{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "4_evaluate_other_recs.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UGfRczMXt-Tt"
   },
   "source": [
    "# Installation with pip\n",
    "Every dependency needed by the framework will be downloaded and installed automatically"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_z0TVVHmQ2sU",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "87e036d8-5455-4cad-92d6-c3303906bb78"
   },
   "source": [
    "!pip install clayrs==0.5.0"
   ],
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting git+https://github.com/swapUniba/ClayRS.git@perfect_replicability_vbpr\n",
      "  Cloning https://github.com/swapUniba/ClayRS.git (to revision perfect_replicability_vbpr) to /tmp/pip-req-build-tyakdf8q\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/swapUniba/ClayRS.git /tmp/pip-req-build-tyakdf8q\n",
      "  Running command git checkout -b perfect_replicability_vbpr --track origin/perfect_replicability_vbpr\n",
      "  Switched to a new branch 'perfect_replicability_vbpr'\n",
      "  Branch 'perfect_replicability_vbpr' set up to track remote branch 'perfect_replicability_vbpr' from 'origin'.\n",
      "  Resolved https://github.com/swapUniba/ClayRS.git to commit 435030e6a6ebd4252e9b7357511b42b50673bda0\n",
      "  Installing build dependencies ... \u001B[?25l\u001B[?25hdone\n",
      "  Getting requirements to build wheel ... \u001B[?25l\u001B[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001B[?25l\u001B[?25hdone\n",
      "Collecting whoosh~=2.7.4\n",
      "  Downloading Whoosh-2.7.4-py2.py3-none-any.whl (468 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m468.8/468.8 KB\u001B[0m \u001B[31m19.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: textblob~=0.15.3 in /usr/local/lib/python3.8/dist-packages (from clayrs==0.4.0) (0.15.3)\n",
      "Collecting sentence-transformers~=1.2.0\n",
      "  Downloading sentence-transformers-1.2.1.tar.gz (80 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m80.8/80.8 KB\u001B[0m \u001B[31m6.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h  Preparing metadata (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "Collecting pyaml~=21.10.1\n",
      "  Downloading pyaml-21.10.1-py2.py3-none-any.whl (24 kB)\n",
      "Collecting ekphrasis~=0.5.4\n",
      "  Downloading ekphrasis-0.5.4-py3-none-any.whl (83 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m83.8/83.8 KB\u001B[0m \u001B[31m5.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: scikit-learn==1.0.2 in /usr/local/lib/python3.8/dist-packages (from clayrs==0.4.0) (1.0.2)\n",
      "Collecting scikit-image~=0.19.3\n",
      "  Downloading scikit_image-0.19.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.0 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m14.0/14.0 MB\u001B[0m \u001B[31m68.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting spacy~=3.2.1\n",
      "  Downloading spacy-3.2.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.2 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m6.2/6.2 MB\u001B[0m \u001B[31m87.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting wn~=0.0.23\n",
      "  Downloading wn-0.0.23.tar.gz (31.6 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m31.6/31.6 MB\u001B[0m \u001B[31m18.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h  Preparing metadata (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "Collecting matplotlib~=3.2.2\n",
      "  Downloading matplotlib-3.2.2-cp38-cp38-manylinux1_x86_64.whl (12.4 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m12.4/12.4 MB\u001B[0m \u001B[31m99.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting networkx~=2.6.3\n",
      "  Downloading networkx-2.6.3-py3-none-any.whl (1.9 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.9/1.9 MB\u001B[0m \u001B[31m67.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting validators~=0.20.0\n",
      "  Downloading validators-0.20.0.tar.gz (30 kB)\n",
      "  Preparing metadata (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "Collecting PyYAML~=5.3.1\n",
      "  Downloading PyYAML-5.3.1.tar.gz (269 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m269.4/269.4 KB\u001B[0m \u001B[31m31.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h  Preparing metadata (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "Collecting distex~=0.7.1\n",
      "  Downloading distex-0.7.2-py3-none-any.whl (19 kB)\n",
      "Collecting colorama~=0.4.4\n",
      "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Collecting SPARQLWrapper~=1.8.5\n",
      "  Downloading SPARQLWrapper-1.8.5-py3-none-any.whl (26 kB)\n",
      "Collecting gensim~=4.1.2\n",
      "  Downloading gensim-4.1.2-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (24.1 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m24.1/24.1 MB\u001B[0m \u001B[31m88.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting babelpy~=1.0.1\n",
      "  Downloading BabelPy-1.0.1.tar.gz (8.0 kB)\n",
      "  Preparing metadata (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "Collecting mysql~=0.0.3\n",
      "  Downloading mysql-0.0.3-py3-none-any.whl (1.2 kB)\n",
      "Collecting transformers~=4.15.0\n",
      "  Downloading transformers-4.15.0-py3-none-any.whl (3.4 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.4/3.4 MB\u001B[0m \u001B[31m101.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting numpy~=1.21.6\n",
      "  Downloading numpy-1.21.6-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m15.7/15.7 MB\u001B[0m \u001B[31m102.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting pywsd~=1.2.4\n",
      "  Downloading pywsd-1.2.5-py3-none-any.whl (26.9 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m26.9/26.9 MB\u001B[0m \u001B[31m15.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: scipy~=1.7.3 in /usr/local/lib/python3.8/dist-packages (from clayrs==0.4.0) (1.7.3)\n",
      "Collecting Pillow~=9.4.0\n",
      "  Downloading Pillow-9.4.0-cp38-cp38-manylinux_2_28_x86_64.whl (3.4 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.4/3.4 MB\u001B[0m \u001B[31m109.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting requests~=2.28.2\n",
      "  Downloading requests-2.28.2-py3-none-any.whl (62 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m62.8/62.8 KB\u001B[0m \u001B[31m7.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting numpy-indexed~=0.3.5\n",
      "  Downloading numpy_indexed-0.3.7-py2.py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: nltk~=3.5 in /usr/local/lib/python3.8/dist-packages (from clayrs==0.4.0) (3.7)\n",
      "Requirement already satisfied: torch~=1.13.0 in /usr/local/lib/python3.8/dist-packages (from clayrs==0.4.0) (1.13.1+cu116)\n",
      "Requirement already satisfied: torchvision~=0.14.1 in /usr/local/lib/python3.8/dist-packages (from clayrs==0.4.0) (0.14.1+cu116)\n",
      "Collecting timm~=0.6.12\n",
      "  Downloading timm-0.6.12-py3-none-any.whl (549 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m549.1/549.1 KB\u001B[0m \u001B[31m39.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting pandas~=1.2.4\n",
      "  Downloading pandas-1.2.5-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (9.7 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m9.7/9.7 MB\u001B[0m \u001B[31m97.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting tqdm~=4.62.2\n",
      "  Downloading tqdm-4.62.3-py2.py3-none-any.whl (76 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m76.2/76.2 KB\u001B[0m \u001B[31m9.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting mysql-connector-python~=8.0.20\n",
      "  Downloading mysql_connector_python-8.0.32-cp38-cp38-manylinux1_x86_64.whl (23.5 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m23.5/23.5 MB\u001B[0m \u001B[31m91.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting nest-asyncio~=1.5.5\n",
      "  Downloading nest_asyncio-1.5.6-py3-none-any.whl (5.2 kB)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn==1.0.2->clayrs==0.4.0) (3.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn==1.0.2->clayrs==0.4.0) (1.2.0)\n",
      "Collecting eventkit\n",
      "  Downloading eventkit-1.0.0-py3-none-any.whl (31 kB)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.8/dist-packages (from distex~=0.7.1->clayrs==0.4.0) (0.3.6)\n",
      "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.8/dist-packages (from distex~=0.7.1->clayrs==0.4.0) (2.2.1)\n",
      "Collecting ujson\n",
      "  Downloading ujson-5.7.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (52 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m52.8/52.8 KB\u001B[0m \u001B[31m6.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: termcolor in /usr/local/lib/python3.8/dist-packages (from ekphrasis~=0.5.4->clayrs==0.4.0) (2.2.0)\n",
      "Collecting ftfy\n",
      "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m53.1/53.1 KB\u001B[0m \u001B[31m6.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.8/dist-packages (from gensim~=4.1.2->clayrs==0.4.0) (6.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib~=3.2.2->clayrs==0.4.0) (2.8.2)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib~=3.2.2->clayrs==0.4.0) (3.0.9)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib~=3.2.2->clayrs==0.4.0) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib~=3.2.2->clayrs==0.4.0) (1.4.4)\n",
      "Collecting mysqlclient\n",
      "  Downloading mysqlclient-2.1.1.tar.gz (88 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m88.1/88.1 KB\u001B[0m \u001B[31m9.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h  Preparing metadata (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "Requirement already satisfied: protobuf<=3.20.3,>=3.11.0 in /usr/local/lib/python3.8/dist-packages (from mysql-connector-python~=8.0.20->clayrs==0.4.0) (3.19.6)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk~=3.5->clayrs==0.4.0) (2022.6.2)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk~=3.5->clayrs==0.4.0) (8.1.3)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.8/dist-packages (from numpy-indexed~=0.3.5->clayrs==0.4.0) (0.16.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas~=1.2.4->clayrs==0.4.0) (2022.7.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from pywsd~=1.2.4->clayrs==0.4.0) (1.15.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests~=2.28.2->clayrs==0.4.0) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests~=2.28.2->clayrs==0.4.0) (2.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests~=2.28.2->clayrs==0.4.0) (3.0.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests~=2.28.2->clayrs==0.4.0) (1.26.14)\n",
      "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.8/dist-packages (from scikit-image~=0.19.3->clayrs==0.4.0) (2.9.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image~=0.19.3->clayrs==0.4.0) (23.0)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from scikit-image~=0.19.3->clayrs==0.4.0) (1.4.1)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.8/dist-packages (from scikit-image~=0.19.3->clayrs==0.4.0) (2023.2.3)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.3/1.3 MB\u001B[0m \u001B[31m59.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy~=3.2.1->clayrs==0.4.0) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy~=3.2.1->clayrs==0.4.0) (3.0.8)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy~=3.2.1->clayrs==0.4.0) (1.0.4)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy~=3.2.1->clayrs==0.4.0) (2.0.8)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from spacy~=3.2.1->clayrs==0.4.0) (0.7.9)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy~=3.2.1->clayrs==0.4.0) (3.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.8/dist-packages (from spacy~=3.2.1->clayrs==0.4.0) (2.4.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy~=3.2.1->clayrs==0.4.0) (1.0.9)\n",
      "Collecting thinc<8.1.0,>=8.0.12\n",
      "  Downloading thinc-8.0.17-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (671 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m671.1/671.1 KB\u001B[0m \u001B[31m57.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy~=3.2.1->clayrs==0.4.0) (3.3.0)\n",
      "Collecting typer<0.5.0,>=0.3.0\n",
      "  Downloading typer-0.4.2-py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.8/dist-packages (from spacy~=3.2.1->clayrs==0.4.0) (3.0.12)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy~=3.2.1->clayrs==0.4.0) (0.10.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.8/dist-packages (from spacy~=3.2.1->clayrs==0.4.0) (0.10.1)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
      "  Downloading pydantic-1.8.2-cp38-cp38-manylinux2014_x86_64.whl (13.7 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m13.7/13.7 MB\u001B[0m \u001B[31m118.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy~=3.2.1->clayrs==0.4.0) (57.4.0)\n",
      "Collecting rdflib>=4.0\n",
      "  Downloading rdflib-6.2.0-py3-none-any.whl (500 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m500.3/500.3 KB\u001B[0m \u001B[31m47.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting huggingface-hub\n",
      "  Downloading huggingface_hub-0.12.1-py3-none-any.whl (190 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m190.3/190.3 KB\u001B[0m \u001B[31m24.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch~=1.13.0->clayrs==0.4.0) (4.5.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers~=4.15.0->clayrs==0.4.0) (3.9.0)\n",
      "Collecting tokenizers<0.11,>=0.10.1\n",
      "  Downloading tokenizers-0.10.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.3/3.3 MB\u001B[0m \u001B[31m111.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting sacremoses\n",
      "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m880.6/880.6 KB\u001B[0m \u001B[31m73.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h  Preparing metadata (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "Requirement already satisfied: decorator>=3.4.0 in /usr/local/lib/python3.8/dist-packages (from validators~=0.20.0->clayrs==0.4.0) (4.4.2)\n",
      "Collecting isodate\n",
      "  Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m41.7/41.7 KB\u001B[0m \u001B[31m5.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.8/dist-packages (from ftfy->ekphrasis~=0.5.4->clayrs==0.4.0) (0.2.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy~=3.2.1->clayrs==0.4.0) (2.1.2)\n",
      "Building wheels for collected packages: clayrs, babelpy, wn, PyYAML, sentence-transformers, validators, mysqlclient, sacremoses\n",
      "  Building wheel for clayrs (pyproject.toml) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for clayrs: filename=clayrs-0.4.0-py3-none-any.whl size=375709 sha256=9419eec51e2d7f6c18fcbc9647c685651f1968fc56f60ed4e34d0808163069ac\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-7a4sfrnr/wheels/bc/47/ec/34f153171beec4c7c9a0e30877ca9edf9e1c0616a25b769ec5\n",
      "  Building wheel for babelpy (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for babelpy: filename=BabelPy-1.0.1-py3-none-any.whl size=9723 sha256=6fd802b45b3178581a74d8df1954521212a4fbf5b3e29352afb8e2d73c252739\n",
      "  Stored in directory: /root/.cache/pip/wheels/67/8f/30/b724e0e8f88e79a32dc7358b52397421d71eac263bdb09e39a\n",
      "  Building wheel for wn (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for wn: filename=wn-0.0.23-py3-none-any.whl size=31792926 sha256=d3c0d27f6054816f1a354cc3971947ccc3087c5390e0ea0b59738e749b195447\n",
      "  Stored in directory: /root/.cache/pip/wheels/6b/eb/fe/eb7c7be28c29ee90dd9d6f58c116673d0eb07b2d83dfb72a37\n",
      "  Building wheel for PyYAML (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for PyYAML: filename=PyYAML-5.3.1-cp38-cp38-linux_x86_64.whl size=44634 sha256=ccc602693a32df573b56ccc4122e1b8ea9c508e2f1f94f0061af1cf42a0cec7d\n",
      "  Stored in directory: /root/.cache/pip/wheels/13/90/db/290ab3a34f2ef0b5a0f89235dc2d40fea83e77de84ed2dc05c\n",
      "  Building wheel for sentence-transformers (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for sentence-transformers: filename=sentence_transformers-1.2.1-py3-none-any.whl size=123300 sha256=f44d190d3a353f31e61695dba072257d45eb1014d1a151d4dc4240725ca36b16\n",
      "  Stored in directory: /root/.cache/pip/wheels/92/e0/93/a0f6582848b87b07fa73c0f1fe2f43b6328fd3171fe852f381\n",
      "  Building wheel for validators (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for validators: filename=validators-0.20.0-py3-none-any.whl size=19581 sha256=7b33d5cc77732e56077b9eb409adc77eb1c852ab715869f1050a39d28c92b3dd\n",
      "  Stored in directory: /root/.cache/pip/wheels/19/09/72/3eb74d236bb48bd0f3c6c3c83e4e0c5bbfcbcad7c6c3539db8\n",
      "  Building wheel for mysqlclient (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for mysqlclient: filename=mysqlclient-2.1.1-cp38-cp38-linux_x86_64.whl size=109205 sha256=3620fe2d1394d48378475ffb177748ceea5b2e0d62471f12e63869b7bfa19512\n",
      "  Stored in directory: /root/.cache/pip/wheels/5b/e1/84/a6185eaec318899f59a32d393af7729a0719cd93695d71f9a1\n",
      "  Building wheel for sacremoses (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=1081d6c7e1ac2dc73313b4da708472b0aaecd457c17665c5ae0503c38f78157c\n",
      "  Stored in directory: /root/.cache/pip/wheels/82/ab/9b/c15899bf659ba74f623ac776e861cf2eb8608c1825ddec66a4\n",
      "Successfully built clayrs babelpy wn PyYAML sentence-transformers validators mysqlclient sacremoses\n",
      "Installing collected packages: wn, whoosh, tokenizers, sentencepiece, babelpy, validators, ujson, typer, tqdm, requests, PyYAML, pydantic, Pillow, numpy, networkx, nest-asyncio, mysqlclient, mysql-connector-python, isodate, ftfy, colorama, sacremoses, rdflib, pyaml, pandas, numpy-indexed, mysql, matplotlib, huggingface-hub, eventkit, transformers, timm, thinc, SPARQLWrapper, scikit-image, pywsd, gensim, ekphrasis, distex, spacy, sentence-transformers, clayrs\n",
      "  Attempting uninstall: typer\n",
      "    Found existing installation: typer 0.7.0\n",
      "    Uninstalling typer-0.7.0:\n",
      "      Successfully uninstalled typer-0.7.0\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.64.1\n",
      "    Uninstalling tqdm-4.64.1:\n",
      "      Successfully uninstalled tqdm-4.64.1\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.25.1\n",
      "    Uninstalling requests-2.25.1:\n",
      "      Successfully uninstalled requests-2.25.1\n",
      "  Attempting uninstall: PyYAML\n",
      "    Found existing installation: PyYAML 6.0\n",
      "    Uninstalling PyYAML-6.0:\n",
      "      Successfully uninstalled PyYAML-6.0\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 1.10.5\n",
      "    Uninstalling pydantic-1.10.5:\n",
      "      Successfully uninstalled pydantic-1.10.5\n",
      "  Attempting uninstall: Pillow\n",
      "    Found existing installation: Pillow 8.4.0\n",
      "    Uninstalling Pillow-8.4.0:\n",
      "      Successfully uninstalled Pillow-8.4.0\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.22.4\n",
      "    Uninstalling numpy-1.22.4:\n",
      "      Successfully uninstalled numpy-1.22.4\n",
      "  Attempting uninstall: networkx\n",
      "    Found existing installation: networkx 3.0\n",
      "    Uninstalling networkx-3.0:\n",
      "      Successfully uninstalled networkx-3.0\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.3.5\n",
      "    Uninstalling pandas-1.3.5:\n",
      "      Successfully uninstalled pandas-1.3.5\n",
      "  Attempting uninstall: matplotlib\n",
      "    Found existing installation: matplotlib 3.5.3\n",
      "    Uninstalling matplotlib-3.5.3:\n",
      "      Successfully uninstalled matplotlib-3.5.3\n",
      "  Attempting uninstall: thinc\n",
      "    Found existing installation: thinc 8.1.7\n",
      "    Uninstalling thinc-8.1.7:\n",
      "      Successfully uninstalled thinc-8.1.7\n",
      "  Attempting uninstall: scikit-image\n",
      "    Found existing installation: scikit-image 0.18.3\n",
      "    Uninstalling scikit-image-0.18.3:\n",
      "      Successfully uninstalled scikit-image-0.18.3\n",
      "  Attempting uninstall: gensim\n",
      "    Found existing installation: gensim 3.6.0\n",
      "    Uninstalling gensim-3.6.0:\n",
      "      Successfully uninstalled gensim-3.6.0\n",
      "  Attempting uninstall: spacy\n",
      "    Found existing installation: spacy 3.4.4\n",
      "    Uninstalling spacy-3.4.4:\n",
      "      Successfully uninstalled spacy-3.4.4\n",
      "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "xarray 2022.12.0 requires pandas>=1.3, but you have pandas 1.2.5 which is incompatible.\n",
      "en-core-web-sm 3.4.1 requires spacy<3.5.0,>=3.4.0, but you have spacy 3.2.5 which is incompatible.\u001B[0m\u001B[31m\n",
      "\u001B[0mSuccessfully installed Pillow-9.4.0 PyYAML-5.3.1 SPARQLWrapper-1.8.5 babelpy-1.0.1 clayrs-0.4.0 colorama-0.4.6 distex-0.7.2 ekphrasis-0.5.4 eventkit-1.0.0 ftfy-6.1.1 gensim-4.1.2 huggingface-hub-0.12.1 isodate-0.6.1 matplotlib-3.2.2 mysql-0.0.3 mysql-connector-python-8.0.32 mysqlclient-2.1.1 nest-asyncio-1.5.6 networkx-2.6.3 numpy-1.21.6 numpy-indexed-0.3.7 pandas-1.2.5 pyaml-21.10.1 pydantic-1.8.2 pywsd-1.2.5 rdflib-6.2.0 requests-2.28.2 sacremoses-0.0.53 scikit-image-0.19.3 sentence-transformers-1.2.1 sentencepiece-0.1.97 spacy-3.2.5 thinc-8.0.17 timm-0.6.12 tokenizers-0.10.3 tqdm-4.62.3 transformers-4.15.0 typer-0.4.2 ujson-5.7.0 validators-0.20.0 whoosh-2.7.4 wn-0.0.23\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G1MOZctwVyW5"
   },
   "source": [
    "# **! RESTART RUNTIME !**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VTZB50Y3rN-8"
   },
   "source": [
    "# Correct order log and prints for IPython\n",
    "This is necessary only for IPython environments (Colab, Jupyter, etc.), since they mess up the order of  ```print``` and ```logging```\n",
    "\n",
    "```python\n",
    "# EXAMPLE of the issue\n",
    ">>> import logging\n",
    ">>> print(\"Should go first\")\n",
    ">>> logging.warning(\"Should go second\")\n",
    "WARNING:root:Should go second\n",
    "Should go first\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "GOTUezxnqzn7"
   },
   "source": [
    "import functools\n",
    "print = functools.partial(print, flush=True)"
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uVKotTbvf_rC"
   },
   "source": [
    "# Import and datasets download\n",
    "\n",
    "The framework is made of three modules:\n",
    "> 1.   Content Analyzer\n",
    "> 2.   Recommender System\n",
    "> 3.   Evaluation\n",
    "\n",
    "We import every module as a library and use classes and methods by using the dot notation:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "SctkIBio9dhe"
   },
   "source": [
    "from clayrs import content_analyzer as ca\n",
    "from clayrs import recsys as rs\n",
    "from clayrs import evaluation as eva\n",
    "\n",
    "# Usage:\n",
    "# ...\n",
    "# ca.Ratings()\n",
    "# rs.ContentBasedRS()\n",
    "# eva.EvalModel()\n",
    "# ..."
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zYjcSfTtCXEw"
   },
   "source": [
    "We use **Movielens 100k** as dataset, with items info expanded thanks to imdb"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "HwEnaPj4pvCS",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "84d62adb-be2d-4a6d-8d84-f117f6534ddd"
   },
   "source": [
    "# Dataset: Movielens-100k\n",
    "\n",
    "# download items_info\n",
    "!wget https://raw.githubusercontent.com/swapUniba/clayrs/master/datasets/ml-100k/items_info.json\n",
    "\n",
    "# download users_info\n",
    "!wget https://raw.githubusercontent.com/swapUniba/clayrs/master/datasets/ml-100k/users_info.csv\n",
    "\n",
    "# download ratings\n",
    "!wget https://raw.githubusercontent.com/swapUniba/clayrs/master/datasets/ml-100k/ratings.csv"
   ],
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--2023-02-28 20:12:51--  https://raw.githubusercontent.com/swapUniba/clayrs/master/datasets/ml-100k/items_info.json\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2222967 (2.1M) [text/plain]\n",
      "Saving to: ‘items_info.json’\n",
      "\n",
      "\ritems_info.json       0%[                    ]       0  --.-KB/s               \ritems_info.json     100%[===================>]   2.12M  --.-KB/s    in 0.01s   \n",
      "\n",
      "2023-02-28 20:12:51 (152 MB/s) - ‘items_info.json’ saved [2222967/2222967]\n",
      "\n",
      "--2023-02-28 20:12:51--  https://raw.githubusercontent.com/swapUniba/clayrs/master/datasets/ml-100k/users_info.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 22667 (22K) [text/plain]\n",
      "Saving to: ‘users_info.csv’\n",
      "\n",
      "users_info.csv      100%[===================>]  22.14K  --.-KB/s    in 0s      \n",
      "\n",
      "2023-02-28 20:12:51 (109 MB/s) - ‘users_info.csv’ saved [22667/22667]\n",
      "\n",
      "--2023-02-28 20:12:51--  https://raw.githubusercontent.com/swapUniba/clayrs/master/datasets/ml-100k/ratings.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1979206 (1.9M) [text/plain]\n",
      "Saving to: ‘ratings.csv’\n",
      "\n",
      "ratings.csv         100%[===================>]   1.89M  --.-KB/s    in 0.04s   \n",
      "\n",
      "2023-02-28 20:12:52 (51.6 MB/s) - ‘ratings.csv’ saved [1979206/1979206]\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EBOImtHzxTlQ"
   },
   "source": [
    "### Check items file\n",
    "In this example, the file containing items info is a JSON where every entry corresponds to a movie.\n",
    "\n",
    "For every movie there are various information, such as *genres, directors, cast, etc.*"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eAMUA6B6wSje",
    "outputId": "9f9842c5-f960-4a0d-d3fe-cef32d58f654"
   },
   "source": [
    "with open(\"items_info.json\", \"r\") as f:\n",
    "  # 25 lines but in these 23 lines there are only 2 entries:\n",
    "  # 'Toy Story', and 'Golden Eye'\n",
    "  for _ in range(25):\n",
    "    print(f.readline(), end='')\n"
   ],
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[\n",
      "    {\n",
      "        \"movielens_id\": \"1\",\n",
      "        \"imdb_id\": \"0114709\",\n",
      "        \"title\": \"Toy Story\",\n",
      "        \"plot\": \"A cowboy doll is profoundly threatened and jealous when a new spaceman figure supplants him as top toy in a boy's room.\",\n",
      "        \"genres\": \"Animation, Adventure, Comedy, Family, Fantasy\",\n",
      "        \"year\": \"1995\",\n",
      "        \"rating\": \"8.3\",\n",
      "        \"directors\": \"John Lasseter\",\n",
      "        \"cast\": \"Tom Hanks, Tim Allen, Don Rickles, Jim Varney, Wallace Shawn, John Ratzenberger, Annie Potts, John Morris, Erik von Detten, Laurie Metcalf, R. Lee Ermey, Sarah Rayne, Penn Jillette, Jack Angel, Spencer Aste, Greg Berg, Lisa Bradley, Kendall Cunningham, Debi Derryberry, Cody Dorkin, Bill Farmer, Craig Good, Gregory Grudt, Danielle Judovits, Sam Lasseter, Brittany Levenbrown, Sherry Lynn, Scott McAfee, Mickie McGowan, Ryan O'Donohue, Jeff Pidgeon, Patrick Pinney, Phil Proctor, Jan Rabson, Joe Ranft, Andrew Stanton, Shane Sweet, Wayne Allwine, Tony Anselmo, Jonathan Benair, Anthony Burch, John Lasseter, Billy West\",\n",
      "        \"dbpedia_uri\": \"http://dbpedia.org/resource/Toy_Story\",\n",
      "        \"dbpedia_label\": \"Toy Story\"\n",
      "    },\n",
      "    {\n",
      "        \"movielens_id\": \"2\",\n",
      "        \"imdb_id\": \"0113189\",\n",
      "        \"title\": \"GoldenEye\",\n",
      "        \"plot\": \"Years after a friend and fellow 00 agent is killed on a joint mission, a secret space based weapons program known as \\\"_GoldenEye_ (qv)\\\" is stolen. James Bond sets out to stop a Russian crime syndicate from using the weapon.\",\n",
      "        \"genres\": \"Action, Adventure, Thriller\",\n",
      "        \"year\": \"1995\",\n",
      "        \"rating\": \"7.2\",\n",
      "        \"directors\": \"Martin Campbell\",\n",
      "        \"cast\": \"Pierce Brosnan, Sean Bean, Izabella Scorupco, Famke Janssen, Joe Don Baker, Judi Dench, Robbie Coltrane, Tchéky Karyo, Gottfried John, Alan Cumming, Desmond Llewelyn, Samantha Bond, Michael Kitchen, Serena Gordon, Simon Kunz, Pavel Douglas, Olivier Lajous, Billy J. Mitchell, Constantine Gregory, Minnie Driver, Michelle Arthur, Ravil Isyanov, Vladimir Milanovich, Trevor Byfield, Peter Majer, Paul Bannon, Simone Bechtel, Martin Campbell, Mark Chapman, Kenneth Coombs, Simon Crane, Terrance Denville, Ian Durrant, Max Faulkner, Juliet Forester, Stefan Kopiecki, Jo Anna Lee, Derek Lyons, Wayne Michaels, Bhasker Patel, Paul Sacks, Michael G. Wilson\",\n",
      "        \"dbpedia_uri\": \"http://dbpedia.org/resource/GoldenEye\",\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NrGycV8NxkwF"
   },
   "source": [
    "### Check users file\n",
    "In this example, the file containing users info is a CSV file where the first column is the *user id*, while the other columns are side information for that user (*gender, occupation, zip code*)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vUKLNAnMxEnh",
    "outputId": "782062de-0052-4aeb-bb58-a813882a7dcd"
   },
   "source": [
    "with open(\"users_info.csv\", \"r\") as f:\n",
    "\n",
    "  # print the header and the first 2 entries\n",
    "  for _ in range(3):\n",
    "    print(f.readline(), end='')"
   ],
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "user_id,age,gender,occupation,zip_code\n",
      "1,24,M,technician,85711\n",
      "2,53,F,other,94043\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dteEOqLxkDGR"
   },
   "source": [
    "<a name=\"cell-id\"></a>\n",
    "### Check ratings\n",
    "In this example, the file containing the interactions between the users and the movies is a CSV, where every interaction is a rating in the **[1, 5]** Likert scale"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "Q3FvQsaIkbtz",
    "outputId": "165c988a-c8ed-44f5-d23f-28cb9ff1d3c2"
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.read_csv('ratings.csv')"
   ],
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       user_id  item_id  rating  timestamp\n",
       "0          196      242       3  881250949\n",
       "1          186      302       3  891717742\n",
       "2           22      377       1  878887116\n",
       "3          244       51       2  880606923\n",
       "4          166      346       1  886397596\n",
       "...        ...      ...     ...        ...\n",
       "99995      880      476       3  880175444\n",
       "99996      716      204       5  879795543\n",
       "99997      276     1090       1  874795795\n",
       "99998       13      225       2  882399156\n",
       "99999       12      203       3  879959583\n",
       "\n",
       "[100000 rows x 4 columns]"
      ],
      "text/html": [
       "\n",
       "  <div id=\"df-3300d788-c7af-4c12-a9b6-1079d0129aa4\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>196</td>\n",
       "      <td>242</td>\n",
       "      <td>3</td>\n",
       "      <td>881250949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>186</td>\n",
       "      <td>302</td>\n",
       "      <td>3</td>\n",
       "      <td>891717742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>377</td>\n",
       "      <td>1</td>\n",
       "      <td>878887116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>244</td>\n",
       "      <td>51</td>\n",
       "      <td>2</td>\n",
       "      <td>880606923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>166</td>\n",
       "      <td>346</td>\n",
       "      <td>1</td>\n",
       "      <td>886397596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>880</td>\n",
       "      <td>476</td>\n",
       "      <td>3</td>\n",
       "      <td>880175444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>716</td>\n",
       "      <td>204</td>\n",
       "      <td>5</td>\n",
       "      <td>879795543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>276</td>\n",
       "      <td>1090</td>\n",
       "      <td>1</td>\n",
       "      <td>874795795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>13</td>\n",
       "      <td>225</td>\n",
       "      <td>2</td>\n",
       "      <td>882399156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>12</td>\n",
       "      <td>203</td>\n",
       "      <td>3</td>\n",
       "      <td>879959583</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 4 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3300d788-c7af-4c12-a9b6-1079d0129aa4')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-3300d788-c7af-4c12-a9b6-1079d0129aa4 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-3300d788-c7af-4c12-a9b6-1079d0129aa4');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ilGpvNnPgHay"
   },
   "source": [
    "# Content Analyzer: representation of Items and export to json\n",
    "In order to define the *item representation*, the following parameters should be defined:\n",
    "*   ***source***: the path of the file containing items info\n",
    "*   ***id***: the field that uniquely identifies an item\n",
    "*   ***output_directory***: the path where serialized representations are saved\n",
    "\n",
    "There is also the optional parameter ***export_json***, which allows to create a file `contents.json` in the output directory containing the serialization of all representations of the content."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "gORk6J3wqJGB"
   },
   "source": [
    "# Configuration of item representation \n",
    "movies_ca_config = ca.ItemAnalyzerConfig(\n",
    "    source=ca.JSONFile('items_info.json'),\n",
    "    id='movielens_id',\n",
    "    output_directory='movies_codified/',\n",
    "    export_json=True\n",
    ")"
   ],
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YfnkSpfTjuDq"
   },
   "source": [
    "<a name=\"ca_id\"></a>\n",
    "Each item can be represented using a set of fields.\n",
    "Every field can be **represented** using several techniques, such as *'tfidf'*, *'entity linking'*, *'embeddings'*, etc.\n",
    "\n",
    "It is possible to process the content of each field using a **Natural Language Processing (NLP) pipeline**.  \n",
    "It is also possible to assign a **custom id** for each generated representation, in order to allow a simpler reference in the recommendation phase. Both NLP pipeline and custom id are optional parameters.\n",
    "\n",
    "> In the following example, we process the *'plot'* field by performing **lemmatization** and **stopwords removal** through [NLTK](https://www.nltk.org/), and we represent it using **tfidf**:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "YA8GfrkrqJ4S",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "1fe1aed4-5911-4398-ce6d-31926c8c03cb"
   },
   "source": [
    "movies_ca_config.add_single_config(\n",
    "    'plot',\n",
    "    ca.FieldConfig(ca.SkLearnTfIdf(),\n",
    "                   preprocessing=ca.NLTK(stopwords_removal=True, lemmatization=True)) \n",
    ")"
   ],
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
      "[nltk_data] Downloading package words to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/words.zip.\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OAgjyRfVYRqG"
   },
   "source": [
    "In this example, we also add an exogenous representation to the content by extracting the 'year' field from the local source \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "m9KBOFO-XAJ2"
   },
   "source": [
    "movies_ca_config.add_single_exogenous(\n",
    "    ca.ExogenousConfig(ca.PropertiesFromDataset(field_name_list=['year']))\n",
    ")"
   ],
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0e24qhQVkXIc"
   },
   "source": [
    "At the end of the configuration step, we provide the configuration to the *'Content Analyzer'* and call the `fit()` method:\n",
    "\n",
    "*   The Content Analyzer will **represent** and **serialize** every item (and create the json file containing representations for every content).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "yXU-Vuw2twuN",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "9254870a-4ba6-4ee1-86ee-6e8283f67955"
   },
   "source": [
    "content_analyzer = ca.ContentAnalyzer(config=movies_ca_config)\n",
    "content_analyzer.fit()"
   ],
   "execution_count": 9,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001B[39mINFO\u001B[0m - Extracting exogenous properties from local dataset\n",
      "\u001B[39mINFO\u001B[0m - ***********   Processing field: plot   ***********\n",
      "\u001B[39mINFO\u001B[0m - Computing tf-idf with SkLearnTfIdf\n",
      "Serializing contents:  100%|██████████| 1682/1682 [00:03<00:00]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yrGYxny3g9sW"
   },
   "source": [
    "This is the JSON file which has been created:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RfdoIwichaaF",
    "outputId": "6705a2e6-2b8d-41dc-a67f-ac3d57f2bb87"
   },
   "source": [
    "with open('movies_codified/contents.json', 'r') as f:\n",
    "    for _ in range(11):\n",
    "      print(f.readline(), end='')"
   ],
   "execution_count": 10,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[\n",
      "    {\n",
      "        \"content_id\": \"1\",\n",
      "        \"Exo#0\": \"{'year': '1995'}\",\n",
      "        \"plot#0\": {\n",
      "            \"sparse_tfidf\": \"[[(0, 1024),0.1719365702112526],\\n [(0, 1729),0.31336848582007987],\\n [(0, 2160),0.3010691777558571],\\n [(0, 2721),0.25782007590969463],\\n [(0, 3752),0.2915290932564375],\\n [(0, 4769),0.1484604944785959],\\n [(0, 5439),0.31336848582007987],\\n [(0, 5932),0.2618948840931845],\\n [(0, 6417),0.3307033869191101],\\n [(0, 6655),0.3307033869191101],\\n [(0, 6853),0.25410006749357394],\\n [(0, 6909),0.2445599829941543],\\n [(0, 6941),0.31336848582007987]]\",\n",
      "            \"pos_word_tuples\": \"[(5932, 'room'), (1024, 'boy'), (6941, 'toy'), (6909, 'top'), (6655, 'supplants'), (2721, 'figure'), (6417, 'spaceman'), (4769, 'new'), (3752, 'jealous'), (6853, 'threaten'), (5439, 'profoundly'), (2160, 'doll'), (1729, 'cowboy')]\",\n",
      "            \"len_vocabulary\": 7614\n",
      "        }\n",
      "    },\n",
      "    {\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H_3ff69ZhFj7"
   },
   "source": [
    "# [Optional] Content Analyzer: representation of Users and export to json\n",
    "In order to define the *'user representation'*, we could use the same process performed for *'item representation'*. In this case we don't want to represent in a complex way users, so this step is completely optional\n",
    "\n",
    "In this example, the ID for users is the column `user_id`.\n",
    "\n",
    "Also for users, it is ossible to export the representation in a ***json file***"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "HhwOj4s-uzOk"
   },
   "source": [
    "#Configuration of user representation\n",
    "users_ca_config = ca.UserAnalyzerConfig(\n",
    "    ca.CSVFile('users_info.csv'),\n",
    "    id='user_id',\n",
    "    output_directory='users_codified/',\n",
    "    export_json=True\n",
    ")"
   ],
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l8xsWBz8eYHF"
   },
   "source": [
    "We also add an exogenous representation for each user by extracting the 'gender' field from the local source\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4BELoRbCckKn",
    "outputId": "802fea63-f955-4bc9-8a18-f30016d482b9"
   },
   "source": [
    "users_ca_config.add_single_exogenous(\n",
    "    ca.ExogenousConfig(ca.PropertiesFromDataset(field_name_list=['gender']))\n",
    ")\n",
    "\n",
    "ca.ContentAnalyzer(config=users_ca_config).fit()"
   ],
   "execution_count": 12,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001B[39mINFO\u001B[0m - Extracting exogenous properties from local dataset\n",
      "Serializing contents:  100%|██████████| 943/943 [00:01<00:00]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QnhE2zqnfykN"
   },
   "source": [
    "This is the JSON file which has been created:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8avOGAFvgGQy",
    "outputId": "3b7a7daf-5e41-45ce-ce7f-0e12a6e63b26"
   },
   "source": [
    "with open('users_codified/contents.json', 'r') as f:\n",
    "    for _ in range(9):\n",
    "      print(f.readline(), end='')"
   ],
   "execution_count": 13,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[\n",
      "    {\n",
      "        \"content_id\": \"1\",\n",
      "        \"Exo#0\": \"{'gender': 'M'}\"\n",
      "    },\n",
      "    {\n",
      "        \"content_id\": \"2\",\n",
      "        \"Exo#0\": \"{'gender': 'F'}\"\n",
      "    },\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dr0qljcRhpW6"
   },
   "source": [
    "# Recommender System: centroid vector algorithm and export to csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ms6kU0TEr_ps"
   },
   "source": [
    "The Recommender System module needs information about users, items and ratings. \n",
    "\n",
    "The **Ratings** class allows you to import rating from a source file (or also from an existent dataframe) into a custom object.   **If** the source file contains users (U), items (I) and ratings (R) in this order, no additional parameters are needed, **otherwise**  the mapping must be explictly specified using:\n",
    "\n",
    "*   **'user_id'** column,\n",
    "*   **'item_id'** column,\n",
    "*   **'score'** column\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XjxZ9yTnvVW-",
    "outputId": "c7ee52ff-b788-4c3a-9c61-84cc5ebaa541"
   },
   "source": [
    "ratings = ca.Ratings(ca.CSVFile('ratings.csv'))\n",
    "\n",
    "print(ratings)"
   ],
   "execution_count": 14,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Importing ratings:  100%|██████████| 100000/100000 [00:01<00:00]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "      user_id item_id  score\n",
      "0         196     242    3.0\n",
      "1         186     302    3.0\n",
      "2          22     377    1.0\n",
      "3         244      51    2.0\n",
      "4         166     346    1.0\n",
      "...       ...     ...    ...\n",
      "99995     880     476    3.0\n",
      "99996     716     204    5.0\n",
      "99997     276    1090    1.0\n",
      "99998      13     225    2.0\n",
      "99999      12     203    3.0\n",
      "\n",
      "[100000 rows x 3 columns]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1SfXQx4y01u7"
   },
   "source": [
    "# (mapping by index) EQUIVALENT:\n",
    "#\n",
    "# ratings = ca.Ratings(\n",
    "#     ca.CSVFile('ratings.csv'),\n",
    "#     user_id_column=0,\n",
    "#     item_id_column=1,\n",
    "#     score_column=2\n",
    "# )"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Ww9kKa3608pK"
   },
   "source": [
    "# (mapping by column name) EQUIVALENT:\n",
    "\n",
    "# ratings = ca.Ratings(\n",
    "#     ca.CSVFile('ratings.csv'),\n",
    "#     user_id_column='user_id',\n",
    "#     item_id_column='item_id',\n",
    "#     score_column='rating'\n",
    "# )"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O9dYu-H9uUa5"
   },
   "source": [
    "The Recommender System also needs an algorithm for ranking or predicting items to users. In the following example we use the **CentroidVector** algorithm:\n",
    "\n",
    "*   It computes the centroid vector of the features of items *liked by the user*\n",
    "*   It computes the similarity between the centroid vector and unrated items\n",
    "\n",
    "The items liked by a user are those having a rating higher or equal than a specific **threshold**. If the threshold is not specified, the average score of all items liked by the user is used.\n",
    "\n",
    "The Recommender System leverages the representations defined by the Content Analyzer. In the current example, we use the representation of the field 'plot'. More representations could be adopted for a single field.\n",
    "\n",
    "\n",
    "```python\n",
    "# Example with multiple representations for a single field\n",
    "{\n",
    "  'plot': ['tfidf', 'word_embedding'],\n",
    "  'genre': 'doc_embedding',\n",
    "  ...\n",
    "}\n",
    "```\n",
    "\n",
    "Representations can be referenced using the **external id** (if specified, see [here](#ca_id)) or the **internal id**:\n",
    "\n",
    "\n",
    "```\n",
    "For the field 'plot':\n",
    "First representation created -> internal_id = 0\n",
    "Second representation created -> internal_id = 1\n",
    "...\n",
    "Nth representation created -> internal_id = n-1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "GbblAapgmOL6"
   },
   "source": [
    "centroid_vec = rs.CentroidVector(\n",
    "    {'plot': 0}, # the first and only representation codifed for the 'plot' field\n",
    "    similarity=rs.CosineSimilarity()\n",
    ")\n",
    "\n",
    "# no threshold parameter specified, the average rating given by\n",
    "# the user wil be used"
   ],
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Before we can instantiate the recommender system, we should perform the splitting of the dataset: let's perform a **KFold with 2 splits**\n",
    "\n",
    "*   The output of the partition module are two lists. One containing the two train set (in this case), the other containing the two test set (in this case)"
   ],
   "metadata": {
    "id": "un-SstzA4iEY"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "kf = rs.KFoldPartitioning(n_splits=2)\n",
    "train_list, test_list = kf.split_all(ratings)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LmTecQWq4iuN",
    "outputId": "6acfea11-0801-4648-e79c-ca6467657b33"
   },
   "execution_count": 16,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Performing KFoldPartitioning:  100%|██████████| 943/943 [00:00<00:00]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now the ***cbrs*** must be fit before we can compute the rank:\n",
    "\n",
    "*   We could do this in two separate steps, by first calling the `fit(..)` method and then the `rank(...)` method \n",
    "\n",
    "*   Or by calling directly the `fit_rank(...)` method, which performs both in one step\n",
    "\n",
    "We use the second approach and we compute the rank for all users of our train set: we will use both the two train set and two test set obtained thanks to the KFold technique\n",
    "\n",
    "In order to compute a rank for all users, you simply do not specify the *user_list* parameter\n",
    "\n",
    "***Note:*** by default top-10 recommendations are returned for each user. In order to produce *unbounded ranking*, simply set `n_recs` parameter to `None`. In this case we are fine with the top-10 recs"
   ],
   "metadata": {
    "id": "ZNnqFrBZ4w7X"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "result_list = []\n",
    "\n",
    "for train_set, test_set in zip(train_list, test_list):\n",
    "  \n",
    "  cbrs = rs.ContentBasedRS(centroid_vec, train_set, 'movies_codified/')\n",
    "  rank_to_append = cbrs.fit_rank(test_set)\n",
    "\n",
    "  result_list.append(rank_to_append)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LdMCfW1Z493a",
    "outputId": "0b6d776e-6aa4-4252-f07f-9e04ed8dd50b"
   },
   "execution_count": 17,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001B[39mINFO\u001B[0m - Don't worry if it looks stuck at first\n",
      "\u001B[39mINFO\u001B[0m - First iterations will stabilize the estimated remaining time\n",
      "Computing fit_rank for user 942:  100%|██████████| 943/943 [00:28<00:00]\n",
      "\u001B[39mINFO\u001B[0m - Don't worry if it looks stuck at first\n",
      "\u001B[39mINFO\u001B[0m - First iterations will stabilize the estimated remaining time\n",
      "Computing fit_rank for user 942:  100%|██████████| 943/943 [00:28<00:00]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CIQtO-Ah1Ty8"
   },
   "source": [
    "Let's export each ranking generated to a csv file\n",
    "\n",
    "*   The `rank()` method (and the `fit_rank()` method) returns a **Rank** object, that has a useful exporting method `to_csv()`\n",
    "\n",
    "We will save also the *test set* of each split, we need them later on in the *EvalModel part*"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2z88KA1D1Xni"
   },
   "source": [
    "# we save the result of each split numbered\n",
    "for i, rank_generated in enumerate(result_list, start=1):\n",
    "  rank_generated.to_csv(file_name=f'rank_split_{i}')\n",
    "\n",
    "# we save  the result of each split numbered\n",
    "for i, test_set in enumerate(test_list, start=1):\n",
    "  test_set.to_csv(file_name=f'truth_split_{i}')"
   ],
   "execution_count": 18,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4IG3B5sUloNF"
   },
   "source": [
    "We can import recommendations we just exported via pandas, or any other library which reads csv file (also the framework itself):"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qU72C15hl03T",
    "outputId": "0be88561-50fe-43bd-f367-310a04ceadd5"
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "rank_split_1 = pd.read_csv('rank_split_1.csv')\n",
    "rank_split_2 = pd.read_csv('rank_split_2.csv')\n",
    "\n",
    "print(\"Result for first split:\")\n",
    "print(rank_split_1)\n",
    "print(\"----------------------------------------\")\n",
    "print(\"Result for second split:\")\n",
    "print(rank_split_2)"
   ],
   "execution_count": 19,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Result for first split:\n",
      "      user_id  item_id     score\n",
      "0         196      251  0.069504\n",
      "1         196      762  0.061897\n",
      "2         196      428  0.051793\n",
      "3         196      202  0.049060\n",
      "4         196      382  0.044702\n",
      "...       ...      ...       ...\n",
      "9425      941      455  0.000000\n",
      "9426      941      257  0.000000\n",
      "9427      941      475  0.000000\n",
      "9428      941        7  0.000000\n",
      "9429      941      181  0.000000\n",
      "\n",
      "[9430 rows x 3 columns]\n",
      "----------------------------------------\n",
      "Result for second split:\n",
      "      user_id  item_id     score\n",
      "0         196      285  0.055741\n",
      "1         196      655  0.053111\n",
      "2         196       25  0.048641\n",
      "3         196       70  0.041518\n",
      "4         196      381  0.034722\n",
      "...       ...      ...       ...\n",
      "9425      941     1007  0.009110\n",
      "9426      941      300  0.008939\n",
      "9427      941      919  0.000000\n",
      "9428      941      408  0.000000\n",
      "9429      941      763  0.000000\n",
      "\n",
      "[9430 rows x 3 columns]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1VlytYEsAEEh"
   },
   "source": [
    "# Evaluation module: evaluation of external recommendations\n",
    "\n",
    "Recommendations can be evaluated with several metrics using the **EvalModel** module of the framework. The nice part of it is that it can evaluate easily also (multiple) recommendations generated via external tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D_oXZn3QAEEp"
   },
   "source": [
    "The Evaluation module needs the following parameters:\n",
    "\n",
    "*   A list of computed rank/predictions (in case multiple splits must be evaluated)\n",
    "*   A list of truths (in case multiple splits must be evaluated)\n",
    "*   List of metrics to compute\n",
    "\n",
    "Obviously the list of computed rank/predictions and list of truths must have the same length, and the rank/prediction in position $i$ will be compared with the truth at position $i$\n",
    "\n",
    "Let's suppose we have recommendations (and related truths) generated via other tools in a csv format. We first import them into the framework and then pass them to the EvalModel class\n",
    "\n",
    "*   In this case we will use the recommendations generated earlier in the RecSys phase of this colab, but they are simple csv files and they could be the output of any other tool!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"Importing split 1\")\n",
    "rank_1 = ca.Ratings(ca.CSVFile('rank_split_1.csv'))\n",
    "truth_1 = ca.Ratings(ca.CSVFile('truth_split_1.csv'))\n",
    "\n",
    "print(\"Importing split 2\")\n",
    "rank_2 = ca.Ratings(ca.CSVFile('rank_split_2.csv'))\n",
    "truth_2 = ca.Ratings(ca.CSVFile('truth_split_2.csv'))\n",
    "\n",
    "# since multiple splits, we wrap ranks and truths in lists\n",
    "imported_ranks = [rank_1, rank_2]\n",
    "imported_truths = [truth_1, truth_2]"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TRiA2LiiCRzU",
    "outputId": "61f57436-d6a7-45b8-b12f-6c2dfcbc7810"
   },
   "execution_count": 20,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Importing split 1\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Importing ratings:  100%|██████████| 9430/9430 [00:00<00:00]\n",
      "Importing ratings:  100%|██████████| 50240/50240 [00:00<00:00]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Importing split 2\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Importing ratings:  100%|██████████| 9430/9430 [00:00<00:00]\n",
      "Importing ratings:  100%|██████████| 49760/49760 [00:00<00:00]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We are ready to instantiate the EvalModel class\n",
    "\n",
    "*   We also need to define a metric list, suppose we want to compute ***Pearson Correlation***, ***MRR*** and ***NDCG***\n",
    "\n"
   ],
   "metadata": {
    "id": "DQDcSQJmDxQM"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "SVtf_pFAAEEp"
   },
   "source": [
    "em = eva.EvalModel(\n",
    "    result_list,\n",
    "    test_list,\n",
    "    metric_list=[\n",
    "        eva.Correlation('pearson'),\n",
    "        eva.MRR(),\n",
    "        eva.NDCG()\n",
    "    ]\n",
    ")"
   ],
   "execution_count": 21,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoFTI1T83TVV"
   },
   "source": [
    "The fit() method returns two pandas DataFrame: the first one contains the metrics aggregated for the system, while the second contains the metrics computed for each user (where possible)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "o9QXMFl02iDK",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "59fb8dec-72a0-4f25-cdbb-543202eb1f20"
   },
   "source": [
    "sys_result, users_result =  em.fit()"
   ],
   "execution_count": 22,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001B[39mINFO\u001B[0m - Performing evaluation on metrics chosen\n",
      "Performing NDCG:  100%|██████████| 3/3 [00:02<00:00]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "For the DataFrame which contains system results, the results are also grouped by splits"
   ],
   "metadata": {
    "id": "Fq8kTXb11SvG"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "id": "yIpALBnr4Azi",
    "outputId": "3f23ff84-5a8a-4997-a3bf-3f0e7f42c0ca"
   },
   "source": [
    "sys_result"
   ],
   "execution_count": 23,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "              pearson       MRR      NDCG\n",
       "user_id                                  \n",
       "sys - fold1  0.025621  0.764719  0.924090\n",
       "sys - fold2  0.045971  0.766976  0.924882\n",
       "sys - mean   0.035796  0.765847  0.924486"
      ],
      "text/html": [
       "\n",
       "  <div id=\"df-b707d66b-8168-47ef-997a-dfbb0d9ce2e8\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pearson</th>\n",
       "      <th>MRR</th>\n",
       "      <th>NDCG</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sys - fold1</th>\n",
       "      <td>0.025621</td>\n",
       "      <td>0.764719</td>\n",
       "      <td>0.924090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sys - fold2</th>\n",
       "      <td>0.045971</td>\n",
       "      <td>0.766976</td>\n",
       "      <td>0.924882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sys - mean</th>\n",
       "      <td>0.035796</td>\n",
       "      <td>0.765847</td>\n",
       "      <td>0.924486</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b707d66b-8168-47ef-997a-dfbb0d9ce2e8')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-b707d66b-8168-47ef-997a-dfbb0d9ce2e8 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-b707d66b-8168-47ef-997a-dfbb0d9ce2e8');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "id": "R_-unvKt4BzC",
    "outputId": "a4c6d191-20ac-4ee5-9a9f-9a3796a9c295"
   },
   "source": [
    "users_result"
   ],
   "execution_count": 24,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "          pearson      NDCG\n",
       "user_id                    \n",
       "1        0.010149  0.897007\n",
       "10       0.023204  0.945943\n",
       "100      0.022389  0.851947\n",
       "101      0.131692  0.912930\n",
       "102      0.106734  0.911586\n",
       "...           ...       ...\n",
       "95      -0.425661  0.861850\n",
       "96       0.229102  0.983621\n",
       "97       0.041533  0.941043\n",
       "98       0.135876  0.940887\n",
       "99       0.136496  0.917057\n",
       "\n",
       "[943 rows x 2 columns]"
      ],
      "text/html": [
       "\n",
       "  <div id=\"df-92c72bef-aae2-453b-a2a7-36a7136ce4d8\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pearson</th>\n",
       "      <th>NDCG</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.010149</td>\n",
       "      <td>0.897007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.023204</td>\n",
       "      <td>0.945943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.022389</td>\n",
       "      <td>0.851947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>0.131692</td>\n",
       "      <td>0.912930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0.106734</td>\n",
       "      <td>0.911586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>-0.425661</td>\n",
       "      <td>0.861850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.229102</td>\n",
       "      <td>0.983621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.041533</td>\n",
       "      <td>0.941043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.135876</td>\n",
       "      <td>0.940887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.136496</td>\n",
       "      <td>0.917057</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>943 rows × 2 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-92c72bef-aae2-453b-a2a7-36a7136ce4d8')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-92c72bef-aae2-453b-a2a7-36a7136ce4d8 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-92c72bef-aae2-453b-a2a7-36a7136ce4d8');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wBeOgvjY2ECV"
   },
   "source": [
    "# Other modules: export splitted dataset and methodology module\n",
    "\n",
    "*    During recommendation phase we have already seen how to split the dataset. We will see a further example on how to split the dataset by considering only some users and how to export said splitted dataset.\n",
    "*    A peculiar parameter may be changed in the recommendation phase, the `methodology` parameter: it enables you to choose which items need to be predicted using a specific methodology. We'll see how to use it manually and how to export its output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F6rHhaW-AXN_"
   },
   "source": [
    "## Split dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sDYYVUWM2hZi"
   },
   "source": [
    "In this example we split with a KFold all the users having a **mean average score >= 3.5**:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "81yNWR0p2baT",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "9299f4fd-0e2b-4acb-f856-e4187097bdd4"
   },
   "source": [
    "import statistics\n",
    "\n",
    "# valid_users will contain all users that have a mean average score given >= 3.5\n",
    "valid_users = set()\n",
    "\n",
    "all_users = set(ratings.user_idx_column)\n",
    "for u in all_users:\n",
    "  user_ratings = ratings.get_user_interactions(u)\n",
    "  all_users_scores = user_ratings[:, 2]\n",
    "  if statistics.mean(all_users_scores) >= 3.5:\n",
    "    valid_users.add(u)\n",
    "\n",
    "\n",
    "# Print the length of the two sets to check that they are different\n",
    "print(f\"n_valid_users = {len(valid_users)}\")\n",
    "print(f\"n_all_users = {len(all_users)}\")"
   ],
   "execution_count": 28,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "n_valid_users = 575\n",
      "n_all_users = 943\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8SeA6PTI411J",
    "outputId": "9fa654e8-fa55-4b24-92e2-103770dfd036"
   },
   "source": [
    "part_technique = rs.KFoldPartitioning(n_splits=3)\n",
    "\n",
    "train_list, test_list = part_technique.split_all(ratings,\n",
    "                                                 user_list=valid_users)"
   ],
   "execution_count": 30,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Performing KFoldPartitioning:  100%|██████████| 575/575 [00:00<00:00]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "puqxwcMo_41H"
   },
   "source": [
    "We can check the training and test set of the first split:  \n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_zxUsgcE_zGv",
    "outputId": "a978a0b8-c11d-49d6-9443-3f79b6a0d2b0"
   },
   "source": [
    "first_train = train_list[0]\n",
    "first_test = test_list[0]\n",
    "\n",
    "print(\"First split - train set:\\n\")\n",
    "print(first_train)\n",
    "print(\"--------------------------------\")\n",
    "print(\"\\nFirst split - test set:\\n\")\n",
    "print(first_test)"
   ],
   "execution_count": 31,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "First split - train set:\n",
      "\n",
      "      user_id item_id  score\n",
      "0         196     242    3.0\n",
      "1         196     381    4.0\n",
      "2         196     306    4.0\n",
      "3         196     238    4.0\n",
      "4         196      25    4.0\n",
      "...       ...     ...    ...\n",
      "36817     941     763    3.0\n",
      "36818     941     298    5.0\n",
      "36819     941     919    5.0\n",
      "36820     941     273    3.0\n",
      "36821     941     294    4.0\n",
      "\n",
      "[36822 rows x 3 columns]\n",
      "--------------------------------\n",
      "\n",
      "First split - test set:\n",
      "\n",
      "      user_id item_id  score\n",
      "0         196     393    4.0\n",
      "1         196     251    3.0\n",
      "2         196     655    5.0\n",
      "3         196      67    5.0\n",
      "4         196     663    5.0\n",
      "...       ...     ...    ...\n",
      "18680     941     222    2.0\n",
      "18681     941     408    5.0\n",
      "18682     941     300    4.0\n",
      "18683     941       1    5.0\n",
      "18684     941    1007    4.0\n",
      "\n",
      "[18685 rows x 3 columns]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Each object in the two lists is a **Rating** object, which has a useful exporting method `to_csv()`"
   ],
   "metadata": {
    "id": "_jmsk35NJWQ3"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "first_train.to_csv(file_name='exported_split_train')\n",
    "first_test.to_csv(file_name='exported_split_test')"
   ],
   "metadata": {
    "id": "JHuPP2bxJjED"
   },
   "execution_count": 32,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "st2i48dXBpZ3"
   },
   "source": [
    "## Methodology module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NknvogOFCLp7"
   },
   "source": [
    "During recommendation phase, it is also possible to specify an optional parameter in the `rank()` or `fit_rank()` method, called ***methodology***, for choosing which items must be ranked.\n",
    "For each target user **u**, the following 4 different methodologies are available for defining those lists:\n",
    "\n",
    "1.   **TestRatings** (default): the list of items to be evaluated consists of items rated by u in the test set\n",
    "2.   **TestItems**: every item in the test set of every user except those in the training set of the target user will be predicted\n",
    "3.   **TrainingItems**: every item in the training set of every user will be predicted except those in the training set of the target user\n",
    "4.   **AllItems**: the whole set of items, except those in the training set of the target user, will be predicted\n",
    "\n",
    "More information on [this paper](https://repositorio.uam.es/bitstream/handle/10486/665121/precision-oriented_bellogin_recsys_2011_ps.pdf;jsessionid=85982302D4DA9FF4DD7F21E4AC4F3391?sequence=1).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you have multiple splits you need to iterate over your train set and test set, since the `filter_all()` method of each methodology works on a single pair of train set and test set\n",
    "\n",
    "* You must call the `setup()` method before calling the `filter_all()` one!\n",
    "\n"
   ],
   "metadata": {
    "id": "xbBAXD5ZMVIp"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CjtUGJoiEhpo",
    "outputId": "1766b137-f52e-49c9-efb7-72f6ba47f864"
   },
   "source": [
    "test_r = rs.TestRatingsMethodology().setup(first_train, first_test).filter_all(first_train, first_test)\n",
    "\n",
    "train_i = rs.TrainingItemsMethodology().setup(first_train, first_test).filter_all(first_train, first_test)"
   ],
   "execution_count": 35,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Filtering items based on TestRatingsMethodology:  100%|██████████| 575/575 [00:00<00:00]\n",
      "Filtering items based on TrainingItemsMethodology:  100%|██████████| 575/575 [00:00<00:00]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4R1xu8mCF6sK"
   },
   "source": [
    "The output of the `filter_all()` method is a list of pandas DataFrame (one for every split in the split_list), so they can easily be exported to .csv, .tsv, etc.\n",
    "\n",
    "Let's check the item to predict with the test ratings methodology for the first split:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oyGcZ-50Hgrj",
    "outputId": "8b870fd3-1f81-43bc-d203-71f1736dd6f5"
   },
   "source": [
    "print(test_r)"
   ],
   "execution_count": 36,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "      user_id item_id\n",
      "0         196     393\n",
      "1         196     251\n",
      "2         196     655\n",
      "3         196      67\n",
      "4         196     663\n",
      "...       ...     ...\n",
      "18680     941     222\n",
      "18681     941     408\n",
      "18682     941     300\n",
      "18683     941       1\n",
      "18684     941    1007\n",
      "\n",
      "[18685 rows x 2 columns]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Report module"
   ],
   "metadata": {
    "id": "XHYkCpFUgBtZ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Via the `Report` class, you can generate several ***yml*** files containing all the parameters passed to key classes and functions during your experiment.\n",
    "\n",
    "The mentioned class is very flexible and it is able to document various parts of the experiment, based on the module you use:\n",
    "\n",
    "* You can document how you preprocessed items and complexly represented them via the *Content Analyzer*\n",
    "* Or maybe the experimental setup of your recommendation pipeline by passing key objects of it to the Report class\n",
    "* And lastly how you evaluated your recommender systems and on which metrics\n",
    "\n",
    "The aim is to give you all the tools to ***reproduce*** the experiment also in a different experimental setup"
   ],
   "metadata": {
    "id": "thWR8EOugHZg"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "First we import the utils package which contains the `Report` class"
   ],
   "metadata": {
    "id": "kWFlxu9jirze"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from clayrs import utils as ut"
   ],
   "metadata": {
    "id": "jDEHRrXAgDXU"
   },
   "execution_count": 37,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then you can instantiate the Report class. It needs the following parameters:\n",
    "* ***output_dir***: where the reports generated will be stored, by default it points to the current directory **('.')**\n",
    "* ***ca_report_filename***: how to rename the report of the content analyzer (if one is generated). Default is **'ca_report'**\n",
    "* ***rs_report_filename***: how to rename the report of the recsys module (if one is generated). Default is **'rs_report'**\n",
    "* ***eva_report_filename***: how to rename the report of the evaluation module (if one is generated). Default is **'eva_report'**"
   ],
   "metadata": {
    "id": "UQUiYnnjiyAq"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# in this case we are satisfied with the default parameters\n",
    "rep = ut.Report()"
   ],
   "metadata": {
    "id": "X_Vnei_dixcb"
   },
   "execution_count": 38,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then simply call the *yaml()* method which will produce the three yaml reports.\n",
    "\n",
    "\n",
    "> You need to pass to it the objects instantiated and used to execute your experiment. For the recsys module and the evaluation module you first need to perform the actual experiment before you are able to obtain a report for them\n",
    "\n",
    "All the parameters of the function are *optional* so that you decide for which module a yaml report must be produced, in case you performed a partial experiment and only used *some* of the modules offered by the framework"
   ],
   "metadata": {
    "id": "KWsICicGkeVF"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# In this case we generate a full report for the three modules used in the\n",
    "# experiment in this notebook\n",
    "rep.yaml(content_analyzer=content_analyzer,\n",
    "         original_ratings=ratings,\n",
    "         partitioning_technique=kf,\n",
    "         recsys=cbrs,\n",
    "         eval_model=em)"
   ],
   "metadata": {
    "id": "D-liANM5kTn_"
   },
   "execution_count": 39,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "The three yml files are generated in the current directory"
   ],
   "metadata": {
    "id": "TA7sgavnlpum"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "assert os.path.isfile('ca_report.yml')\n",
    "assert os.path.isfile('rs_report.yml')\n",
    "assert os.path.isfile('eva_report.yml')"
   ],
   "metadata": {
    "id": "Ms8KI8WAlyNO"
   },
   "execution_count": 40,
   "outputs": []
  }
 ]
}
