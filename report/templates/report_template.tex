\documentclass[12pt, a4paper]{article}
\title{Report ClayRS}
\author{Semantic Web Access and Personalization Research Group}
\usepackage{hyperref}
\usepackage{amsmath}


\begin{document}
\maketitle
This \LaTeX{} document was generated from yaml files for the purpose of replicability of experiments done with
\href{https://github.com/swapUniba/ClayRS}{ClayRS},
it contains information about the dataset, preprocessing methods, analysis algorithms and
the results of the experimental evaluation.
\VAR{'genres_0' in dict['field_representations'] }

\BLOCK{ if 'source_file' in dict }
\section{ClayRS configurations of the experiment}\label{sec:clayrs-configurations-of-the-experiment}

\subsection{Dataset}\label{subsec:dataset}
In this experiment, the \BLOCK{ if dict['id_each_content'] == ['movielens_id']} \textbf{Movielens}\BLOCK{endif}
 \textbf{Dataset} was used.

\hfill\break
\BLOCK{if 'interactions' in dict}
The statistics of the dataset used are reported in the following table \ref{tab:dataset_table}:

\begin{table}[ht]
    \centering
  \begin{tabular}{|c|c|}
    \hline
    \textbf{Parameter}& \textbf{Value} \\ \hline
    n\_users  & \VAR{dict['interactions']['n_users']|safe_text}\\ \hline
    n\_items  & \VAR{dict['interactions']['n_items']|safe_text}\\ \hline
    total\_interactions  & \VAR{dict['interactions']['total_interactions']|safe_text}\\ \hline
    min\_score  & \VAR{dict['interactions']['min_score']|safe_text}\\ \hline
    max\_score  & \VAR{dict['interactions']['max_score']|safe_text}\\ \hline
    mean\_score  & \VAR{dict['interactions']['mean_score']|safe_text}\\ \hline
    sparsity  & \VAR{dict['interactions']['sparsity']|truncate|safe_text}\\ \hline
  \end{tabular}
   \caption{Table of the Interactions}\label{tab:dataset_table}
\end{table}
\BLOCK{endif}

\hfill\break
\hfill\break

\BLOCK{ if 'WordEmbeddingTechnique' in dict['field_representations']['genres_0'] }
The following techniques have been used:
\VAR{dict['field_representations']['genres_0']['WordEmbeddingTechnique']['embedding_source']|safe_text}
has been used as word embedding.
\BLOCK{endif}
\BLOCK{if 'SentenceEmbeddingTechnique' in dict['field_representations']['genres_1'] }
\VAR{dict['field_representations']['genres_1']['SentenceEmbeddingTechnique']['embedding_source']|safe_text}
has been used as sentence embedding
\BLOCK{endif}
\hfill\break

\subsection{Preprocessing}\label{subsec:preprocessing}
\BLOCK{if 'plot_0' in dict['field_representations']} %# Beginning OF THE dict['field_representations']['plot_0']['SkLearnTfIdf']
\BLOCK{if 'NLTK' in dict['field_representations']['plot_0']['preprocessing']}
The preprocessing used is NLTK, a leading platform for building Python programs to work with human language data.
It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet,
along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing,
and semantic reasoning.

\hfill\break
In this experiment, those operations of NLTK were used:

\BLOCK{if dict['field_representations']['plot_0']['preprocessing']['NLTK']['strip_multiple_whitespace'] == True}
\textbf{Strip multiple whitespace}, an operation that removes multiple whitspaces between words.
\BLOCK{endif}


\BLOCK{if dict['field_representations']['plot_0']['preprocessing']['NLTK']['remove_punctuation'] == True}
\textbf{Remove punctuation}, an operation that removes the punctuation characters occuring in the text.
\BLOCK{endif}


\BLOCK{if dict['field_representations']['plot_0']['preprocessing']['NLTK']['stopwords_removal'] == True}
\textbf{Stopwords removal}, used to remove the stopwords occurring in the text.
\BLOCK{endif}


\BLOCK{if dict['field_representations']['plot_0']['preprocessing']['NLTK']['url_tagging'] == True}
\textbf{Url tagging}, tags the urls occuring in the text, replacing them with "<URL>".
\BLOCK{endif}


\BLOCK{if dict['field_representations']['plot_0']['preprocessing']['NLTK']['lemmatization'] == True}
\textbf{Lemmatization}, an operation that determines the lemma of a word based on its intended meaning, for example "troubled" becomes "trouble" after lemmatization.
\BLOCK{endif}


\BLOCK{if dict['field_representations']['plot_0']['preprocessing']['NLTK']['stemming'] == True}
Stemming, an operation that reduces a word to its root, for example "Flying" becomes "Fly" after the stemming operations is applied.
\BLOCK{endif}


\BLOCK{if dict['field_representations']['plot_0']['preprocessing']['NLTK']['named_entity_recognition'] == True}
Named entity recognition, an operation that detects the named entities such as person names, location names, company names, etc., from the text.
\BLOCK{endif}
\hfill\break




\BLOCK{ endif }
\BLOCK{ endif }

\BLOCK{if 'genres_0' in dict['field_representations']} %# Beginning OF THE dict['field_representations']['plot_0']['SkLearnTfIdf']
\BLOCK{if 'NLTK' in dict['field_representations']['genres_0']['preprocessing']}
The preprocessing used is NLTK, a leading platform for building Python programs to work with human language data.
It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet,
along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing,
and semantic reasoning.


\hfill\break
In this experiment, those operations of NLTK were used:

\BLOCK{if dict['field_representations']['genres_0']['preprocessing']['NLTK']['strip_multiple_whitespace'] == True}
\textbf{Strip multiple whitespace}, an operation that removes multiple whitspaces between words.
\BLOCK{endif}


\BLOCK{if dict['field_representations']['genres_0']['preprocessing']['NLTK']['remove_punctuation'] == True}
\textbf{Remove punctuation}, an operation that removes the punctuation characters occuring in the text.
\BLOCK{endif}


\BLOCK{if dict['field_representations']['genres_0']['preprocessing']['NLTK']['stopwords_removal'] == True}
\textbf{Stopwords removal}, used to remove the stopwords occurring in the text.
\BLOCK{endif}


\BLOCK{if dict['field_representations']['genres_0']['preprocessing']['NLTK']['url_tagging'] == True}
\textbf{Url tagging}, tags the urls occuring in the text, replacing them with "<URL>".
\BLOCK{endif}


\BLOCK{if dict['field_representations']['genres_0']['preprocessing']['NLTK']['lemmatization'] == True}
\textbf{Lemmatization}, an operation that determines the lemma of a word based on its intended meaning, for example "troubled" becomes "trouble" after lemmatization.
\BLOCK{endif}


\BLOCK{if dict['field_representations']['genres_0']['preprocessing']['NLTK']['stemming'] == True}
Stemming, an operation that reduces a word to its root, for example "Flying" becomes "Fly" after the stemming operations is applied.
\BLOCK{endif}


\BLOCK{if dict['field_representations']['genres_0']['preprocessing']['NLTK']['named_entity_recognition'] == True}
Named entity recognition, an operation that detects the named entities such as person names, location names, company names, etc., from the text.
\BLOCK{endif}
\hfill\break

\hfill\break


\BLOCK{ endif }
\BLOCK{ endif } %# END OF THE dict['field_representations']['plot_0']['Ekphrasis']




\BLOCK{if 'plot_0' in dict['field_representations']} %# Beginning OF THE dict['field_representations']['plot_0']['SkLearnTfIdf']
\BLOCK{if 'Ekphrasis' in dict['field_representations']['plot_0']['preprocessing'] }
The preprocessing used is Ekphrasis, ........................

\hfill\break
The Ekphrasis configuration for this experiment is :

\BLOCK{ endif }

\BLOCK{ endif } %# END OF THE dict['field_representations']['plot_0']['Ekphrasis']

\BLOCK{if 'genres_0' in dict['field_representations']} %# Beginning OF THE dict['field_representations']['plot_0']['SkLearnTfIdf']
\BLOCK{if 'Ekphrasis' in dict['field_representations']['genres_0']['preprocessing'] }
The preprocessing used is Ekphrasis, ........................

\hfill\break
The Ekphrasis configuration for this experiment is :

\BLOCK{ endif }

\BLOCK{ endif } %# END OF THE dict['field_representations']['plot_0']['Ekphrasis']

\BLOCK{if 'genres_0' in dict['field_representations']} %# Beginning OF THE dict['field_representations']['plot_0']['SkLearnTfIdf']
\BLOCK{if 'Spacy' in dict['field_representations']['genres_0']['preprocessing'] }

spaCy is a free, open-source library for advanced Natural Language Processing (NLP) in Python.
spaCy is designed specifically for production use and helps you build applications that process and “understand”
large volumes of text. It can be used to build information extraction or natural language understanding systems,
or to pre-process text for deep learning.
In this experiment, those spaCy operations have been used:

\hfill\break
In this experiment, those spaCy operations were used:

\BLOCK{if dict['field_representations']['genres_0']['preprocessing']['Spacy']['strip_multiple_whitespace'] == True}
\textbf{Strip multiple whitespace}, an operation that removes multiple whitspaces between words.
\BLOCK{endif}


\BLOCK{if dict['field_representations']['genres_0']['preprocessing']['Spacy']['remove_punctuation'] == True}
\textbf{Remove punctuation}, an operation that removes the punctuation characters occuring in the text.
\BLOCK{endif}

\BLOCK{if dict['field_representations']['genres_0']['preprocessing']['Spacy']['stopwords_removal'] == True}
\textbf{Stopwords removal}, used to remove the stopwords occurring in the text.
\BLOCK{endif}


\BLOCK{if dict['field_representations']['genres_0']['preprocessing']['Spacy']['url_tagging'] == True}
\textbf{Url tagging}, tags the urls occuring in the text, replacing them with "<URL>".
\BLOCK{endif}


\BLOCK{if dict['field_representations']['genres_0']['preprocessing']['Spacy']['lemmatization'] == True}
\textbf{Lemmatization}, an operation that determines the lemma of a word based on its intended meaning, for example "troubled" becomes "trouble" after lemmatization.
\BLOCK{endif}


\BLOCK{if dict['field_representations']['genres_0']['preprocessing']['Spacy']['named_entity_recognition'] == True}
Named entity recognition, an operation that detects the named entities such as person names, location names, company names, etc., from the text.
\BLOCK{endif}



\BLOCK{ endif }
\BLOCK{ endif } %# END OF THE dict['field_representations']['plot_0']['SkLearnTfIdf']

\BLOCK{ endif } %end of the content analyzer section

\BLOCK{ if dict['interactions'] is defined } %beginning of the recommender system section recsys
%\subsection{Interactions}

%\newpage
\subsection{Partitioning}\label{subsec:partitioning}
\BLOCK{ if dict['partitioning']['HoldOutPartitioning'] is defined}
%\subsection{Partitioning}

The partitioning used is the Hold-Out Partitioning.
This approach splits the dataset in use into a ‘train’ set and a ‘test’ set.
The training set is what the model is trained on, and the test set is used to see how
well the model will perform on new, unseen data.
\hfill\break
\hfill\break
The train set size of this experiment is the \VAR{dict['partitioning']['HoldOutPartitioning']['train_set_size'] * 100}\%
of the original dataset, while the test set is the remaining \VAR{(100 - (dict['partitioning']['HoldOutPartitioning']['train_set_size'] * 100))}\%.
\hfill\break
\hfill\break
\BLOCK{if dict['partitioning']['HoldOutPartitioning']['shuffle'] == True}
The data has been shuffled before being splitted into batches.
\BLOCK{endif}


%The Hold-Out Partitioning has been used in this experiment with the following configuration:
%\begin{table}[ht]
%    \centering
%  \begin{tabular}{|c|c|}
%    \hline
%    \textbf{Parameter}& \textbf{Value} \\ \hline
%    train\_set\_size  & \VAR{dict['partitioning']['HoldOutPartitioning']['train_set_size']|safe_text}\\ \hline
%    shuffle  & \VAR{dict['partitioning']['HoldOutPartitioning']['shuffle']|safe_text}\\ \hline
    %\VAR total\_interactions  & \VAR{dict['partitioning']['HoldOutPartitioning']['random_state']|safe_text}\\
    %\VAR skip\_user\_error  & \VAR{dict['partitioning']['HoldOutPartitioning']['skip_user_error']|safe_text}\\
%  \end{tabular}
%    \caption{Table of Hold-Out Partitioning}
%\end{table}

\BLOCK{endif}% end of the partitioning section

\BLOCK{ if dict['partitioning']['KFoldPartitioning'] is defined}
%\subsection{Partitioning}

The partitioning used is the KFoldPartitioning Partitioning.
...............................
\hfill\break
\hfill\break
%The train set size of this experiment is the \VAR{dict['partitioning']['KFoldPartitioning']['train_set_size'] * 100}\%
%of the original dataset, while the test set is the remaining \VAR{(100 - (dict['partitioning']['KFoldPartitioning']['train_set_size'] * 100))}\%.
\hfill\break
\hfill\break
\BLOCK{if dict['partitioning']['HoldOutPartitioning']['shuffle'] == True}
%The data has been shuffled before being splitted into batches.
\BLOCK{endif}


%The Hold-Out Partitioning has been used in this experiment with the following configuration:
%\begin{table}[ht]
%    \centering
%  \begin{tabular}{|c|c|}
%    \hline
%    \textbf{Parameter}& \textbf{Value} \\ \hline
%    train\_set\_size  & \VAR{dict['partitioning']['HoldOutPartitioning']['train_set_size']|safe_text}\\ \hline
%    shuffle  & \VAR{dict['partitioning']['HoldOutPartitioning']['shuffle']|safe_text}\\ \hline
    %\VAR total\_interactions  & \VAR{dict['partitioning']['HoldOutPartitioning']['random_state']|safe_text}\\
    %\VAR skip\_user\_error  & \VAR{dict['partitioning']['HoldOutPartitioning']['skip_user_error']|safe_text}\\
%  \end{tabular}
%    \caption{Table of Hold-Out Partitioning}
%\end{table}

\BLOCK{endif}% end of the partitioning section


\BLOCK{if dict['recsys']['GraphBasedRS']['algorithm']['NXPageRank'] is defined} %beginning recsys graph based area
\subsection{Algorithm Used}
In this experiment a Graph Based Recommender Algorithm has been used.
Specifically the NXPageRank algorithm, a Page Rank algorithm based on the networkx implementation
The PageRank can be 'personalized', in this case the PageRank will be calculated with Priors.
The alpha value used was: \VAR{dict['recsys']['GraphBasedRS']['algorithm']['NXPageRank']['alpha']|safe_text}


%\BLOCK{if 'SkLearnTfIdf' in dict['field_representations']['plot_0']} %# Beginning OF THE dict['field_representations']['plot_0']['SkLearnTfIdf']

%\begin{table}[htpb]
%    \centering
%  \begin{tabular}{|c|c|}

%    \hline
%    \textbf{Parameter}& \textbf{Value} \\ \hline
%    max\_df  & \VAR{dict['field_representations']['plot_0']['SkLearnTfIdf']['max_df']|safe_text} \\ \hline
%    min\_df  & \VAR{dict['field_representations']['plot_0']['SkLearnTfIdf']['min_df']|safe_text}\\ \hline
%    max\_features  & \VAR{dict['field_representations']['plot_0']['SkLearnTfIdf']['max_features']|safe_text}\\ \hline
%    vocabulary  & \VAR{dict['field_representations']['plot_0']['SkLearnTfIdf']['vocabulary']|safe_text}\\ \hline
%    binary  & \VAR{dict['field_representations']['plot_0']['SkLearnTfIdf']['binary']|safe_text}\\ \hline
    %\VAR dtype  & \VAR{dict['field_representations']['plot_0']['SkLearnTfIdf']['dtype']|safe_text}\\
%    norm  & \VAR{dict['field_representations']['plot_0']['SkLearnTfIdf']['norm']|safe_text}\\ \hline
%    use\_idf  & \VAR{dict['field_representations']['plot_0']['SkLearnTfIdf']['use_idf']|safe_text}\\ \hline
%    smooth\_idf  & \VAR{dict['field_representations']['plot_0']['SkLearnTfIdf']['smooth_idf']|safe_text}\\ \hline
%    sublinear\_tf  & \VAR{dict['field_representations']['plot_0']['SkLearnTfIdf']['sublinear_tf']|safe_text}\\ \hline
%  \end{tabular}
%    \caption{Table of the parameters used with SkLearnTfIdf}
%\end{table}
%\BLOCK{ endif } %# END OF THE dict['field_representations']['plot_0']['SkLearnTfIdf']



\BLOCK{endif} %end recsys area
\\
\BLOCK{endif} %end of the recsys section
%\newpage
\BLOCK{ if dict['metrics'] is defined } %beginning of the eval model section
\subsection{Metrics}


\BLOCK{if dict['metrics']['Precision'] is defined} %beginning of the precision section
%\subsection{Precision}



\hfill\break
In ClayRS the Precision metric is calculated as such for the \textbf{single user}:

    \[
    Precision_u = \frac{tp_u}{tp_u + fp_u}
    \]

    Where:

    - $tp_u$ is the number of items which are in the recommendation list of the user and have a
      ratingasdjiasd $\geq$ \BLOCK{if dict['metrics']['Precision']['relevant_threshold'] != None}
        \textbf{\VAR{dict['metrics']['Precision']['relevant_threshold']}}\BLOCK{endif}
        \BLOCK{if dict['metrics']['Precision']['relevant_threshold'] == None}
        \textbf{\VAR{dict['interactions']['mean_score']}}\BLOCK{endif}


    - $fp_u$ is the number of items which are in the recommendation list of the user and have a
      rating $<$ \BLOCK{if dict['metrics']['Precision']['relevant_threshold'] != None}
        \textbf{\VAR{dict['metrics']['Precision']['relevant_threshold']}}\BLOCK{endif}
        \BLOCK{if dict['metrics']['Precision']['relevant_threshold'] == None}
        \textbf{\VAR{dict['interactions']['mean_score']}}\BLOCK{endif}
    \hfill\break



In ClayRS, Precision needs those parameters:
\hfill\break
the \textbf{relevant\_threshold}, is a parameter needed to discern relevant items and non-relevant items for every user.
If not specified, the mean rating score of every user will be used, in this experiment it has been set to
\textbf{\VAR{dict['metrics']['Precision']['relevant_threshold']|safe_text}}.
\hfill\break\hfill\break
\textbf{sys\_average}, a parameter that specifies how the system average must be computed the default value is 'macro',
in this experiment the value of the sys\_average is \textbf{\VAR{dict['metrics']['Precision']['sys_average']|safe_text}}.
%\begin{table}[ht]
%    \centering
%  \begin{tabular}{|c|c|}
%    \hline
%    \textbf{Parameter}& \textbf{Value} \\ \hline
%    relevant\_threshold  & \VAR{dict['metrics']['Precision']['relevant_threshold']|safe_text}\\ \hline
%    sys\_average  & \VAR{dict['metrics']['Precision']['sys_average']|safe_text}\\ \hline
    %\VAR precision  & \VAR{dict['metrics']['Precision']['precision']|safe_text}\\
%  \end{tabular}
%    \caption{Table of the precision configuration}
%\end{table}

\BLOCK{endif} %end of the precision section



\BLOCK{if dict['metrics']['PrecisionAtK'] is defined} %beginning of the precision at k section
%\subsection{Metrics: Precision at K}
\hfill\break
\textbf{Precision at k} is the proportion of recommended items in the top-k set that are relevant.
 The Precision@K metric is calculated as such for the \textbf{single user}:

    \[
    Precision@K_u = \frac{tp@K_u}{tp@K_u + fp@K_u}
    \]

    Where:

    - $tp@K_u$ is the number of items which are in the recommendation list  of the user
      \textbf{cutoff to the first K items} and have a rating $\geq$
        \BLOCK{if dict['metrics']['Precision']['relevant_threshold'] != None}
        \textbf{\VAR{dict['metrics']['Precision']['relevant_threshold']}}\BLOCK{endif}
        \BLOCK{if dict['metrics']['Precision']['relevant_threshold'] == None}
        \textbf{\VAR{dict['interactions']['mean_score']}}\BLOCK{endif}


    - $tp@K_u$ is the number of items which are in the recommendation list  of the user
      \textbf{cutoff to the first K items} and have a rating $<$
        \BLOCK{if dict['metrics']['Precision']['relevant_threshold'] != None}
        \textbf{\VAR{dict['metrics']['Precision']['relevant_threshold']}}\BLOCK{endif}
        \BLOCK{if dict['metrics']['Precision']['relevant_threshold'] == None}
        \textbf{\VAR{dict['interactions']['mean_score']}}\BLOCK{endif}



In this experiment the value \textbf{k is \VAR{dict['metrics']['PrecisionAtK']['k']|safe_text}},
the sys\_average is \textbf{\VAR{dict['metrics']['PrecisionAtK']['sys_average']|safe_text}}


%\begin{table}[ht]
%    \centering
%  \begin{tabular}{|c|c|}
%    \hline
%    \textbf{Parameter}& \textbf{Value} \\ \hline
%    k  & \VAR{dict['metrics']['PrecisionAtK']['k']|safe_text}\\ \hline
%    sys\_average  & \VAR{dict['metrics']['PrecisionAtK']['sys_average']|safe_text}\\ \hline
    %\VAR precision  & \VAR{dict['metrics']['PrecisionAtK']['precision']|safe_text}\\
%  \end{tabular}
%   \caption{Table of the precision at k configuration}
%\end{table}
\BLOCK{endif} %end of the precision at ksection

\BLOCK{if dict['metrics']['FMeasureAtK'] is defined} %beginning of the fmeasure at k section
%\subsection{Metrics: F - measure at K}
\hfill\break\hfill\break
 The FMeasure@K metric combines Precision@K and Recall@K into a single metric.
 It is calculated as such for the
    \textbf{single user}:

    \[
    FMeasure_u = (1 + \beta^2) \cdot \frac{P@K_u \cdot R@K_u}{(\beta^2 \cdot P@K_u) + R@K_u}
    \]

    Where:

    - $P@K_u$ is the Precision at K calculated for the user \textbf{u}
    - $R@K_u$ is the Recall at K calculated for the user \textbf{u}
    - $\beta$ is a real factor which could weight differently Recall or Precision based on its value:

        - $\beta = 1$: Equally weight Precision and Recall
        - $\beta > 1$: Weight Recall more
        - $\beta < 1$: Weight Precision more

    A famous FMeasure@K is the F1@K Metric, where $\beta$ = 1, which basically is the harmonic mean of recall and
    precision:
    \hfill\break\hfill\break


    \[
    F1@K_u = \frac{2 \cdot P@K_u \cdot R@K_u}{P@K_u + R@K_u}
    \]




\hfill\break
\hfill\break
In this experiment \textbf{k = \VAR{dict['metrics']['FMeasureAtK']['k']|safe_text}},
\text{\boldmath$\beta$} = \textbf{\VAR{dict['metrics']['FMeasureAtK']['beta']|safe_text}} and
\textbf{sys\_average} is \textbf{\VAR{dict['metrics']['FMeasureAtK']['sys_average']|safe_text}}.

%\begin{table}[ht]
%    \centering
%  \begin{tabular}{|c|c|}
%    \hline
%    \textbf{Parameter}& \textbf{Value} \\ \hline
%    k  & \VAR{dict['metrics']['FMeasureAtK']['k']|safe_text}\\ \hline
%    $\beta$  & \VAR{dict['metrics']['FMeasureAtK']['beta']|safe_text}\\ \hline
%    sys\_average  & \VAR{dict['metrics']['FMeasureAtK']['sys_average']|safe_text}\\ \hline
%    \end{tabular}
%    \caption{Table of the fmeasure at k configuration}
%\end{table}
%\BLOCK{endif} %end of the fmeasure at k section

\BLOCK{if dict['sys_results']['sys - fold1'] is defined} %beginning of sysfold 1 results
%\subsection{Sys Results fold1 :}
%\newpage
\subsection{Results}\label{sec:results}
In the following table, we present the results of the evaluation \ref{tab:results_table}
\begin{table}[!hbp]\label{tab:results_table}
    \centering
  \begin{tabular}{|c|c|}
    \hline
    \textbf{Metric}& \textbf{Value} \\ \hline
    Precision - macro & \VAR{dict['sys_results']['sys - fold1']['Precision - macro']|truncate|safe_text}\\ \hline
    Precision@2 - macro  & \VAR{dict['sys_results']['sys - fold1']['Precision@2 - macro']|truncate|safe_text}\\ \hline
    NDCG  & \VAR{dict['sys_results']['sys - fold1']['NDCG']|truncate|safe_text}\\ \hline
    MRR  & \VAR{dict['sys_results']['sys - fold1']['MRR']|truncate|safe_text}\\ \hline
    F1@1 - macro  & \VAR{dict['sys_results']['sys - fold1']['F1@1 - macro']|truncate|safe_text}\\ \hline
  \end{tabular}
  \caption{Table of the results}
\end{table}
\BLOCK{endif} % end of sysfold 1 results

\BLOCK{endif} %end of the recsys section


\end{document}