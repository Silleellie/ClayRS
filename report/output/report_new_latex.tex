

%! Author = DIEGO
%! Date = 21/12/2023

% Preamble
\documentclass[11pt]{article}

% Packages
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{comment}
\usepackage{graphicx}
\usepackage{caption}

\title{Report Experiment ClayRS Framework}
\author{SWAP research group UniBa}
\date{30-12-2023}

% -------------------- DOCUMENT: REPORT ON THE ExPERIMENT WITH CLAYRS FRAMEWORK STARTED -------------------------------
\begin{document}

\maketitle
This LaTex document was generated automatically from yaml files for the purpose of replicability of experiments done with
\href{https://github.com/swapUniba/ClayRS}{ClayRS},
it contains information about the experiment that has been conducted and the results obtained.
The report is divided in 3 principal section dedicated each one for the 3 principal module of the ClayRS framework
and a conclusion section to highlights what have been achieved from the experiment.
\hfill\break
\hfill\break



% ----------------------------------------- OPENING CONTENT ANALYZER SECTION -----------------------------------------
\section{Content Analyzer Module}\label{sec:ca}
The content analyzer module will deal with raw source document or more in general data which could be
video or audio data and give a representation of these data which will be used by the other two module.
The text data source could be represented with exogenous technique or with a specified representation
and each field given could be treated with preprocessing techniques and postprocessing technique.
In this experiment the following techniques have been used on specific field in order to achieve the
representation wanted:
\hfill\break
\hfill\break
% --- TECNIQUE USED TO REPPRESENT DATA FIELD ---



% field: plot_0 representation subsection
$ plot_0 $  has been represented with the following techniques:
\hfill\break
\hfill\break



% SkLearnTfIdf used as representation
The SkLearnTfIdf technique has been used.
The technique has been settled with the following parameter:
\begin{itemize}
    \item max df: 1.0
    \item min df: 1
    \item max features: None
    \item vocabulary: None
    \item binary: False
    \item norm: l2
    \item use idf: True
    \item smooth idf: True
    \item sublinear tf: False
\end{itemize}
\hfill\break
\hfill\break
% SkLearnTfIdf usege end___



% NLTK preprocessing
NLTK has been used to preprocess data with the following settings:
\begin{itemize}
    \item strip multiple whitespace: True
    \item remove punctuation: False
    \item stopwords removal: True
    \item url tagging: False
    \item lemmatization: False
    \item stemming: False
    \item pos tag: no setting used
    \item lang: english
\end{itemize}
\hfill\break
\hfill\break
% NLTK preprocessing end___



% field: genres_0 representation subsection
$ genres_0 $  has been represented with the following techniques:
\hfill\break
\hfill\break



% WordEmbeddingTechnique used as representation
The Word Embedding Technique has been used and
the embedding source used is
Gensim glove-twitter-25
\hfill\break
\hfill\break
% WordEmbeddingTechnique usege end___



% Spacy preprocessing
Spacy has been used to preprocess data with the following settings:
\begin{itemize}
    \item model: en\_core\_web\_sm
    \item strip multiple whitespace: True
    \item remove punctuation:False
    \item stopwords removal: True
    \item new stopwords: None
    \item not stopwords: None
    \item lemmatization: False
    \item url tagging: False
    \item named entity recognition: False
\end{itemize}
\hfill\break
\hfill\break
% Spacy preprocessing end___



% field: genres_1 representation subsection
$ genres_1 $  has been represented with the following techniques:
\hfill\break
\hfill\break



% SentenceEmbeddingTechnique used as representation
The Sentence Embedding Technique has been used and
the embedding source used is
Sbert
\hfill\break
\hfill\break
% SentenceEmbeddingTechnique usege end___



% Spacy preprocessing
Spacy has been used to preprocess data with the following settings:
\begin{itemize}
    \item model: en\_core\_web\_sm
    \item strip multiple whitespace: True
    \item remove punctuation:False
    \item stopwords removal: True
    \item new stopwords: None
    \item not stopwords: None
    \item lemmatization: False
    \item url tagging: False
    \item named entity recognition: False
\end{itemize}
\hfill\break
\hfill\break
% Spacy preprocessing end___






% ----------------------------------------- START RECSYS MODULE ------------------------------------------------------
\section{Recommender System module: RecSys}\label{sec:recsys}
The \textbf{RecSys module} allows to instantiate a recommender system and make it work on items and users serialized
by the Content Analyzer module, despite this is also possible using other serialization made with other framework and
give them as input to the recommender system and obtain score prediction or recommend items for the active user(s).
In particular this module allows has to get some general statistics on the data used, the scheme used to split the data
and train the recommender system and the settings beloning to the algorithm chosen.
\hfill\break
\hfill\break

\subsection{Statistics on data used}\label{subsec:stats}
% --- DATA STATS ---
In this experiment the statistics of the dataset used are reported in the following table:~\ref{tab:dataset_table}:
\begin{table}[ht]
    \centering
  \begin{tabular}{|c|c|}
    \hline
    \textbf{Parameter}& \textbf{Value} \\ \hline
    n\_users  & 943\\ \hline
    n\_items  & 1682\\ \hline
    total\_interactions  & 100000\\ \hline
    min\_score  & 1.0\\ \hline
    max\_score  & 5.0\\ \hline
    mean\_score  & 3.52986\\ \hline
    sparsity  & 0.93695\\ \hline
  \end{tabular}
   \caption{Table of the Interactions}\label{tab:dataset_table}
\end{table}
\hfill\break
\hfill\break


% ------------------------------ START SUBSECTION OF PARTITIONING OF RECSYS --------------------------------------------
\subsection{Partitioning techinque used}\label{subsec:partitioning}


%  HOLD-OUT PARTIONING TECNIQUE
The partitioning used is the Hold-Out Partitioning.
This approach splits the dataset in use into a train set and a test set.
The training set is what the model is trained on, and the test set is used to see how
well the model will perform on new, unseen data.
\hfill\break
\hfill\break
The train set size of this experiment is the 80.0\%
of the original dataset, while the test set is the remaining 20.0\%.
\hfill\break
\hfill\break
The data has been shuffled before being split into batches.
\hfill\break
\hfill\break
%  HOLD-OUT PARTIONING TECNIQUE ended
% end partitioning section___________


% ---------------------------------- ALGORITHM FOR RECOMMENDER SYSTEM -------------------------------------------------
\subsection{Algorithm used for the recommender system}\label{subsec:algo}
% ---RECSYS ALGORITHM ---
The framework of ClayRs allows to instantiate a recommender system in order to make list of recommendation, to achieve
this target the system need to be based on a chosen algorithm that will work with the representation of data that
we have.
In this section we will analyse which algorithm has been used for the experiment and what are the settings
given.
\hfill\break
\hfill\break

% Dealing with the principal class where the algorithm used belong to.
The recommender system is based on a graph representation.
\hfill\break
\hfill\break

% ----------------------------------------- CONTENT BASE ALGO ---------------------------------------------------------










% ------------------------ GRAPH BASED ALGO ------------------------------------------
% NX PAGE RANK ALGO
The algorithm used is NX Page Rank.
\begin{itemize}
    \item alpha: 0.85
    \item personalized: False
    \item max iter: 100
    \item tol: 1e-06
    \item nstart: None
    \item weight: weight
\end{itemize}
\hfill\break
\hfill\break
The type of graph used in the algorithm is a: NXFullGraph,
the mode used is rank and the number of recommendation given is
None.
\hfill\break
\hfill\break
% NX page rank end____


% In case of the RecSys in not been used



% -------------------------------------- OPENING THE EVALUATION MODULE SECTION ---------------------------------------
\section{Evaluation Module}\label{sec:eva-module}
The \textbf{EvalModel} which is the abbreviation for Evaluation Model has the task of evaluating a recommender system,
using several state-of-the-art metrics, this allows to compare different recommender system and different algorithm of
recommendation and find out which are the strength points and which the weak ones.

\subsection{Metrics}\label{subsec:metrics}
% --- Metrics ---
During the experiment a bunch of formal metrics have been performed on the recommendation produced in order to evaluate
the performance of the system.
The metrics used are the followings:
\hfill\break
\hfill\break

% ---------------------------------------- CLASSIFICATION METRICS STARTED --------------------------------------------
% Precision report start
\subsubsection{Precision}\label{subsubsec:precision}
In ClayRS, the Precision metric is calculated as such for the \textbf{single user}:
\hfill\break
\hfill\break
    \[
         Precision_u = \frac{tp_u}{tp_u + fp_u}
    \]
\hfill\break
\hfill\break
    Where:
\begin{itemize}
    \item $tp_u$ is the number of items which are in the recommendation list of the user and have a
       $\geq$         \textbf{3.52986}.
            \item $fp_u$ is the number of items which are in the recommendation list of the user and have a
      rating $<$         \textbf{no relevant threshold used}.
        \end{itemize}
\hfill\break
\hfill\break
In ClayRS, Precision needs those parameters:
the \textbf{relevant\_threshold}, is a parameter needed to discern relevant items and non-relevant items for every user.
If not specified, the mean rating score of every user will be used, in this experiment it has been set to
\textbf{None}.
\hfill\break
\hfill\break
% precision report ended___



% PRECISION @K REPORT start
\subsubsection{Precision@K}\label{subsubsec:prec-k}
The Precision@K metric is calculated as such for the \textbf{single user}:
\hfill\break
\hfill\break
    \[
        Precision@K_u = \frac{tp@K_u}{tp@K_u + fp@K_u}
    \]
\hfill\break
\hfill\break
    Where:
\begin{itemize}
    \item $tp@K_u$ is the number of items which are in the recommendation list of the user
      cutoff to the first K items and have a rating $>=$ relevant threshold in its ground truth.
    \item $tp@K_u$ is the number of items which are in the recommendation list of the user
      cutoff to the first K items** and have a rating $<$ relevant threshold in its ground truth.
\end{itemize}
\hfill\break
\hfill\break
And it is calculated as such for the entire system, depending on if \textbf{macro} average or \textbf{micro} average
has been chosen:
\hfill\break
\hfill\break
   \[
       Precision@K_{sys} - micro = \frac{\sum_{u \in U} tp@K_u}{\sum_{u \in U} tp@K_u + \sum_{u \in U} fp@K_u}
   \]
\hfill\break
\hfill\break
    \[
       Precision@K_{sys} - macro = \frac{\sum_{u \in U} Precision@K_u}{|U|}
   \]
\hfill\break
\hfill\break
During the experiment Precision@K has been used with the following settings:
\begin{itemize}
    \item \textbf{K: 2 }
    \item \textbf{relevant threshold:None }
    \item \textbf{sys average: macro }
\end{itemize}
\hfill\break
\hfill\break
% precision@k report end___


% FMEASURE @K REPORT start
\subsubsection{FMeasure@K }\label{subsubsec:f-meas-k}
The FMeasure@K metric combines Precision@K and Recall@K into a single metric.
It is calculated as such for the \textbf{single user}:
\hfill\break
\hfill\break
    \[
        FMeasure@K_u = (1 + \beta^2) \cdot \frac{P@K_u \cdot R@K_u}{(\beta^2 \cdot P@K_u) + R@K_u}
    \]
\hfill\break
\hfill\break
    Where:
\begin{itemize}
    \item $P@K_u$ is the Precision at K calculated for the user \textbf{u}.
    \item $R@K_u$ is the Recall at K calculated for the user \textbf{u}.
    \item $\beta$ is a real factor which could weight differently Recall or Precision based on its value:
    \begin{itemize}
        \item $\beta = 1$: Equally weight Precision and Recall.
        \item $\beta > 1$: Weight Recall more.
        \item $\beta < 1$: Weight Precision more.
    \end{itemize}
\end{itemize}
\hfill\break
\hfill\break
A famous FMeasure@K is the F1@K Metric, where $\beta = 1$, which basically is the harmonic mean of recall and
precision:
\hfill\break
\hfill\break
    \[
        F1@K_u = \frac{2 \cdot P@K_u \cdot R@K_u}{P@K_u + R@K_u}
    \]
\hfill\break
\hfill\break
The FMeasure@K metric is calculated as such for the entire system, depending on if \textbf{macro} average or
\textbf{micro} average has been chosen:
\hfill\break
\hfill\break
    \[
        FMeasure@K_{sys} - micro = (1 + \beta^2) \cdot \frac{P@K_u \cdot R@K_u}{(\beta^2 \cdot P@K_u) + R@K_u}
    \]
\hfill\break
\hfill\break
    \[
        FMeasure@K_{sys} - macro = \frac{\sum_{u \in U} FMeasure@K_u}{|U|}
    \]
\hfill\break
\hfill\break
During the experiment FMeasure@K has been used with the following settings:
\begin{itemize}
    \item \textbf{K: 1}
    \item  \textbf{$\beta$: 1}
    \item \textbf{relevant threshold: None }
    \item \textbf{sys average: macro }
\end{itemize}
\hfill\break
\hfill\break
% fmeasure@k report end___

% <----- classification metrics end ------->

% -------------------------------------------- ERROR METRICS STARTED ------------------------------------------------


% <------ error metrics ended -------->

% ------------------------------------------- RANKING METRICS STARTED -----------------------------------------------
% Normalized Discounted Cumulative Gain REPORT start
\subsubsection{NDCG}\label{subsubsec:ndcg}
The NDCG abbreviation of Normalized Discounted Cumulative Gain metric is calculated for the \textbf{single user}
by first computing the DCG score using the following formula:
\hfill\break
\hfill\break
    \[
        DCG_{u}(scores_{u}) = \sum_{r\in scores_{u}}{\frac{f(r)}{log_x(2 + i)}}
    \]
\hfill\break
\hfill\break
    Where:
\begin{itemize}
    \item $scores_{u}$ are the ground truth scores for predicted items, ordered according to the order of said items in the
        ranking for the user $u$.
    \item $f$ is a gain function \textit{linear or exponential, in particular}.
    \item $x$ is the base of the logarithm.
    \item $i$ is the index of the truth score $r$ in the list of scores $scores_{u}$.
\end{itemize}
\hfill\break
\hfill\break
If $f$ is "linear", then the truth score $r$ is returned as is. Otherwise, in the "exponentia" case, the following
formula is applied to $r$:
\hfill\break
\hfill\break
    \[
        f(r) = 2^{r} - 1
    \]
\hfill\break
\hfill\break
The NDCG for a single user is then calculated using the following formula:
\hfill\break
\hfill\break
    \[
        NDCG_u(scores_{u}) = \frac{DCG_{u}(scores_{u})}{IDCG_{u}(scores_{u})}
    \]
\hfill\break
\hfill\break
    Where:
\begin{itemize}
    \item $IDCG_{u}$ is the DCG of the ideal ranking for the truth scores.
\end{itemize}
\hfill\break
\hfill\break
So the basic idea is to compare the actual ranking with the ideal one.
Finally, the NDCG of the entire system is calculated instead as such:
\hfill\break
\hfill\break
    \[
        NDCG_{sys} = \frac{\sum_{u} NDCG_u}{|U|}
    \]
\hfill\break
\hfill\break
    Where:
\begin{itemize}
    \item $NDCG_u$ is the NDCG calculated for user $u$.
    \item $U$ is the set of all users.
\end{itemize}
\hfill\break
\hfill\break
The system average excludes NaN values.
\hfill\break
\hfill\break
% ndcg report end___


% MRR REPORT start
\subsubsection{MRR}\label{subsubsec:mrr}
The MRR abbreviation of Mean Reciprocal Rank metric is a system-wide metric, so only its result it will be returned
and not those of every user.
MRR is calculated as such:
\hfill\break
\hfill\break
    \[
        MRR_{sys} = \frac{1}{|Q|}\cdot\sum_{i=1}^{|Q|}\frac{1}{rank(i)}
    \]
\hfill\break
\hfill\break
    Where:
\begin{itemize}
    \item $Q$ is the set of recommendation lists.
    \item $rank(i)$ is the position of the first relevant item in the i-th recommendation list.
\end{itemize}
\hfill\break
\hfill\break
% ATTENTION here we modified with \_ in case change
The MRR metric needs to discern relevant items from the not relevant ones. To achieve this, one could pass a
custom \texttt{relevant\_threshold} parameter that will be applied to every user. If the rating of an item
is $\geq$ \texttt{relevant\_threshold}, then it is considered relevant; otherwise, it is not.
If no \texttt{relevant\_threshold} parameter is passed, then for every user, its mean rating score will be used.
In this experiment, the relevant threshold used is
None.
\hfill\break
\hfill\break
% mrr report end___




% <------- ranking metrics ended ------->

% -------------------------------------------- FAIRNESS METRICS ------------------------------------------------------



% <------- fairness metrics end ------>

% ------------------------------------------------ PLOT METRICS STARTED ----------------------------------------------


% <------- plot metric end ----------->

% <---------------------- closing the only metric section ------------------------->



% ------------------------------- RESULT OF PERFORMACE OF THE SYSTEM ON THE FOLD -------------------------------------
\subsection{sys - fold1}\label{subsec:sys - fold1}
In the following table, we present the results of the evaluation~\ref{tab:results_table_sys - fold1}

\begin{center}
    \begin{tabular}{|c|c|}
        \hline
        \textbf{Metric} & \textbf{Value} \\ \hline
        Precision - macro & 0.55697\\ \hline
                                        NDCG  & 0.93529\\ \hline
            MRR  & 0.79602\\ \hline
                                 \end{tabular}
    \captionsetup{type=table}
    \caption{Table of the results}
    \label{tab:results_table_sys - fold1}
\end{center}
\hfill\break
\hfill\break
% end table of performance on the fold___



% ------------------------------- RESULT OF PERFORMACE OF THE SYSTEM ON THE FOLD -------------------------------------
\subsection{sys - mean}\label{subsec:sys - mean}
In the following table, we present the results of the evaluation~\ref{tab:results_table_sys - mean}

\begin{center}
    \begin{tabular}{|c|c|}
        \hline
        \textbf{Metric} & \textbf{Value} \\ \hline
        Precision - macro & 0.55697\\ \hline
                                        NDCG  & 0.93529\\ \hline
            MRR  & 0.79602\\ \hline
                                 \end{tabular}
    \captionsetup{type=table}
    \caption{Table of the results}
    \label{tab:results_table_sys - mean}
\end{center}
\hfill\break
\hfill\break
% end table of performance on the fold___



% In case of eval module is not used



\section{Conclusion on the experiment}\label{sec:conclution}
This part is for conclusion to be sum up as needed.
\hfill\break
\hfill\break

% ------------------- END OF THE REPORT COMPLETED ---------------
% closing the document
\end{document}
