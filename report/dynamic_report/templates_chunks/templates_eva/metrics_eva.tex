\usepackage{amsmath}
###

% ------------------ CLASSIFICATION METRICS STARTED ------------------------------------

% Precision report
\BLOCK{if my_dict['metrics']['Precision'] is defined }
In ClayRS, the Precision metric is calculated as such for the \textbf{single user}:

    \[
    Precision_u = \frac{tp_u}{tp_u + fp_u}
    \]

    Where:

    - $tp_u$ is the number of items which are in the recommendation list of the user and have a
       $\geq$ \BLOCK{ if my_dict.get('metrics', {}).get('Precision', {}).get('relevant_threshold') is not none}
        \textbf{\VAR{my_dict['metrics']['Precision']['relevant_threshold']}}
        \BLOCK{else}
        \textbf{\VAR{my_dict['interactions']['mean_score']|default('no relevant threshold used')}}
        \BLOCK{endif}


    - $fp_u$ is the number of items which are in the recommendation list of the user and have a
      rating $<$ \BLOCK{if my_dict.get('metrics', {}).get('Precision', {}).get('relevant_threshold') is not none}
        \textbf{\VAR{my_dict['metrics']['Precision']['relevant_threshold']}}
        \BLOCK{else}
        \textbf{\VAR{dict['interactions']['mean_score']|default('no relevant threshold used')}}
        \BLOCK{endif}

\hfill\break

In ClayRS, Precision needs those parameters:
\hfill\break
the \textbf{relevant\_threshold}, is a parameter needed to discern relevant items and non-relevant items for every user.
If not specified, the mean rating score of every user will be used, in this experiment it has been set to
\textbf{\VAR{my_dict['metrics']['Precision']['relevant_threshold']|safe_text}}.
\hfill\break



% Recall report
\BLOCK{elif my_dict['metrics']['Recall'] is defined}
The Recall metric is calculated as such for the \textbf{single user}:

    \[
    Recall_u = \frac{tp_u}{tp_u + fn_u}
    \]

    Where:

    - $tp_u$ is the number of items which are in the recommendation list of the user and have a
      rating $>=$ \BLOCK{ if my_dict.get('metrics', {}).get('Recall', {}).get('relevant_threshold') is not none}
        \textbf{\VAR{my_dict['metrics']['Recall']['relevant_threshold']}}
        \BLOCK{else}
        \textbf{\VAR{my_dict['interactions']['mean_score']|default('no relevant threshold used')}}
        \BLOCK{endif}

    - $fn_u$ is the number of items which are not in the recommendation list of the user and have a
      rating $>=$ \BLOCK{if my_dict.get('metrics', {}).get('Recall', {}).get('relevant_threshold') is not none}
        \textbf{\VAR{my_dict['metrics']['Recall']['relevant_threshold']}}
        \BLOCK{else}
        \textbf{\VAR{dict['interactions']['mean_score']|default('no relevant threshold used')}}
        \BLOCK{endif}

\hfill\break

In ClayRS, Recall needs those parameters:
\hfill\break
the \textbf{relevant\_threshold}, is a parameter needed to discern relevant items and non-relevant items for every user.
If not specified, the mean rating score of every user will be used, in this experiment it has been set to
\textbf{\VAR{my_dict['metrics']['Recall']['relevant_threshold']|default('no relevant threshold used')|safe_text}}.
\hfill\break



%FMeasure report
\BLOCK{elif my_dict['metrics']['FMeasure'] is defined}
The FMeasure metric combines Precision and Recall into a single metric.
It is calculated as such for the \textbf{single user}.

    \[
    FMeasure_u = (1 + \beta^2) \cdot \frac{P_u \cdot R_u}{(\beta^2 \cdot P_u) + R_u}
    \]

    Where:

    - $P_u$ is the Precision calculated for the user \textbf{u}
    - $R_u$ is the Recall calculated for the user \textbf{u}
    - $\beta$ is a real factor which could weight differently Recall or Precision based on its value:

        - $\beta = 1$: Equally weight Precision and Recall
        - $\beta > 1$: Weight Recall more
        - $\beta < 1$: Weight Precision more

\hfill\break

A famous FMeasure is the F1 Metric, where $\beta = 1$, which basically is the harmonic mean of recall and
precision:

    \[
    F1_u = \frac{2 \cdot P_u \cdot R_u}{P_u + R_u}
    \]

\hfill\break

The FMeasure metric is calculated as such for the **entire system**, depending on if 'macro' average or 'micro'
average has been chosen:

    \begin{gather*}
        FMeasure_{sys} - micro = (1 + \beta^2) \cdot \frac{P_u \cdot R_u}{(\beta^2 \cdot P_u) + R_u}\\
        FMeasure_{sys} - macro = \frac{\sum_{u \in U} FMeasure_u}{|U|}\\
    \end{gather*}

\hfill\break
During the experiment the FMeasure has been calculated with $\beta = $
\VAR{my_dict['metrics']['FMeasure']['beta']|safe_text} and the relevant threshold is
\textbf{\VAR{my_dict['metrics']['FMeasure']['relevant_threshold']|default('no relevant threshold used')|safe_text}}.


% PRECISION @K REPORT
\BLOCK{elif my_dict['metrics']['PrecisionAtK'] is defined}
The Precision@K metric is calculated as such for the \textbf{single user}:

    \[
    Precision@K_u = \frac{tp@K_u}{tp@K_u + fp@K_u}
    \]

    Where:

    - $tp@K_u$ is the number of items which are in the recommendation list of the user
      cutoff to the first K items and have a rating >= relevant threshold in its 'ground truth'

    - $tp@K_u$ is the number of items which are in the recommendation list of the user
      **cutoff to the first K items** and have a rating < relevant threshold in its 'ground truth'

\hfill\break

And it is calculated as such for the **entire system**, depending on if 'macro' average or 'micro' average has been
chosen:

   \begin{gather*}
       Precision@K_{sys} - micro = \frac{\sum_{u \in U} tp@K_u}{\sum_{u \in U} tp@K_u + \sum_{u \in U} fp@K_u}\\
       Precision@K_{sys} - macro = \frac{\sum_{u \in U} Precision@K_u}{|U|}\\
   \end{gather*}

\hfill\break

During the experiment Precision@K has been used with the following settings:
\begin{itemize}
    \item \textbf{K: \VAR{my_dict['metrics']['PrecisionAtK']['k']|safe_text} }
    \item \textbf{relevant threshold:\VAR{my_dict['metrics']['PrecisionAtK']['relevant_threshold']|default('no relevant threshold used')|safe_text} }
    \item \textbf{sys average: \VAR{my_dict['metrics']['PrecisionAtK']['sys_average']|safe_text} }
\end{itemize}


% RECALL @K REPORT
\BLOCK{elif my_dict['metrics']['RecallAtK'] is defined}
The Recall@K metric is calculated as such for the \textbf{single user}:

    \[
    Recall@K_u = \frac{tp@K_u}{tp@K_u + fn@K_u}
    \]

    Where:

    - $tp@K_u$ is the number of items which are in the recommendation list of the user
      \textbf{cutoff to the first K items} and have a rating >= relevant threshold in its 'ground truth'

    - $tp@K_u$ is the number of items which are NOT in the recommendation list of the user
      \textbf{cutoff to the first K items} and have a rating >= relevant threshold in its 'ground truth'

\hfill\break

And it is calculated as such for the **entire system**, depending on if 'macro' average or 'micro' average has been
chosen:

    \begin{gather*}
        Recall@K_{sys} - micro = \frac{\sum_{u \in U} tp@K_u}{\sum_{u \in U} tp@K_u + \sum_{u \in U} fn@K_u}\\
        Recall@K_{sys} - macro = \frac{\sum_{u \in U} Recall@K_u}{|U|}\\
    \end{gather*}

\hfill\break

During the experiment Recall@K has been used with the following settings:
\begin{itemize}
    \item \textbf{K: \VAR{my_dict['metrics']['RecallAtK']['k']|safe_text} }
    \item \textbf{relevant threshold: \VAR{my_dict['metrics']['RecallAtK']['relevant_threshold']|default('no relevant threshold used')|safe_text} }
    \item \textbf{sys average: \VAR{my_dict['metrics']['RecallAtK']['sys_average']|safe_text} }
\end{itemize}


% FMEASURE @K REPORT
\BLOCK{elif my_dict['metrics']['FMeasureAtK'] is defined}
The FMeasure@K metric combines Precision@K and Recall@K into a single metric.
It is calculated as such for the \textbf{single user}:

    \[
    FMeasure@K_u = (1 + \beta^2) \cdot \frac{P@K_u \cdot R@K_u}{(\beta^2 \cdot P@K_u) + R@K_u}
    \]

    Where:

    - $P@K_u$ is the Precision at K calculated for the user \textbf{u}
    - $R@K_u$ is the Recall at K calculated for the user \textbf{u}
    - $\beta$ is a real factor which could weight differently Recall or Precision based on its value:

        - $\beta = 1$: Equally weight Precision and Recall
        - $\beta > 1$: Weight Recall more
        - $\beta < 1$: Weight Precision more

\hfill\break

A famous FMeasure@K is the F1@K Metric, where :math:`\beta = 1`, which basically is the harmonic mean of recall and
precision:

    \[
    F1@K_u = \frac{2 \cdot P@K_u \cdot R@K_u}{P@K_u + R@K_u}
    \]

\hfill\break

The FMeasure@K metric is calculated as such for the entire system, depending on if 'macro' average or 'micro'
average has been chosen:

    \begin{gather*}
        FMeasure@K_{sys} - micro = (1 + \beta^2) \cdot \frac{P@K_u \cdot R@K_u}{(\beta^2 \cdot P@K_u) + R@K_u}\\
        FMeasure@K_{sys} - macro = \frac{\sum_{u \in U} FMeasure@K_u}{|U|}\\
    \end{gather*}

\hfill\break

During the experiment FMeasure@K has been used with the following settings:
\begin{itemize}
    \item \textbf{K: \VAR{my_dict['metrics']['FMeasureAtK']['k']|safe_text}}
    \item  \textbf{$\beta$: \VAR{my_dict['metrics']['FMeasureAtK']['beta']|safe_text}}
    \item \textbf{relevant threshold: \VAR{my_dict['metrics']['FMeasureAtK']['relevant_threshold']|default('no relevant threshold used')|safe_text} }
    \item \textbf{sys average: \VAR{my_dict['metrics']['FMeasureAtK']['sys_average']|safe_text} }
\end{itemize}


% R-PRECISION REPORT
\BLOCK{elif my_dict['metrics']['RPrecision'] is defined}
The R-Precision metric is calculated as such for the \textbf{single user}:

    \[
    R-Precision_u = \frac{tp@R_u}{tp@R_u + fp@R_u}
    \]

    Where:

    - $R$ it's the number of relevant items for the user \textbf{u}.

    - $tp@R_u$ is the number of items which are in the recommendation list of the user
      \textbf{cutoff to the first R items} and have a rating >= relevant_threshold in its 'ground truth'

    - $tp@R_u$ is the number of items which are in the recommendation list of the user
      \textbf{cutoff to the first R items} and have a rating < relevant_threshold in its 'ground truth'

\hfill\break

And it is calculated as such for the entire system, depending on if 'macro' average or 'micro' average has been
chosen:

    \begin{gather*}
        Precision@R_{sys} - micro = \frac{\sum_{u \in U} tp@R_u}{\sum_{u \in U} tp@R_u + \sum_{u \in U} fp@R_u}\\
        Precision@R_{sys} - macro = \frac{\sum_{u \in U} R-Precision_u}{|U|}\\
    \end{gather*}

\hfill\break

During the experiment R-Precision has been used with the following settings:
\begin{itemize}
    \item \textbf{relevant threshold:\VAR{my_dict['metrics']['RPrecision']['relevant_threshold']|default('no relevant threshold used')|safe_text} }
    \item \textbf{sys average: \VAR{my_dict['metrics']['RPrecision']['sys_average']|safe_text} }
\end{itemize}


% ------------------ CLASSIFICATION METRICS ENDED ------------------------------------

% ------------------ ERROR METRICS STARTED -------------------------------------------


% MSE REPORT
\BLOCK{elif my_dict['metrics']['MSE'] is defined }
The MSE (Mean Squared Error) metric is calculated as such for the \textbf{single user}:

    \[
    MSE_u = \sum_{i \in T_u} \frac{(r_{u,i} - \hat{r}_{u,i})^2}{|T_u|}
    \]

    Where:

    - $T_u$ is the *test set* of the user \textbf{u}
    - $r_{u, i}$ is the actual score give by user \textbf{u} to item \textbf{i}
    - $\hat{r}_{u, i}$ is the predicted score give by user \textbf{u} to item \textbf{i}

\hfill\break

And it is calculated as such for the entire system:

    \[
    MSE_{sys} = \sum_{u \in T} \frac{MSE_u}{|T|}
    \]

    Where:

    - $T$ is the \textbf{test set}
    - $MSE_u$ is the MSE calculated for user \textbf{u}

\hfill\break

There may be cases in which some items of the *test set* of the user could not be predicted (eg. A CBRS was chosen
and items were not present locally)

    In those cases, the $MSE_u$ formula becomes

    \[
    MSE_u = \sum_{i \in T_u} \frac{(r_{u,i} - \hat{r}_{u,i})^2}{|T_u| - unk}
    \]

    Where:

    - $unk$ (unknown) is the number of items of the user test set that could not be predicted

\hfill\break

If no items of the user test set have been predicted ($|T_u| - unk = 0$), then:

    \[
    MSE_u = NaN
    \]

\hfill\break


% RMSE REPORT
\BLOCK{elif my_dict['metrics']['RMSE'] is defined}
The RMSE (Root Mean Squared Error) metric is calculated as such for the \textbf{single user}:

    \[
    RMSE_u = \sqrt{\sum_{i \in T_u} \frac{(r_{u,i} - \hat{r}_{u,i})^2}{|T_u|}}
    \]

    Where:

    - $T_u$ is the *test set* of the user $u$
    - $r_{u, i}$ is the actual score give by user $u$ to item $i$
    - $\hat{r}_{u, i}$ is the predicted score give by user $u$ to item $i$

\hfill\break

And it is calculated as such for the entire system:

    \[
    RMSE_{sys} = \sum_{u \in T} \frac{RMSE_u}{|T|}
    \]

    Where:

    - $T$ is the test set
    - $RMSE_u$ is the RMSE calculated for user $u$

\hfill\break

There may be cases in which some items of the *test set* of the user could not be predicted (eg. A CBRS was chosen
and items were not present locally, a methodology different from TestRatings was chosen).

In those cases, the $RMSE_u$ formula becomes

    \[
    RMSE_u = \sqrt{\sum_{i \in T_u} \frac{(r_{u,i} - \hat{r}_{u,i})^2}{|T_u| - unk}}
    \]

    Where:

    - $unk$ (unknown) is the number of items of the user test set that could not be predicted

    If no items of the user test set have been predicted ($|T_u| - unk = 0$), then:

    \[
    RMSE_u = NaN
    \]

\hfill\break



% MAE REPORT
\BLOCK{elif my_dict['metrics']['MAE'] is defined}
The MAE (Mean Absolute Error) metric is calculated as such for the \textbf{single user}:

    \[
    MAE_u = \sum_{i \in T_u} \frac{|r_{u,i} - \hat{r}_{u,i}|}{|T_u|}
    \]

    Where:

    - $T_u$ is the test set of the user $u$
    - $r_{u, i}$ is the actual score give by user $u$ to item $i$
    - $\hat{r}_{u, i}$ is the predicted score give by user $u$ to item $i$

\hfill\break

And it is calculated as such for the entire system:

    \[
    MAE_{sys} = \sum_{u \in T} \frac{MAE_u}{|T|}
    \]

    Where:

    - $T$ is the test set
    - $MAE_u$ is the MAE calculated for user $u$

\hfill\break

There may be cases in which some items of the *test set* of the user could not be predicted (eg. A CBRS was chosen
and items were not present locally, a methodology different from *TestRatings* was chosen).
In those cases, the $MAE_u$ formula becomes

    \[
    MAE_u = \sum_{i \in T_u} \frac{|r_{u,i} - \hat{r}_{u,i}|}{|T_u| - unk}
    \]

    Where:

    - $unk$ (unknown) is the number of items of the user test set that could not be predicted

    If no items of the user test set have been predicted ($|T_u| - unk = 0$), then:

    \[
    MAE_u = NaN
    \]

\hfill\break

% ------------------------- error metrics ended -------------------------------------------


% ------------------------ RANKING METRICS STARTED ----------------------------------------

% Normalized Discounted Cumulative Gain REPORT
\BLOCK{elif my_dict['metrics']['NDCG'] is defined}
The NDCG abbreviation of Normalized Discounted Cumulative Gain metric is calculated for the \textbf{single user}
by first computing the DCG score using the following formula:

    \[
    DCG_{u}(scores_{u}) = \sum_{r\in scores_{u}}{\frac{f(r)}{log_x(2 + i)}}
    \]

    Where:

    - $scores_{u}$ are the ground truth scores for predicted items, ordered according to the order of said items in the
        ranking for the user $u$
    - $f$ is a gain function (linear or exponential, in particular)
    - $x$ is the base of the logarithm
    - $i$ is the index of the truth score $r$ in the list of scores $scores_{u}$

\hfill\break

If $f$ is "linear", then the truth score $r$ is returned as is. Otherwise, in the "exponential" case, the following
formula is applied to $r$:

    \[
    f(r) = 2^{r} - 1
    \]

\hfill\break

The NDCG for a single user is then calculated using the following formula:

    \[
    NDCG_u(scores_{u}) = \frac{DCG_{u}(scores_{u})}{IDCG_{u}(scores_{u})}
    \]

    Where:

    - $IDCG_{u}$ is the DCG of the ideal ranking for the truth scores

\hfill\break

So the basic idea is to compare the actual ranking with the ideal one.
Finally, the NDCG of the entire system is calculated instead as such:

    \[
    NDCG_{sys} = \frac{\sum_{u} NDCG_u}{|U|}
    \]

    Where:

    - $NDCG_u$ is the NDCG calculated for user :math:`u`
    - $U$ is the set of all users

\hfill\break

The system average excludes NaN values.



% NDCG@K REPORT
\BLOCK{elif my_dict['metrics']['NDCGAtK'] is defined}
The NDCG@K abbreviation of Normalized Discounted Cumulative Gain at K) metric is calculated for the \textbf{single user}
by using the [framework implementation of the NDCG][clayrs.evaluation.NDCG] but considering $scores_{u}$ cut at the
first $k$ predictions.

The K used for the experiment is \VAR{my_dict['metrics']['MRRAtK']['k']|safe_text}.


% MRR REPORT
\BLOCK{elif my_dict['metrics']['MRR'] is defined}
The MRR abbreviation of Mean Reciprocal Rank metric is a system-wide metric, so only its result it will be returned and not those
of every user. MRR is calculated as such:

    \[
    MRR_{sys} = \frac{1}{|Q|}\cdot\sum_{i=1}^{|Q|}\frac{1}{rank(i)}
    \]

    Where:

    - $Q$ is the set of recommendation lists
    - $rank(i)$ is the position of the first relevant item in the i-th recommendation list

\hfill\break

The MRR metric needs to discern relevant items from the not relevant ones: in order to do that, one could pass a
custom 'relevant_threshold' parameter that will be applied to every user, so that if a rating of an item
is >= relevant_threshold, then it's relevant, otherwise it's not.
If no 'relevant_threshold' parameter is passed then, for every user, its mean rating score will be used

In this experiment, the relevant threshod used is
\VAR{my_dict['metrics']['MRR']['relevant_threshold']|default('no threshold has been setted')|safe_text}.



\BLOCK{elif my_dict['metrics']['MRRAtK'] is defined}
The MRR@K (Mean Reciprocal Rank at K) metric is a system-wide metric, so only its result will be returned and
not those of every user. MRR@K is calculated as such

    \[
    MRR@K_{sys} = \frac{1}{|Q|}\cdot\sum_{i=1}^{K}\frac{1}{rank(i)}
    \]

    Where:

    - $K$ is the cutoff parameter
    - $Q$ is the set of recommendation lists
    - $rank(i)$ is the position of the first relevant item in the i-th recommendation list

\hfill\break
In this experiment, the relevant threshold used is
\VAR{my_dict['metrics']['MRRAtK']['relevant_threshold']|default('no threshold has been setted')|safe_text}.



% MAP REPORT
\BLOCK{elif my_dict['metrics']['MAP'] is defined}
The MAP metric abbreviation of Mean average Precision is a ranking metric computed by first calculating the AP
abbreviation of Average Precision for each user and then taking its mean.
The AP is calculated as such for the \textbf{single user}:

    \[
    AP_u = \frac{1}{m_u}\sum_{i=1}^{N_u}P(i)\cdot rel(i)
    \]

    Where:

    - $m_u$ is the number of relevant items for the user $u$
    - $N_u$ is the number of recommended items for the user $u$
    - $P(i)$ is the precision computed at cutoff $i$
    - $rel(i)$ is an indicator variable that says whether the i-th item is relevant ($rel(i)=1$) or not ($rel(i)=0$)

\hfill\break

After computing the AP for each user, we can compute the MAP for the whole system:

    \[
    MAP_{sys} = \frac{1}{|U|}\sum_{u}AP_u
    \]

\hfill\break

This metric will return the AP computed for each user in the dataframe containing users results, and the MAP
computed for the whole system in the dataframe containing system results. In this experiment the MAP has been calculeted
using a relevant threshold:
\VAR{my_dict['metrics']['MAP']['relevant_threshold']|default('no threshold has been setted')|safe_text}.


% MAP@K REPORT
\BLOCK{elif my_dict['metrics']['MAPAtK'] is defined}
The MAP@K metric abbreviation of Mean average Precision At K is a ranking metric computed by first calculating the
AP@K abbreviation of Average Precision At K for each user and then taking its mean.
The AP@K is calculated as such for the \text\[{single user}:

    \[
        AP@K_u = \frac{1}{m_u}\sum_{i=1}^{K}P(i)\cdot rel(i)
    \]

    Where:

    - $m_u$ is the number of relevant items for the user $u$
    - $K$ is the cutoff value
    - $P(i)$ is the precision computed at cutoff $i$
    - $rel(i)$ is an indicator variable that says whether the i-th item is relevant ($rel(i)=1$) or not ($rel(i)=0$)

\hfill\break

After computing the AP@K for each user, we can compute the MAP@K for the whole system:

    \[
        MAP@K_{sys} = \frac{1}{|U|}\sum_{u}AP@K_u
    \]

\hfill\break

This metric will return the AP@K computed for each user in the dataframe containing users results, and the MAP@K
computed for the whole system in the dataframe containing system results.


% CORRELATION REPORT
\BLOCK{elif my_dict['metrics']['Correlation'] is defined}
The Correlation metric calculates the correlation between the ranking of a user and its ideal ranking.
The currently correlation methods implemented are:

    - `pearson`
    - `kendall`
    - `spearman`

Every correlation method is implemented by the pandas library, so read its [documentation][pd_link] for more

[pd_link]: https://pandas.pydata.org/docs/reference/api/pandas.Series.corr.html

The correlation metric is calculated as such for the \textbf{single user}:

    \[
    Corr_u = Corr(ranking_u, ideal\_ranking_u)
    \]

    Where:

    - $ranking_u$ is ranking of the user
    - $ideal\_ranking_u$ is the ideal ranking for the user

\hfill\break

The ideal ranking is calculated based on the rating inside the *ground truth* of the user.
The Correlation metric calculated for the **entire system** is simply the average of every $Corr$:

    \[
    Corr_{sys} = \frac{\sum_{u} Corr_u}{|U|}
    \]

    Where:

    - $Corr_u$ is the correlation of the user $u$
    - $U$ is the set of all users

\hfill\break

The system average excludes NaN values.
It's also possible to specify a cutoff parameter thanks to the 'top_n' parameter: if specified, only the first
$n$ results of the recommendation list will be used in order to calculate the correlation.
For this experiment the setting of the correlation metrics are :
\begin{itemize}
    \item method: \VAR{my_dict['metrics']['Correlation']['method']|default('no method for correlation specified')|safe_text}.
    \item top_n: \VAR{my_dict['metrics']['Correlation']['top_n']|default('no cutoff setted')|safe_text}.
\end{itemize}

% ------------------------- ranking metrics ended ---------------------------


% ------------------------- FAIRNESS METRICS ------------------------------

% Gini Index REPORT
\BLOCK{elif my_dict['metrics']['GiniIndex'] is defined}
The Gini Index metric measures inequality in recommendation lists. It's a system wide metric, so only its
result it will be returned and not those of every user. The metric is calculated as such:

    \[
    Gini_{sys} = \frac{\sum_i(2i - n - 1)x_i}{n\cdot\sum_i x_i}
    \]

    Where:

    - $n$ is the total number of distinct items that are being recommended
    - $x_i$ is the number of times that the item $i$ has been recommended

\hfill\break

A perfectly equal recommender system should recommend every item the same number of times, in which case the Gini
index would be equal to 0. The more the recsys is "disegual", the more the Gini Index is closer to 1. If the 'top_n'
parameter is specified, then the Gini index will measure inequality considering only the first n items of every
recommendation list of all users. For this experiment the top_n:
\VAR{my_dict['metrics']['GiniIndex']['top_n']|default('no list of top n item has setted')|safe_text}.


% PREDICTION COVERAGE REPORT
\BLOCK{elif my_dict['metrics']['PredictionCoverage'] is defined}
The Prediction Coverage metric measures in percentage how many distinct items are being recommended in relation
to all available items. It's a system wise metric, so only its result it will be returned and not those of every
user. The metric is calculated as such:

    \[
    Prediction Coverage_{sys} = (\frac{|I_p|}{|I|})\cdot100
    \]

    Where:

    - $I$ is the set of all available items
    - $I_p$ is the set of recommended items

\hfill\break

The $I$ must be specified through the 'catalog' parameter.



% CATALOG COVERAGE REPORT
\BLOCK{elif my_dict['metrics']['CatalogCoverage'] is defined}
The Catalog Coverage metric measures in percentage how many distinct items are being recommended in relation
to all available items. It's a system wide metric, so only its result it will be returned and not those of every
user. It differs from the Prediction Coverage since it allows for different parameters to come into play. If no
parameter is passed then it's a simple Prediction Coverage.
The metric is calculated as such:

    \[
    Catalog Coverage_{sys} = (\frac{|\bigcup_{j=1...N}reclist(u_j)|}{|I|})\cdot100
    \]

    Where:

    - $N$ is the total number of users
    - $reclist(u_j)$ is the set of items contained in the recommendation list of user $j$
    - $I$ is the set of all available items

\hfill\break

The $I$ must be specified through the 'catalog' parameter. The recommendation list of every user ($reclist(u_j)$)
can be reduced to the first n parameter with the top-n parameter, so that catalog coverage is measured considering
only the most highest ranked items. With the 'k' parameter one could specify the number of users that will be used to
calculate catalog coverage: k users will be randomly sampled and their recommendation lists will be used.
The formula above becomes:

    \[
    Catalog Coverage_{sys} = (\frac{|\bigcup_{j=1\dotsk}reclist(u_j)|}{|I|})\cdot100
    \]

    Where:

    - $k$ is the parameter specified

\hfill\break

Obviously 'k' < N, else simply recommendation lists of all users will be used


% DELTA GAP REPORT
\BLOCK{elif my_dict['metrics']['DeltaGap'] is defined}
The Delta GAP (Group Average popularity) metric lets you compare the average popularity "requested" by one or
multiple groups of users and the average popularity "obtained" with the recommendation given by the recsys.
It's a system wise metric and results of every group will be returned. It is calculated as such:

    \[
    \Delta GAP = \frac{recs_GAP - profile_GAP}{profile_GAP}
    \]

Users are split into groups based on the user_groups parameter, which contains names of the groups as keys,
and percentage of how many user must contain a group as values. For example:

\hfill\break

\begin{itemize}
        \item user_groups = {'popular_users': 0.3, 'medium_popular_users': 0.2, 'low_popular_users': 0.5}
\end{itemize}

\hfill\break

Every user will be inserted in a group based on how many popular items the user has rated (in relation to the
percentage of users we specified as value in the dictionary):

\begin{itemize}
    \item users with many popular items will be inserted into the first group
    \item users with niche items rated will be inserted into one of the last groups.
\end{itemize}

In general users are grouped by $Popularity\_ratio$ in a descending order. $Popularity\_ratio$ for a single user $u$
is defined as:

    \[
    Popularity\_ratio_u = n\_most\_popular\_items\_rated_u / n\_items\_rated_u
    \]

The most popular items are the first 'pop_percentage', items of all items ordered in a descending order by
popularity. The popularity of an item is defined as the number of times it is rated in the 'original_ratings' parameter
divided by the total number of users in the 'original_ratings'.

\hfill\break

It can happen that for a particular user of a group no recommendation are available: in that case it will be skipped
and it won't be considered in the $\Delta GAP$ computation of its group. In case no user of a group has recs
available, a warning will be printed and the whole group won't be considered.

If the 'top_n' parameter is specified, then the $\Delta GAP$ will be calculated considering only the first
n items of every recommendation list of all users



\BLOCK{elif my_dict['metrics']['Precision'] is defined}

\BLOCK{elif my_dict['metrics']['Precision'] is defined}
\BLOCK{else}

\BLOCK{endif}

###