\usepackage{amsmath}
###

% ------------------ CLASSIFICATION METRICS STARTED ------------------------------------

% Precision report
\BLOCK{if my_dict['metrics']['Precision'] is defined }
In ClayRS, the Precision metric is calculated as such for the \textbf{single user}:

    \[
    Precision_u = \frac{tp_u}{tp_u + fp_u}
    \]

    Where:

    - $tp_u$ is the number of items which are in the recommendation list of the user and have a
       $\geq$ \BLOCK{ if my_dict.get('metrics', {}).get('Precision', {}).get('relevant_threshold') is not none}
        \textbf{\VAR{my_dict['metrics']['Precision']['relevant_threshold']}}
        \BLOCK{else}
        \textbf{\VAR{my_dict['interactions']['mean_score']|default('no relevant threshold used')}}
        \BLOCK{endif}


    - $fp_u$ is the number of items which are in the recommendation list of the user and have a
      rating $<$ \BLOCK{if my_dict.get('metrics', {}).get('Precision', {}).get('relevant_threshold') is not none}
        \textbf{\VAR{my_dict['metrics']['Precision']['relevant_threshold']}}
        \BLOCK{else}
        \textbf{\VAR{dict['interactions']['mean_score']|default('no relevant threshold used')}}
        \BLOCK{endif}

\hfill\break

In ClayRS, Precision needs those parameters:
\hfill\break
the \textbf{relevant\_threshold}, is a parameter needed to discern relevant items and non-relevant items for every user.
If not specified, the mean rating score of every user will be used, in this experiment it has been set to
\textbf{\VAR{my_dict['metrics']['Precision']['relevant_threshold']|safe_text}}.
\hfill\break



% Recall report
\BLOCK{elif my_dict['metrics']['Recall'] is defined}
The Recall metric is calculated as such for the \textbf{single user}:

    \[
    Recall_u = \frac{tp_u}{tp_u + fn_u}
    \]

    Where:

    - $tp_u$ is the number of items which are in the recommendation list of the user and have a
      rating $>=$ \BLOCK{ if my_dict.get('metrics', {}).get('Recall', {}).get('relevant_threshold') is not none}
        \textbf{\VAR{my_dict['metrics']['Recall']['relevant_threshold']}}
        \BLOCK{else}
        \textbf{\VAR{my_dict['interactions']['mean_score']|default('no relevant threshold used')}}
        \BLOCK{endif}

    - $fn_u$ is the number of items which are not in the recommendation list of the user and have a
      rating $>=$ \BLOCK{if my_dict.get('metrics', {}).get('Recall', {}).get('relevant_threshold') is not none}
        \textbf{\VAR{my_dict['metrics']['Recall']['relevant_threshold']}}
        \BLOCK{else}
        \textbf{\VAR{dict['interactions']['mean_score']|default('no relevant threshold used')}}
        \BLOCK{endif}

\hfill\break

In ClayRS, Recall needs those parameters:
\hfill\break
the \textbf{relevant\_threshold}, is a parameter needed to discern relevant items and non-relevant items for every user.
If not specified, the mean rating score of every user will be used, in this experiment it has been set to
\textbf{\VAR{my_dict['metrics']['Recall']['relevant_threshold']|default('no relevant threshold used')|safe_text}}.
\hfill\break



%FMeasure report
\BLOCK{elif my_dict['metrics']['FMeasure'] is defined}
The FMeasure metric combines Precision and Recall into a single metric.
It is calculated as such for the \textbf{single user}.

    \[
    FMeasure_u = (1 + \beta^2) \cdot \frac{P_u \cdot R_u}{(\beta^2 \cdot P_u) + R_u}
    \]

    Where:

    - $P_u$ is the Precision calculated for the user \textbf{u}
    - $R_u$ is the Recall calculated for the user \textbf{u}
    - $\beta$ is a real factor which could weight differently Recall or Precision based on its value:

        - $\beta = 1$: Equally weight Precision and Recall
        - $\beta > 1$: Weight Recall more
        - $\beta < 1$: Weight Precision more

\hfill\break

A famous FMeasure is the F1 Metric, where $\beta = 1$, which basically is the harmonic mean of recall and
precision:

    \[
    F1_u = \frac{2 \cdot P_u \cdot R_u}{P_u + R_u}
    \]

\hfill\break

The FMeasure metric is calculated as such for the **entire system**, depending on if 'macro' average or 'micro'
average has been chosen:

    \begin{gather*}
        FMeasure_{sys} - micro = (1 + \beta^2) \cdot \frac{P_u \cdot R_u}{(\beta^2 \cdot P_u) + R_u}\\
        FMeasure_{sys} - macro = \frac{\sum_{u \in U} FMeasure_u}{|U|}\\
    \end{gather*}

\hfill\break
During the experiment the FMeasure has been calculated with $\beta = $
\VAR{my_dict['metrics']['FMeasure']['beta']|safe_text} and the relevant threshold is
\textbf{\VAR{my_dict['metrics']['FMeasure']['relevant_threshold']|default('no relevant threshold used')|safe_text}}.


% PRECISION @K REPORT
\BLOCK{elif my_dict['metrics']['PrecisionAtK'] is defined}
The Precision@K metric is calculated as such for the \textbf{single user}:

    \[
    Precision@K_u = \frac{tp@K_u}{tp@K_u + fp@K_u}
    \]

    Where:

    - $tp@K_u$ is the number of items which are in the recommendation list of the user
      cutoff to the first K items and have a rating >= relevant threshold in its 'ground truth'

    - $tp@K_u$ is the number of items which are in the recommendation list of the user
      **cutoff to the first K items** and have a rating < relevant threshold in its 'ground truth'

\hfill\break

And it is calculated as such for the **entire system**, depending on if 'macro' average or 'micro' average has been
chosen:

   \begin{gather*}
       Precision@K_{sys} - micro = \frac{\sum_{u \in U} tp@K_u}{\sum_{u \in U} tp@K_u + \sum_{u \in U} fp@K_u}\\
       Precision@K_{sys} - macro = \frac{\sum_{u \in U} Precision@K_u}{|U|}\\
   \end{gather*}

\hfill\break

During the experiment Precision@K has been used with the following settings:
\begin{itemize}
    \item \textbf{K: \VAR{my_dict['metrics']['PrecisionAtK']['k']|safe_text} }
    \item \textbf{relevant threshold:\VAR{my_dict['metrics']['PrecisionAtK']['relevant_threshold']|default('no relevant threshold used')|safe_text} }
    \item \textbf{sys average: \VAR{my_dict['metrics']['PrecisionAtK']['sys_average']|safe_text} }
\end{itemize}


% RECALL @K REPORT
\BLOCK{elif my_dict['metrics']['RecallAtK'] is defined}
The Recall@K metric is calculated as such for the \textbf{single user}:

    \[
    Recall@K_u = \frac{tp@K_u}{tp@K_u + fn@K_u}
    \]

    Where:

    - $tp@K_u$ is the number of items which are in the recommendation list of the user
      \textbf{cutoff to the first K items} and have a rating >= relevant threshold in its 'ground truth'

    - $tp@K_u$ is the number of items which are NOT in the recommendation list of the user
      \textbf{cutoff to the first K items} and have a rating >= relevant threshold in its 'ground truth'

\hfill\break

And it is calculated as such for the **entire system**, depending on if 'macro' average or 'micro' average has been
chosen:

    \begin{gather*}
        Recall@K_{sys} - micro = \frac{\sum_{u \in U} tp@K_u}{\sum_{u \in U} tp@K_u + \sum_{u \in U} fn@K_u}\\
        Recall@K_{sys} - macro = \frac{\sum_{u \in U} Recall@K_u}{|U|}\\
    \end{gather*}

\hfill\break

During the experiment Recall@K has been used with the following settings:
\begin{itemize}
    \item \textbf{K: \VAR{my_dict['metrics']['RecallAtK']['k']|safe_text} }
    \item \textbf{relevant threshold: \VAR{my_dict['metrics']['RecallAtK']['relevant_threshold']|default('no relevant threshold used')|safe_text} }
    \item \textbf{sys average: \VAR{my_dict['metrics']['RecallAtK']['sys_average']|safe_text} }
\end{itemize}


% FMEASURE @K REPORT
\BLOCK{elif my_dict['metrics']['FMeasureAtK'] is defined}
The FMeasure@K metric combines Precision@K and Recall@K into a single metric.
It is calculated as such for the \textbf{single user}:

    \[
    FMeasure@K_u = (1 + \beta^2) \cdot \frac{P@K_u \cdot R@K_u}{(\beta^2 \cdot P@K_u) + R@K_u}
    \]

    Where:

    - $P@K_u$ is the Precision at K calculated for the user \textbf{u}
    - $R@K_u$ is the Recall at K calculated for the user \textbf{u}
    - $\beta$ is a real factor which could weight differently Recall or Precision based on its value:

        - $\beta = 1$: Equally weight Precision and Recall
        - $\beta > 1$: Weight Recall more
        - $\beta < 1$: Weight Precision more

\hfill\break

A famous FMeasure@K is the F1@K Metric, where :math:`\beta = 1`, which basically is the harmonic mean of recall and
precision:

    \[
    F1@K_u = \frac{2 \cdot P@K_u \cdot R@K_u}{P@K_u + R@K_u}
    \]

\hfill\break

The FMeasure@K metric is calculated as such for the entire system, depending on if 'macro' average or 'micro'
average has been chosen:

    \begin{gather*}
        FMeasure@K_{sys} - micro = (1 + \beta^2) \cdot \frac{P@K_u \cdot R@K_u}{(\beta^2 \cdot P@K_u) + R@K_u}\\
        FMeasure@K_{sys} - macro = \frac{\sum_{u \in U} FMeasure@K_u}{|U|}\\
    \end{gather*}

\hfill\break

During the experiment FMeasure@K has been used with the following settings:
\begin{itemize}
    \item \textbf{K: \VAR{my_dict['metrics']['FMeasureAtK']['k']|safe_text}}
    \item  \textbf{$\beta$: \VAR{my_dict['metrics']['FMeasureAtK']['beta']|safe_text}}
    \item \textbf{relevant threshold: \VAR{my_dict['metrics']['FMeasureAtK']['relevant_threshold']|default('no relevant threshold used')|safe_text} }
    \item \textbf{sys average: \VAR{my_dict['metrics']['FMeasureAtK']['sys_average']|safe_text} }
\end{itemize}


% R-PRECISION REPORT
\BLOCK{elif my_dict['metrics']['RPrecision'] is defined}
The R-Precision metric is calculated as such for the \textbf{single user}:

    \[
    R-Precision_u = \frac{tp@R_u}{tp@R_u + fp@R_u}
    \]

    Where:

    - $R$ it's the number of relevant items for the user \textbf{u}.

    - $tp@R_u$ is the number of items which are in the recommendation list of the user
      \textbf{cutoff to the first R items} and have a rating >= relevant_threshold in its 'ground truth'

    - $tp@R_u$ is the number of items which are in the recommendation list of the user
      \textbf{cutoff to the first R items} and have a rating < relevant_threshold in its 'ground truth'

\hfill\break

And it is calculated as such for the entire system, depending on if 'macro' average or 'micro' average has been
chosen:

    \begin{gather*}
        Precision@R_{sys} - micro = \frac{\sum_{u \in U} tp@R_u}{\sum_{u \in U} tp@R_u + \sum_{u \in U} fp@R_u}\\
        Precision@R_{sys} - macro = \frac{\sum_{u \in U} R-Precision_u}{|U|}\\
    \end{gather*}

\hfill\break

During the experiment R-Precision has been used with the following settings:
\begin{itemize}
    \item \textbf{relevant threshold:\VAR{my_dict['metrics']['RPrecision']['relevant_threshold']|default('no relevant threshold used')|safe_text} }
    \item \textbf{sys average: \VAR{my_dict['metrics']['RPrecision']['sys_average']|safe_text} }
\end{itemize}


% ------------------ CLASSIFICATION METRICS ENDED ------------------------------------

% ------------------ ERROR METRICS STARTED -------------------------------------------


% MSE REPORT
\BLOCK{elif my_dict['metrics']['MSE'] is defined }
The MSE (Mean Squared Error) metric is calculated as such for the \textbf{single user}:

    \[
    MSE_u = \sum_{i \in T_u} \frac{(r_{u,i} - \hat{r}_{u,i})^2}{|T_u|}
    \]

    Where:

    - $T_u$ is the *test set* of the user \textbf{u}
    - $r_{u, i}$ is the actual score give by user \textbf{u} to item \textbf{i}
    - $\hat{r}_{u, i}$ is the predicted score give by user \textbf{u} to item \textbf{i}

\hfill\break

And it is calculated as such for the entire system:

    \[
    MSE_{sys} = \sum_{u \in T} \frac{MSE_u}{|T|}
    \]

    Where:

    - $T$ is the \textbf{test set}
    - $MSE_u$ is the MSE calculated for user \textbf{u}

\hfill\break

There may be cases in which some items of the *test set* of the user could not be predicted (eg. A CBRS was chosen
and items were not present locally)

    In those cases, the $MSE_u$ formula becomes

    \[
    MSE_u = \sum_{i \in T_u} \frac{(r_{u,i} - \hat{r}_{u,i})^2}{|T_u| - unk}
    \]

    Where:

    - $unk$ (unknown) is the number of items of the user test set that could not be predicted

\hfill\break

If no items of the user test set have been predicted ($|T_u| - unk = 0$), then:

    \[
    MSE_u = NaN
    \]

\hfill\break


% RMSE REPORT
\BLOCK{elif my_dict['metrics']['RMSE'] is defined}
The RMSE (Root Mean Squared Error) metric is calculated as such for the \textbf{single user}:

    \[
    RMSE_u = \sqrt{\sum_{i \in T_u} \frac{(r_{u,i} - \hat{r}_{u,i})^2}{|T_u|}}
    \]

    Where:

    - $T_u$ is the *test set* of the user $u$
    - $r_{u, i}$ is the actual score give by user $u$ to item $i$
    - $\hat{r}_{u, i}$ is the predicted score give by user $u$ to item $i$

\hfill\break

And it is calculated as such for the entire system:

    \[
    RMSE_{sys} = \sum_{u \in T} \frac{RMSE_u}{|T|}
    \]

    Where:

    - $T$ is the test set
    - $RMSE_u$ is the RMSE calculated for user $u$

\hfill\break

There may be cases in which some items of the *test set* of the user could not be predicted (eg. A CBRS was chosen
and items were not present locally, a methodology different from TestRatings was chosen).

In those cases, the $RMSE_u$ formula becomes

    \[
    RMSE_u = \sqrt{\sum_{i \in T_u} \frac{(r_{u,i} - \hat{r}_{u,i})^2}{|T_u| - unk}}
    \]

    Where:

    - $unk$ (unknown) is the number of items of the user test set that could not be predicted

    If no items of the user test set have been predicted ($|T_u| - unk = 0$), then:

    \[
    RMSE_u = NaN
    \]

\hfill\break



% MAE REPORT
\BLOCK{elif my_dict['metrics']['MAE'] is defined}
The MAE (Mean Absolute Error) metric is calculated as such for the \textbf{single user}:

    \[
    MAE_u = \sum_{i \in T_u} \frac{|r_{u,i} - \hat{r}_{u,i}|}{|T_u|}
    \]

    Where:

    - $T_u$ is the test set of the user $u$
    - $r_{u, i}$ is the actual score give by user $u$ to item $i$
    - $\hat{r}_{u, i}$ is the predicted score give by user $u$ to item $i$

\hfill\break

And it is calculated as such for the entire system:

    \[
    MAE_{sys} = \sum_{u \in T} \frac{MAE_u}{|T|}
    \]

    Where:

    - $T$ is the test set
    - $MAE_u$ is the MAE calculated for user $u$

\hfill\break

There may be cases in which some items of the *test set* of the user could not be predicted (eg. A CBRS was chosen
and items were not present locally, a methodology different from *TestRatings* was chosen).
In those cases, the $MAE_u$ formula becomes

    \[
    MAE_u = \sum_{i \in T_u} \frac{|r_{u,i} - \hat{r}_{u,i}|}{|T_u| - unk}
    \]

    Where:

    - $unk$ (unknown) is the number of items of the user test set that could not be predicted

    If no items of the user test set have been predicted ($|T_u| - unk = 0$), then:

    \[
    MAE_u = NaN
    \]

\hfill\break

% ------------------------- error metrics ended -------------------------------------------


% ------------------------ RANKING METRICS STARTED ----------------------------------------

% Normalized Discounted Cumulative Gain REPORT
\BLOCK{elif my_dict['metrics']['NDCG'] is defined}
The NDCG abbreviation of Normalized Discounted Cumulative Gain metric is calculated for the \textbf{single user}
by first computing the DCG score using the following formula:

    \[
    DCG_{u}(scores_{u}) = \sum_{r\in scores_{u}}{\frac{f(r)}{log_x(2 + i)}}
    \]

    Where:

    - $scores_{u}$ are the ground truth scores for predicted items, ordered according to the order of said items in the
        ranking for the user $u$
    - $f$ is a gain function (linear or exponential, in particular)
    - $x$ is the base of the logarithm
    - $i$ is the index of the truth score $r$ in the list of scores $scores_{u}$

\hfill\break

If $f$ is "linear", then the truth score $r$ is returned as is. Otherwise, in the "exponential" case, the following
formula is applied to $r$:

    \[
    f(r) = 2^{r} - 1
    \]

\hfill\break

The NDCG for a single user is then calculated using the following formula:

    \[
    NDCG_u(scores_{u}) = \frac{DCG_{u}(scores_{u})}{IDCG_{u}(scores_{u})}
    \]

    Where:

    - $IDCG_{u}$ is the DCG of the ideal ranking for the truth scores

\hfill\break

So the basic idea is to compare the actual ranking with the ideal one.
Finally, the NDCG of the entire system is calculated instead as such:

    \[
    NDCG_{sys} = \frac{\sum_{u} NDCG_u}{|U|}
    \]

    Where:

    - $NDCG_u$ is the NDCG calculated for user :math:`u`
    - $U$ is the set of all users

\hfill\break

The system average excludes NaN values.



% NDCG@K REPORT
\BLOCK{elif my_dict['metrics']['NDCGAtK'] is defined}
The NDCG@K abbreviation of Normalized Discounted Cumulative Gain at K) metric is calculated for the \textbf{single user}
by using the [framework implementation of the NDCG][clayrs.evaluation.NDCG] but considering $scores_{u}$ cut at the
first $k$ predictions.

The K used for the experiment is \VAR{my_dict['metrics']['MRRAtK']['k']|safe_text}.


% MRR REPORT
\BLOCK{elif my_dict['metrics']['MRR'] is defined}
The MRR abbreviation of Mean Reciprocal Rank metric is a system-wide metric, so only its result it will be returned and not those
of every user. MRR is calculated as such:

    \[
    MRR_{sys} = \frac{1}{|Q|}\cdot\sum_{i=1}^{|Q|}\frac{1}{rank(i)}
    \]

    Where:

    - $Q$ is the set of recommendation lists
    - $rank(i)$ is the position of the first relevant item in the i-th recommendation list

\hfill\break

The MRR metric needs to discern relevant items from the not relevant ones: in order to do that, one could pass a
custom 'relevant_threshold' parameter that will be applied to every user, so that if a rating of an item
is >= relevant_threshold, then it's relevant, otherwise it's not.
If no 'relevant_threshold' parameter is passed then, for every user, its mean rating score will be used

In this experiment the relevant threshod used is
\VAR{my_dict['metrics']['MRR']['relevant_threshold']|default('no threshold has been setted')|safe_text}.



\BLOCK{elif my_dict['metrics']['MRRAtK'] is defined}
The MRR@K (Mean Reciprocal Rank at K) metric is a system-wide metric, so only its result will be returned and
not those of every user. MRR@K is calculated as such

    \[
    MRR@K_{sys} = \frac{1}{|Q|}\cdot\sum_{i=1}^{K}\frac{1}{rank(i)}
    \]

    Where:

    - $K$ is the cutoff parameter
    - $Q$ is the set of recommendation lists
    - $rank(i)$ is the position of the first relevant item in the i-th recommendation list

\hfill\break
In this experiment, the relevant threshold used is
\VAR{my_dict['metrics']['MRRAtK']['relevant_threshold']|default('no threshold has been setted')|safe_text}.


\BLOCK{elif my_dict['metrics']['Precision'] is defined}

\BLOCK{elif my_dict['metrics']['Precision'] is defined}

\BLOCK{elif my_dict['metrics']['Precision'] is defined}

\BLOCK{elif my_dict['metrics']['Precision'] is defined}

\BLOCK{elif my_dict['metrics']['Precision'] is defined}

\BLOCK{elif my_dict['metrics']['Precision'] is defined}

\BLOCK{elif my_dict['metrics']['Precision'] is defined}

\BLOCK{elif my_dict['metrics']['Precision'] is defined}

\BLOCK{elif my_dict['metrics']['Precision'] is defined}
\BLOCK{else}

\BLOCK{endif}

###