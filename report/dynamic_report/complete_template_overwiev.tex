%! Author = DIEGO
%! Date = 21/12/2023

% Preamble
\documentclass[11pt]{article}

% Packages
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{graphicx}

\title{REPORT EXPERIMENT CLAYRS FRAMEWORK}
\author{SWAP research group UniBa}
%\date{27 December 2023}

% ------------ DOCUMENT: REPORT ON THE EXPERIMENT WITH CLAYRS FRAMEWORK STARTED -------------------------
\begin{document}

\maketitle
This \LaTeX{} document was generated automatically from yaml files for the purpose of replicability of experiments done with
\href{https://github.com/swapUniba/ClayRS}{ClayRS},
it contains information about the experiment that has been conducted and the results obtained.
The report is divided in 3 principal section dedicated each one for the 3 principal module of the ClayRS framework
and a conclusion section to highlights what have been achieved from the experiment.
\hfill\break
\hfill\break



\BLOCK{if my_dict['source_file'] is defined}
% ---------OPENING CONTENT ANALYZER SECTION ----------------
\section{CONTENT ANALYZER MODULE}\label{sec:ca}
The content analyzer module will deal with raw source document or more in general data which could be
video or audio data and give a rappresentation of these data which will be used by the other two module.
The text data source could be rappresented with exogenous technique or with a specified rappresentation
and each field given could be treated with preprocessing techiniques and postprocessing technique. In
this experiment the following techinques have been used on specific field in order to achieve the
rappresentation wanted:
\hfill\break
\hfill\break



% ------ TECNIQUE USED TO REPPRESENT DATA FIELD ------------

% field: X reppresentation subsection
X has been represented with the following techniques:
\hfill\break
\hfill\break


\BLOCK{if 'X' in my_dict['field_representations'] and 'OriginalData' in my_dict['field_representations']['X']}
% check if non particular technique of data reppresentation has been used
The data has been represented as the original.
\hfill\break
\hfill\break


\BLOCK{elif 'X' in my_dict['field_representations'] and 'WhooshTfIdf' in my_dict['field_representations']['X']}
% check WhooshTfIdf used as representation
The WhooshTfIdf technique has been used.
\hfill\break
\hfill\break


\BLOCK{elif 'X' in my_dict['field_representations'] and 'SkLearnTfIdf' in my_dict['field_representations']['X'] }
% check SkLearnTfIdf used as representation
The SkLearnTfIdf technique has been used.
The technique has been settled with the following parameter:
\begin{itemize}
    \item max df: \VAR{my_dict['field_representations']['X']['SkLearnTfIdf']['max_df']|safe_text}
    \item min df: \VAR{my_dict['field_representations']['X']['SkLearnTfIdf']['min_df']|safe_text}
    \item max features: \VAR{my_dict['field_representations']['X']['SkLearnTfIdf']['max_features']|safe_text}
    \item vocabulary: \VAR{my_dict['field_representations']['X']['SkLearnTfIdf']['vocabulary']|safe_text}
    \item binary: \VAR{my_dict['field_representations']['X']['SkLearnTfIdf']['binary']|safe_text}
    \item norm: \VAR{my_dict['field_representations']['X']['SkLearnTfIdf']['norm']|safe_text}
    \item use idf: \VAR{my_dict['field_representations']['X']['SkLearnTfIdf']['use_idf']|safe_text}
    \item smooth idf: \VAR{my_dict['field_representations']['X']['SkLearnTfIdf']['smooth_idf']|safe_text}
    \item sublinear tf: \VAR{my_dict['field_representations']['X']['SkLearnTfIdf']['sublinear_tf']|safe_text}
\end{itemize}
\hfill\break
\hfill\break


\BLOCK{ elif 'X' in my_dict['field_representations'] and 'WordEmbeddingTechnique' in my_dict['field_representations']['X'] }
% check WordEmbeddingTechnique used as representation
The Word Embedding Technique has been used and
\BLOCK{ set emb_source = my_dict.get('field_representations', {}).get('X', {}).get('WordEmbeddingTechnique', {}).get('embedding_source', None) }
\BLOCK{ if emb_source is not none }
the embedding source used is
\VAR{my_dict['field_representations']['X']['WordEmbeddingTechnique']['embedding_source']|safe_text}
\BLOCK{else}
no embedding source has been specified.
\BLOCK{endif}
\hfill\break
\hfill\break


\BLOCK{elif 'X' in my_dict['field_representations'] and 'SentenceEmbeddingTechnique' in my_dict['field_representations']['X'] }
% check SentenceEmbeddingTechnique used as representation
The Sentence Embedding Technique has been used and
\BLOCK{ set emb_src = my_dict.get('field_representations', {}).get('X', {}).get('SentenceEmbeddingTechnique', {}).get('embedding_source', None) }
\BLOCK{ if emb_src is not none }
the embedding source used is
\VAR{my_dict['field_representations']['X']['SentenceEmbeddingTechnique']['embedding_source']|safe_text}
\BLOCK{else}
no embedding source has been specified.
\BLOCK{endif}
\hfill\break
\hfill\break


\BLOCK{elif 'X' in my_dict['field_representations'] and 'DocumentEmbeddingTechnique' in my_dict['field_representations']['X'] }
% check DocumentEmbeddingTechnique used as representation
The Document Embedding Technique has been used and
\BLOCK{ set emb_src1 = my_dict.get('field_representations', {}).get('X', {}).get('DocumentEmbeddingTechnique', {}).get('embedding_source', None) }
\BLOCK{ if emb_src1 is not none }
the embedding source used is
\VAR{my_dict['field_representations']['X']['DocumentEmbeddingTechnique']['embedding_source']|safe_text}
\BLOCK{else}
no embedding source has been specified.
\BLOCK{endif}
\hfill\break
\hfill\break


\BLOCK{elif 'X' in my_dict['field_representations'] and 'Word2SentenceEmbedding' in my_dict['field_representations']['X'] }
% check Word2SentenceEmbeddingused as representation
The Word2Sentence Embedding Technique has been used and
\BLOCK{ set emb_src2 = my_dict.get('field_representations', {}).get('X', {}).get('Word2SentenceEmbedding', {}).get('embedding_source', None) }
\BLOCK{ if emb_src2 is not none }
the embedding source used is
\VAR{my_dict['field_representations']['X']['Word2SentenceEmbedding']['embedding_source']|safe_text}.
\BLOCK{else}
no embedding source has been specified.
\BLOCK{endif}
\BLOCK{ set ct = my_dict.get('field_representations', {}).get('X', {}).get('Word2SentenceEmbedding', {}).get('combining_technique', None) }
\BLOCK{ if ct is not none }
Combining technique used is
\VAR{my_dict['field_representations']['X']['Word2SentenceEmbedding']['combining_technique']|safe_text}.
\BLOCK{else}
No combining technique has been used.
\BLOCK{endif}
\hfill\break
\hfill\break


\BLOCK{elif 'X' in my_dict['field_representations'] and 'Word2DocEmbedding' in my_dict['field_representations']['X'] }
% check Word2DocEmbedding used as representation
The Word2Doc Embedding Technique has been used and
\BLOCK{ set emb_src3 = my_dict.get('field_representations', {}).get('X', {}).get('Word2DocEmbedding', {}).get('embedding_source', None) }
\BLOCK{ if emb_src3 is not none }
the embedding source used is
\VAR{my_dict['field_representations']['X']['Word2DocEmbedding']['embedding_source']|safe_text}.
\BLOCK{else}
no embedding source has been specified.
\BLOCK{endif}
\BLOCK{ set ct1 = my_dict.get('field_representations', {}).get('X', {}).get('Word2DocEmbedding', {}).get('combining_technique', None) }
\BLOCK{ if ct1 is not none }
Combining technique used is
\VAR{my_dict['field_representations']['X']['Word2DocEmbedding']['combining_technique']|safe_text}.
\BLOCK{else}
No combining technique has been used.
\BLOCK{endif}
\hfill\break
\hfill\break


\BLOCK{elif 'X' in my_dict['field_representations'] and 'Sentence2DocEmbedding' in my_dict['field_representations']['X'] }
% check Sentence2DocEmbedding used as representation
The Sentence2Doc Embedding Technique has been used and
\BLOCK{ set emb_src4 = my_dict.get('field_representations', {}).get('X', {}).get('Sentence2DocEmbedding', {}).get('embedding_source', None) }
\BLOCK{ if emb_src4 is not none }
the embedding source used is
\VAR{my_dict['field_representations']['X']['Sentence2DocEmbedding']['embedding_source']|safe_text}.
\BLOCK{else}
no embedding source has been specified.
\BLOCK{endif}
\BLOCK{ set ct2 = my_dict.get('field_representations', {}).get('X', {}).get('Sentence2DocEmbedding', {}).get('combining_technique', None) }
\BLOCK{ if ct2 is not none }
Combining technique used is
\VAR{my_dict['field_representations']['X']['Sentence2DocEmbedding']['combining_technique']|safe_text}.
\BLOCK{else}
No combining technique has been used.
\BLOCK{endif}
\hfill\break
\hfill\break


\BLOCK{elif 'X' in my_dict['field_representations'] and 'PyWSDSynsetDocumentFrequency' in my_dict['field_representations']['X'] }
% check PyWSDSynsetDocumentFrequency used as representation
The PyWSD Synset Document Frequency Technique has been used.
\hfill\break
\hfill\break


\BLOCK{elif 'X' in my_dict['field_representations'] and 'FromNPY' in my_dict['field_representations']['X'] }
% check FromNPY used as representation
The FromNPY Technique has been used.
\hfill\break
\hfill\break


\BLOCK{elif 'X' in my_dict['field_representations'] and 'SkImageHogDescriptor' in my_dict['field_representations']['X'] }
% check SkImageHogDescriptorused as representation
SKImage Hog Descriptor technique has been applied to process data with the following settings:
\begin{itemize}
    \item orientations: \VAR{my_dict['field_representations']['X']['SkImageHogDescriptor']['orientations']|default('no setting used')|safe_text}
    \item pixels per cell:
    \item cells per block:
    \item block norm: \VAR{my_dict['field_representations']['X']['SkImageHogDescriptor']['block_norm']|default('no setting used')|safe_text}
    \item transform sqrt: \VAR{my_dict['field_representations']['X']['SkImageHogDescriptor']['transform_sqrt']|default('no setting used')|safe_text}
    \item flatten: \VAR{my_dict['field_representations']['X']['SkImageHogDescriptor']['flatten']|default('no setting used')|safe_text}
    \item contents dirs: \VAR{my_dict['field_representations']['X']['SkImageHogDescriptor']['contents_dirs']|default('no setting used')|safe_text}
    \item time tuple:
    \item max timeout: \VAR{my_dict['field_representations']['X']['SkImageHogDescriptor']['max_timeout']|default('no setting used')|safe_text}
    \item max retries: \VAR{my_dict['field_representations']['X']['SkImageHogDescriptor']['max_retries']|default('no setting used')|safe_text}
    \item max workers: \VAR{my_dict['field_representations']['X']['SkImageHogDescriptor']['max_workers']|default('no setting used')|safe_text}
\end{itemize}
\hfill\break
\hfill\break


\BLOCK{elif 'X' in my_dict['field_representations'] and 'MFCC' in my_dict['field_representations']['X'] }
% check MFCC used as representation
MFCC technique has been applied to process data with the following settings:
\begin{itemize}
    \item contents dirs: \VAR{my_dict['field_representations']['X']['MFCC']['contents_dirs']|safe_text}
    \item time tuple:
    \item flatten: \VAR{my_dict['field_representations']['X']['MFCC']['flatten']|default('no setting used')|safe_text}
    \item mean: \VAR{my_dict['field_representations']['X']['MFCC']['mean']|default('no setting used')|safe_text}
    \item max timeout: \VAR{my_dict['field_representations']['X']['MFCC']['max_timeout']|default('no setting used')|safe_text}
    \item max retries: \VAR{my_dict['field_representations']['X']['MFCC']['max_retries']|default('no setting used')|safe_text}
    \item max workers: \VAR{my_dict['field_representations']['X']['MFCC']['max_workers']|default('no setting used')|safe_text}
    \item n mfcc: \VAR{my_dict['field_representations']['X']['MFCC']['n_mfcc']|default('no setting used')|safe_text}
    \item dct type: \VAR{my_dict['field_representations']['X']['MFCC']['dct_type']|default('no setting used')|safe_text}
    \item norm: \VAR{my_dict['field_representations']['X']['MFCC']['norm']|default('no setting used')|safe_text}
    \item log mels: \VAR{my_dict['field_representations']['X']['MFCC']['log_mels']|default('no setting used')|safe_text}
    \item melkwargs: \VAR{my_dict['field_representations']['X']['MFCC']['melkwargs']|default('no setting used')|safe_text}
\end{itemize}
\hfill\break
\hfill\break


\BLOCK{elif 'X' in my_dict['field_representations'] and 'VGGISH' in my_dict['field_representations']['X'] }
% check VGGISH used as representation
VGGISH technique has been applied to process data with the following settings:
\begin{itemize}
    \item feature layer: \VAR{my_dict['field_representations']['X']['VGGISH']['feature_layer']|default('no setting used')|safe_text}
    \item flatten: \VAR{my_dict['field_representations']['X']['VGGISH']['flatten']|default('no setting used')|safe_text}
    \item device:  \VAR{my_dict['field_representations']['X']['VGGISH']['device']|default('no setting used')|safe_text}
    \item apply on output: \VAR{my_dict['field_representations']['X']['VGGISH']['apply_on_output']|default('no setting used')|safe_text}
    \item contents dirs: \VAR{my_dict['field_representations']['X']['VGGISH']['contents_dirs']|default('no setting used')|safe_text}
    \item time tuple:
    \item max timeout: \VAR{my_dict['field_representations']['X']['VGGISH']['max_timeout']|default('no setting used')|safe_text}
    \item max retries: \VAR{my_dict['field_representations']['X']['VGGISH']['max_retries']|default('no setting used')|safe_text}
    \item max workers: \VAR{my_dict['field_representations']['X']['VGGISH']['max_workers']|default('no setting used')|safe_text}
    \item batch size:  \VAR{my_dict['field_representations']['X']['VGGISH']['batch_size']|default('no setting used')|safe_text}
\end{itemize}
\hfill\break
\hfill\break


\BLOCK{elif 'X' in my_dict['field_representations'] and 'PytorchImageModels' in my_dict['field_representations']['X'] }
% check PytorchImageModels used as representation
Pytorch Image Models technique has been applied to process data with the following settings:
\begin{itemize}
    \item model name: \VAR{my_dict['field_representations']['X']['PytorchImageModels']['model_name']|default('no setting used')|safe_text}
    \item feature layer: \VAR{my_dict['field_representations']['X']['PytorchImageModels']['feature_layer']|default('no setting used')|safe_text}
    \item flatten: \VAR{my_dict['field_representations']['X']['PytorchImageModels']['flatten']|default('no setting used')|safe_text}
    \item device:  \VAR{my_dict['field_representations']['X']['PytorchImageModels']['device']|default('no setting used')|safe_text}
    \item apply on output: \VAR{my_dict['field_representations']['X']['PytorchImageModels']['apply_on_output']|default('no setting used')|safe_text}
    \item contents dirs: \VAR{my_dict['field_representations']['X']['PytorchImageModels']['contents_dirs']|default('no setting used')|safe_text}
    \item time tuple:
    \item custom weights path: \VAR{my_dict['field_representations']['X']['PytorchImageModels']['custom_weights_path']|default('no setting used')|safe_text}
    \item use default transforms: \VAR{my_dict['field_representations']['X']['PytorchImageModels']['use_default_transforms']|default('no setting used')|safe_text}
    \item num classes: \VAR{my_dict['field_representations']['X']['PytorchImageModels']['num_classes']|default('no setting used')|safe_text}
    \item max timeout: \VAR{my_dict['field_representations']['X']['PytorchImageModels']['max_timeout']|default('no setting used')|safe_text}
    \item max retries: \VAR{my_dict['field_representations']['X']['PytorchImageModels']['max_retries']|default('no setting used')|safe_text}
    \item max workers: \VAR{my_dict['field_representations']['X']['PytorchImageModels']['max_workers']|default('no setting used')|safe_text}
    \item batch size:  \VAR{my_dict['field_representations']['X']['PytorchImageModels']['batch_size']|default('no setting used')|safe_text}
\end{itemize}
\hfill\break
\hfill\break


\BLOCK{elif  'X' in my_dict['field_representations'] and 'TorchVisionVideoModels' in my_dict['field_representations']['X']}
% check TorchVisionVideoModels used as representation
Torch Vision Video Models technique has been applied to process data with the following settings:
\begin{itemize}
    \item model name: \VAR{my_dict['field_representations']['X']['TorchVisionVideoModels']['model_name']|default('no setting used')|safe_text}
    \item feature layer: \VAR{my_dict['field_representations']['X']['TorchVisionVideoModels']['feature_layer']|default('no setting used')|safe_text}
    \item weights: \VAR{my_dict['field_representations']['X']['TorchVisionVideoModels']['weights']|default('no setting used')|safe_text}
    \item flatten: \VAR{my_dict['field_representations']['X']['TorchVisionVideoModels']['flatten']|default('no setting used')|safe_text}
    \item device:  \VAR{my_dict['field_representations']['X']['TorchVisionVideoModels']['device']|default('no setting used')|safe_text}
    \item apply on output: \VAR{my_dict['field_representations']['X']['TorchVisionVideoModels']['apply_on_output']|default('no setting used')|safe_text}
    \item contents dirs: \VAR{my_dict['field_representations']['X']['TorchVisionVideoModels']['contents_dirs']|default('no setting used')|safe_text}
    \item time tuple:
    \item max timeout: \VAR{my_dict['field_representations']['X']['TorchVisionVideoModels']['max_timeout']|default('no setting used')|safe_text}
    \item max retries: \VAR{my_dict['field_representations']['X']['TorchVisionVideoModels']['max_retries']|default('no setting used')|safe_text}
    \item max workers: \VAR{my_dict['field_representations']['X']['TorchVisionVideoModels']['max_workers']|default('no setting used')|safe_text}
    \item batch size:  \VAR{my_dict['field_representations']['X']['TorchVisionVideoModels']['batch_size']|default('no setting used')|safe_text}
    \item mini batch size: \VAR{my_dict['field_representations']['X']['TorchVisionVideoModels']['mini_batch_size']|default('no setting used')|safe_text}
\end{itemize}
\hfill\break
\hfill\break

\BLOCK{else}
The representation technique used has not been recognised from the framework check statement flow control
for chunks template used for rendering by jinja.
\hfill\break
\hfill\break
\BLOCK{endif}



% ------ PREPROCESSING OF DATA FIELD ------------

\BLOCK{if 'X' in my_dict['field_representations'] and 'Spacy' in my_dict['field_representations']['X']['preprocessing'] }
% Spacy
Spacy has been used to preprocess data with the following settings:
\begin{itemize}
    \item model: \VAR{my_dict['field_representations']['X']['preprocessing']['Spacy']['model']|default('no setting used')|safe_text}
    \item strip multiple whitespace: \VAR{my_dict['field_representations']['X']['preprocessing']['Spacy']['strip_multiple_whitespace']|default('no setting used')|safe_text}
    \item remove punctuation:\VAR{my_dict['field_representations']['X']['preprocessing']['Spacy']['remove_punctuation']|default('no setting used')|safe_text}
    \item stopwords removal: \VAR{my_dict['field_representations']['X']['preprocessing']['Spacy']['stopwords_removal']|default('no setting used')|safe_text}
    \item new stopwords: \VAR{my_dict['field_representations']['X']['preprocessing']['Spacy']['new_stopwords']|default('no setting used')|safe_text}
    \item not stopwords: \VAR{my_dict['field_representations']['X']['preprocessing']['Spacy']['not_stopwords']|default('no setting used')|safe_text}
    \item lemmatization: \VAR{my_dict['field_representations']['X']['preprocessing']['Spacy']['lemmatization']|default('no setting used')|safe_text}
    \item url tagging: \VAR{my_dict['field_representations']['X']['preprocessing']['Spacy']['url_tagging']|default('no setting used')|safe_text}
    \item named entity recognition: \VAR{my_dict['field_representations']['X']['preprocessing']['Spacy']['named_entity_recognition']|default('no setting used')|safe_text}
\end{itemize}
\hfill\break
\hfill\break


\BLOCK{ elif 'X' in my_dict['field_representations'] and 'Ekphrasis' in my_dict['field_representations']['X']['preprocessing'] }
% Ekphrasis
Ekphrasis has been used to preprocess data with the following settings:
\begin{itemize}
    \item omit: \VAR{my_dict['field_representations']['X']['preprocessing']['Ekphrasis']['omit']|default('no setting used')|safe_text}
    \item normalize: \VAR{my_dict['field_representations']['X']['preprocessing']['Ekphrasis']['normalize']|default('no setting used')|safe_text}
    \item unpack contractions: \VAR{my_dict['field_representations']['X']['preprocessing']['Ekphrasis']['unpack_contractions']|default('no setting used')|safe_text}
    \item unpack hashtags: \VAR{my_dict['field_representations']['X']['preprocessing']['Ekphrasis']['unpack_hashtags']|default('no setting used')|safe_text}
    \item annotate: \VAR{my_dict['field_representations']['X']['preprocessing']['Ekphrasis']['annotate']|default('no setting used')|safe_text}
    \item corrector: \VAR{my_dict['field_representations']['X']['preprocessing']['Ekphrasis']['corrector']|default('no setting used')|safe_text}
    \item tokenizer:
    \item segmenter: \VAR{my_dict['field_representations']['X']['preprocessing']['Ekphrasis']['segmenter']|default('no setting used')|safe_text}
    \item all caps tag: \VAR{my_dict['field_representations']['X']['preprocessing']['Ekphrasis']['all_caps_tag']|default('no setting used')|safe_text}
    \item spell correction: \VAR{my_dict['field_representations']['X']['preprocessing']['Ekphrasis']['spell_correction']|default('no setting used')|safe_text}
    \item segmentation: \VAR{my_dict['field_representations']['X']['preprocessing']['Ekphrasis']['segmentation']|default('no setting used')|safe_text}
    \item dicts: \VAR{my_dict['field_representations']['X']['preprocessing']['Ekphrasis']['dicts']|default('no setting used')|safe_text}
    \item spell correct elong: \VAR{my_dict['field_representations']['X']['preprocessing']['Ekphrasis']['spell_correct_elong']|default('no setting used')|safe_text}
\end{itemize}
\hfill\break
\hfill\break


\BLOCK{ elif 'X' in my_dict['field_representations'] and 'NLTK' in my_dict['field_representations']['X']['preprocessing'] }
% NLTK preprocessing
NLTK has been used to preprocess data with the following settings:
\begin{itemize}
    \item strip multiple whitespace: \VAR{my_dict['field_representations']['X']['preprocessing']['NLTK']['strip_multiple_whitespace']|default('no setting used')|safe_text}
    \item remove punctuation: \VAR{my_dict['field_representations']['X']['preprocessing']['NLTK']['remove_punctuation']|default('no setting used')|safe_text}
    \item stopwords removal: \VAR{my_dict['field_representations']['X']['preprocessing']['NLTK']['stopwords_removal']|default('no setting used')|safe_text}
    \item url tagging: \VAR{my_dict['field_representations']['X']['preprocessing']['NLTK']['url_tagging']|default('no setting used')|safe_text}
    \item lemmatization: \VAR{my_dict['field_representations']['X']['preprocessing']['NLTK']['lemmatization']|default('no setting used')|safe_text}
    \item stemming: \VAR{my_dict['field_representations']['X']['preprocessing']['NLTK']['stemming']|default('no setting used')|safe_text}
    \item pos tag: \VAR{my_dict['field_representations']['X']['preprocessing']['NLTK']['pos_tag']|default('no setting used')|safe_text}
    \item lang: \VAR{my_dict['field_representations']['X']['preprocessing']['NLTK']['lang']|default('no setting used')|safe_text}
\end{itemize}
\hfill\break
\hfill\break


\BLOCK{ elif 'X' in my_dict['field_representations'] and 'TorchUniformTemporalSubSampler' in my_dict['field_representations']['X']['preprocessing'] }
% TorchUniformTemporalSubSampler
TorchUniformTemporalSubSampler has been used to preprocess data with the following settings:
\begin{itemize}
    \item number samples: \VAR{my_dict['field_representations']['X']['preprocessing']['TorchUniformTemporalSubSampler']['num_samples']|default('no setting used')|safe_text}
\end{itemize}
\hfill\break
\hfill\break


\BLOCK{ elif 'X' in my_dict['field_representations'] and 'TorchResample' in my_dict['field_representations']['X']['preprocessing'] }
% TorchResample preproceessing
TorchResample has been used to preprocess data with the following settings:
\begin{itemize}
    \item new freq: \VAR{my_dict['field_representations']['X']['preprocessing']['TorchResample']['new_freq']|default('no setting used')|safe_text}
    \item resampling method: \VAR{my_dict['field_representations']['X']['preprocessing']['TorchResample']['resampling_method']|default('no setting used')|safe_text}
    \item lowpass filter width: \VAR{my_dict['field_representations']['X']['preprocessing']['TorchResample']['lowpass_filter_width']|default('no setting used')|safe_text}
    \item roll off: \VAR{my_dict['field_representations']['X']['preprocessing']['TorchResample']['rolloff']|default('no setting used')|safe_text}
    \item beta: \VAR{my_dict['field_representations']['X']['preprocessing']['TorchResample']['beta']|default('no setting used')|safe_text}
\end{itemize}
\hfill\break
\hfill\break


\BLOCK{ elif 'X' in my_dict['field_representations'] and 'ConvertToMono' in my_dict['field_representations']['X']['preprocessing'] }
% ConvertToMono preprocessing
ConvertToMono has been used to preprocess data with the following settings:
\hfill\break
\hfill\break


\BLOCK{ elif 'X' in my_dict['field_representations'] and 'Resize' in my_dict['field_representations']['X']['preprocessing'] }
% Resize preprocessing
Resize has been used to preprocess data with the following settings:
\begin{itemize}
    \item size:
    \item interpolation: \VAR{my_dict['field_representations']['X']['preprocessing']['Resize']['interpolation']|default('no setting used')|safe_text}
    \item max size: \VAR{my_dict['field_representations']['X']['preprocessing']['Resize']['max_size']|default('no setting used')|safe_text}
    \item antialias: \VAR{my_dict['field_representations']['X']['preprocessing']['Resize']['antialias']|default('no setting used')|safe_text}
\end{itemize}
\hfill\break
\hfill\break


\BLOCK{ elif 'X' in my_dict['field_representations'] and 'CenterCrop' in my_dict['field_representations']['X']['preprocessing'] }
% CenterCrop preprocessing
CenterCrop has been used to preprocess data with the following settings:
\begin{itemize}
    \item size:
\end{itemize}
\hfill\break
\hfill\break


\BLOCK{ elif 'X' in my_dict['field_representations'] and 'Lambda' in my_dict['field_representations']['X']['preprocessing'] }
% Lambda preprocessing
Lambda has been used to preprocess data with the following settings:
\hfill\break
\hfill\break


\BLOCK{ elif 'X' in my_dict['field_representations'] and 'Normalize' in my_dict['field_representations']['X']['preprocessing'] }
% Normalize preprocessing
Normalize has been used to preprocess data with the following settings:
\begin{itemize}
    \item mean: % understand how to deal with list as value of a key in jinja --> solved use index to gain access
    \item std:
\end{itemize}
\hfill\break
\hfill\break


\BLOCK{ elif 'X' in my_dict['field_representations'] and 'ClipSampler' in my_dict['field_representations']['X']['preprocessing'] }
% ClipSampler preprocessing
ClipSampler has been used to preprocess data with the following settings:
\begin{itemize}
    \item number of frames for clip: \VAR{my_dict['field_representations']['X']['preprocessing']['ClipSampler']['num_frames_for_clip']|default('no setting used')|safe_text}
    \item number of clips: \VAR{my_dict['field_representations']['X']['preprocessing']['ClipSampler']['num_clips']|default('no setting used')|safe_text}
    \item selection strategy: \VAR{my_dict['field_representations']['X']['preprocessing']['ClipSampler']['selection_strategy']|default('no setting used')|safe_text}
\end{itemize}
\hfill\break
\hfill\break

\BLOCK{else}
No preprocessing techniques have been used to preprocess data X during the experiment.
\hfill\break
\hfill\break
\BLOCK{endif}


% ------------------- POSTPROCESSING OF DATA FIELD ------------


\BLOCK{ if in my_dict['field_representations'] and 'FVGMM' in my_dict['field_representations']['X']['postprocessing']['1'] }
% FVGMM posttprocessing
FVGMM has been used to preprocess data with the following settings:
\begin{itemize}
    \item number of components: \VAR{my_dict['field_representations']['X']['postprocessing']['1']['FVGMM']['n_components']|default('no setting used')|safe_text}
    \item covariance type: \VAR{my_dict['field_representations']['X']['postprocessing']['1']['FVGMM']['covariance_type']|default('no setting used')|safe_text}
    \item tol: \VAR{my_dict['field_representations']['X']['postprocessing']['1']['FVGMM']['tol']|default('no setting used')|safe_text}
    \item reg covar: \VAR{my_dict['field_representations']['X']['postprocessing']['1']['FVGMM']['reg_covar']|default('no setting used')|safe_text}
    \item max iter: \VAR{my_dict['field_representations']['X']['postprocessing']['1']['FVGMM']['max_iter']|default('no setting used')|safe_text}
    \item n init: \VAR{my_dict['field_representations']['X']['postprocessing']['1']['FVGMM']['n_init']|default('no setting used')|safe_text}
    \item init params: \VAR{my_dict['field_representations']['X']['postprocessing']['1']['FVGMM']['init_params']|default('no setting used')|safe_text}
    \item weights init: \VAR{my_dict['field_representations']['X']['postprocessing']['1']['FVGMM']['weights_init']|default('no setting used')|safe_text}
    \item means init: \VAR{my_dict['field_representations']['X']['postprocessing']['1']['FVGMM']['means_init']|default('no setting used')|safe_text}
    \item precisions init: \VAR{my_dict['field_representations']['X']['postprocessing']['1']['FVGMM']['precisions_init']|default('no setting used')|safe_text}
    \item random state: \VAR{my_dict['field_representations']['X']['postprocessing']['1']['FVGMM']['random_state']|default('no setting used')|safe_text}
    \item warm start: \VAR{my_dict['field_representations']['X']['postprocessing']['1']['FVGMM']['warm_start']|default('no setting used')|safe_text}
    \item verbose: \VAR{my_dict['field_representations']['X']['postprocessing']['1']['FVGMM']['verbose']|default('no setting used')|safe_text}
    \item verbose interval: \VAR{my_dict['field_representations']['X']['postprocessing']['1']['FVGMM']['verbose_interval']|default('no setting used')|safe_text}
    \item improved: \VAR{my_dict['field_representations']['X']['postprocessing']['1']['FVGMM']['improved']|default('no setting used')|safe_text}
    \item alpha: \VAR{my_dict['field_representations']['X']['postprocessing']['1']['FVGMM']['alpha']|default('no setting used')|safe_text}
    \item with mean: \VAR{my_dict['field_representations']['X']['postprocessing']['1']['FVGMM']['with_mean']|default('no setting used')|safe_text}
    \item with std: \VAR{my_dict['field_representations']['X']['postprocessing']['1']['FVGMM']['with_std']|default('no setting used')|safe_text}
\end{itemize}
\hfill\break
\hfill\break


\BLOCK{elif in my_dict['field_representations'] and 'VLADGMM' in my_dict['field_representations']['X']['postprocessing']['2']}
% VLADGMM postprocessing
VLADGMM has been used to preprocess data with the following settings:
\begin{itemize}
    \item number of components: \VAR{my_dict['field_representations']['X']['postprocessing']['2']['VLADGMM']['n_components']|default('no setting used')|safe_text}
    \item covariance type: \VAR{my_dict['field_representations']['X']['postprocessing']['2']['VLADGMM']['covariance_type']|default('no setting used')|safe_text}
    \item tol: \VAR{my_dict['field_representations']['X']['postprocessing']['2']['VLADGMM']['tol']|default('no setting used')|safe_text}
    \item reg covar: \VAR{my_dict['field_representations']['X']['postprocessing']['2']['VLADGMM']['reg_covar']|default('no setting used')|safe_text}
    \item max iter: \VAR{my_dict['field_representations']['X']['postprocessing']['2']['VLADGMM']['max_iter']|default('no setting used')|safe_text}
    \item n init: \VAR{my_dict['field_representations']['X']['postprocessing']['2']['VLADGMM']['n_init']|default('no setting used')|safe_text}
    \item init params: \VAR{my_dict['field_representations']['X']['postprocessing']['2']['VLADGMM']['init_params']|default('no setting used')|safe_text}
    \item weights init: \VAR{my_dict['field_representations']['X']['postprocessing']['2']['VLADGMM']['weights_init']|default('no setting used')|safe_text}
    \item means init: \VAR{my_dict['field_representations']['X']['postprocessing']['2']['VLADGMM']['means_init']|default('no setting used')|safe_text}
    \item precisions init: \VAR{my_dict['field_representations']['X']['postprocessing']['2']['VLADGMM']['precisions_init']|default('no setting used')|safe_text}
    \item random state: \VAR{my_dict['field_representations']['X']['postprocessing']['2']['VLADGMM']['random_state']|default('no setting used')|safe_text}
    \item warm start: \VAR{my_dict['field_representations']['X']['postprocessing']['2']['VLADGMM']['warm_start']|default('no setting used')|safe_text}
    \item verbose: \VAR{my_dict['field_representations']['X']['postprocessing']['2']['VLADGMM']['verbose']|default('no setting used')|safe_text}
    \item verbose interval: \VAR{my_dict['field_representations']['X']['postprocessing']['2']['VLADGMM']['verbose_interval']|default('no setting used')|safe_text}
    \item improved: \VAR{my_dict['field_representations']['X']['postprocessing']['2']['VLADGMM']['improved']|default('no setting used')|safe_text}
    \item alpha: \VAR{my_dict['field_representations']['X']['postprocessing']['2']['VLADGMM']['alpha']|default('no setting used')|safe_text}
    \item with mean: \VAR{my_dict['field_representations']['X']['postprocessing']['2']['VLADGMM']['with_mean']|default('no setting used')|safe_text}
    \item with std: \VAR{my_dict['field_representations']['X']['postprocessing']['2']['VLADGMM']['with_std']|default('no setting used')|safe_text}
\end{itemize}
\hfill\break
\hfill\break


\BLOCK{elif in my_dict['field_representations'] and 'SkLearnPCA' in my_dict['field_representations']['X']['postprocessing']['3']}
% SkLearnPCA postprocessing
SkLearnPCA has been used to preprocess data with the following settings:
\begin{itemize}
    \item number of components: \VAR{my_dict['field_representations']['X']['postprocessing']['3']['SkLearnPCA']['n_components']|default('no setting used')|safe_text}
    \item copy: \VAR{my_dict['field_representations']['X']['postprocessing']['3']['SkLearnPCA']['copy']|default('no setting used')|safe_text}
    \item whiten: \VAR{my_dict['field_representations']['X']['postprocessing']['3']['SkLearnPCA']['whiten']|default('no setting used')|safe_text}
    \item svd solver: \VAR{my_dict['field_representations']['X']['postprocessing']['3']['SkLearnPCA']['svd_solver']|default('no setting used')|safe_text}
    \item tol: \VAR{my_dict['field_representations']['X']['postprocessing']['3']['SkLearnPCA']['tol']|default('no setting used')|safe_text}
    \item iterated power: \VAR{my_dict['field_representations']['X']['postprocessing']['3']['SkLearnPCA']['iterated_power']|default('no setting used')|safe_text}
    \item random state: \VAR{my_dict['field_representations']['X']['postprocessing']['3']['SkLearnPCA']['random_state']|default('no setting used')|safe_text}
\end{itemize}
\hfill\break
\hfill\break


\BLOCK{else}
No postprocessing techniques have been used on the data X in this experiment.
\hfill\break
\hfill\break

% -------------- ends post processing techniques ---------------------------------
\BLOCK{endif}

% EXOGENOUS TECHNIQUE REPRESENTATION OF DATA

This exogenous technique expands each content by using as external source the DBPedia ontology.
It needs the entity of the contents for which a mapping is required (e.g. entity\_type=`dbo:Film`)
and the field of the raw source that will be used for the actual mapping.
This technique can be performed with four different modalities as follows:
\begin{itemize}
 \item
       \textbf{mode='only\_retrieved\_evaluated'}, all properties from DBPedia will be retrieved but discarding the
        ones with a blank value.
 \item
       \textbf{mode='all'}, all properties in DBPedia più all properties in local raw source will be retrieved.
        Local properties will be overwritten by dbpedia values if there's a conflict.
 \item
       \textbf{mode='all\_retrieved'}, all properties in DBPedia *only* will be retrieved.
 \item
       \textbf{mode='original\_retrieved'}, all local properties with their DBPedia value will be retrieved.
\end{itemize}
\hfill\break
\hfill\break
In this experiment the DBpedia Mapping Technique has been used with the mode:
\VAR{my_dict['exogenous_representations']['DBPediaMappingTechnique']['mode']|safe_text}, the label_field_used is
\VAR{my_dict['exogenous_representations']['DBPediaMappingTechnique']['label_field']|safe_text} and the timeout used
to expand the content with dbpedia is set to
\VAR{my_dict['exogenous_representations']['DBPediaMappingTechnique']['max_timeout']|safe_text}.
\BLOCK{if my_dict['exogenous_representations']['DBPediaMappingTechnique']['prop_as_uri'] is defined and
my_dict['exogenous_representations']['DBPediaMappingTechnique']['prop_as_uri'] == true}
The properties have been returned in their full uri form.
\BLOCK{else}
The properties have been returned in their rdfs:label form.
\BLOCK{endif}
\hfill\break
\hfill\break


% In case of the content analyzer is not used
\BLOCK{else}
For this experiment the module of the content analyzer has not been used.
\hfill\break
\hfill\break
% closing the controll block for the content analyzer section
\BLOCK{endif}


% ------------------------------- START RECSYS MODULE ----------------------------------

\BLOCK{if 'interactions' in my_dict}
\section{Recommender System module: RecSys}\label{sec:recsys}
The \textbf{RecSys module} allows to instantiate a recommender system and make it work on items and users serialized
by the Content Analyzer module, despite this is also possible using other serialization made with other framework and
give them as input to the recommender system and obtain score prediction or recommend items for the active user(s).
In particular this module allows has to get some general statistics on the data used, the scheme used to split the data
and train the recommender system and the settings beloning to the algorithm chosen.
\hfill\break
\hfill\break

\subsection{Statistics on data used}\label{subsec:stats}
% --- DATA STATS ---
In this experiment the statistics of the dataset used are reported in the following table:~\ref{tab:dataset_table}:
\begin{table}[ht]
    \centering
  \begin{tabular}{|c|c|}
    \hline
    \textbf{Parameter}& \textbf{Value} \\ \hline
    n\_users  & \VAR{my_dict['interactions']['n_users']|default('no users')|safe_text}\\ \hline
    n\_items  & \VAR{my_dict['interactions']['n_items']|default('no items')|safe_text}\\ \hline
    total\_interactions  & \VAR{my_dict['interactions']['total_interactions']|safe_text}\\ \hline
    min\_score  & \VAR{my_dict['interactions']['min_score']|safe_text}\\ \hline
    max\_score  & \VAR{my_dict['interactions']['max_score']|safe_text}\\ \hline
    mean\_score  & \VAR{my_dict['interactions']['mean_score']|safe_text}\\ \hline
    sparsity  & \VAR{my_dict['interactions']['sparsity']|truncate|safe_text}\\ \hline
  \end{tabular}
   \caption{Table of the Interactions}\label{tab:dataset_table}
\end{table}
\hfill\break
\hfill\break


% --------------- START SUBSECTION OF PARTITIONING OF RECSYS ------------------------------------

\subsection{Partitioning techinque used}\label{subsec:partitioning}
\BLOCK{if  my_dict['partitioning']['KFoldPartitioning'] is defined}
% KFOLD PARTITIONING TECNIQUE
K-fold cross-validation is a technique used in machine learning to assess
the performance of a predictive model. The basic idea is to divide the dataset
into K subsets, or folds. The model is then trained K times, each time using K-1
folds for training and the remaining fold for validation. This process is
repeated K times, with a different fold used as the validation set in each iteration
\hfill\break
\hfill\break
The KFoldPartitioning has been used with the following setting:
\hfill\break
\hfill\break
The train set size of this experiment is the \VAR{my_dict['partitioning']['KFoldPartitioning']['train_set_size'] * 100}\%
of the original dataset, while the test set is the remaining \VAR{(100 - (my_dict['partitioning']['KFoldPartitioning']['train_set_size'] * 100))}\%.
\hfill\break
\hfill\break
\BLOCK{if my_dict.get('partitioning', {}).get('KFoldPartitioning', {}).get('shuffle') == True}
The data has been shuffled before being split into batches.
The partitioning technique has been executed with the following settings:
\begin{itemize}
    \item number of splits: \VAR{my_dict['partitioning']['KFoldPartitioning']['n_splits']}
    \item shuffle: \VAR{my_dict['partitioning']['KFoldPartitioning']['shuffle']}
    \item random state: \VAR{my_dict['partitioning']['KFoldPartitioning']['random_state']|default('no random state applied')}
    \item skip user error: \VAR{my_dict['partitioning']['KFoldPartitioning']['skip_user_error']|default('no setted')}
\end{itemize}
\hfill\break
\hfill\break
\BLOCK{endif}
% KFOLD PARTITIONING TECNIQUE ended


\BLOCK{elif  my_dict['partitioning']['HoldOutPartitioning'] is defined}
%  HOLD-OUT PARTIONING TECNIQUE
The partitioning used is the Hold-Out Partitioning.
This approach splits the dataset in use into a ‘train’ set and a ‘test’ set.
The training set is what the model is trained on, and the test set is used to see how
well the model will perform on new, unseen data.
\hfill\break
\hfill\break
The train set size of this experiment is the \VAR{my_dict['partitioning']['HoldOutPartitioning']['train_set_size'] * 100}\%
of the original dataset, while the test set is the remaining \VAR{(100 - (my_dict['partitioning']['HoldOutPartitioning']['train_set_size'] * 100))}\%.
\hfill\break
\hfill\break
\BLOCK{ if my_dict.get('partitioning', {}).get('HoldOutPartitioning', {}).get('shuffle') == True }
The data has been shuffled before being split into batches.
\hfill\break
\hfill\break
%  HOLD-OUT PARTIONING TECNIQUE ended

\BLOCK{endif}
\hfill\break
\hfill\break

% ---------------------------------- ALGORITHM FOR RECOMMENDER SYSTEM -------------------------------------------------

\subsection{Algorithm used for the recommender system}\label{subsec:algo}
% ---RECSYS ALGORITHM ---
The framework of ClayRs allows to instantiate a recommender system in order to make list of recommendation, to achive
this target the system need to be based on a chosen algorithm that will work with the representation of data that
we have.
In this section we will analyse which algorithm has been used for the experiment and what are the settings
given.
\hfill\break
\hfill\break
\BLOCK{if my_dict['recsys']['ContentBasedRS'] id defined}
The recommender system is based on content.
\hfill\break
\hfill\break
\BLOCK{elif my_dict['recsys']['GraphBasedRS'] id defined}
The recommender system is based on a graph representation.
\hfill\break
\hfill\break

% -------------------------------- CONTENT BASE ALGO ----------------------------------------

\BLOCK{elif my_dict['recsys']['ContentBasedRS']['algorithm']['AmarDoubleSource'] is defined}
% AMAR DOUBLE SOURCE ALGO
The algorithm used is AmarDoubleSource
\begin{itemize}
    \item item fields: 'list of field used' TO BE ADJUSTED
    \item user fields: TO BE ADJUSTED
    \item batch size: \VAR{my_dict['recsys']['ContentBasedRS']['algorithm']['AmarDoubleSource']['batch_size']|default('no batch size') }
    \item epochs: \VAR{my_dict['recsys']['ContentBasedRS']['algorithm']['AmarDoubleSource']['epochs']|default('no batch size')}
    \item threshold: \VAR{my_dict['recsys']['ContentBasedRS']['algorithm']['AmarDoubleSource']['threshold']|default('no batch size')}
    \item embedding combiner: TO ADJUST
    \item seed: \VAR{my_dict['recsys']['ContentBasedRS']['algorithm']['AmarDoubleSource']['seed']|default('no batch size')}
    \item additional dl parameters: TO ADJUST
\end{itemize}
\hfill\break
\hfill\break
The mode used is \VAR{ my_dict['recsys']['ContentBasedRS']['mode']} and the number of recommendation given is
\VAR{ my_dict['recsys']['ContentBasedRS']['n_recs']|default('no number has been setted')}.
\hfill\break
\hfill\break


\BLOCK{elif my_dict['recsys']['ContentBasedRS']['algorithm']['CentroidVector'] is defined}
% CENTROID VECTOR ALGO
The algorithm used is the centroid vector
\begin{itemize}
    \item item fields: TO BE ADJUSTED
    \item user fields: TO BE ADJUSTED
    \item similarity:  \VAR{my_dict['recsys']['ContentBasedRS']['algorithm']['CentroidVector']['similarity']|default('no similarity')|safe_text}
    \item threshold: \VAR{my_dict['recsys']['ContentBasedRS']['algorithm']['CentroidVector']['threshold']|default('no threshold')}
    \item embedding combiner: TO ADJUST
\end{itemize}
\hfill\break
\hfill\break
The mode used is \VAR{ my_dict['recsys']['ContentBasedRS']['mode']} and the number of recommendation given is
\VAR{ my_dict['recsys']['ContentBasedRS']['n_recs']|default('no number has been setted')}.
\hfill\break
\hfill\break


\BLOCK{elif my_dict['recsys']['ContentBasedRS']['algorithm']['ClassifierRecommender'] is defined}
% CLASSIFIER ALGO
The algorithm used is a classifier \VAR{my_dict['recsys']['ContentBasedRS']['algorithm']['ClassifierRecommender']['classifier']|safe_text)}
\begin{itemize}
    \item item fields: TO BE ADJUSTED
    \item user fields: TO BE ADJUSTED
    \item threshold: \VAR{my_dict['recsys']['ContentBasedRS']['algorithm']['ClassifierRecommender']['threshold']|default('no threshold')}
    \item embedding combiner: TO ADJUST
\end{itemize}
\hfill\break
\hfill\break
The mode used is \VAR{ my_dict['recsys']['ContentBasedRS']['mode']} and the number of recommendation given is
\VAR{ my_dict['recsys']['ContentBasedRS']['n_recs']|default('no number has been setted')}.
\hfill\break
\hfill\break


\BLOCK{elif my_dict['recsys']['ContentBasedRS']['algorithm']['IndexQuery'] is defined}
% INDEX QUERY ALGO
The algorithm used is the Index Query.
\begin{itemize}
    \item item fields: TO BE ADJUSTED
    \item user fields: TO BE ADJUSTED
    \item classic similarity: \VAR{my_dict['recsys']['ContentBasedRS']['algorithm']['IndexQuery']['classic_similarity']|default('no used')}
    \item threshold: \VAR{my_dict['recsys']['ContentBasedRS']['algorithm']['IndexQuery']['threshold']|default('no threshold')}
    \item embedding combiner: TO ADJUST
\end{itemize}
\hfill\break
\hfill\break
The mode used is \VAR{ my_dict['recsys']['ContentBasedRS']['mode']} and the number of recommendation given is
\VAR{ my_dict['recsys']['ContentBasedRS']['n_recs']|default('no number has been setted')}.
\hfill\break
\hfill\break


\BLOCK{elif my_dict['recsys']['ContentBasedRS']['algorithm']['LinearPredictor'] is defined}
% LINEAR PREDICTOR ALGO
The algorithm used is a regressor \VAR{my_dict['recsys']['ContentBasedRS']['algorithm']['LinearPredictor']['regressor']|safe_text)}
\begin{itemize}
    \item item fields: TO BE ADJUSTED
    \item user fields: TO BE ADJUSTED
    \item only greater eq: \VAR{my_dict['recsys']['ContentBasedRS']['algorithm']['LinearPredictor']['only_greater_eq']|default('no used')}
    \item embedding combiner: TO ADJUST
\end{itemize}
\hfill\break
\hfill\break
The mode used is \VAR{ my_dict['recsys']['ContentBasedRS']['mode']} and the number of recommendation given is
\VAR{ my_dict['recsys']['ContentBasedRS']['n_recs']|default('no number has been setted')}.
\hfill\break
\hfill\break


% ------------------------ GRAPH BASED ALGO ------------------------------------------
\BLOCK{elif my_dict['recsys']['GraphBasedRS']['algorithm']['NXPageRank'] is defined}
% NX PAGE RANK ALGO
The algorithm used is NX Page Rank.
\begin{itemize}
    \item alpha: \VAR{my_dict['recsys']['GraphBasedRS']['algorithm']['NXPageRank']['alpha']|default('no alpha used')}
    \item personalized: \VAR{my_dict['recsys']['GraphBasedRS']['algorithm']['NXPageRank']['personalized']|default('no used')}
    \item max iter: \VAR{my_dict['recsys']['GraphBasedRS']['algorithm']['NXPageRank']['max_iter']|default('no max iter')}
    \item tol: \VAR{my_dict['recsys']['GraphBasedRS']['algorithm']['NXPageRank']['tol']|default('no used')}
    \item nstart: \VAR{my_dict['recsys']['GraphBasedRS']['algorithm']['NXPageRank']['nstart']|default('no used')}
    \item weight: \VAR{my_dict['recsys']['GraphBasedRS']['algorithm']['NXPageRank']['weight']|default('no used')|safe_text}
\end{itemize}
\hfill\break
\hfill\break
The type of graph used in the algorithm is a \VAR{ my_dict['recsys']['GraphBasedRS']['mode']},
the mode used is \VAR{ my_dict['recsys']['GraphBasedRS']['mode']} and the number of recommendation given is
\VAR{ my_dict['recsys']['GraphBasedRS']['n_recs']|default('no number has been setted')}.
\hfill\break
\hfill\break


% In case of the RecSys in not been used
\BLOCK{else}
The RecSys module has noit been used during this experiment.
\hfill\break
\hfill\break
% ------------------ RECSYS SECTION ENDED-------------------
\BLOCK{endif}


% --------------------- OPENING THE EVALUATION MODULE SECTION -------------------------------------

\BLOCK{if my_dict['metrics'] is defined}
% --- EVA MODULE start ---
\section{EVALUATION MODULE}\label{sec:eva-module}
The \textbf{EvalModel} which is the abbreviation for Evaluation Model has the task of evaluating a recommender system,
using several state-of-the-art metrics, this allows to compare different recommender system and different algorithm of
recommendation and find out which are the strength points and which the weak ones.

\subsection{Metrics}\label{subsec:metrics}
% --- Metrics ---
During the experiment a bunch of formal metrics have been performed on the recommendation produced in order to evaluete
the performace of the system.
The metrics used are the followings:
\hfill\break
\hfill\break


% ------------------ CLASSIFICATION METRICS STARTED ------------------------------------

\BLOCK{if my_dict['metrics']['Precision'] is defined }
% Precision report
\subsubsection{Precision}\label{sec:precision}
In ClayRS, the Precision metric is calculated as such for the \textbf{single user}:
\hfill\break
\hfill\break
    \[
         Precision_u = \frac{tp_u}{tp_u + fp_u}
    \]
\hfill\break
\hfill\break
    Where:
\begin{itemize}
    \item $tp_u$ is the number of items which are in the recommendation list of the user and have a
       $\geq$ \BLOCK{ if my_dict.get('metrics', {}).get('Precision', {}).get('relevant_threshold') is not none}
        \textbf{\VAR{my_dict['metrics']['Precision']['relevant_threshold']}}.
        \BLOCK{else}
        \textbf{\VAR{my_dict['interactions']['mean_score']|default('no relevant threshold used')}}.
        \BLOCK{endif}
    \item $fp_u$ is the number of items which are in the recommendation list of the user and have a
      rating $<$ \BLOCK{if my_dict.get('metrics', {}).get('Precision', {}).get('relevant_threshold') is not none}
        \textbf{\VAR{my_dict['metrics']['Precision']['relevant_threshold']}}.
        \BLOCK{else}
        \textbf{\VAR{dict['interactions']['mean_score']|default('no relevant threshold used')}}.
        \BLOCK{endif}
\end{itemize}
\hfill\break
\hfill\break
In ClayRS, Precision needs those parameters:
the \textbf{relevant\_threshold}, is a parameter needed to discern relevant items and non-relevant items for every user.
If not specified, the mean rating score of every user will be used, in this experiment it has been set to
\textbf{\VAR{my_dict['metrics']['Precision']['relevant_threshold']|safe_text}}.
\hfill\break
\hfill\break



\BLOCK{elif my_dict['metrics']['Recall'] is defined}
% Recall report
\subsubsection{Recall}\label{sec:recall}
The Recall metric is calculated as such for the \textbf{single user}:
\hfill\break
\hfill\break
    \[
        Recall_u = \frac{tp_u}{tp_u + fn_u}
    \]
\hfill\break
\hfill\break
    Where:
\begin{itemize}
    \item $tp_u$ is the number of items which are in the recommendation list of the user and have a
      rating $>=$ \BLOCK{ if my_dict.get('metrics', {}).get('Recall', {}).get('relevant_threshold') is not none}
        \textbf{\VAR{my_dict['metrics']['Recall']['relevant_threshold']}}.
        \BLOCK{else}
        \textbf{\VAR{my_dict['interactions']['mean_score']|default('no relevant threshold used')}}.
        \BLOCK{endif}
    \item $fn_u$ is the number of items which are not in the recommendation list of the user and have a
      rating $>=$ \BLOCK{if my_dict.get('metrics', {}).get('Recall', {}).get('relevant_threshold') is not none}
        \textbf{\VAR{my_dict['metrics']['Recall']['relevant_threshold']}}.
        \BLOCK{else}
        \textbf{\VAR{dict['interactions']['mean_score']|default('no relevant threshold used')}}.
        \BLOCK{endif}
\end{itemize}
\hfill\break
\hfill\break
In ClayRS, Recall needs those parameters:
the \textbf{relevant\_threshold}, is a parameter needed to discern relevant items and non-relevant items for every user.
If not specified, the mean rating score of every user will be used, in this experiment it has been set to
\textbf{\VAR{my_dict['metrics']['Recall']['relevant_threshold']|default('no relevant threshold used')|safe_text}}.
\hfill\break
\hfill\break



\BLOCK{elif my_dict['metrics']['FMeasure'] is defined}
%FMeasure report
\subsubsection{FMeasure}\label{sec:f-meas}
The FMeasure metric combines Precision and Recall into a single metric.
It is calculated as such for the \textbf{single user}:
\hfill\break
\hfill\break
    \[
        FMeasure_u = (1 + \beta^2) \cdot \frac{P_u \cdot R_u}{(\beta^2 \cdot P_u) + R_u}
    \]
\hfill\break
\hfill\break
    Where:
\begin{itemize}
    \item $P_u$ is the Precision calculated for the user \textbf{u}.
    \item $R_u$ is the Recall calculated for the user \textbf{u}.
    \item $\beta$ is a real factor which could weight differently Recall or Precision based on its value:
    \begin{itemize}
        \item $\beta = 1$: Equally weight Precision and Recall.
        \item $\beta > 1$: Weight Recall more.
        \item $\beta < 1$: Weight Precision more.
    \end{itemize}
\end{itemize}
\hfill\break
\hfill\break
A famous FMeasure is the F1 Metric, where $\beta = 1$, which basically is the harmonic mean of recall and
precision:
\hfill\break
\hfill\break
    \[
         F1_u = \frac{2 \cdot P_u \cdot R_u}{P_u + R_u}
    \]
\hfill\break
\hfill\break
The FMeasure metric is calculated as such for the entire system, depending on if \textbf{macro} average or
\textbf{micro} average has been chosen:
\hfill\break
\hfill\break
    \[
        FMeasure_{sys} - micro = (1 + \beta^2) \cdot \frac{P_u \cdot R_u}{(\beta^2 \cdot P_u) + R_u}
    \]
\hfill\break
\hfill\break
    \[
        FMeasure_{sys} - macro = \frac{\sum_{u \in U} FMeasure_u}{|U|}
    \]
\hfill\break
\hfill\break
During the experiment the FMeasure has been calculated with $\beta = $
\VAR{my_dict['metrics']['FMeasure']['beta']|safe_text} and the relevant threshold is
\textbf{\VAR{my_dict['metrics']['FMeasure']['relevant_threshold']|default('no relevant threshold used')|safe_text}}.
\hfill\break
\hfill\break



\BLOCK{elif my_dict['metrics']['PrecisionAtK'] is defined}
% PRECISION @K REPORT
\subsubsection{Precision@K}\label{sec:prec-k}
The Precision@K metric is calculated as such for the \textbf{single user}:
\hfill\break
\hfill\break
    \[
        Precision@K_u = \frac{tp@K_u}{tp@K_u + fp@K_u}
    \]
\hfill\break
\hfill\break
    Where:
\begin{itemize}
    \item $tp@K_u$ is the number of items which are in the recommendation list of the user
      cutoff to the first K items and have a rating $>=$ relevant threshold in its ground truth.
    \item $tp@K_u$ is the number of items which are in the recommendation list of the user
      cutoff to the first K items** and have a rating $<$ relevant threshold in its ground truth.
\end{itemize}
\hfill\break
\hfill\break
And it is calculated as such for the entire system, depending on if \textbf{macro} average or \textbf{micro} average
has been chosen:
\hfill\break
\hfill\break
   \[
       Precision@K_{sys} - micro = \frac{\sum_{u \in U} tp@K_u}{\sum_{u \in U} tp@K_u + \sum_{u \in U} fp@K_u}
   \]
\hfill\break
\hfill\break
    \[
       Precision@K_{sys} - macro = \frac{\sum_{u \in U} Precision@K_u}{|U|}
   \]
\hfill\break
\hfill\break
During the experiment Precision@K has been used with the following settings:
\begin{itemize}
    \item \textbf{K: \VAR{my_dict['metrics']['PrecisionAtK']['k']|safe_text} }
    \item \textbf{relevant threshold:\VAR{my_dict['metrics']['PrecisionAtK']['relevant_threshold']|default('no relevant threshold used')|safe_text} }
    \item \textbf{sys average: \VAR{my_dict['metrics']['PrecisionAtK']['sys_average']|safe_text} }
\end{itemize}
\hfill\break
\hfill\break



\BLOCK{elif my_dict['metrics']['RecallAtK'] is defined}
% RECALL @K REPORT
\subsubsection{Recall@K}\label{sec:rec-k}
The Recall@K metric is calculated as such for the \textbf{single user}:
\hfill\break
\hfill\break
    \[
        Recall@K_u = \frac{tp@K_u}{tp@K_u + fn@K_u}
    \]
\hfill\break
\hfill\break
    Where:
\begin{itemize}
    \item $tp@K_u$ is the number of items which are in the recommendation list of the user
      \textbf{cutoff to the first K items} and have a rating $>=$ relevant threshold in its ground truth
    \item $tp@K_u$ is the number of items which are NOT in the recommendation list of the user
      \textbf{cutoff to the first K items} and have a rating $>=$ relevant threshold in its ground truth
\end{itemize}
\hfill\break
\hfill\break
And it is calculated as such for the entire system, depending on if \textbf{macro} average or \textbf{micro} average
has been chosen:
\hfill\break
\hfill\break
    \[
        Recall@K_{sys} - micro = \frac{\sum_{u \in U} tp@K_u}{\sum_{u \in U} tp@K_u + \sum_{u \in U} fn@K_u}
    \]
\hfill\break
\hfill\break
    \[
        Recall@K_{sys} - macro = \frac{\sum_{u \in U} Recall@K_u}{|U|}
    \]
\hfill\break
\hfill\break
During the experiment Recall@K has been used with the following settings:
\begin{itemize}
    \item \textbf{K: \VAR{my_dict['metrics']['RecallAtK']['k']|safe_text} }
    \item \textbf{relevant threshold: \VAR{my_dict['metrics']['RecallAtK']['relevant_threshold']|default('no relevant threshold used')|safe_text} }
    \item \textbf{sys average: \VAR{my_dict['metrics']['RecallAtK']['sys_average']|safe_text} }
\end{itemize}
\hfill\break
\hfill\break



\BLOCK{elif my_dict['metrics']['FMeasureAtK'] is defined}
% FMEASURE @K REPORT
\subsubsection{FMeasure@K }\label{sec:f-meas-k}
The FMeasure@K metric combines Precision@K and Recall@K into a single metric.
It is calculated as such for the \textbf{single user}:
\hfill\break
\hfill\break
    \[
        FMeasure@K_u = (1 + \beta^2) \cdot \frac{P@K_u \cdot R@K_u}{(\beta^2 \cdot P@K_u) + R@K_u}
    \]
\hfill\break
\hfill\break
    Where:
\begin{itemize}
    \item $P@K_u$ is the Precision at K calculated for the user \textbf{u}.
    \item $R@K_u$ is the Recall at K calculated for the user \textbf{u}.
    \item $\beta$ is a real factor which could weight differently Recall or Precision based on its value:
    \begin{itemize}
        \item $\beta = 1$: Equally weight Precision and Recall.
        \item $\beta > 1$: Weight Recall more.
        \item $\beta < 1$: Weight Precision more.
    \end{itemize}
\end{itemize}
\hfill\break
\hfill\break
A famous FMeasure@K is the F1@K Metric, where $\beta = 1$, which basically is the harmonic mean of recall and
precision:
\hfill\break
\hfill\break
    \[
        F1@K_u = \frac{2 \cdot P@K_u \cdot R@K_u}{P@K_u + R@K_u}
    \]
\hfill\break
\hfill\break
The FMeasure@K metric is calculated as such for the entire system, depending on if \textbf{macro} average or
\textbf{micro} average has been chosen:
\hfill\break
\hfill\break
    \[
        FMeasure@K_{sys} - micro = (1 + \beta^2) \cdot \frac{P@K_u \cdot R@K_u}{(\beta^2 \cdot P@K_u) + R@K_u}
    \]
\hfill\break
\hfill\break
    \[
        FMeasure@K_{sys} - macro = \frac{\sum_{u \in U} FMeasure@K_u}{|U|}
    \]
\hfill\break
\hfill\break
During the experiment FMeasure@K has been used with the following settings:
\begin{itemize}
    \item \textbf{K: \VAR{my_dict['metrics']['FMeasureAtK']['k']|safe_text}}
    \item  \textbf{$\beta$: \VAR{my_dict['metrics']['FMeasureAtK']['beta']|safe_text}}
    \item \textbf{relevant threshold: \VAR{my_dict['metrics']['FMeasureAtK']['relevant_threshold']|default('no relevant threshold used')|safe_text} }
    \item \textbf{sys average: \VAR{my_dict['metrics']['FMeasureAtK']['sys_average']|safe_text} }
\end{itemize}
\hfill\break
\hfill\break



\BLOCK{elif my_dict['metrics']['RPrecision'] is defined}
% R-PRECISION REPORT
\subsubsection{R-Precision }\label{sec:r-prec}
The R-Precision metric is calculated as such for the \textbf{single user}:
\hfill\break
\hfill\break
    \[
        R-Precision_u = \frac{tp@R_u}{tp@R_u + fp@R_u}
    \]
\hfill\break
\hfill\break
    Where:
\begin{itemize}
    \item $R$ it's the number of relevant items for the user \textbf{u}.
    \item $tp@R_u$ is the number of items in the recommendation list of the user, up to the first   $R$ items,
        that have a rating $\geq$ the relevant threshold in its ground truth.
    \item $tp@R_u$ is the number of items in the recommendation list of the user, up to the first $R$ items,
        that have a rating $<$ \texttt{relevant\_threshold} in their ground truth.
\end{itemize}
\hfill\break
\hfill\break
And it is calculated as such for the entire system, depending on if \textbf{macro} average or \textbf{micro} average
has been chosen:
\hfill\break
\hfill\break
    \[
        Precision@R_{sys} - micro = \frac{\sum_{u \in U} tp@R_u}{\sum_{u \in U} tp@R_u + \sum_{u \in U} fp@R_u}
    \]
\hfill\break
\hfill\break
    \[
        Precision@R_{sys} - macro = \frac{\sum_{u \in U} R-Precision_u}{|U|}
    \]
\hfill\break
\hfill\break
During the experiment R-Precision has been used with the following settings:
\begin{itemize}
    \item \textbf{relevant threshold:\VAR{my_dict['metrics']['RPrecision']['relevant_threshold']|default('no relevant threshold used')|safe_text} }
    \item \textbf{sys average: \VAR{my_dict['metrics']['RPrecision']['sys_average']|safe_text} }
\end{itemize}
\hfill\break
\hfill\break

% ------------------ CLASSIFICATION METRICS ENDED ------------------------------------

% ------------------ ERROR METRICS STARTED -------------------------------------------

\BLOCK{elif my_dict['metrics']['MSE'] is defined }
% MSE REPORT
\subsubsection{MSE }\label{sec:mse}
The MSE abbreviation Mean Squared Error metric is calculated as such for the \textbf{single user}:
\hfill\break
\hfill\break
    \[
        MSE_u = \sum_{i \in T_u} \frac{(r_{u,i} - \hat{r}_{u,i})^2}{|T_u|}
    \]
\hfill\break
\hfill\break
    Where:
\begin{itemize}
    \item $T_u$ is the test set of the user \textbf{u}.
    \item $r_{u, i}$ is the actual score give by user \textbf{u} to item \textbf{i}.
    \item $\hat{r}_{u, i}$ is the predicted score give by user \textbf{u} to item \textbf{i}.
\end{itemize}
\hfill\break
\hfill\break
It is calculated as such for the entire system:
\hfill\break
\hfill\break
    \[
        MSE_{sys} = \sum_{u \in T} \frac{MSE_u}{|T|}
    \]
\hfill\break
\hfill\break
    Where:
\begin{itemize}
    \item $T$ is the \textbf{test set}.
    \item $MSE_u$ is the MSE calculated for user \textbf{u}.
\end{itemize}
\hfill\break
\hfill\break
There may be cases in which some items of the test set of the user could not be predicted
\textit{eg. a CBRS was chosen and items were not present locally}. In those cases, the $MSE_u$ formula becomes:
\hfill\break
\hfill\break
    \[
        MSE_u = \sum_{i \in T_u} \frac{(r_{u,i} - \hat{r}_{u,i})^2}{|T_u| - unk}
    \]
\hfill\break
\hfill\break
    Where:
\begin{itemize}
    \item $unk$ stay for unknown is the number of items of the user test set that could not be predicted.
\end{itemize}
\hfill\break
\hfill\break
If no items of the user test set have been predicted $|T_u| - unk = 0$, then:
\hfill\break
\hfill\break
    \[
        MSE_u = NaN
    \]
\hfill\break
\hfill\break



\BLOCK{elif my_dict['metrics']['RMSE'] is defined}
% RMSE REPORT
\subsubsection{RMSE}\label{sec:rmse}
The RMSE (Root Mean Squared Error) metric is calculated as such for the \textbf{single user}:
\hfill\break
\hfill\break
    \[
        RMSE_u = \sqrt{\sum_{i \in T_u} \frac{(r_{u,i} - \hat{r}_{u,i})^2}{|T_u|}}
    \]
\hfill\break
\hfill\break
    Where:
\begin{itemize}
    \item $T_u$ is the *test set* of the user $u$.
    \item $r_{u, i}$ is the actual score give by user $u$ to item $i$.
    \item $\hat{r}_{u, i}$ is the predicted score give by user $u$ to item $i$.
\end{itemize}
\hfill\break
\hfill\break
It is calculated as such for the entire system:
\hfill\break
\hfill\break
    \[
        RMSE_{sys} = \sum_{u \in T} \frac{RMSE_u}{|T|}
    \]
\hfill\break
\hfill\break
    Where:
\begin{itemize}
    \item $T$ is the test set.
    \item $RMSE_u$ is the RMSE calculated for user $u$.
\end{itemize}
\hfill\break
\hfill\break
There may be cases in which some items of the test set of the user could not be predicted
\textit{eg. a CBRS was chosen and items were not present locally, a methodology different from TestRatings was chosen}.
In those cases, the $RMSE_u$ formula becomes:
\hfill\break
\hfill\break
    \[
        RMSE_u = \sqrt{\sum_{i \in T_u} \frac{(r_{u,i} - \hat{r}_{u,i})^2}{|T_u| - unk}}
    \]
\hfill\break
\hfill\break
    Where:
\begin{itemize}
    \item $unk$ (unknown) is the number of items of the user test set that could not be predicted.
\end{itemize}
\hfill\break
\hfill\break
If no items of the user test set have been predicted $|T_u| - unk = 0$, then:
\hfill\break
\hfill\break
    \[
        RMSE_u = NaN
    \]
\hfill\break
\hfill\break



\BLOCK{elif my_dict['metrics']['MAE'] is defined}
% MAE REPORT
\subsubsection{MAE}\label{sec:mae}
The MAE (Mean Absolute Error) metric is calculated as such for the \textbf{single user}:
\hfill\break
\hfill\break
    \[
        MAE_u = \sum_{i \in T_u} \frac{|r_{u,i} - \hat{r}_{u,i}|}{|T_u|}
    \]
\hfill\break
\hfill\break
    Where:
\begin{itemize}
    \item $T_u$ is the test set of the user $u$.
    \item $r_{u, i}$ is the actual score give by user $u$ to item $i$.
    \item $\hat{r}_{u, i}$ is the predicted score give by user $u$ to item $i$.
\end{itemize}
\hfill\break
\hfill\break
It is calculated as such for the entire system:
\hfill\break
\hfill\break
    \[
        MAE_{sys} = \sum_{u \in T} \frac{MAE_u}{|T|}
    \]
\hfill\break
\hfill\break
    Where:
\begin{itemize}
    \item $T$ is the test set.
    \item $MAE_u$ is the MAE calculated for user $u$.
\end{itemize}
\hfill\break
\hfill\break
There may be cases in which some items of the test set of the user could not be predicted
\textit{eg. a CBRS was chosen and items were not present locally, a methodology different from TestRatings was chosen}.
In those cases, the $MAE_u$ formula becomes:
\hfill\break
\hfill\break
    \[
        MAE_u = \sum_{i \in T_u} \frac{|r_{u,i} - \hat{r}_{u,i}|}{|T_u| - unk}
    \]
\hfill\break
\hfill\break
    Where:
\begin{itemize}
    \item $unk$ (unknown) is the number of items of the user test set that could not be predicted.
\end{itemize}
\hfill\break
\hfill\break
If no items of the user test set have been predicted $|T_u| - unk = 0$, then:
\hfill\break
\hfill\break
    \[
        MAE_u = NaN
    \]
\hfill\break
\hfill\break

% ------------------------- error metrics ended -------------------------------------------


% ------------------------ RANKING METRICS STARTED ----------------------------------------

\BLOCK{elif my_dict['metrics']['NDCG'] is defined}
% Normalized Discounted Cumulative Gain REPORT
\subsubsection{NDCG}\label{sec:ndcg}
The NDCG abbreviation of Normalized Discounted Cumulative Gain metric is calculated for the \textbf{single user}
by first computing the DCG score using the following formula:
\hfill\break
\hfill\break
    \[
        DCG_{u}(scores_{u}) = \sum_{r\in scores_{u}}{\frac{f(r)}{log_x(2 + i)}}
    \]
\hfill\break
\hfill\break
    Where:
\begin{itemize}
    \item $scores_{u}$ are the ground truth scores for predicted items, ordered according to the order of said items in the
        ranking for the user $u$.
    \item $f$ is a gain function \textit{linear or exponential, in particular}.
    \item $x$ is the base of the logarithm.
    \item $i$ is the index of the truth score $r$ in the list of scores $scores_{u}$.
\end{itemize}
\hfill\break
\hfill\break
If $f$ is "linear", then the truth score $r$ is returned as is. Otherwise, in the "exponentia" case, the following
formula is applied to $r$:
\hfill\break
\hfill\break
    \[
        f(r) = 2^{r} - 1
    \]
\hfill\break
\hfill\break
The NDCG for a single user is then calculated using the following formula:
\hfill\break
\hfill\break
    \[
        NDCG_u(scores_{u}) = \frac{DCG_{u}(scores_{u})}{IDCG_{u}(scores_{u})}
    \]
\hfill\break
\hfill\break
    Where:
\begin{itemize}
    \item $IDCG_{u}$ is the DCG of the ideal ranking for the truth scores.
\end{itemize}
\hfill\break
\hfill\break
So the basic idea is to compare the actual ranking with the ideal one.
Finally, the NDCG of the entire system is calculated instead as such:
\hfill\break
\hfill\break
    \[
        NDCG_{sys} = \frac{\sum_{u} NDCG_u}{|U|}
    \]
\hfill\break
\hfill\break
    Where:
\begin{itemize}
    \item $NDCG_u$ is the NDCG calculated for user $u$.
    \item $U$ is the set of all users.
\end{itemize}
\hfill\break
\hfill\break
The system average excludes NaN values.
\hfill\break
\hfill\break



\BLOCK{elif my_dict['metrics']['NDCGAtK'] is defined}
% NDCG@K REPORT
\subsubsection{NDCG@k}\label{sec:ndcg-k}
The NDCG@K abbreviation of Normalized Discounted Cumulative Gain at K) metric is calculated for the \textbf{single user}
by using the [framework implementation of the NDCG][clayrs.evaluation.NDCG] but considering $scores_{u}$ cut at the
first $k$ predictions.
The K used for the experiment is \VAR{my_dict['metrics']['MRRAtK']['k']|safe_text}.
\hfill\break
\hfill\break



\BLOCK{elif my_dict['metrics']['MRR'] is defined}
% MRR REPORT
\subsubsection{MRR}\label{sec:mrr}
The MRR abbreviation of Mean Reciprocal Rank metric is a system-wide metric, so only its result it will be returned
and not those of every user.
MRR is calculated as such:
\hfill\break
\hfill\break
    \[
        MRR_{sys} = \frac{1}{|Q|}\cdot\sum_{i=1}^{|Q|}\frac{1}{rank(i)}
    \]
\hfill\break
\hfill\break
    Where:
\begin{itemize}
    \item $Q$ is the set of recommendation lists.
    \item $rank(i)$ is the position of the first relevant item in the i-th recommendation list.
\end{itemize}
\hfill\break
\hfill\break
% ATTENTION here we modified with \_ in case change
The MRR metric needs to discern relevant items from the not relevant ones. To achieve this, one could pass a
custom \texttt{relevant\_threshold} parameter that will be applied to every user. If the rating of an item
is $\geq$ \texttt{relevant\_threshold}, then it is considered relevant; otherwise, it is not.
If no \texttt{relevant\_threshold} parameter is passed, then for every user, its mean rating score will be used.
In this experiment, the relevant threshold used is
\VAR{my_dict['metrics']['MRR']['relevant_threshold']|default('no threshold has been setted')|safe_text}.
\hfill\break
\hfill\break



\BLOCK{elif my_dict['metrics']['MRRAtK'] is defined}
% MRR@K REPORT
\subsubsection{MRR@K}\label{sec:mrr-k}
The MRR@K (Mean Reciprocal Rank at K) metric is a system-wide metric, so only its result will be returned and
not those of every user. MRR@K is calculated as such:
\hfill\break
\hfill\break
    \[
        MRR@K_{sys} = \frac{1}{|Q|}\cdot\sum_{i=1}^{K}\frac{1}{rank(i)}
    \]
\hfill\break
\hfill\break
    Where:
\begin{itemize}
    \item $K$ is the cutoff parameter.
    \item $Q$ is the set of recommendation lists.
    \item $rank(i)$ is the position of the first relevant item in the i-th recommendation list.
\end{itemize}
\hfill\break
\hfill\break
In this experiment, the relevant threshold used is
\VAR{my_dict['metrics']['MRRAtK']['relevant_threshold']|default('no threshold has been setted')|safe_text}.
\hfill\break
\hfill\break



\BLOCK{elif my_dict['metrics']['MAP'] is defined}
% MAP REPORT
\subsubsection{MAP}\label{sec:map}
The MAP metric abbreviation of Mean average Precision is a ranking metric computed by first calculating the AP
abbreviation of Average Precision for each user and then taking its mean.
The AP is calculated as such for the \textbf{single user}:
\hfill\break
\hfill\break
    \[
        AP_u = \frac{1}{m_u}\sum_{i=1}^{N_u}P(i)\cdot rel(i)
    \]
\hfill\break
\hfill\break
    Where:
\begin{itemize}
    \item $m_u$ is the number of relevant items for the user $u$.
    \item $N_u$ is the number of recommended items for the user $u$.
    \item $P(i)$ is the precision computed at cutoff $i$.
    \item $rel(i)$ is an indicator variable that says whether the i-th item is relevant ($rel(i)=1$) or not ($rel(i)=0$).
\end{itemize}
\hfill\break
\hfill\break
After computing the AP for each user, we can compute the MAP for the whole system:
\hfill\break
\hfill\break
    \[
        MAP_{sys} = \frac{1}{|U|}\sum_{u}AP_u
    \]
\hfill\break
\hfill\break
This metric will return the AP computed for each user in the dataframe containing users results, and the MAP
computed for the whole system in the dataframe containing system results. In this experiment the MAP has been calculeted
using a relevant threshold:
\VAR{my_dict['metrics']['MAP']['relevant_threshold']|default('no threshold has been setted')|safe_text}.
\hfill\break
\hfill\break



\BLOCK{elif my_dict['metrics']['MAPAtK'] is defined}
% MAP@K REPORT
\subsubsection{MAP@K}\label{sec:map-k}
The MAP@K metric abbreviation of Mean average Precision At K is a ranking metric computed by first calculating the
AP@K abbreviation of Average Precision At K for each user and then taking its mean.
The AP@K is calculated as such for the \textbf{single user}:
\hfill\break
\hfill\break
    \[
        AP@K_u = \frac{1}{m_u}\sum_{i=1}^{K}P(i)\cdot rel(i)
    \]
\hfill\break
\hfill\break
    Where:
\begin{itemize}
    \item $m_u$ is the number of relevant items for the user $u$.
    \item $K$ is the cutoff value.
    \item $P(i)$ is the precision computed at cutoff $i$.
    \item $rel(i)$ is an indicator variable that says whether the i-th item is relevant ($rel(i)=1$) or not ($rel(i)=0$).
\end{itemize}
\hfill\break
\hfill\break
After computing the AP@K for each user, we can compute the MAP@K for the whole system:
\hfill\break
\hfill\break
    \[
        MAP@K_{sys} = \frac{1}{|U|}\sum_{u}AP@K_u
    \]
\hfill\break
\hfill\break
This metric will return the AP@K computed for each user in the dataframe containing users results, and the MAP@K
computed for the whole system in the dataframe containing system results.
\hfill\break
\hfill\break



\BLOCK{elif my_dict['metrics']['Correlation'] is defined}
% CORRELATION REPORT
\subsubsection{Correlation}\label{sec:corr}
The Correlation metric calculates the correlation between the ranking of a user and its ideal ranking.
The currently correlation methods implemented are:
\begin{itemize}
    \item `pearson`
    \item `kendall`
    \item `spearman`
\end{itemize}
\hfill\break
\hfill\break
Every correlation method is implemented by the pandas library, so refer to its
\href{https://pandas.pydata.org/docs/reference/api/pandas.Series.corr.html}{documentation} for more information.
\hfill\break
\hfill\break
The correlation metric is calculated as such for the \textbf{single user}:
\hfill\break
\hfill\break
    \[
        Corr_u = Corr(ranking_u, ideal_ranking_u)
    \]
\hfill\break
\hfill\break
    Where:
\begin{itemize}
    \item $ranking_u$ is ranking of the user.
    \item $ideal_ranking_u$ is the ideal ranking for the user.
\end{itemize}
\hfill\break
\hfill\break
The ideal ranking is calculated based on the rating inside the *ground truth* of the user.
The Correlation metric calculated for the entire system is simply the average of every $Corr$:
\hfill\break
\hfill\break
    \[
        Corr_{sys} = \frac{\sum_{u} Corr_u}{|U|}
    \]
\hfill\break
\hfill\break
    Where:
\begin{itemize}
    \item $Corr_u$ is the correlation of the user $u$.
    \item $U$ is the set of all users.
\end{itemize}
\hfill\break
\hfill\break
% ATTETNION here there are already \_
The system average excludes NaN values.
It's also possible to specify a cutoff parameter using the \texttt{top\_n} parameter. If specified, only the first
$n$ results of the recommendation list will be used to calculate the correlation.
For this experiment, the settings for the correlation metrics are:
\begin{itemize}
    \item method: \VAR{my_dict['metrics']['Correlation']['method']|default('no method for correlation specified')|safe_text}.
    \item top_n: \VAR{my_dict['metrics']['Correlation']['top_n']|default('no cutoff setted')|safe_text}.
\end{itemize}
\hfill\break
\hfill\break

% ------------------------- ranking metrics ended ---------------------------


% ------------------------- FAIRNESS METRICS ------------------------------

\BLOCK{elif my_dict['metrics']['GiniIndex'] is defined}
% Gini Index REPORT
\subsubsection{Gini Index}\label{sec:gini}
The Gini Index metric measures inequality in recommendation lists. It's a system wide metric, so only its
result it will be returned and not those of every user. The metric is calculated as such:
\hfill\break
\hfill\break
    \[
        Gini_{sys} = \frac{\sum_i(2i - n - 1)x_i}{n\cdot\sum_i x_i}
    \]
\hfill\break
\hfill\break
    Where:
\begin{itemize}
    \item $n$ is the total number of distinct items that are being recommended.
    \item $x_i$ is the number of times that the item $i$ has been recommended.
\end{itemize}
\hfill\break
\hfill\break
A perfectly equal recommender system should recommend every item the same number of times, in which case the Gini
index would be equal to 0. The more the recsys is "disegual", the more the Gini Index is closer to 1. If the 'top_n'
parameter is specified, then the Gini index will measure inequality considering only the first n items of every
recommendation list of all users. For this experiment the top_n:
\VAR{my_dict['metrics']['GiniIndex']['top_n']|default('no list of top n item has setted')|safe_text}.
\hfill\break
\hfill\break



\BLOCK{elif my_dict['metrics']['PredictionCoverage'] is defined}
% PREDICTION COVERAGE REPORT
\subsubsection{Prediction Coverage}\label{sec:pred_cov}
The Prediction Coverage metric measures in percentage how many distinct items are being recommended in relation
to all available items. It's a system wise metric, so only its result it will be returned and not those of every
user. The metric is calculated as such:
\hfill\break
\hfill\break
    \[
         Prediction Coverage_{sys} = (\frac{|I_p|}{|I|})\cdot100
    \]
\hfill\break
\hfill\break
    Where:
\begin{itemize}
    \item $I$ is the set of all available items.
    \item $I_p$ is the set of recommended items.
\end{itemize}
\hfill\break
\hfill\break
The $I$ must be specified through the 'catalog' parameter.
\hfill\break
\hfill\break



\BLOCK{elif my_dict['metrics']['CatalogCoverage'] is defined}
% CATALOG COVERAGE REPORT
\subsubsection{Catalog Coverage}\label{sec:cat_cov}
The Catalog Coverage metric measures in percentage how many distinct items are being recommended in relation
to all available items. It's a system wide metric, so only its result it will be returned and not those of every
user. It differs from the Prediction Coverage since it allows for different parameters to come into play. If no
parameter is passed then it's a simple Prediction Coverage.
The metric is calculated as such:
\hfill\break
\hfill\break
    \[
         Catalog Coverage_{sys} = (\frac{|\bigcup_{j=1...N}reclist(u_j)|}{|I|})\cdot100
    \]
\hfill\break
\hfill\break
    Where:
\begin{itemize}
    \item $N$ is the total number of users.
    \item $reclist(u_j)$ is the set of items contained in the recommendation list of user $j$.
    \item $I$ is the set of all available items.
\end{itemize}
\hfill\break
\hfill\break
The $I$ must be specified through the 'catalog' parameter. The recommendation list of every user ($reclist(u_j)$)
can be reduced to the first n parameter with the top-n parameter, so that catalog coverage is measured considering
only the most highest ranked items. With the 'k' parameter one could specify the number of users that will be used to
calculate catalog coverage: k users will be randomly sampled and their recommendation lists will be used.
The formula above becomes:
\hfill\break
\hfill\break
    \[
        Catalog Coverage_{\text{sys}} = \left(\frac{|\bigcup_{j=1\ldots k} \text{reclist}(u_j)|}{|I|}\right) \cdot 100
    \]
\hfill\break
\hfill\break
    Where:
\begin{itemize}
    \item $k$ is the parameter specified.
\end{itemize}
\hfill\break
\hfill\break
Obviously 'k' $<$ N, else simply recommendation lists of all users will be used.
\hfill\break
\hfill\break



\BLOCK{elif my_dict['metrics']['DeltaGap'] is defined}
% DELTA GAP REPORT
\subsubsection{Delta GAP}\label{sec:DG}
The Delta GAP abbreviation of Group Average popularity metric lets you compare the average popularity "requested" by
one or multiple groups of users and the average popularity "obtained" with the recommendation given by the recsys.
It's a system wise metric and results of every group will be returned. It is calculated as such:
\hfill\break
\hfill\break
    \[
        \Delta GAP = \frac{recs_GAP - profile_GAP}{profile_GAP}
    \]
\hfill\break
\hfill\break
Users are split into groups based on the user_groups parameter, which contains names of the groups as keys,
and percentage of how many user must contain a group as values. For example:
\begin{itemize}
    \item user groups = \{'popular\_users': 0.3, 'medium\_popular\_users': 0.2, 'low\_popular\_users': 0.5\}
\end{itemize}
\hfill\break
\hfill\break
Every user will be inserted in a group based on how many popular items the user has rated
\textit{in relation to the percentage of users we specified as value in the dictionary}:
\begin{itemize}
    \item users with many popular items will be inserted into the first group.
    \item users with niche items rated will be inserted into one of the last groups.
\end{itemize}
\hfill\break
\hfill\break
In general users are grouped by $Popularity_ratio$ in a descending order. $Popularity_ratio$ for a single user $u$
is defined as:
\hfill\break
\hfill\break
    \[
        Popularity_ratio_u = n_most_popular_items_rated_u / n_items_rated_u
    \]
\hfill\break
\hfill\break
The most popular items are the first 'pop_percentage', items of all items ordered in a descending order by
popularity. The popularity of an item is defined as the number of times it is rated in the 'original_ratings' parameter
divided by the total number of users in the 'original_ratings'.
\hfill\break
\hfill\break
It can happen that for a particular user of a group no recommendation are available: in that case it will be skipped
and it won't be considered in the $\Delta GAP$ computation of its group. In case no user of a group has recs
available, a warning will be printed and the whole group won't be considered.
If the 'top_n' parameter is specified, then the $\Delta GAP$ will be calculated considering only the first
n items of every recommendation list of all users
\hfill\break
\hfill\break

% ---------------------------------- FAIRNESS METRICS ENDED --------------------------------------------


% --------------------------------- PLOT METRICS STARTED ----------------------------------------------

\BLOCK{elif my_dict['metrics']['LongTailDistr'] is defined}
% LONG TAIL DISTIBUTION REPORT
\subsubsection{Long Tail Distribution}\label{sec:LTD}
This metric generates the Long Tail Distribution plot and saves it in the output directory with the file name
specified. The plot can be generated both for the truth set or the predictions set based on the on parameter:
\begin{itemize}
    \item \textbf{on = 'truth'}: in this case the long tail distribution is useful to see which are the most popular items
       the most rated ones.
    \item \textbf{on = 'pred'}: in this case the long tail distribution is useful to see which are the most recommended
        items.
\end{itemize}
\hfill\break
\hfill\break
The plot file will be saved as `out_dir/file_name.format`.
Since multiple split could be evaluated at once, the overwrite parameter comes into play:
if is set to False, file with the same name will be saved as `file_name (1).format`, `file_name (2).format`, etc.
so that for every split a plot is generated without overwriting any file previously generated.
\hfill\break
\hfill\break
For this experiments the Long Tail Distribution has been used with the following settings:
\begin{itemize}
    \item on: \VAR{my_dict['metrics']['LongTailDistr'][''on'']|safe_text}.
    \item format: \VAR{my_dict['metrics']['LongTailDistr']['format']|safe_text}.
    \item overwrite: \VAR{my_dict['metrics']['LongTailDistr']['overwrite']|safe_text}.
\end{itemize}
\hfill\break
\hfill\break



\BLOCK{elif my_dict['metrics']['PopRatioProfileVsRecs'] is defined}
% PopRatioProfileVsRecs REPORT
\subsubsection{Pop Ratio Profile Vs Recs}\label{sec:popRatio}
This metric generates a plot where users are split into groups and, for every group, a boxplot comparing
profile popularity ratio and recommendations popularity ratio is drawn.
Users are split into groups based on the user_groups parameter, which contains names of the groups as keys,
and percentage of how many user must contain a group as values. For example:
\begin{itemize}
       \item user groups = {'popular_users': 0.3, 'medium_popular_users': 0.2, 'low_popular_users': 0.5}
\end{itemize}
\hfill\break
\hfill\break
Every user will be inserted in a group based on how many popular items the user has rated
\textit{in relation to the percentage of users we specified as value in the dictionary}:
\begin{itemize}
    \item users with many popular items will be inserted into the first group
    \item users with niche items rated will be inserted into one of the last groups.
\end{itemize}
\hfill\break
\hfill\break
In general users are grouped by $Popularity_ratio$ in a descending order. $Popularity_ratio$ for a single user $u$
is defined as:
\hfill\break
\hfill\break
    \[
        Popularity_ratio_u = n_most_popular_items_rated_u / n_items_rated_u
    \]
\hfill\break
\hfill\break
The most popular items are the first 'pop_percentage', items of all items ordered in a descending order by
popularity.
The popularity of an item is defined as the number of times it is rated in the 'original_ratings' parameter
divided by the total number of users in the 'original_ratings'.
It can happen that for a particular user of a group no recommendation are available: in that case it will be skipped
and it won't be considered in the $Popularity\_ratio$ computation of its group. In case no user of a group has recs
available, a warning will be printed and the whole group won't be considered.
\begin{itemize}
    \item The plot file will be saved as `out\_dir/file\_name.format`
\end{itemize}
\hfill\break
\hfill\break
Since multiple split could be evaluated at once, the `overwrite` parameter comes into play:
if is set to False, file with the same name will be saved as `file_name (1).format`, `file_name (2).format`, etc.
so that for every split a plot is generated without overwriting any file previously generated.
Thanks to the 'store_frame' parameter it's also possible to store a csv containing the calculations done in order
to build every boxplot. Will be saved in the same directory and with the same file name as the plot itself (but
with the .csv format):
\begin{itemize}
    \item The csv will be saved as `out_dir/file_name.csv`
\end{itemize}
\hfill\break
\hfill\break



\BLOCK{elif my_dict['metrics']['PopRecsCorrelation'] is defined}
% PopRecsCorrelation REPORT
\subsubsection{Pop Recs Correlation}\label{sec:popRC}
This metric generates a plot which has as the X-axis the popularity of each item and as Y-axis the recommendation
frequency, so that it can be easily seen the correlation between popular niche items and how many times are being
recommended.
The popularity of an item is defined as the number of times it is rated in the 'original_ratings' parameter
divided by the total number of users in the 'original_ratings'.
\begin{itemize}
    \item The plot file will be saved as out_dir/file_name.format'
\end{itemize}
\hfill\break
\hfill\break
Since multiple split could be evaluated at once, the overwrite parameter comes into play:
if is set to False, file with the same name will be saved as 'file_name (1).format, 'file_name (2).format', etc.
so that for every split a plot is generated without overwriting any file previously generated
\hfill\break
\hfill\break
There exists cases in which some items are not recommended even once, so in the graph could appear
zero recommendations. One could change this behaviour thanks to the 'mode' parameter:
\begin{itemize}
    \item \textbf{mode='both'}: two graphs will be created, the first one containing eventual zero recommendations, the
      second one where zero recommendations are excluded. This additional graph will be stored as
      out_dir/file_name_no_zeros.format \textit{the string '_no_zeros' will be added to the file_name chosen automatically}
    \item \textbf{mode='w_zeros'}: only a graph containing eventual zero recommendations will be created
    \item \text{mode='no_zeros'}: only a graph excluding eventual zero recommendations will be created. The graph will be
      saved as out_dir/file_name_no_zeros.format \textit{the string '_no_zeros' will be added to the file_name chosen automatically}
\end{itemize}
\hfill\break
\hfill\break
For this experiments the PopRecsCorrelation has been used with the following settings:
\begin{itemize}
    \item mode: \VAR{my_dict['metrics']['PopRecsCorrelation']['mode']|safe_text}.
    \item format: \VAR{my_dict['metrics']['PopRecsCorrelation']['format']|safe_text}.
    \item overwrite: \VAR{my_dict['metrics']['PopRecsCorrelation']['overwrite']|safe_text}.
\end{itemize}
\hfill\break
\hfill\break


% ----------------------------- PLOT METRICS ENDED --------------------------------------

\BLOCK{else}
No metrics have been used during this experiment.
\hfill\break
\hfill\break
\BLOCK{endif}



% THIS LATEC TEMPLATE REPORT THE RISULTS OF THE SYSTEMS ON THE FOLD USED

\BLOCK{if my_dict['sys_results'] is defined}
\subsection{Results}\label{sec:results}
In the following table, we present the results of the evaluation \ref{tab:results_table}
\begin{table}[!hbp]\label{tab:results_table}
    \centering
  \begin{tabular}{|c|c|}
    \hline
    \textbf{Metric}& \textbf{Value} \\ \hline
    Precision - macro & \VAR{my_dict['sys_results']['sys - fold1']['Precision - macro']|truncate|safe_text}\\ \hline
    Precision - micro & \VAR{my_dict['sys_results']['sys - fold1']['Precision - micro']|truncate|safe_text}\\ \hline
    Recall - macro & \VAR{my_dict['sys_results']['sys - fold1']['Recall - macro']|truncate|safe_text}\\ \hline
    Recall - micro & \VAR{my_dict['sys_results']['sys - fold1']['Recall - micro']|truncate|safe_text}\\ \hline
    R-Precision - macro & \VAR{my_dict['sys_results']['sys - fold1']['R-Precision - macro']|truncate|safe_text}\\ \hline
    R-Precision - micro & \VAR{my_dict['sys_results']['sys - fold1']['R-Precision - micro']|truncate|safe_text}\\ \hline
    F1 - macro &  \VAR{my_dict['sys_results']['sys - fold1']['F1 - macro']|truncate|safe_text}\\ \hline
    F1 - micro & \VAR{my_dict['sys_results']['sys - fold1']['F1 - micro']|truncate|safe_text}\\ \hline
    NDCG  & \VAR{my_dict['sys_results']['sys - fold1']['NDCG']|truncate|safe_text}\\ \hline
    MRR  & \VAR{my_dict['sys_results']['sys - fold1']['MRR']|truncate|safe_text}\\ \hline
    RMSE & \VAR{my_dict['sys_results']['sys - fold1']['RMSE']|truncate|safe_text}\\ \hline
    MSE & \VAR{my_dict['sys_results']['sys - fold1']['MSE']|truncate|safe_text}\\ \hline
    MAE & \VAR{my_dict['sys_results']['sys - fold1']['MAE']|truncate|safe_text}\\ \hline
    MRR & \VAR{my_dict['sys_results']['sys - fold1']['MRR']|truncate|safe_text}\\ \hline
    MAP  & \VAR{my_dict['sys_results']['sys - fold1']['MAP']|truncate|safe_text}\\ \hline
    Gini & \VAR{my_dict['sys_results']['sys - fold1']['Gini']|truncate|safe_text}\\ \hline
    PredictionCoverage & \VAR{my_dict['sys_results']['sys - fold1']['PredictionCoverage']|truncate|safe_text}\\ \hline
  \end{tabular}
  \caption{Table of the results}
\end{table}
\hfill\break
\hfill\break
\BLOCK{endif}

% In case of eval modulke is not used
\BLOCK{else}
The evaluation module has not been used during the experiment conducted.
\hfill\break
\hfill\break
\BLOCK{endif}

% ------------ END OF EVAL MODULE RREPORT -------------------------------


\section{CONCLUSION ON THE EXPERIMENT}\label{sec:conclution}
This part is for conclution to be sum up as needed.
\hfill\break
\hfill\break
% ------------------- END OF THE REPORT COMPLETED ---------------
% closing the document
\end{document}
