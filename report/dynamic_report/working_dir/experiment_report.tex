
% Created by Diego Miccoli

% Preamble
\documentclass[11pt]{article}
% [twocolumn]

\usepackage{graphicx} % Required for inserting images
\usepackage{amsmath}
\usepackage{verbatim}
\usepackage{hyperref}
\usepackage{comment}
\usepackage{caption}
\usepackage{url}
\usepackage{listings}
\usepackage{float}
\usepackage{array}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{geometry}
\usepackage{changepage}
\usepackage{longtable}
\usepackage{soul}

\usepackage{background}

\backgroundsetup{
    scale=1,
    angle=0,
    opacity=1,
    color=black,
    position=current page.north east,
    vshift=-0.3cm, % Regola questa dimensione per posizionare il testo verticalmente
    hshift=-3.5cm, % Regola questa dimensione per posizionare il testo orizzontalmente
    contents={%
        Aldo Moro University of studies of Bari
    }
}

\geometry{top=1cm, bottom=1cm}

\title{\textbf{ ClayRS in practice: Experimental scenarios }\\ [1cm] Department of Computer Science}
\author{ SWAP research group UniBa \thanks{Experiment conduct by Diego Miccoli}}
\date{23-02-2024}

\begin{document}

\maketitle

\section{Introduction}\label{sec:intro}
Recommender Systems (RS) are designed to assist users in various decision-making tasks by acquiring
information about their needs, interests, and preferences in order to personalize the user experience
based on such data.
These systems are based on paradigms that allow managing user information and processing
it to provide decision support.
In particular, two successful paradigms are Collaborative Filtering and Content-based Filtering.
In this document, we will present the results obtained using a recommendation system called \textbf{ClayRS},
a framework developed by the SWAP research group at the Department of Computer Science of the University of Bari.\\
\hfill\break

\textit{\ul{The following report has been automatically generated from YAML configuration files.}}

\hfill\break




% ----------------------------------------- OPENING CONTENT ANALYZER SECTION -----------------------------------------
\section{Content Analyzer Module}\label{sec:ca}
The content analyzer module will deal with raw source document or more in general data which could be
video or audio data and give a representation of these data which will be used by the other two module.
The text data source could be represented with exogenous technique or with a specified representation
and each field given could be treated with preprocessing techniques and postprocessing technique.
In this experiment the following techniques have been used on specific field in order to achieve the
representation wanted:
\hfill\break
\hfill\break
% --- TECNIQUE USED TO REPPRESENT DATA FIELD ---


\begin{itemize}
    \item \textbf{fields used}:  idx0, plot0, plot1, plot10, plot11, plot2, plot3, plot4, plot5, plot6, plot7, plot8, plot9, video\_path0, video\_path1, video\_path2, video\_path3, video\_path4

    \item \textbf{fields representation}:  FromNPY, OriginalData, WhooshTfIdf, SkLearnTfIdf, WordEmbeddingTechnique, SentenceEmbeddingTechnique, DocumentEmbeddingTechnique, Word2SentenceEmbedding, Word2DocEmbedding, Sentence2DocEmbedding, PyWSDSynsetDocumentFrequency, SkImageHogDescriptor, MFCC, VGGISH, PytorchImageModels, TorchVisionVideoModels

    \item \textbf{fields preprocessing}:  Spacy, NLTK, Ekphrasis, TorchUniformTemporalSubSampler, ConvertToMono, TorchResample, CenterCrop, Resize, Lambda, Normalize, ClipSampler

    \item \textbf{fields postprocessing}:  FVGMM, VLADGMM, SkLearnPCA
\end{itemize}





\hfill\break
% subsection of Dataset identification
\subsection{Dataset used and its statistics}
The dataset used for the experiment is  \lstinline[style=verbatim-text]| 1000K data video movie |,
its source file is \lstinline[style=verbatim-text]| items_extra_small.json | ,and the content used during the experiment:
     movielens\_id,
.


% --- DATA STATS ---
The statistics of the dataset used are reported in the following table:~\ref{tab:dataset_table}:
\begin{table}[ht]
    \centering
  \begin{tabular}{|c|c|}
    \hline
    \textbf{Parameter}& \textbf{Value} \\ \hline
    n\_users  & 2\\ \hline
    n\_items  & 6\\ \hline
    total\_interactions  & 6\\ \hline
    min\_score  & 1.0\\ \hline
    max\_score  & 4.0\\ \hline
    mean\_score  & 2.66667\\ \hline
    sparsity  & 0.5\\ \hline
  \end{tabular}
   \caption{Stats on the dataset}\label{tab:dataset_table}
\end{table}

\hfill\break



% ------------------------------ START SUBSECTION OF PARTITIONING OF RECSYS --------------------------------------------
% subsection of the splitting technique used, referred to partition protocol
\subsection{Data splitting technique}\label{subsec:partitioning}
% KFOLD PARTITIONING TECNIQUE
K-fold cross-validation is a technique used in machine learning to assess the performance of a predictive model.
The basic idea is to divide the dataset into K subsets, or folds.
The model is then trained K times, each time using K-1 folds for training and the remaining fold for validation.
This process is repeated K times, with a different fold used as the validation set in each iteration.
\hfill\break
The KFoldPartitioning has been used with the following setting:
\hfill\break
The data has been shuffled before being split into batches.
The partitioning technique has been executed with the following settings:
\begin{itemize}
    \item number of splits: 2
    \item shuffle: True
    \item random state: None
    \item skip user error: True
\end{itemize}
\hfill\break
% KFOLD PARTITIONING TECNIQUE ended

% end partitioning section___________


% ----------------------------------------- START RECSYS MODULE ------------------------------------------------------
\section{Recommender System}\label{sec:recsys}
The RecSys module allows to instantiate a recommender system in order to work on items and users serialized
the Content Analyzer module, and its aim is to be trained on the data used in order to produce recommendation.
In this experiment the following are the algorithm used.



% ---------------------------------- ALGORITHM CentroidVector FOR RECOMMENDER SYSTEM -------------------------------------------------
\subsection{Algorithm CentroidVector used in the experiment}\label{subsec:algo_CentroidVector}
% ---RECSYS ALGORITHM CentroidVector ---

% ----------------------------------------- CONTENT BASED ALGO ---------------------------------------------------------
%specify the algorithm used and with which parameters
% The keys are: ['algorithm', 'mode', 'n_recs', 'methodology'] debug istruction to check the keys
The recommender system is based on content, the algorithm used
is \verb| CentroidVector | and the following are its settings:
\begin{itemize}
 \item \begin{verbatim}
  item_field: {'plot': ['tfidf_sk']}
\end{verbatim}
 \item \begin{verbatim}
  similarity: CosineSimilarity
\end{verbatim}
 \item \begin{verbatim}
  threshold: 4
\end{verbatim}
 \item \begin{verbatim}
  embedding_combiner: Centroid
\end{verbatim}
\end{itemize}
\hfill\break
The mode used is rank and the number of recommendations given
is 10.
The methodology used:
\begin{itemize}
    \item \verb| TestRatingsMethodology:|
    \begin{itemize}
                    \item \verb| only_greater_eq: None |
            \end{itemize}
\end{itemize}
% CONTENT BASED ALGO end___
\hfill\break






% ---------------------------------- ALGORITHM LinearPredictor FOR RECOMMENDER SYSTEM -------------------------------------------------
\subsection{Algorithm LinearPredictor used in the experiment}\label{subsec:algo_LinearPredictor}
% ---RECSYS ALGORITHM LinearPredictor ---

% ----------------------------------------- CONTENT BASED ALGO ---------------------------------------------------------
%specify the algorithm used and with which parameters
% The keys are: ['algorithm', 'mode', 'n_recs', 'methodology'] debug istruction to check the keys
The recommender system is based on content, the algorithm used
is \verb| LinearPredictor | and the following are its settings:
\begin{itemize}
 \item \begin{verbatim}
  item_field: {'plot': ['tfidf_sk']}
\end{verbatim}
 \item \begin{verbatim}
  regressor: SkLinearRegression
\end{verbatim}
 \item \begin{verbatim}
  only_greater_eq: None
\end{verbatim}
 \item \begin{verbatim}
  embedding_combiner: Centroid
\end{verbatim}
\end{itemize}
\hfill\break
The mode used is rank and the number of recommendations given
is 10.
The methodology used:
\begin{itemize}
    \item \verb| TestRatingsMethodology:|
    \begin{itemize}
                    \item \verb| only_greater_eq: None |
            \end{itemize}
\end{itemize}
% CONTENT BASED ALGO end___
\hfill\break






% ---------------------------------- ALGORITHM IndexQuery FOR RECOMMENDER SYSTEM -------------------------------------------------
\subsection{Algorithm IndexQuery used in the experiment}\label{subsec:algo_IndexQuery}
% ---RECSYS ALGORITHM IndexQuery ---

% ----------------------------------------- CONTENT BASED ALGO ---------------------------------------------------------
%specify the algorithm used and with which parameters
% The keys are: ['algorithm', 'mode', 'n_recs', 'methodology'] debug istruction to check the keys
The recommender system is based on content, the algorithm used
is \verb| IndexQuery | and the following are its settings:
\begin{itemize}
 \item \begin{verbatim}
  item_field: {'plot': ['search_i']}
\end{verbatim}
 \item \begin{verbatim}
  classic_similarity: True
\end{verbatim}
 \item \begin{verbatim}
  threshold: 4
\end{verbatim}
\end{itemize}
\hfill\break
The mode used is rank and the number of recommendations given
is 10.
The methodology used:
\begin{itemize}
    \item \verb| TestRatingsMethodology:|
    \begin{itemize}
                    \item \verb| only_greater_eq: None |
            \end{itemize}
\end{itemize}
% CONTENT BASED ALGO end___
\hfill\break






% ---------------------------------- ALGORITHM ClassifierRecommender FOR RECOMMENDER SYSTEM -------------------------------------------------
\subsection{Algorithm ClassifierRecommender used in the experiment}\label{subsec:algo_ClassifierRecommender}
% ---RECSYS ALGORITHM ClassifierRecommender ---

% ----------------------------------------- CONTENT BASED ALGO ---------------------------------------------------------
%specify the algorithm used and with which parameters
% The keys are: ['algorithm', 'mode', 'n_recs', 'methodology'] debug istruction to check the keys
The recommender system is based on content, the algorithm used
is \verb| ClassifierRecommender | and the following are its settings:
\begin{itemize}
 \item \begin{verbatim}
  item_field: {'plot': ['tfidf_sk']}
\end{verbatim}
 \item \begin{verbatim}
  classifier: SkKNN
\end{verbatim}
 \item \begin{verbatim}
  threshold: None
\end{verbatim}
 \item \begin{verbatim}
  embedding_combiner: Centroid
\end{verbatim}
\end{itemize}
\hfill\break
The mode used is rank and the number of recommendations given
is 10.
The methodology used:
\begin{itemize}
    \item \verb| TestRatingsMethodology:|
    \begin{itemize}
                    \item \verb| only_greater_eq: None |
            \end{itemize}
\end{itemize}
% CONTENT BASED ALGO end___
\hfill\break






% ---------------------------------- ALGORITHM AmarDoubleSource FOR RECOMMENDER SYSTEM -------------------------------------------------
\subsection{Algorithm AmarDoubleSource used in the experiment}\label{subsec:algo_AmarDoubleSource}
% ---RECSYS ALGORITHM AmarDoubleSource ---

% ----------------------------------------- CONTENT BASED ALGO ---------------------------------------------------------
%specify the algorithm used and with which parameters
% The keys are: ['algorithm', 'mode', 'n_recs', 'methodology'] debug istruction to check the keys
The recommender system is based on content, the algorithm used
is \verb| AmarDoubleSource | and the following are its settings:
\begin{itemize}
  \item \begin{verbatim}
  item_fields: [{'plot': ['tfidf_sk']}, {'plot': ['tfidf_sk']}]
\end{verbatim}
 \item \begin{verbatim}
  user_fields: [{}, {}]
\end{verbatim}
 \item \begin{verbatim}
  batch_size: 512
\end{verbatim}
 \item \begin{verbatim}
  epochs: 5
\end{verbatim}
 \item \begin{verbatim}
  threshold: 4
\end{verbatim}
 \item \begin{verbatim}
  additional_opt_parameters: {'batch_size': 512}
\end{verbatim}
  \item \begin{verbatim}
  optimizer_class: <class 'torch.optim.adam.Adam'>
\end{verbatim}
 \item \begin{verbatim}
  device: cuda:0
\end{verbatim}
 \item \begin{verbatim}
  embedding_combiner: {'Centroid': {}}
\end{verbatim}
 \item \begin{verbatim}
  seed: None
\end{verbatim}
 \item \begin{verbatim}
  additional_dl_parameters: {}
\end{verbatim}
\end{itemize}
\hfill\break
The mode used is rank and the number of recommendations given
is 10.
The methodology used:
\begin{itemize}
    \item \verb| TestRatingsMethodology:|
    \begin{itemize}
                    \item \verb| only_greater_eq: None |
            \end{itemize}
\end{itemize}
% CONTENT BASED ALGO end___
\hfill\break






% -------------------------------------- OPENING THE EVALUATION MODULE SECTION ---------------------------------------
\section{Evaluation Module}\label{sec:eva-module}
The \textbf{EvalModel} which is the abbreviation for Evaluation Model has the task of evaluating a recommender system,
using several state-of-the-art metrics, this allows to compare different recommender system and different algorithm of
recommendation and find out which are the strength points and which the weak ones.

\subsection{Metrics}\label{subsec:metrics}
% --- Metrics ---
During the experiment a bunch of formal metrics have been performed on the recommendation produced in order to evaluate
the performance of the system.
The metrics used are the followings:
\hfill\break
\hfill\break

% ---------------------------------------- CLASSIFICATION METRICS STARTED --------------------------------------------
% Precision report start
\subsubsection{Precision}\label{subsubsec:precision}
In ClayRS, the Precision metric is calculated as such for the \textbf{single user}:
\hfill\break
\hfill\break
    \[
         Precision_u = \frac{tp_u}{tp_u + fp_u}
    \]

% precision report end___

% Recall report start
\subsubsection{Recall}\label{subsubsec:recall}
The Recall metric is calculated as such for the \textbf{single user}:
\hfill\break
\hfill\break
    \[
        Recall_u = \frac{tp_u}{tp_u + fn_u}
    \]

% recall report end___

%FMeasure report start
\subsubsection{FMeasure}\label{subsubsec:f-meas}
The FMeasure metric combines Precision and Recall into a single metric.
It is calculated as such for the \textbf{single user}:
\hfill\break
\hfill\break
    \[
        FMeasure_u = (1 + \beta^2) \cdot \frac{P_u \cdot R_u}{(\beta^2 \cdot P_u) + R_u}
    \]

% FMeasure report end___

% PRECISION @K REPORT start
\subsubsection{Precision@K}\label{subsubsec:prec-k}
The Precision@K metric is calculated as such for the \textbf{single user}:
\hfill\break
\hfill\break
    \[
        Precision@K_u = \frac{tp@K_u}{tp@K_u + fp@K_u}
    \]

% precision@k report end___

% RECALL @K REPORT start
\subsubsection{Recall@K}\label{subsubsec:rec-k}
The Recall@K metric is calculated as such for the \textbf{single user}:
\hfill\break
\hfill\break
    \[
        Recall@K_u = \frac{tp@K_u}{tp@K_u + fn@K_u}
    \]

% recall@k report end___

% FMEASURE @K REPORT start
\subsubsection{FMeasure@K}\label{subsubsec:f-meas-k}
The FMeasure@K metric combines Precision@K and Recall@K into a single metric.
It is calculated as such for the \textbf{single user}:
\hfill\break
\hfill\break
    \[
        FMeasure@K_u = (1 + \beta^2) \cdot \frac{P@K_u \cdot R@K_u}{(\beta^2 \cdot P@K_u) + R@K_u}
    \]

% fmeasure@k report end___

% R-PRECISION REPORT start
\subsubsection{R-Precision}\label{subsubsec:r-prec}
The R-Precision metric is calculated as such for the \textbf{single user}:
\hfill\break
\hfill\break
    \[
        R-Precision_u = \frac{tp@R_u}{tp@R_u + fp@R_u}
    \]

% r-precision report end___
% <----- classification metrics end ------->

% -------------------------------------------- ERROR METRICS STARTED ------------------------------------------------
% MSE REPORT start
\subsubsection{MSE}\label{subsubsec:mse}
The MSE abbreviation Mean Squared Error metric is calculated as such for the \textbf{single user}:
\hfill\break
\hfill\break
    \[
        MSE_u = \sum_{i \in T_u} \frac{(r_{u,i} - \hat{r}_{u,i})^2}{|T_u|}
    \]

% mse report end___

% RMSE REPORT start
\subsubsection{RMSE}\label{subsubsec:rmse}
The RMSE (Root Mean Squared Error) metric is calculated as such for the \textbf{single user}:
\hfill\break
\hfill\break
    \[
        RMSE_u = \sqrt{\sum_{i \in T_u} \frac{(r_{u,i} - \hat{r}_{u,i})^2}{|T_u|}}
    \]
\hfill\break

% rmse report end___

% MAE REPORT start
\subsubsection{MAE}\label{subsubsec:mae}
The MAE abbreviation of Mean Absolute Error metric is calculated as such for the \textbf{single user}:
\hfill\break
\hfill\break
    \[
        MAE_u = \sum_{i \in T_u} \frac{|r_{u,i} - \hat{r}_{u,i}|}{|T_u|}
    \]
\hfill\break

% mae report end___
% <------ error metrics ended -------->

% ------------------------------------------- RANKING METRICS STARTED -----------------------------------------------
% Normalized Discounted Cumulative Gain REPORT start
\subsubsection{NDCG}\label{subsubsec:ndcg}
The NDCG abbreviation of Normalized Discounted Cumulative Gain metric is calculated for the \textbf{single user}
by first computing the DCG score using the following formula:
\hfill\break
\hfill\break
    \[
        DCG_{u}(scores_{u}) = \sum_{r\in scores_{u}}{\frac{f(r)}{log_x(2 + i)}}
    \]
\hfill\break
% ndcg report end___

% NDCG@K REPORT start
\subsubsection{NDCG@k}\label{subsubsec:ndcg-k}
The NDCG@K abbreviation of Normalized Discounted Cumulative Gain at K metric is calculated for the \textbf{single user}
by using the [framework implementation of the NDCG][clayrs.evaluation.NDCG] but considering $scores_{u}$ cut at the
first $k$ predictions.
The K used for the experiment is .
\hfill\break

% ndcg@k report end___

% MRR REPORT start
\subsubsection{MRR}\label{subsubsec:mrr}
The MRR abbreviation of Mean Reciprocal Rank metric is a system-wide metric, so only its result it will be returned
and not those of every user.
MRR is calculated as such:
\hfill\break
\hfill\break
    \[
        MRR_{sys} = \frac{1}{|Q|}\cdot\sum_{i=1}^{|Q|}\frac{1}{rank(i)}
    \]
\hfill\break

% mrr report end___

% MRR@K REPORT start
\subsubsection{MRR@K}\label{subsubsec:mrr-k}
The MRR@K abbreviation of Mean Reciprocal Rank at K metric is a system-wide metric, so only its result will be returned
and not those of every user.
MRR@K is calculated as such:
\hfill\break
\hfill\break
    \[
        MRR@K_{sys} = \frac{1}{|Q|}\cdot\sum_{i=1}^{K}\frac{1}{rank(i)}
    \]
\hfill\break

% mrr@k report end___

% MAP REPORT start
\subsubsection{MAP}\label{subsubsec:map}
The MAP metric abbreviation of Mean average Precision is a ranking metric computed by first calculating the AP
abbreviation of Average Precision for each user and then taking its mean.
The AP is calculated as such for the \textbf{single user}:
\hfill\break
\hfill\break
    \[
        AP_u = \frac{1}{m_u}\sum_{i=1}^{N_u}P(i)\cdot rel(i)
    \]
\hfill\break

% map report end___


% CORRELATION REPORT start
\subsubsection{Correlation}\label{subsubsec:corr}
The Correlation metric calculates the correlation between the ranking of a user and its ideal ranking.
The current correlation methods implemented are:
\begin{itemize}
    \item `pearson`
    \item `kendall`
    \item `spearman`
\end{itemize}
\hfill\break
\hfill\break
Every correlation method is implemented by the pandas library, so refer to its
\href{https://pandas.pydata.org/docs/reference/api/pandas.Series.corr.html}{documentation} for more information.
\hfill\break
\hfill\break
The correlation metric is calculated as such for the \textbf{single user}:
\hfill\break
\hfill\break
    \[
        Corr_u = Corr(ranking_u, ideal_ranking_u)
    \]
\hfill\break

% correlation report end___
% <------- ranking metrics ended ------->

% -------------------------------------------- FAIRNESS METRICS ------------------------------------------------------
% Gini Index REPORT start
\subsubsection{Gini Index}\label{subsubsec:gini}
The Gini Index metric measures inequality in recommendation lists.
It's a system-wide metric, so only its result it will be returned and not those of every user.
The metric is calculated as such:
\hfill\break
\hfill\break
    \[
        Gini_{sys} = \frac{\sum_i(2i - n - 1)x_i}{n\cdot\sum_i x_i}
    \]
\hfill\break

% gini index report end___

% PREDICTION COVERAGE REPORT start
\subsubsection{Prediction Coverage}\label{subsubsec:pred_cov}
The Prediction Coverage metric measures in percentage how many distinct items are being recommended in relation
to all available items.
It's a system wise metric, so only its result it will be returned and not those of every user.
The metric is calculated as such:
\hfill\break
\hfill\break
    \[
         Prediction Coverage_{sys} = (\frac{|I_p|}{|I|})\cdot100
    \]
\hfill\break

% prediction coverage report end___

% CATALOG COVERAGE REPORT start
\subsubsection{Catalog Coverage}\label{subsubsec:cat_cov}
The Catalog Coverage metric measures in percentage how many distinct items are being recommended in relation
to all available items.
It's a system-wide metric, so only its result it will be returned and not those of every user.
It differs from the Prediction Coverage since it allows for different parameters to come into play.
If no parameter is passed then it's a simple Prediction Coverage.
The metric is calculated as such:
\hfill\break
\hfill\break
    \[
         Catalog Coverage_{sys} = (\frac{|\bigcup_{j=1...N}reclist(u_j)|}{|I|})\cdot100
    \]
\hfill\break

% catalog coverage report end___

% <------- fairness metrics end ------>

% ------------------------------------------------ PLOT METRICS STARTED ----------------------------------------------
% LONG TAIL DISTIBUTION REPORT start
\subsubsection{Long Tail Distribution}\label{subsubsec:ltd}
This metric generates the Long Tail Distribution plot and saves it in the output directory with the file name
specified.
The plot can be generated both for the truth set or the predictions set based on the on parameter:
\begin{itemize}
    \item \textbf{on = 'truth'}: in this case the long tail distribution is useful to see which are the most popular items
       the most rated ones.
    \item \textbf{on = 'pred'}: in this case the long tail distribution is useful to see which are the most recommended
        items.
\end{itemize}

\hfill\break
For this experiment the Long Tail Distribution has been used with the following settings:
\begin{itemize}
    \item on: truth.
    \item format: png.
    \item overwrite: False.
\end{itemize}
\hfill\break

% LTD report end___


% PopRecsCorrelation REPORT start
\subsubsection{Pop Recs Correlation}\label{subsubsec:poprc}
This metric generates a plot which has as the X-axis the popularity of each item and as Y-axis the recommendation
frequency, so that it can be easily seen the correlation between popular niche items and how many times are being
recommended. \\
For this experiment the PopRecsCorrelation has been used with the following settings:
\begin{itemize}
    \item mode: both.
    \item format: png.
    \item overwrite: False.
\end{itemize}
\hfill\break

% PopRecsCorrelation report end___
% <------- plot metric end ----------->

% <---------------------- closing the only metric section ------------------------->



% In case of eval module is not used




% ------------------------------- RESULT OF PERFORMACE OF THE SYSTEM ON THE FOLD CentroidVector -------------------------------------
\subsection{Results for CentroidVector}\label{subsec:CentroidVector}
In this section we show the mean results with \textbf{ CentroidVector }, the metrics used during the experiment will be
grouped into the following table, that represent the results of the evaluation conducted~\ref{tab:results_table_CentroidVector}

\begin{center}
    \begin{tabular}{|c|c|}
        \hline
        \textbf{Metric} & \textbf{Value} \\ \hline
                CatalogCoverage (PredictionCov) & 16.67 \\ \hline
                F1 - macro & 1.0 \\ \hline
                F1@5 - micro & 1.0 \\ \hline
                Gini & 0.0 \\ \hline
                MAE & 3.0 \\ \hline
                MAP & 1.0 \\ \hline
                MRR & 1.0 \\ \hline
                MRR@5 & 1.0 \\ \hline
                MSE & 9.0 \\ \hline
                NDCG & 1.0 \\ \hline
                NDCG@5 & 1.0 \\ \hline
                Precision - macro & 1.0 \\ \hline
                Precision@5 - macro & 1.0 \\ \hline
                PredictionCoverage & 16.67 \\ \hline
                R-Precision - macro & 1.0 \\ \hline
                RMSE & 3.0 \\ \hline
                Recall - macro & 1.0 \\ \hline
                Recall@5 - macro & 1.0 \\ \hline
             \end{tabular}
    \captionsetup{type=table}
    \caption{Table of the results}
    \label{tab:results_table_CentroidVector}
\end{center}
\hfill\break
\hfill\break
% end table of performance with CentroidVector ___




% ------------------------------- RESULT OF PERFORMACE OF THE SYSTEM ON THE FOLD LinearPredictor -------------------------------------
\subsection{Results for LinearPredictor}\label{subsec:LinearPredictor}
In this section we show the mean results with \textbf{ LinearPredictor }, the metrics used during the experiment will be
grouped into the following table, that represent the results of the evaluation conducted~\ref{tab:results_table_LinearPredictor}

\begin{center}
    \begin{tabular}{|c|c|}
        \hline
        \textbf{Metric} & \textbf{Value} \\ \hline
                CatalogCoverage (PredictionCov) & 50.0 \\ \hline
                F1 - macro & 0.91667 \\ \hline
                F1@5 - micro & 0.92857 \\ \hline
                Gini & 0.0 \\ \hline
                MAE & 0.76165 \\ \hline
                MAP & 1.0 \\ \hline
                MRR & 1.0 \\ \hline
                MRR@5 & 1.0 \\ \hline
                MSE & 0.6997 \\ \hline
                NDCG & 1.0 \\ \hline
                NDCG@5 & 1.0 \\ \hline
                Precision - macro & 0.875 \\ \hline
                Precision@5 - macro & 0.875 \\ \hline
                PredictionCoverage & 50.0 \\ \hline
                R-Precision - macro & 1.0 \\ \hline
                RMSE & 0.81343 \\ \hline
                Recall - macro & 1.0 \\ \hline
                Recall@5 - macro & 1.0 \\ \hline
             \end{tabular}
    \captionsetup{type=table}
    \caption{Table of the results}
    \label{tab:results_table_LinearPredictor}
\end{center}
\hfill\break
\hfill\break
% end table of performance with LinearPredictor ___




% ------------------------------- RESULT OF PERFORMACE OF THE SYSTEM ON THE FOLD IndexQuery -------------------------------------
\subsection{Results for IndexQuery}\label{subsec:IndexQuery}
In this section we show the mean results with \textbf{ IndexQuery }, the metrics used during the experiment will be
grouped into the following table, that represent the results of the evaluation conducted~\ref{tab:results_table_IndexQuery}

\begin{center}
    \begin{tabular}{|c|c|}
        \hline
        \textbf{Metric} & \textbf{Value} \\ \hline
                CatalogCoverage (PredictionCov) & 16.67 \\ \hline
                F1 - macro & 1.0 \\ \hline
                F1@5 - micro & 1.0 \\ \hline
                Gini & 0.0 \\ \hline
                MAE & 63.65394 \\ \hline
                MAP & 1.0 \\ \hline
                MRR & 1.0 \\ \hline
                MRR@5 & 1.0 \\ \hline
                MSE & 4051.82448 \\ \hline
                NDCG & 1.0 \\ \hline
                NDCG@5 & 1.0 \\ \hline
                Precision - macro & 1.0 \\ \hline
                Precision@5 - macro & 1.0 \\ \hline
                PredictionCoverage & 16.67 \\ \hline
                R-Precision - macro & 1.0 \\ \hline
                RMSE & 63.65394 \\ \hline
                Recall - macro & 1.0 \\ \hline
                Recall@5 - macro & 1.0 \\ \hline
             \end{tabular}
    \captionsetup{type=table}
    \caption{Table of the results}
    \label{tab:results_table_IndexQuery}
\end{center}
\hfill\break
\hfill\break
% end table of performance with IndexQuery ___




% ------------------------------- RESULT OF PERFORMACE OF THE SYSTEM ON THE FOLD ClassifierRecommender -------------------------------------
\subsection{Results for ClassifierRecommender}\label{subsec:ClassifierRecommender}
In this section we show the mean results with \textbf{ ClassifierRecommender }, the metrics used during the experiment will be
grouped into the following table, that represent the results of the evaluation conducted~\ref{tab:results_table_ClassifierRecommender}

\begin{center}
    \begin{tabular}{|c|c|}
        \hline
        \textbf{Metric} & \textbf{Value} \\ \hline
                CatalogCoverage (PredictionCov) & 16.67 \\ \hline
                F1 - macro & 1.0 \\ \hline
                F1@5 - micro & 1.0 \\ \hline
                Gini & 0.0 \\ \hline
                MAE & 1.5 \\ \hline
                MAP & 1.0 \\ \hline
                MRR & 1.0 \\ \hline
                MRR@5 & 1.0 \\ \hline
                MSE & 2.25 \\ \hline
                NDCG & 1.0 \\ \hline
                NDCG@5 & 1.0 \\ \hline
                Precision - macro & 1.0 \\ \hline
                Precision@5 - macro & 1.0 \\ \hline
                PredictionCoverage & 16.67 \\ \hline
                R-Precision - macro & 1.0 \\ \hline
                RMSE & 1.5 \\ \hline
                Recall - macro & 1.0 \\ \hline
                Recall@5 - macro & 1.0 \\ \hline
             \end{tabular}
    \captionsetup{type=table}
    \caption{Table of the results}
    \label{tab:results_table_ClassifierRecommender}
\end{center}
\hfill\break
\hfill\break
% end table of performance with ClassifierRecommender ___




% ------------------------------- RESULT OF PERFORMACE OF THE SYSTEM ON THE FOLD AmarDoubleSource -------------------------------------
\subsection{Results for AmarDoubleSource}\label{subsec:AmarDoubleSource}
In this section we show the mean results with \textbf{ AmarDoubleSource }, the metrics used during the experiment will be
grouped into the following table, that represent the results of the evaluation conducted~\ref{tab:results_table_AmarDoubleSource}

\begin{center}
    \begin{tabular}{|c|c|}
        \hline
        \textbf{Metric} & \textbf{Value} \\ \hline
                CatalogCoverage (PredictionCov) & 50.0 \\ \hline
                F1 - macro & 0.91667 \\ \hline
                F1@5 - micro & 0.92857 \\ \hline
                Gini & 0.0 \\ \hline
                MAE & 2.16423 \\ \hline
                MAP & 1.0 \\ \hline
                MRR & 1.0 \\ \hline
                MRR@5 & 1.0 \\ \hline
                MSE & 5.67578 \\ \hline
                NDCG & 1.0 \\ \hline
                NDCG@5 & 1.0 \\ \hline
                Precision - macro & 0.875 \\ \hline
                Precision@5 - macro & 0.875 \\ \hline
                PredictionCoverage & 50.0 \\ \hline
                R-Precision - macro & 1.0 \\ \hline
                RMSE & 2.19062 \\ \hline
                Recall - macro & 1.0 \\ \hline
                Recall@5 - macro & 1.0 \\ \hline
             \end{tabular}
    \captionsetup{type=table}
    \caption{Table of the results}
    \label{tab:results_table_AmarDoubleSource}
\end{center}
\hfill\break
\hfill\break
% end table of performance with AmarDoubleSource ___



\section{Algorithms comparison}\label{sec:comparison}
In the following table it's showed a comparison between the algorithms used in these experiments
and their performance are highlighted base on the best result obtained.
In bold the best result for each of the evaluating metrics used, despite underline will be the second-best result
obtained on each metrics and an asterisk netx to the result of the metric means that the result obtained is statistic
relevant based on statistical tests conducted:

\hfill\break
\begin{table}[H]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{@{}c *{3}{p{3.0cm}}@{}}
\toprule
\multirow{2}{*}{Algorithms} & \multicolumn{3}{c}{Columns} \\
\cmidrule{2-4}
& \multirow{2}{*}{\makecell{Precision - macro}} & \multirow{2}{*}{\makecell{Recall - macro}} & \multirow{2}{*}{\makecell{F1 - macro}} \\
\addlinespace[12pt]
\cmidrule{2-4}
 & & & \\
CentroidVector & 0.58 & 0.69 & 0.71 \\
\addlinespace[5pt]
\midrule
LinearPredictor & 0.375* & 0.421 & 0.517* \\
\addlinespace[5pt]
\midrule
IndexQuery & \textbf{0.99}* & \underline{0.985} & \textbf{0.979}* \\
\addlinespace[5pt]
\midrule
ClassifierRecommender & 0.723* & 0.792 & 0.799 \\
\addlinespace[5pt]
\midrule
AmarDoubleSource & \underline{0.875}* & \textbf{1.0} & \underline{0.917} \\
\addlinespace[5pt]
\midrule
\bottomrule
\end{tabular}}
\caption{Comparison between algorithms (Part 1)}
\end{table}
\vspace{10pt}
\begin{table}[H]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{@{}c *{3}{p{3.0cm}}@{}}
\toprule
\multirow{2}{*}{Algorithms} & \multicolumn{3}{c}{Columns} \\
\cmidrule{2-4}
& \multirow{2}{*}{\makecell{Gini}} & \multirow{2}{*}{\makecell{NDCG}} & \multirow{2}{*}{\makecell{R-Precision - macro}} \\
\addlinespace[12pt]
\cmidrule{2-4}
 & & & \\
CentroidVector & 8.0 & \underline{0.95} & 0.853 \\
\addlinespace[5pt]
\midrule
LinearPredictor & 35.0* & 0.521 & 0.423 \\
\addlinespace[5pt]
\midrule
IndexQuery & \underline{0.9}* & 0.91 & \underline{0.965} \\
\addlinespace[5pt]
\midrule
ClassifierRecommender & 11.0* & 0.823 & 0.811 \\
\addlinespace[5pt]
\midrule
AmarDoubleSource & \textbf{0.0}* & \textbf{1.0} & \textbf{1.0} \\
\addlinespace[5pt]
\midrule
\bottomrule
\end{tabular}}
\caption{Comparison between algorithms (Part 2)}
\end{table}
\vspace{10pt}
\begin{table}[H]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{@{}c *{3}{p{3.0cm}}@{}}
\toprule
\multirow{2}{*}{Algorithms} & \multicolumn{3}{c}{Columns} \\
\cmidrule{2-4}
& \multirow{2}{*}{\makecell{RMSE}} & \multirow{2}{*}{\makecell{MSE}} & \multirow{2}{*}{\makecell{MAE}} \\
\addlinespace[12pt]
\cmidrule{2-4}
 & & & \\
CentroidVector & \underline{3.0} & \underline{9.0} & \underline{3.0} \\
\addlinespace[5pt]
\midrule
LinearPredictor & 120.813 & 1540.7* & 2760.762* \\
\addlinespace[5pt]
\midrule
IndexQuery & 43.654 & 51.824* & 3.654* \\
\addlinespace[5pt]
\midrule
ClassifierRecommender & 13.5 & 26.25* & 12.5* \\
\addlinespace[5pt]
\midrule
AmarDoubleSource & \textbf{2.191} & \textbf{5.676}* & \textbf{2.164}* \\
\addlinespace[5pt]
\midrule
\bottomrule
\end{tabular}}
\caption{Comparison between algorithms (Part 3)}
\end{table}
\vspace{10pt}
\begin{table}[H]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{@{}c *{3}{p{3.0cm}}@{}}
\toprule
\multirow{2}{*}{Algorithms} & \multicolumn{3}{c}{Columns} \\
\cmidrule{2-4}
& \multirow{2}{*}{\makecell{MRR}} & \multirow{2}{*}{\makecell{MAP}} & \multirow{2}{*}{\makecell{PredictionCoverage}} \\
\addlinespace[12pt]
\cmidrule{2-4}
 & & & \\
CentroidVector & 0.699 & \underline{0.981} & 16.67 \\
\addlinespace[5pt]
\midrule
LinearPredictor & 0.543 & 0.477 & \underline{150.0} \\
\addlinespace[5pt]
\midrule
IndexQuery & \textbf{1.0} & 0.9 & 36.67 \\
\addlinespace[5pt]
\midrule
ClassifierRecommender & \underline{0.75} & 0.69 & \textbf{169.67} \\
\addlinespace[5pt]
\midrule
AmarDoubleSource & \textbf{1.0} & \textbf{1.0} & 50.0 \\
\addlinespace[5pt]
\midrule
\bottomrule
\end{tabular}}
\caption{Comparison between algorithms (Part 4)}
\end{table}
\vspace{10pt}
\begin{table}[H]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{@{}c *{3}{p{3.0cm}}@{}}
\toprule
\multirow{2}{*}{Algorithms} & \multicolumn{3}{c}{Columns} \\
\cmidrule{2-4}
& \multirow{2}{*}{\makecell{Precision@5 - macro}} & \multirow{2}{*}{\makecell{Recall@5 - macro}} & \multirow{2}{*}{\makecell{F1@5 - micro}} \\
\addlinespace[12pt]
\cmidrule{2-4}
 & & & \\
CentroidVector & \textbf{1.0} & \textbf{1.0} & 0.877 \\
\addlinespace[5pt]
\midrule
LinearPredictor & 0.475* & 0.399* & 0.529 \\
\addlinespace[5pt]
\midrule
IndexQuery & \underline{0.93}* & \underline{0.92}* & \textbf{0.998} \\
\addlinespace[5pt]
\midrule
ClassifierRecommender & 0.771* & 0.721* & 0.789 \\
\addlinespace[5pt]
\midrule
AmarDoubleSource & 0.875* & \textbf{1.0}* & \underline{0.929} \\
\addlinespace[5pt]
\midrule
\bottomrule
\end{tabular}}
\caption{Comparison between algorithms (Part 5)}
\end{table}
\vspace{10pt}
\begin{table}[H]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{@{}c *{2}{p{3.0cm}}@{}}
\toprule
\multirow{2}{*}{Algorithms} & \multicolumn{2}{c}{Columns} \\
\cmidrule{2-3}
& \multirow{2}{*}{\makecell{MRR@5}} & \multirow{2}{*}{\makecell{NDCG@5}} \\
\addlinespace[12pt]
\cmidrule{2-3}
 & & & \\
CentroidVector & 0.877 & 0.811 \\
\addlinespace[5pt]
\midrule
LinearPredictor & 0.444 & 0.454 \\
\addlinespace[5pt]
\midrule
IndexQuery & \underline{0.925} & \underline{0.965} \\
\addlinespace[5pt]
\midrule
ClassifierRecommender & 0.688 & 0.698 \\
\addlinespace[5pt]
\midrule
AmarDoubleSource & \textbf{1.0} & \textbf{1.0} \\
\addlinespace[5pt]
\midrule
\bottomrule
\end{tabular}}
\caption{Comparison between algorithms (Part 6)}
\end{table}



\subsection{Statistic relevance}\label{subsec:stas_rel}
During the experiment some statistic test are been performed and the following table show the result
of the level of p-value used for each metric, this result have been used to mark with an asterisk
the result obtained in the previous table that are statistical relevant.

\hfill\break
\begin{table}[H]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{lrrr}
\toprule
{} & Precision - macro & Recall@5 - macro & Precision@5 - macro \\
{} &            pvalue &           pvalue &              pvalue \\
sys\_pair                                  &                   &                  &                     \\
\midrule
(CentroidVector, LinearPredictor)         &     9.505701e-281 &         0.002248 &       1.022519e-100 \\
(CentroidVector, IndexQuery)              &     2.835562e-276 &         0.002316 &        1.510916e-96 \\
(LinearPredictor, IndexQuery)             &      6.955147e-01 &         0.994123 &        6.314952e-01 \\
(CentroidVector, ClassifierRecommender)   &      1.230000e-03 &         0.023000 &        9.083649e-01 \\
(CentroidVector, AmarDoubleSource)        &      5.000000e-01 &         0.876350 &        2.276869e-01 \\
(LinearPredictor, ClassifierRecommender)  &      1.570000e+00 &         1.450000 &        2.800000e+00 \\
(LinearPredictor, AmarDoubleSource)       &      1.500000e+00 &         1.200000 &        4.530000e+00 \\
(IndexQuery, ClassifierRecommender)       &      2.470000e-02 &         0.063215 &        8.374560e-02 \\
(IndexQuery, AmarDoubleSource)            &      8.800000e-01 &         0.445425 &        8.745600e-01 \\
(ClassifierRecommender, AmarDoubleSource) &      3.200000e-03 &         0.120970 &        3.487561e-03 \\
\bottomrule
\end{tabular}
}
\caption{relevance table - Precision - macro, Recall@5 - macro, Precision@5 - macro}
\end{table}

\begin{table}[H]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{lrrr}
\toprule
{} &    F1 - macro &           MSE &           MAE \\
{} &        pvalue &        pvalue &        pvalue \\
sys\_pair                                  &               &               &               \\
\midrule
(CentroidVector, LinearPredictor)         &  1.744839e-50 &  1.902252e-99 &  1.174484e-49 \\
(CentroidVector, IndexQuery)              &  2.417581e-50 &  1.451092e-95 &  2.417581e-50 \\
(LinearPredictor, IndexQuery)             &  9.931930e-01 &  7.631495e+00 &  6.993193e+00 \\
(CentroidVector, ClassifierRecommender)   &  1.872534e+00 &  2.346754e-04 &  7.893245e-03 \\
(CentroidVector, AmarDoubleSource)        &  2.876354e+00 &  7.653400e-03 &  8.725340e-03 \\
(LinearPredictor, ClassifierRecommender)  &  5.893746e+00 &  7.635400e-03 &  9.872356e-04 \\
(LinearPredictor, AmarDoubleSource)       &  3.129735e+00 &  8.653400e-03 &  2.348220e-03 \\
(IndexQuery, ClassifierRecommender)       &  6.753465e-01 &  1.023646e+00 &  2.863500e-04 \\
(IndexQuery, AmarDoubleSource)            &  9.734000e-03 &  9.864500e-01 &  9.735676e-03 \\
(ClassifierRecommender, AmarDoubleSource) &  8.972355e-03 &  7.655000e-01 &  1.238896e-01 \\
\bottomrule
\end{tabular}
}
\caption{relevance table - F1 - macro, MSE, MAE}
\end{table}

\begin{table}[H]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{lrr}
\toprule
{} &           Gini &  R-Precision - macro \\
{} &         pvalue &               pvalue \\
sys\_pair                                  &                &                      \\
\midrule
(CentroidVector, LinearPredictor)         &  5.505701e-281 &            12.002248 \\
(CentroidVector, IndexQuery)              &  3.835562e-276 &             7.002316 \\
(LinearPredictor, IndexQuery)             &   1.695515e+00 &             3.994123 \\
(CentroidVector, ClassifierRecommender)   &   2.364000e-04 &             3.978640 \\
(CentroidVector, AmarDoubleSource)        &   7.623410e-03 &             1.872534 \\
(LinearPredictor, ClassifierRecommender)  &   9.888000e-01 &             0.665710 \\
(LinearPredictor, AmarDoubleSource)       &   1.232300e+00 &             0.222188 \\
(IndexQuery, ClassifierRecommender)       &   1.564200e+00 &             0.861540 \\
(IndexQuery, AmarDoubleSource)            &   3.261000e-02 &             0.815810 \\
(ClassifierRecommender, AmarDoubleSource) &   1.152672e-02 &             0.122311 \\
\bottomrule
\end{tabular}
}
\caption{relevance table - Gini,  R-Precision - macro}
\end{table}


\begin{table}[H]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{lr}
\toprule
\textbf{Metrics} & \textbf{(CentroidVector, IndexQuery)} \\
\midrule
  Precision - macro - statistic & -35.51548 \\
  Precision - macro - pvalue & 0.0 \\
  Recall@5 - macro - statistic & -3.04635 \\
  Recall@5 - macro - pvalue & 0.00232 \\
  Precision@5 - macro - statistic & -20.85042 \\
  Precision@5 - macro - pvalue & 0.0 \\
  F1 - macro - statistic & -14.92069 \\
  F1 - macro - pvalue & 0.0 \\
  MSE - statistic & -29.85042 \\
  MSE - pvalue & 0.0 \\
  MAE - statistic & -24.92069 \\
  MAE - pvalue & 0.0 \\
  Gini - statistic & -15.51548 \\
  Gini - pvalue & 0.0 \\
   R-Precision - macro - statistic & -3.04635 \\
   R-Precision - macro - pvalue & 7.00232 \\
\bottomrule
\end{tabular}%
}
\caption{CentroidVector and IndexQuery}
\end{table}



\section{Conclusion on the experiment}\label{sec:conclution}
This part is for conclusion to be sum up as needed.
\hfill\break
\hfill\break

% ------------------- END OF THE REPORT COMPLETED ---------------
% closing the document
\end{document}


