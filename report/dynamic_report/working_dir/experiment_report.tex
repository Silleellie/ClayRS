
% Created by Diego Miccoli

% Preamble
\documentclass[11pt]{article}
% [twocolumn]

\usepackage{graphicx} % Required for inserting images
\usepackage{amsmath}
\usepackage{verbatim}
\usepackage{hyperref}
\usepackage{comment}
\usepackage{caption}
\usepackage{url}
\usepackage{listings}
\usepackage{float}
\usepackage{array}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{geometry}
\usepackage{changepage}
\usepackage{longtable}
\usepackage{soul}

\usepackage{background}

\backgroundsetup{
    scale=1,
    angle=0,
    opacity=1,
    color=black,
    position=current page.north east,
    vshift=-0.3cm, % Regola questa dimensione per posizionare il testo verticalmente
    hshift=-3.5cm, % Regola questa dimensione per posizionare il testo orizzontalmente
    contents={%
        Aldo Moro University of studies of Bari
    }
}

\geometry{top=1cm, bottom=1cm}

\title{\textbf{ ClayRS in practice: Experimental scenarios }\\ [1cm] Department of Computer Science}
\author{ SWAP research group UniBa \thanks{Experiment conduct by Diego Miccoli}}
\date{21-02-2024}

\begin{document}

\maketitle

\section{Introduction}\label{sec:intro}
Recommender Systems (RS) are designed to assist users in various decision-making tasks by acquiring
information about their needs, interests, and preferences in order to personalize the user experience
based on such data.
These systems are based on paradigms that allow managing user information and processing
it to provide decision support.
In particular, two successful paradigms are Collaborative Filtering and Content-based Filtering.
In this document, we will present the results obtained using a recommendation system called \textbf{ClayRS},
a framework developed by the SWAP research group at the Department of Computer Science of the University of Bari.\\
\hfill\break

\textit{\ul{The following report has been automatically generated from YAML configuration files.}}

\hfill\break




% ----------------------------------------- OPENING CONTENT ANALYZER SECTION -----------------------------------------
\section{Content Analyzer Module}\label{sec:ca}
The content analyzer module will deal with raw source document or more in general data which could be
video or audio data and give a representation of these data which will be used by the other two module.
The text data source could be represented with exogenous technique or with a specified representation
and each field given could be treated with preprocessing techniques and postprocessing technique.
In this experiment the following techniques have been used on specific field in order to achieve the
representation wanted:
\hfill\break
\hfill\break
% --- TECNIQUE USED TO REPPRESENT DATA FIELD ---


\begin{itemize}
    \item \textbf{fields used}:  idx0, plot0, plot1, plot10, plot11, plot2, plot3, plot4, plot5, plot6, plot7, plot8, plot9, video\_path0, video\_path1, video\_path2, video\_path3, video\_path4

    \item \textbf{fields representation}:  FromNPY, OriginalData, WhooshTfIdf, SkLearnTfIdf, WordEmbeddingTechnique, SentenceEmbeddingTechnique, DocumentEmbeddingTechnique, Word2SentenceEmbedding, Word2DocEmbedding, Sentence2DocEmbedding, PyWSDSynsetDocumentFrequency, SkImageHogDescriptor, MFCC, VGGISH, PytorchImageModels, TorchVisionVideoModels

    \item \textbf{fields preprocessing}:  Spacy, Ekphrasis, NLTK, TorchUniformTemporalSubSampler, TorchResample, ConvertToMono, Normalize, Lambda, Resize, CenterCrop, ClipSampler

    \item \textbf{fields postprocessing}:  FVGMM, VLADGMM, SkLearnPCA
\end{itemize}





\hfill\break
% subsection of Dataset identification
\subsection{Dataset used and its statistics}
The dataset used for the experiment is  \lstinline[style=verbatim-text]| 1000K data video movie |,
its source file is \lstinline[style=verbatim-text]| items_extra_small.json | ,and the content used during the experiment:
     movielens\_id,
.


% --- DATA STATS ---
The statistics of the dataset used are reported in the following table:~\ref{tab:dataset_table}:
\begin{table}[ht]
    \centering
  \begin{tabular}{|c|c|}
    \hline
    \textbf{Parameter}& \textbf{Value} \\ \hline
    n\_users  & 2\\ \hline
    n\_items  & 6\\ \hline
    total\_interactions  & 6\\ \hline
    min\_score  & 1.0\\ \hline
    max\_score  & 4.0\\ \hline
    mean\_score  & 2.66667\\ \hline
    sparsity  & 0.5\\ \hline
  \end{tabular}
   \caption{Stats on the dataset}\label{tab:dataset_table}
\end{table}

\hfill\break



% ------------------------------ START SUBSECTION OF PARTITIONING OF RECSYS --------------------------------------------
% subsection of the splitting technique used, referred to partition protocol
\subsection{Data splitting technique}\label{subsec:partitioning}
% KFOLD PARTITIONING TECNIQUE
K-fold cross-validation is a technique used in machine learning to assess the performance of a predictive model.
The basic idea is to divide the dataset into K subsets, or folds.
The model is then trained K times, each time using K-1 folds for training and the remaining fold for validation.
This process is repeated K times, with a different fold used as the validation set in each iteration.
\hfill\break
The KFoldPartitioning has been used with the following setting:
\hfill\break
The data has been shuffled before being split into batches.
The partitioning technique has been executed with the following settings:
\begin{itemize}
    \item number of splits: 2
    \item shuffle: True
    \item random state: None
    \item skip user error: True
\end{itemize}
\hfill\break
% KFOLD PARTITIONING TECNIQUE ended

% end partitioning section___________


% ----------------------------------------- START RECSYS MODULE ------------------------------------------------------
\section{Recommender System}\label{sec:recsys}
The RecSys module allows to instantiate a recommender system in order to work on items and users serialized
the Content Analyzer module, and its aim is to be trained on the data used in order to produce recommendation.
In this experiment the following are the algorithm used: \\


    \lstinline[style=verbatim-text]| - CentroidVector |  \\


    \lstinline[style=verbatim-text]| - LinearPredictor |  \\


    \lstinline[style=verbatim-text]| - IndexQuery |  \\


    \lstinline[style=verbatim-text]| - ClassifierRecommender |  \\


    \lstinline[style=verbatim-text]| - AmarDoubleSource |  \\



% -------------------------------------- OPENING THE EVALUATION MODULE SECTION ---------------------------------------
\section{Evaluation Module}\label{sec:eva-module}
The \textbf{EvalModel} which is the abbreviation for Evaluation Model has the task of evaluating a recommender system,
using several state-of-the-art metrics, this allows to compare different recommender system and different algorithm of
recommendation and find out which are the strength points and which the weak ones.

\subsection{Metrics}\label{subsec:metrics}
% --- Metrics ---
During the experiment a bunch of formal metrics have been performed on the recommendation produced in order to evaluate
the performance of the system.
The metrics used are the followings:
\hfill\break
\hfill\break

% ---------------------------------------- CLASSIFICATION METRICS STARTED --------------------------------------------
% Precision report start
\subsubsection{Precision}\label{subsubsec:precision}
In ClayRS, the Precision metric is calculated as such for the \textbf{single user}:
\hfill\break
\hfill\break
    \[
        \begin{gathered}
         Precision_u = \frac{tp_u}{tp_u + fp_u}
        \end{gathered}
    \]
\hfill\break
\hfill\break
    Where:
\begin{itemize}
    \item $tp_u$ is the number of items which are in the recommendation list of the user and have a
       $\geq$         \textbf{2.66667}.
            \item $fp_u$ is the number of items which are in the recommendation list of the user and have a
      rating $<$         \textbf{no relevant threshold used}.
        \end{itemize}
\hfill\break
\hfill\break
In ClayRS, Precision needs those parameters:
the \textbf{relevant\_threshold}, is a parameter needed to discern relevant items and non-relevant items for every user.
If not specified, the mean rating score of every user will be used, in this experiment it has been set to
\textbf{None}.
\hfill\break
\hfill\break
% precision report ended___

% Recall report start
\subsubsection{Recall}\label{subsubsec:recall}
The Recall metric is calculated as such for the \textbf{single user}:
\hfill\break
\hfill\break
    \[
       \begin{gathered}
           Recall_u = \frac{tp_u}{tp_u + fn_u}
       \end{gathered}
    \]
\hfill\break
\hfill\break
    Where:
\begin{itemize}
    \item $tp_u$ is the number of items which are in the recommendation list of the user and have a
      rating $>=$         \textbf{2.66667}.
            \item $fn_u$ is the number of items which are not in the recommendation list of the user and have a
      rating $>=$         \textbf{no relevant threshold used}.
        \end{itemize}
\hfill\break
\hfill\break
In ClayRS, Recall needs those parameters:
the \textbf{relevant\_threshold}, is a parameter needed to discern relevant items and non-relevant items for every user.
If not specified, the mean rating score of every user will be used, in this experiment it has been set to
\textbf{None}.
\hfill\break
\hfill\break
% recall report end___

%FMeasure report start
\subsubsection{FMeasure}\label{subsubsec:f-meas}
The FMeasure metric combines Precision and Recall into a single metric.
It is calculated as such for the \textbf{single user}:
\hfill\break
\hfill\break
    \[
       \begin{gathered}
           FMeasure_u = (1 + \beta^2) \cdot \frac{P_u \cdot R_u}{(\beta^2 \cdot P_u) + R_u}
       \end{gathered}
    \]
\hfill\break
\hfill\break
    Where:
\begin{itemize}
    \item $P_u$ is the Precision calculated for the user \textbf{u}.
    \item $R_u$ is the Recall calculated for the user \textbf{u}.
    \item $\beta$ is a real factor which could weight differently Recall or Precision based on its value:
    \begin{itemize}
        \item $\beta = 1$: Equally weight Precision and Recall.
        \item $\beta > 1$: Weight Recall more.
        \item $\beta < 1$: Weight Precision more.
    \end{itemize}
\end{itemize}
\hfill\break
\hfill\break
A famous FMeasure is the F1 Metric, where $\beta = 1$, which basically is the harmonic mean of recall and
precision:
\hfill\break
\hfill\break
    \[
       \begin{gathered}
           F1_u = \frac{2 \cdot P_u \cdot R_u}{P_u + R_u}
       \end:{gathered}
    \]
\hfill\break
\hfill\break
The FMeasure metric is calculated as such for the entire system, depending on if \textbf{macro} average or
\textbf{micro} average has been chosen:
\hfill\break
\hfill\break
    \[
       \begin{gathered}
           FMeasure_{sys} - micro = (1 + \beta^2) \cdot \frac{P_u \cdot R_u}{(\beta^2 \cdot P_u) + R_u}
       \end{gathered}
    \]
\hfill\break
\hfill\break
    \[
      \begin{gathered}
          FMeasure_{sys} - macro = \frac{\sum_{u \in U} FMeasure_u}{|U|}
      \end{gathered}
    \]
\hfill\break
\hfill\break
During the experiment the FMeasure has been calculated with $\beta = $
1 and the relevant threshold is
\textbf{None}.
\hfill\break
\hfill\break
% FMeasure report end___

% PRECISION @K REPORT start
\subsubsection{Precision@K}\label{subsubsec:prec-k}
The Precision@K metric is calculated as such for the \textbf{single user}:
\hfill\break
\hfill\break
    \[
      \begin{gathered}
          Precision@K_u = \frac{tp@K_u}{tp@K_u + fp@K_u}
      \end{gathered}
    \]
\hfill\break
\hfill\break
    Where:
\begin{itemize}
    \item $tp@K_u$ is the number of items which are in the recommendation list of the user
      cutoff to the first K items and have a rating $>=$ relevant threshold in its ground truth.
    \item $tp@K_u$ is the number of items which are in the recommendation list of the user
      cutoff to the first K items** and have a rating $<$ relevant threshold in its ground truth.
\end{itemize}
\hfill\break
\hfill\break
And it is calculated as such for the entire system, depending on if \textbf{macro} average or \textbf{micro} average
has been chosen:
\hfill\break
\hfill\break
   \[
      \begin{gathered}
          Precision@K_{sys} - micro = \frac{\sum_{u \in U} tp@K_u}{\sum_{u \in U} tp@K_u + \sum_{u \in U} fp@K_u}
      \end{gathered}
   \]
\hfill\break
\hfill\break
    \[
      \begin{gathered}
          Precision@K_{sys} - macro = \frac{\sum_{u \in U} Precision@K_u}{|U|}
      \end{gathered}
    \]
\hfill\break
\hfill\break
During the experiment Precision@K has been used with the following settings:
\begin{itemize}
    \item \textbf{K: 5 }
    \item \textbf{relevant threshold:None }
    \item \textbf{sys average: macro }
\end{itemize}
\hfill\break
\hfill\break
% precision@k report end___

% RECALL @K REPORT start
\subsubsection{Recall@K}\label{subsubsec:rec-k}
The Recall@K metric is calculated as such for the \textbf{single user}:
\hfill\break
\hfill\break
    \[
       \begin{gathered}
           Recall@K_u = \frac{tp@K_u}{tp@K_u + fn@K_u}
       \end{gathered}
    \]
\hfill\break
\hfill\break
    Where:
\begin{itemize}
    \item $tp@K_u$ is the number of items which are in the recommendation list of the user
      \textbf{cutoff to the first K items} and have a rating $>=$ relevant threshold in its ground truth
    \item $tp@K_u$ is the number of items which are NOT in the recommendation list of the user
      \textbf{cutoff to the first K items} and have a rating $>=$ relevant threshold in its ground truth
\end{itemize}
\hfill\break
\hfill\break
And it is calculated as such for the entire system, depending on if \textbf{macro} average or \textbf{micro} average
has been chosen:
\hfill\break
\hfill\break
    \[
       \begin{gathered}
           Recall@K_{sys} - micro = \frac{\sum_{u \in U} tp@K_u}{\sum_{u \in U} tp@K_u + \sum_{u \in U} fn@K_u}
       \end{gathered}
    \]
\hfill\break
\hfill\break
    \[
      \begin{gathered}
          ecall@K_{sys} - macro = \frac{\sum_{u \in U} Recall@K_u}{|U|}
      \end{gathered}
    \]
\hfill\break
\hfill\break
During the experiment Recall@K has been used with the following settings:
\begin{itemize}
    \item \textbf{K: 5 }
    \item \textbf{relevant threshold: None }
    \item \textbf{sys average: macro }
\end{itemize}
\hfill\break
\hfill\break
% recall@k report end___

% FMEASURE @K REPORT start
\subsubsection{FMeasure@K}\label{subsubsec:f-meas-k}
The FMeasure@K metric combines Precision@K and Recall@K into a single metric.
It is calculated as such for the \textbf{single user}:
\hfill\break
\hfill\break
    \[
       \begin{gathered}
           FMeasure@K_u = (1 + \beta^2) \cdot \frac{P@K_u \cdot R@K_u}{(\beta^2 \cdot P@K_u) + R@K_u}
       \end{gathered}
    \]
\hfill\break
\hfill\break
    Where:
\begin{itemize}
    \item $P@K_u$ is the Precision at K calculated for the user \textbf{u}.
    \item $R@K_u$ is the Recall at K calculated for the user \textbf{u}.
    \item $\beta$ is a real factor which could weight differently Recall or Precision based on its value:
    \begin{itemize}
        \item $\beta = 1$: Equally weight Precision and Recall.
        \item $\beta > 1$: Weight Recall more.
        \item $\beta < 1$: Weight Precision more.
    \end{itemize}
\end{itemize}
\hfill\break
\hfill\break
A famous FMeasure@K is the F1@K Metric, where $\beta = 1$, which basically is the harmonic mean of recall and
precision:
\hfill\break
\hfill\break
    \[
      \begin{gathered}
          F1@K_u = \frac{2 \cdot P@K_u \cdot R@K_u}{P@K_u + R@K_u}
      \end{gathered}
    \]
\hfill\break
\hfill\break
The FMeasure@K metric is calculated as such for the entire system, depending on if \textbf{macro} average or
\textbf{micro} average has been chosen:
\hfill\break
\hfill\break
    \[
      \begin{gathered}
          FMeasure@K_{sys} - micro = (1 + \beta^2) \cdot \frac{P@K_u \cdot R@K_u}{(\beta^2 \cdot P@K_u) + R@K_u}
      \end{gathered}
    \]
\hfill\break
\hfill\break
    \[
       \begin{gathered}
           FMeasure@K_{sys} - macro = \frac{\sum_{u \in U} FMeasure@K_u}{|U|}
       \end{gathered}
    \]
\hfill\break
\hfill\break
During the experiment FMeasure@K has been used with the following settings:
\begin{itemize}
    \item \textbf{K: 5}
    \item  \textbf{$\beta$: 1}
    \item \textbf{relevant threshold: None }
    \item \textbf{sys average: micro }
\end{itemize}
\hfill\break
\hfill\break
% fmeasure@k report end___

% R-PRECISION REPORT start
\subsubsection{R-Precision}\label{subsubsec:r-prec}
The R-Precision metric is calculated as such for the \textbf{single user}:
\hfill\break
\hfill\break
    \[
       \begin{gathered}
           R-Precision_u = \frac{tp@R_u}{tp@R_u + fp@R_u}
       \end{gathered}
    \]
\hfill\break
\hfill\break
    Where:
\begin{itemize}
    \item $R$ it's the number of relevant items for the user \textbf{u}.
    \item $tp@R_u$ is the number of items in the recommendation list of the user, up to the first   $R$ items,
        that have a rating $\geq$ the relevant threshold in its ground truth.
    \item $tp@R_u$ is the number of items in the recommendation list of the user, up to the first $R$ items,
        that have a rating $<$ \texttt{relevant\_threshold} in their ground truth.
\end{itemize}
\hfill\break
\hfill\break
And it is calculated as such for the entire system, depending on if \textbf{macro} average or \textbf{micro} average
has been chosen:
\hfill\break
\hfill\break
    \[
       \begin{gathered}
           Precision@R_{sys} - micro = \frac{\sum_{u \in U} tp@R_u}{\sum_{u \in U} tp@R_u + \sum_{u \in U} fp@R_u}
       \end{gathered}
    \]
\hfill\break
\hfill\break
    \[
       \begin{gathered}
           Precision@R_{sys} - macro = \frac{\sum_{u \in U} R-Precision_u}{|U|}
       \end{gathered}
    \]
\hfill\break
\hfill\break
During the experiment R-Precision has been used with the following settings:
\begin{itemize}
    \item \textbf{relevant threshold:None }
    \item \textbf{sys average: macro }
\end{itemize}
\hfill\break
\hfill\break
% r-precision report end___
% <----- classification metrics end ------->

% -------------------------------------------- ERROR METRICS STARTED ------------------------------------------------
% MSE REPORT start
\subsubsection{MSE}\label{subsubsec:mse}
The MSE abbreviation Mean Squared Error metric is calculated as such for the \textbf{single user}:
\hfill\break
\hfill\break
    \[
       \begin{gathered}
           MSE_u = \sum_{i \in T_u} \frac{(r_{u,i} - \hat{r}_{u,i})^2}{|T_u|}
       \end{gathered}
    \]
\hfill\break
\hfill\break
    Where:
\begin{itemize}
    \item $T_u$ is the test set of the user \textbf{u}.
    \item $r_{u, i}$ is the actual score give by user \textbf{u} to item \textbf{i}.
    \item $\hat{r}_{u, i}$ is the predicted score give by user \textbf{u} to item \textbf{i}.
\end{itemize}
\hfill\break
\hfill\break
It is calculated as such for the entire system:
\hfill\break
\hfill\break
    \[
       \begin{gathered}
           MSE_{sys} = \sum_{u \in T} \frac{MSE_u}{|T|}
       \end{gathered}
    \]
\hfill\break
\hfill\break
    Where:
\begin{itemize}
    \item $T$ is the \textbf{test set}.
    \item $MSE_u$ is the MSE calculated for user \textbf{u}.
\end{itemize}
\hfill\break
\hfill\break
There may be cases in which some items of the test set of the user could not be predicted
\textit{e.g. a CBRS was chosen and items were not present locally}.
In those cases, the $MSE_u$ formula becomes:
\hfill\break
\hfill\break
    \[
       \begin{gathered}
           MSE_u = \sum_{i \in T_u} \frac{(r_{u,i} - \hat{r}_{u,i})^2}{|T_u| - unk}
       \end{gathered}
    \]
\hfill\break
\hfill\break
    Where:
\begin{itemize}
    \item $unk$ stay for unknown is the number of items of the user test set that could not be predicted.
\end{itemize}
\hfill\break
\hfill\break
If no items of the user test set have been predicted $|T_u| - unk = 0$, then:
\hfill\break
\hfill\break
    \[
        \begin{gathered}
            MSE_u = NaN
        \end{gathered}
    \]
\hfill\break
\hfill\break
% mse report end___

% RMSE REPORT start
\subsubsection{RMSE}\label{subsubsec:rmse}
The RMSE (Root Mean Squared Error) metric is calculated as such for the \textbf{single user}:
\hfill\break
\hfill\break
    \[
       \begin{gathered}
           RMSE_u = \sqrt{\sum_{i \in T_u} \frac{(r_{u,i} - \hat{r}_{u,i})^2}{|T_u|}}
       \end{gathered}
    \]
\hfill\break
\hfill\break
    Where:
\begin{itemize}
    \item $T_u$ is the test set of the user $u$.
    \item $r_{u, i}$ is the actual score give by user $u$ to item $i$.
    \item $\hat{r}_{u, i}$ is the predicted score give by user $u$ to item $i$.
\end{itemize}
\hfill\break
\hfill\break
It is calculated as such for the entire system:
\hfill\break
\hfill\break
    \[
      \begin{gathered}
          RMSE_{sys} = \sum_{u \in T} \frac{RMSE_u}{|T|}
      \end{gathered}
    \]
\hfill\break
\hfill\break
    Where:
\begin{itemize}
    \item $T$ is the test set.
    \item $RMSE_u$ is the RMSE calculated for user $u$.
\end{itemize}
\hfill\break
\hfill\break
There may be cases in which some items of the test set of the user could not be predicted
\textit{e.g. a CBRS was chosen and items were not present locally, a methodology different from TestRatings was chosen}.
In those cases, the $RMSE_u$ formula becomes:
\hfill\break
\hfill\break
    \[
       \begin{gathered}
           RMSE_u = \sqrt{\sum_{i \in T_u} \frac{(r_{u,i} - \hat{r}_{u,i})^2}{|T_u| - unk}}
       \end{gathered}
    \]
\hfill\break
\hfill\break
    Where:
\begin{itemize}
    \item $unk$ (unknown) is the number of items of the user test set that could not be predicted.
\end{itemize}
\hfill\break
\hfill\break
If no items of the user test set have been predicted $|T_u| - unk = 0$, then:
\hfill\break
\hfill\break
    \[
      \begin{gathered}
          RMSE_u = NaN
      \end{gathered}
    \]
\hfill\break
\hfill\break
% rmse report end___

% MAE REPORT start
\subsubsection{MAE}\label{subsubsec:mae}
The MAE abbreviation of Mean Absolute Error metric is calculated as such for the \textbf{single user}:
\hfill\break
\hfill\break
    \[
      \begin{gathered}
          MAE_u = \sum_{i \in T_u} \frac{|r_{u,i} - \hat{r}_{u,i}|}{|T_u|}
      \end{gathered}
    \]
\hfill\break
\hfill\break
    Where:
\begin{itemize}
    \item $T_u$ is the test set of the user $u$.
    \item $r_{u, i}$ is the actual score give by user $u$ to item $i$.
    \item $\hat{r}_{u, i}$ is the predicted score give by user $u$ to item $i$.
\end{itemize}
\hfill\break
\hfill\break
It is calculated as such for the entire system:
\hfill\break
\hfill\break
    \[
       \begin{gathered}
           MAE_{sys} = \sum_{u \in T} \frac{MAE_u}{|T|}
       \end{gathered}
    \]
\hfill\break
\hfill\break
    Where:
\begin{itemize}
    \item $T$ is the test set.
    \item $MAE_u$ is the MAE calculated for user $u$.
\end{itemize}
\hfill\break
\hfill\break
There may be cases in which some items of the test set of the user could not be predicted
\textit{e.g. a CBRS was chosen and items were not present locally, a methodology different from TestRatings was chosen}.
In those cases, the $MAE_u$ formula becomes:
\hfill\break
\hfill\break
    \[
      \begin{gathered}
          MAE_u = \sum_{i \in T_u} \frac{|r_{u,i} - \hat{r}_{u,i}|}{|T_u| - unk}
      \end{gathered}
    \]
\hfill\break
\hfill\break
    Where:
\begin{itemize}
    \item $unk$ (unknown) is the number of items of the user test set that could not be predicted.
\end{itemize}
\hfill\break
\hfill\break
If no items of the user test set have been predicted $|T_u| - unk = 0$, then:
\hfill\break
\hfill\break
    \[
        \begin{gathered}
            MAE_u = NaN
        \end{gathered}
    \]
\hfill\break
\hfill\break
% mae report end___
% <------ error metrics ended -------->

% ------------------------------------------- RANKING METRICS STARTED -----------------------------------------------
% Normalized Discounted Cumulative Gain REPORT start
\subsubsection{NDCG}\label{subsubsec:ndcg}
The NDCG abbreviation of Normalized Discounted Cumulative Gain metric is calculated for the \textbf{single user}
by first computing the DCG score using the following formula:
\hfill\break
\hfill\break
    \[
       \begin{gathered}
           DCG_{u}(scores_{u}) = \sum_{r\in scores_{u}}{\frac{f(r)}{log_x(2 + i)}}
       \end{gathered}
    \]
\hfill\break
\hfill\break
    Where:
\begin{itemize}
    \item $scores_{u}$ are the ground truth scores for predicted items, ordered according to the order of said items in the
        ranking for the user $u$.
    \item $f$ is a gain function \textit{linear or exponential, in particular}.
    \item $x$ is the base of the logarithm.
    \item $i$ is the index of the truth score $r$ in the list of scores $scores_{u}$.
\end{itemize}
\hfill\break
\hfill\break
If $f$ is "linear", then the truth score $r$ is returned as is.
Otherwise, in the "exponentia" case, the following formula is applied to $r$:
\hfill\break
\hfill\break
    \[
       \begin{gathered}
           f(r) = 2^{r} - 1
       \end{gathered}
    \]
\hfill\break
\hfill\break
The NDCG for a single user is then calculated using the following formula:
\hfill\break
\hfill\break
    \[
       \begin{gathered}
           NDCG_u(scores_{u}) = \frac{DCG_{u}(scores_{u})}{IDCG_{u}(scores_{u})}
       \end{gathered}
    \]
\hfill\break
\hfill\break
    Where:
\begin{itemize}
    \item $IDCG_{u}$ is the DCG of the ideal ranking for the truth scores.
\end{itemize}
\hfill\break
\hfill\break
So the basic idea is to compare the actual ranking with the ideal one.
Finally, the NDCG of the entire system is calculated instead as such:
\hfill\break
\hfill\break
    \[
       \begin{gathered}
           NDCG_{sys} = \frac{\sum_{u} NDCG_u}{|U|}
       \end{gathered}
    \]
\hfill\break
\hfill\break
    Where:
\begin{itemize}
    \item $NDCG_u$ is the NDCG calculated for user $u$.
    \item $U$ is the set of all users.
\end{itemize}
\hfill\break
\hfill\break
The system average excludes NaN values.
\hfill\break
\hfill\break
% ndcg report end___

% NDCG@K REPORT start
\subsubsection{NDCG@k}\label{subsubsec:ndcg-k}
The NDCG@K abbreviation of Normalized Discounted Cumulative Gain at K metric is calculated for the \textbf{single user}
by using the [framework implementation of the NDCG][clayrs.evaluation.NDCG] but considering $scores_{u}$ cut at the
first $k$ predictions.
The K used for the experiment is .
\hfill\break
\hfill\break
% ndcg@k report end___

% MRR REPORT start
\subsubsection{MRR}\label{subsubsec:mrr}
The MRR abbreviation of Mean Reciprocal Rank metric is a system-wide metric, so only its result it will be returned
and not those of every user.
MRR is calculated as such:
\hfill\break
\hfill\break
    \[
       \begin{gathered}
           MRR_{sys} = \frac{1}{|Q|}\cdot\sum_{i=1}^{|Q|}\frac{1}{rank(i)}
       \end{gathered}
    \]
\hfill\break
\hfill\break
    Where:
\begin{itemize}
    \item $Q$ is the set of recommendation lists.
    \item $rank(i)$ is the position of the first relevant item in the i-th recommendation list.
\end{itemize}
\hfill\break
\hfill\break
% ATTENTION here we modified with \_ in case change
The MRR metric needs to discern relevant items from the not relevant ones.
To achieve this, one could pass a custom \texttt{relevant\_threshold} parameter that will be applied to every user.
If the rating of an item is $\geq$ \texttt{relevant\_threshold}, then it is considered relevant; otherwise, it is not.
If no \texttt{relevant\_threshold} parameter is passed, then for every user, its mean rating score will be used.
In this experiment, the relevant threshold used is
None.
\hfill\break
\hfill\break
% mrr report end___

% MRR@K REPORT start
\subsubsection{MRR@K}\label{subsubsec:mrr-k}
The MRR@K abbreviation of Mean Reciprocal Rank at K metric is a system-wide metric, so only its result will be returned
and not those of every user.
MRR@K is calculated as such:
\hfill\break
\hfill\break
    \[
       \begin{gathered}
           MRR@K_{sys} = \frac{1}{|Q|}\cdot\sum_{i=1}^{K}\frac{1}{rank(i)}
       \end{gathered}
    \]
\hfill\break
\hfill\break
    Where:
\begin{itemize}
    \item $K$ is the cutoff parameter.
    \item $Q$ is the set of recommendation lists.
    \item $rank(i)$ is the position of the first relevant item in the i-th recommendation list.
\end{itemize}
\hfill\break
\hfill\break
In this experiment, the relevant threshold used is
None.
\hfill\break
\hfill\break
% mrr@k report end___

% MAP REPORT start
\subsubsection{MAP}\label{subsubsec:map}
The MAP metric abbreviation of Mean average Precision is a ranking metric computed by first calculating the AP
abbreviation of Average Precision for each user and then taking its mean.
The AP is calculated as such for the \textbf{single user}:
\hfill\break
\hfill\break
    \[
       \begin{gathered}
           AP_u = \frac{1}{m_u}\sum_{i=1}^{N_u}P(i)\cdot rel(i)
       \end{gathered}
    \]
\hfill\break
\hfill\break
    Where:
\begin{itemize}
    \item $m_u$ is the number of relevant items for the user $u$.
    \item $N_u$ is the number of recommended items for the user $u$.
    \item $P(i)$ is the precision computed at cutoff $i$.
    \item $rel(i)$ is an indicator variable that says whether the i-th item is relevant ($rel(i)=1$) or not ($rel(i)=0$).
\end{itemize}
\hfill\break
\hfill\break
After computing the AP for each user, we can compute the MAP for the whole system:
\hfill\break
\hfill\break
    \[
       \begin{gathered}
           MAP_{sys} = \frac{1}{|U|}\sum_{u}AP_u
       \end{gathered}
    \]
\hfill\break
\hfill\break
This metric will return the AP computed for each user in the dataframe containing users results, and the MAP
computed for the whole system in the dataframe containing system results.
In this experiment the MAP has been calculated using a relevant threshold:
None.
\hfill\break
\hfill\break
% map report end___


% CORRELATION REPORT start
\subsubsection{Correlation}\label{subsubsec:corr}
The Correlation metric calculates the correlation between the ranking of a user and its ideal ranking.
The current correlation methods implemented are:
\begin{itemize}
    \item `pearson`
    \item `kendall`
    \item `spearman`
\end{itemize}
\hfill\break
\hfill\break
Every correlation method is implemented by the pandas library, so refer to its
\href{https://pandas.pydata.org/docs/reference/api/pandas.Series.corr.html}{documentation} for more information.
\hfill\break
\hfill\break
The correlation metric is calculated as such for the \textbf{single user}:
\hfill\break
\hfill\break
    \[
      \begin{gathered}
          Corr_u = Corr(ranking_u, ideal_ranking_u)
      \end{gathered}
    \]
\hfill\break
\hfill\break
    Where:
\begin{itemize}
    \item $ranking_u$ is ranking of the user.
    \item $ideal_ranking_u$ is the ideal ranking for the user.
\end{itemize}
\hfill\break
\hfill\break
The ideal ranking is calculated based on the rating inside the ground truth of the user.
The Correlation metric calculated for the entire system is simply the average of every $Corr$:
\hfill\break
\hfill\break
    \[
      \begin{gathered}
          Corr_{sys} = \frac{\sum_{u} Corr_u}{|U|}
      \end{gathered}
    \]
\hfill\break
\hfill\break
    Where:
\begin{itemize}
    \item $Corr_u$ is the correlation of the user $u$.
    \item $U$ is the set of all users.
\end{itemize}
\hfill\break
\hfill\break
% ATTETNION here there are already \_
The system average excludes NaN values.
It's also possible to specify a cutoff parameter using the \texttt{top\_n} parameter.
If specified, only the first $n$ results of the recommendation list will be used to calculate the correlation.
For this experiment, the settings for the correlation metrics are:
\begin{itemize}
    \item method: pearson.
    \item top\_n: None.
\end{itemize}
\hfill\break
\hfill\break
% correlation report end___
% <------- ranking metrics ended ------->

% -------------------------------------------- FAIRNESS METRICS ------------------------------------------------------
% Gini Index REPORT start
\subsubsection{Gini Index}\label{subsubsec:gini}
The Gini Index metric measures inequality in recommendation lists.
It's a system-wide metric, so only its result it will be returned and not those of every user.
The metric is calculated as such:
\hfill\break
\hfill\break
    \[
       \begin{gathered}
           Gini_{sys} = \frac{\sum_i(2i - n - 1)x_i}{n\cdot\sum_i x_i}
       \end{gathered}
    \]
\hfill\break
\hfill\break
    Where:
\begin{itemize}
    \item $n$ is the total number of distinct items that are being recommended.
    \item $x_i$ is the number of times that the item $i$ has been recommended.
\end{itemize}
\hfill\break
\hfill\break
A perfectly equal recommender system should recommend every item the same number of times, in which case the Gini
index would be equal to 0.
The more the recsys is "unequal", the more the Gini Index is closer to 1.
If the \texttt{'top\_n'} parameter is specified, then the Gini index will measure inequality considering only
the first $n$ items of every recommendation list of all users.
For this experiment, the \texttt{top\_n}:
None.
\hfill\break
\hfill\break
% gini index report end___

% PREDICTION COVERAGE REPORT start
\subsubsection{Prediction Coverage}\label{subsubsec:pred_cov}
The Prediction Coverage metric measures in percentage how many distinct items are being recommended in relation
to all available items.
It's a system wise metric, so only its result it will be returned and not those of every user.
The metric is calculated as such:
\hfill\break
\hfill\break
    \[
       \begin{gathered}
           Prediction Coverage_{sys} = (\frac{|I_p|}{|I|})\cdot100
       \end{gathered}
    \]
\hfill\break
\hfill\break
    Where:
\begin{itemize}
    \item $I$ is the set of all available items.
    \item $I_p$ is the set of recommended items.
\end{itemize}
\hfill\break
\hfill\break
The $I$ must be specified through the 'catalog' parameter.
\hfill\break
\hfill\break
% prediction coverage report end___

% CATALOG COVERAGE REPORT start
\subsubsection{Catalog Coverage}\label{subsubsec:cat_cov}
The Catalog Coverage metric measures in percentage how many distinct items are being recommended in relation
to all available items.
It's a system-wide metric, so only its result it will be returned and not those of every user.
It differs from the Prediction Coverage since it allows for different parameters to come into play.
If no parameter is passed then it's a simple Prediction Coverage.
The metric is calculated as such:
\hfill\break
\hfill\break
    \[
        \begin{gathered}
            Catalog Coverage_{sys} = (\frac{|\bigcup_{j=1...N}reclist(u_j)|}{|I|})\cdot100
        \end{gathered}
    \]
\hfill\break
\hfill\break
    Where:
\begin{itemize}
    \item $N$ is the total number of users.
    \item $reclist(u_j)$ is the set of items contained in the recommendation list of user $j$.
    \item $I$ is the set of all available items.
\end{itemize}
\hfill\break
\hfill\break
The $I$ must be specified through the 'catalog' parameter.
The recommendation list of every user ($reclist(u_j)$) can be reduced to the first n parameter with the top-n parameter,
so that catalog coverage is measured considering only the highest ranked items.
With the 'k' parameter one could specify the number of users that will be used to
calculate catalog coverage: k users will be randomly sampled and their recommendation lists will be used.
The formula above becomes:
\hfill\break
\hfill\break
    \[
       \begin{gathered}
         Catalog Coverage_{\text{sys}} = \left(\frac{|\bigcup_{j=1\ldots k} \text{reclist}(u_j)|}{|I|}\right) \cdot 100
       \end{gathered}
    \]
\hfill\break
\hfill\break
    Where:
\begin{itemize}
    \item $k$ is the parameter specified.
\end{itemize}
\hfill\break
\hfill\break
Obviously 'k' $<$ N, else simply recommendation lists of all users will be used.
\hfill\break
\hfill\break
% catalog coverage report end___

% <------- fairness metrics end ------>

% ------------------------------------------------ PLOT METRICS STARTED ----------------------------------------------
% LONG TAIL DISTIBUTION REPORT start
\subsubsection{Long Tail Distribution}\label{subsubsec:ltd}
This metric generates the Long Tail Distribution plot and saves it in the output directory with the file name
specified.
The plot can be generated both for the truth set or the predictions set based on the on parameter:
\begin{itemize}
    \item \textbf{on = 'truth'}: in this case the long tail distribution is useful to see which are the most popular items
       the most rated ones.
    \item \textbf{on = 'pred'}: in this case the long tail distribution is useful to see which are the most recommended
        items.
\end{itemize}
\hfill\break
\hfill\break
The plot file will be saved as \texttt{out\_dir/file\_name.format}.
Since multiple splits could be evaluated at once, the overwrite parameter comes into play:
if set to \texttt{False}, files with the same name will be saved as \texttt{file\_name\_(1).format}, \texttt{file\_name\_(2).format}, etc.
so that for every split a plot is generated without overwriting any previously generated files.
\hfill\break
\hfill\break
For this experiment the Long Tail Distribution has been used with the following settings:
\begin{itemize}
    \item on: truth.
    \item format: png.
    \item overwrite: False.
\end{itemize}
\hfill\break
\hfill\break
% LTD report end___


% PopRecsCorrelation REPORT start
\subsubsection{Pop Recs Correlation}\label{subsubsec:poprc}
This metric generates a plot which has as the X-axis the popularity of each item and as Y-axis the recommendation
frequency, so that it can be easily seen the correlation between popular niche items and how many times are being
recommended.
The popularity of an item is defined as the number of times it is rated in the 'original\_ratings' parameter
divided by the total number of users in the 'original\_ratings'.
\begin{itemize}
    \item The plot file will be saved as out\_dir/file\_name.format'
\end{itemize}
\hfill\break
\hfill\break
Since multiple splits could be evaluated at once, the overwrite parameter comes into play: if set to \texttt{False},
files with the same name will be saved as \texttt{'file\_name\_(1).format'}, \texttt{'file\_name\_(2).format'}, etc.,
so that for every split, a plot is generated without overwriting any previously generated files.
\hfill\break
\hfill\break
There exists cases in which some items are not recommended even once, so in the graph could appear
zero recommendations.
One could change this behaviour thanks to the 'mode' parameter:
\begin{itemize}
    \item \textbf{mode='both'}: Two graphs will be created, the first one containing eventual zero recommendations, the
      second one where zero recommendations are excluded. This additional graph will be stored as
      \texttt{out\_dir/file\_name\_no\_zeros.format} \textit{the string '\_no\_zeros' will be added to the file\_name chosen automatically}.
    \item \textbf{mode='w\_zeros'}: Only a graph containing eventual zero recommendations will be created.
    \item \textbf{mode='no\_zeros'}: Only a graph excluding eventual zero recommendations will be created. The graph will be
      saved as \texttt{out\_dir/file\_name\_no\_zeros.format} \textit{the string '\_no\_zeros' will be added to the file\_name chosen automatically}.
\end{itemize}
\hfill\break
\hfill\break
For this experiment the PopRecsCorrelation has been used with the following settings:
\begin{itemize}
    \item mode: both.
    \item format: png.
    \item overwrite: False.
\end{itemize}
\hfill\break
\hfill\break
% PopRecsCorrelation report end___
% <------- plot metric end ----------->

% <---------------------- closing the only metric section ------------------------->



% In case of eval module is not used




% ------------------------------- RESULT OF PERFORMACE OF THE SYSTEM ON THE FOLD CentroidVector -------------------------------------
\subsection{Results for CentroidVector}\label{subsec:CentroidVector}
In this section we show the mean results with \textbf{ CentroidVector }, the metrics used during the experiment will be
grouped into the following table, that represent the results of the evaluation conducted~\ref{tab:results_table_CentroidVector}

\begin{center}
    \begin{tabular}{|c|c|}
        \hline
        \textbf{Metric} & \textbf{Value} \\ \hline
                CatalogCoverage (PredictionCov) & 16.67 \\ \hline
                F1 - macro & 1.0 \\ \hline
                F1@5 - micro & 1.0 \\ \hline
                Gini & 0.0 \\ \hline
                MAE & 3.0 \\ \hline
                MAP & 1.0 \\ \hline
                MRR & 1.0 \\ \hline
                MRR@5 & 1.0 \\ \hline
                MSE & 9.0 \\ \hline
                NDCG & 1.0 \\ \hline
                NDCG@5 & 1.0 \\ \hline
                Precision - macro & 1.0 \\ \hline
                Precision@5 - macro & 1.0 \\ \hline
                PredictionCoverage & 16.67 \\ \hline
                R-Precision - macro & 1.0 \\ \hline
                RMSE & 3.0 \\ \hline
                Recall - macro & 1.0 \\ \hline
                Recall@5 - macro & 1.0 \\ \hline
             \end{tabular}
    \captionsetup{type=table}
    \caption{Table of the results}
    \label{tab:results_table_CentroidVector}
\end{center}
\hfill\break
\hfill\break
% end table of performance with CentroidVector ___




% ------------------------------- RESULT OF PERFORMACE OF THE SYSTEM ON THE FOLD LinearPredictor -------------------------------------
\subsection{Results for LinearPredictor}\label{subsec:LinearPredictor}
In this section we show the mean results with \textbf{ LinearPredictor }, the metrics used during the experiment will be
grouped into the following table, that represent the results of the evaluation conducted~\ref{tab:results_table_LinearPredictor}

\begin{center}
    \begin{tabular}{|c|c|}
        \hline
        \textbf{Metric} & \textbf{Value} \\ \hline
                CatalogCoverage (PredictionCov) & 50.0 \\ \hline
                F1 - macro & 0.91667 \\ \hline
                F1@5 - micro & 0.92857 \\ \hline
                Gini & 0.0 \\ \hline
                MAE & 0.76165 \\ \hline
                MAP & 1.0 \\ \hline
                MRR & 1.0 \\ \hline
                MRR@5 & 1.0 \\ \hline
                MSE & 0.6997 \\ \hline
                NDCG & 1.0 \\ \hline
                NDCG@5 & 1.0 \\ \hline
                Precision - macro & 0.875 \\ \hline
                Precision@5 - macro & 0.875 \\ \hline
                PredictionCoverage & 50.0 \\ \hline
                R-Precision - macro & 1.0 \\ \hline
                RMSE & 0.81343 \\ \hline
                Recall - macro & 1.0 \\ \hline
                Recall@5 - macro & 1.0 \\ \hline
             \end{tabular}
    \captionsetup{type=table}
    \caption{Table of the results}
    \label{tab:results_table_LinearPredictor}
\end{center}
\hfill\break
\hfill\break
% end table of performance with LinearPredictor ___




% ------------------------------- RESULT OF PERFORMACE OF THE SYSTEM ON THE FOLD IndexQuery -------------------------------------
\subsection{Results for IndexQuery}\label{subsec:IndexQuery}
In this section we show the mean results with \textbf{ IndexQuery }, the metrics used during the experiment will be
grouped into the following table, that represent the results of the evaluation conducted~\ref{tab:results_table_IndexQuery}

\begin{center}
    \begin{tabular}{|c|c|}
        \hline
        \textbf{Metric} & \textbf{Value} \\ \hline
                CatalogCoverage (PredictionCov) & 16.67 \\ \hline
                F1 - macro & 1.0 \\ \hline
                F1@5 - micro & 1.0 \\ \hline
                Gini & 0.0 \\ \hline
                MAE & 63.65394 \\ \hline
                MAP & 1.0 \\ \hline
                MRR & 1.0 \\ \hline
                MRR@5 & 1.0 \\ \hline
                MSE & 4051.82448 \\ \hline
                NDCG & 1.0 \\ \hline
                NDCG@5 & 1.0 \\ \hline
                Precision - macro & 1.0 \\ \hline
                Precision@5 - macro & 1.0 \\ \hline
                PredictionCoverage & 16.67 \\ \hline
                R-Precision - macro & 1.0 \\ \hline
                RMSE & 63.65394 \\ \hline
                Recall - macro & 1.0 \\ \hline
                Recall@5 - macro & 1.0 \\ \hline
             \end{tabular}
    \captionsetup{type=table}
    \caption{Table of the results}
    \label{tab:results_table_IndexQuery}
\end{center}
\hfill\break
\hfill\break
% end table of performance with IndexQuery ___




% ------------------------------- RESULT OF PERFORMACE OF THE SYSTEM ON THE FOLD ClassifierRecommender -------------------------------------
\subsection{Results for ClassifierRecommender}\label{subsec:ClassifierRecommender}
In this section we show the mean results with \textbf{ ClassifierRecommender }, the metrics used during the experiment will be
grouped into the following table, that represent the results of the evaluation conducted~\ref{tab:results_table_ClassifierRecommender}

\begin{center}
    \begin{tabular}{|c|c|}
        \hline
        \textbf{Metric} & \textbf{Value} \\ \hline
                CatalogCoverage (PredictionCov) & 16.67 \\ \hline
                F1 - macro & 1.0 \\ \hline
                F1@5 - micro & 1.0 \\ \hline
                Gini & 0.0 \\ \hline
                MAE & 1.5 \\ \hline
                MAP & 1.0 \\ \hline
                MRR & 1.0 \\ \hline
                MRR@5 & 1.0 \\ \hline
                MSE & 2.25 \\ \hline
                NDCG & 1.0 \\ \hline
                NDCG@5 & 1.0 \\ \hline
                Precision - macro & 1.0 \\ \hline
                Precision@5 - macro & 1.0 \\ \hline
                PredictionCoverage & 16.67 \\ \hline
                R-Precision - macro & 1.0 \\ \hline
                RMSE & 1.5 \\ \hline
                Recall - macro & 1.0 \\ \hline
                Recall@5 - macro & 1.0 \\ \hline
             \end{tabular}
    \captionsetup{type=table}
    \caption{Table of the results}
    \label{tab:results_table_ClassifierRecommender}
\end{center}
\hfill\break
\hfill\break
% end table of performance with ClassifierRecommender ___




% ------------------------------- RESULT OF PERFORMACE OF THE SYSTEM ON THE FOLD AmarDoubleSource -------------------------------------
\subsection{Results for AmarDoubleSource}\label{subsec:AmarDoubleSource}
In this section we show the mean results with \textbf{ AmarDoubleSource }, the metrics used during the experiment will be
grouped into the following table, that represent the results of the evaluation conducted~\ref{tab:results_table_AmarDoubleSource}

\begin{center}
    \begin{tabular}{|c|c|}
        \hline
        \textbf{Metric} & \textbf{Value} \\ \hline
                CatalogCoverage (PredictionCov) & 50.0 \\ \hline
                F1 - macro & 0.91667 \\ \hline
                F1@5 - micro & 0.92857 \\ \hline
                Gini & 0.0 \\ \hline
                MAE & 2.16423 \\ \hline
                MAP & 1.0 \\ \hline
                MRR & 1.0 \\ \hline
                MRR@5 & 1.0 \\ \hline
                MSE & 5.67578 \\ \hline
                NDCG & 1.0 \\ \hline
                NDCG@5 & 1.0 \\ \hline
                Precision - macro & 0.875 \\ \hline
                Precision@5 - macro & 0.875 \\ \hline
                PredictionCoverage & 50.0 \\ \hline
                R-Precision - macro & 1.0 \\ \hline
                RMSE & 2.19062 \\ \hline
                Recall - macro & 1.0 \\ \hline
                Recall@5 - macro & 1.0 \\ \hline
             \end{tabular}
    \captionsetup{type=table}
    \caption{Table of the results}
    \label{tab:results_table_AmarDoubleSource}
\end{center}
\hfill\break
\hfill\break
% end table of performance with AmarDoubleSource ___



\section{Algorithms comparison}\label{sec:comparison}
In the following table it's showed a comparison between the algorithms used in these experiments
and their performance are highlighted base on the best result obtained.
In bold the best result for each of the evaluating metrics used, despite underline will be the second-best result
obtained on each metrics and an asterisk netx to the result of the metric means that the result obtained is statistic
relevant based on statistical tests conducted:

\hfill\break
\begin{table}[H]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{@{}c *{3}{p{3.0cm}}@{}}
\toprule
\multirow{2}{*}{Algorithms} & \multicolumn{3}{c}{Columns} \\
\cmidrule{2-4}
& \multirow{2}{*}{\makecell{Precision - macro}} & \multirow{2}{*}{\makecell{Recall - macro}} & \multirow{2}{*}{\makecell{F1 - macro}} \\
\addlinespace[12pt]
\cmidrule{2-4}
 & & & \\
CentroidVector & 0.58 & 0.69 & 0.71 \\
\addlinespace[5pt]
\midrule
LinearPredictor & 0.375* & 0.421 & 0.517* \\
\addlinespace[5pt]
\midrule
IndexQuery & \textbf{0.99}* & \underline{0.985} & \textbf{0.979}* \\
\addlinespace[5pt]
\midrule
ClassifierRecommender & 0.723* & 0.792 & 0.799 \\
\addlinespace[5pt]
\midrule
AmarDoubleSource & \underline{0.875}* & \textbf{1.0} & \underline{0.917} \\
\addlinespace[5pt]
\midrule
\bottomrule
\end{tabular}}
\caption{Comparison between algorithms (Part 1)}
\end{table}
\vspace{10pt}
\begin{table}[H]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{@{}c *{3}{p{3.0cm}}@{}}
\toprule
\multirow{2}{*}{Algorithms} & \multicolumn{3}{c}{Columns} \\
\cmidrule{2-4}
& \multirow{2}{*}{\makecell{Gini}} & \multirow{2}{*}{\makecell{NDCG}} & \multirow{2}{*}{\makecell{R-Precision - macro}} \\
\addlinespace[12pt]
\cmidrule{2-4}
 & & & \\
CentroidVector & 8.0 & \underline{0.95} & 0.853 \\
\addlinespace[5pt]
\midrule
LinearPredictor & 35.0* & 0.521 & 0.423 \\
\addlinespace[5pt]
\midrule
IndexQuery & \underline{0.9}* & 0.91 & \underline{0.965} \\
\addlinespace[5pt]
\midrule
ClassifierRecommender & 11.0* & 0.823 & 0.811 \\
\addlinespace[5pt]
\midrule
AmarDoubleSource & \textbf{0.0}* & \textbf{1.0} & \textbf{1.0} \\
\addlinespace[5pt]
\midrule
\bottomrule
\end{tabular}}
\caption{Comparison between algorithms (Part 2)}
\end{table}
\vspace{10pt}
\begin{table}[H]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{@{}c *{3}{p{3.0cm}}@{}}
\toprule
\multirow{2}{*}{Algorithms} & \multicolumn{3}{c}{Columns} \\
\cmidrule{2-4}
& \multirow{2}{*}{\makecell{RMSE}} & \multirow{2}{*}{\makecell{MSE}} & \multirow{2}{*}{\makecell{MAE}} \\
\addlinespace[12pt]
\cmidrule{2-4}
 & & & \\
CentroidVector & \underline{3.0} & \underline{9.0} & \underline{3.0} \\
\addlinespace[5pt]
\midrule
LinearPredictor & 120.813 & 1540.7* & 2760.762* \\
\addlinespace[5pt]
\midrule
IndexQuery & 43.654 & 51.824* & 3.654* \\
\addlinespace[5pt]
\midrule
ClassifierRecommender & 13.5 & 26.25* & 12.5* \\
\addlinespace[5pt]
\midrule
AmarDoubleSource & \textbf{2.191} & \textbf{5.676}* & \textbf{2.164}* \\
\addlinespace[5pt]
\midrule
\bottomrule
\end{tabular}}
\caption{Comparison between algorithms (Part 3)}
\end{table}
\vspace{10pt}
\begin{table}[H]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{@{}c *{3}{p{3.0cm}}@{}}
\toprule
\multirow{2}{*}{Algorithms} & \multicolumn{3}{c}{Columns} \\
\cmidrule{2-4}
& \multirow{2}{*}{\makecell{MRR}} & \multirow{2}{*}{\makecell{MAP}} & \multirow{2}{*}{\makecell{PredictionCoverage}} \\
\addlinespace[12pt]
\cmidrule{2-4}
 & & & \\
CentroidVector & 0.699 & \underline{0.981} & 16.67 \\
\addlinespace[5pt]
\midrule
LinearPredictor & 0.543 & 0.477 & \underline{150.0} \\
\addlinespace[5pt]
\midrule
IndexQuery & \textbf{1.0} & 0.9 & 36.67 \\
\addlinespace[5pt]
\midrule
ClassifierRecommender & \underline{0.75} & 0.69 & \textbf{169.67} \\
\addlinespace[5pt]
\midrule
AmarDoubleSource & \textbf{1.0} & \textbf{1.0} & 50.0 \\
\addlinespace[5pt]
\midrule
\bottomrule
\end{tabular}}
\caption{Comparison between algorithms (Part 4)}
\end{table}
\vspace{10pt}
\begin{table}[H]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{@{}c *{3}{p{3.0cm}}@{}}
\toprule
\multirow{2}{*}{Algorithms} & \multicolumn{3}{c}{Columns} \\
\cmidrule{2-4}
& \multirow{2}{*}{\makecell{Precision@5 - macro}} & \multirow{2}{*}{\makecell{Recall@5 - macro}} & \multirow{2}{*}{\makecell{F1@5 - micro}} \\
\addlinespace[12pt]
\cmidrule{2-4}
 & & & \\
CentroidVector & \textbf{1.0} & \textbf{1.0} & 0.877 \\
\addlinespace[5pt]
\midrule
LinearPredictor & 0.475* & 0.399* & 0.529 \\
\addlinespace[5pt]
\midrule
IndexQuery & \underline{0.93}* & \underline{0.92}* & \textbf{0.998} \\
\addlinespace[5pt]
\midrule
ClassifierRecommender & 0.771* & 0.721* & 0.789 \\
\addlinespace[5pt]
\midrule
AmarDoubleSource & 0.875* & \textbf{1.0}* & \underline{0.929} \\
\addlinespace[5pt]
\midrule
\bottomrule
\end{tabular}}
\caption{Comparison between algorithms (Part 5)}
\end{table}
\vspace{10pt}
\begin{table}[H]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{@{}c *{2}{p{3.0cm}}@{}}
\toprule
\multirow{2}{*}{Algorithms} & \multicolumn{2}{c}{Columns} \\
\cmidrule{2-3}
& \multirow{2}{*}{\makecell{MRR@5}} & \multirow{2}{*}{\makecell{NDCG@5}} \\
\addlinespace[12pt]
\cmidrule{2-3}
 & & & \\
CentroidVector & 0.877 & 0.811 \\
\addlinespace[5pt]
\midrule
LinearPredictor & 0.444 & 0.454 \\
\addlinespace[5pt]
\midrule
IndexQuery & \underline{0.925} & \underline{0.965} \\
\addlinespace[5pt]
\midrule
ClassifierRecommender & 0.688 & 0.698 \\
\addlinespace[5pt]
\midrule
AmarDoubleSource & \textbf{1.0} & \textbf{1.0} \\
\addlinespace[5pt]
\midrule
\bottomrule
\end{tabular}}
\caption{Comparison between algorithms (Part 6)}
\end{table}



\subsection{Statistic relevance}\label{subsec:stas_rel}
During the experiment some statistic test are been performed and the following table show the result
of the level of p-value used for each metric, this result have been used to mark with an asterisk
the result obtained in the previous table that are statistical relevant.

\hfill\break
\begin{table}[H]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{lrr}
\toprule
{} & Precision - macro & Recall@5 - macro \\
{} &            pvalue &           pvalue \\
sys\_pair                                  &                   &                  \\
\midrule
(CentroidVector, LinearPredictor)         &     9.505701e-281 &         0.002248 \\
(CentroidVector, IndexQuery)              &     2.835562e-276 &         0.002316 \\
(LinearPredictor, IndexQuery)             &      6.955147e-01 &         0.994123 \\
(CentroidVector, ClassifierRecommender)   &      1.230000e-03 &         0.023000 \\
(CentroidVector, AmarDoubleSource)        &      5.000000e-01 &         0.876350 \\
(LinearPredictor, ClassifierRecommender)  &      1.570000e+00 &         1.450000 \\
(LinearPredictor, AmarDoubleSource)       &      1.500000e+00 &         1.200000 \\
(IndexQuery, ClassifierRecommender)       &      2.470000e-02 &         0.063215 \\
(IndexQuery, AmarDoubleSource)            &      8.800000e-01 &         0.445425 \\
(ClassifierRecommender, AmarDoubleSource) &      3.200000e-03 &         0.120970 \\
\bottomrule
\end{tabular}
}
\caption{relevance table - Precision - macro, Recall@5 - macro}
\end{table}

\begin{table}[H]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{lrr}
\toprule
{} & Precision@5 - macro &    F1 - macro \\
{} &              pvalue &        pvalue \\
sys\_pair                                  &                     &               \\
\midrule
(CentroidVector, LinearPredictor)         &       1.022519e-100 &  1.744839e-50 \\
(CentroidVector, IndexQuery)              &        1.510916e-96 &  2.417581e-50 \\
(LinearPredictor, IndexQuery)             &        6.314952e-01 &  9.931930e-01 \\
(CentroidVector, ClassifierRecommender)   &        9.083649e-01 &  1.872534e+00 \\
(CentroidVector, AmarDoubleSource)        &        2.276869e-01 &  2.876354e+00 \\
(LinearPredictor, ClassifierRecommender)  &        2.800000e+00 &  5.893746e+00 \\
(LinearPredictor, AmarDoubleSource)       &        4.530000e+00 &  3.129735e+00 \\
(IndexQuery, ClassifierRecommender)       &        8.374560e-02 &  6.753465e-01 \\
(IndexQuery, AmarDoubleSource)            &        8.745600e-01 &  9.734000e-03 \\
(ClassifierRecommender, AmarDoubleSource) &        3.487561e-03 &  8.972355e-03 \\
\bottomrule
\end{tabular}
}
\caption{relevance table - Precision@5 - macro, F1 - macro}
\end{table}

\begin{table}[H]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{lrr}
\toprule
{} &           MSE &           MAE \\
{} &        pvalue &        pvalue \\
sys\_pair                                  &               &               \\
\midrule
(CentroidVector, LinearPredictor)         &  1.902252e-99 &  1.174484e-49 \\
(CentroidVector, IndexQuery)              &  1.451092e-95 &  2.417581e-50 \\
(LinearPredictor, IndexQuery)             &  7.631495e+00 &  6.993193e+00 \\
(CentroidVector, ClassifierRecommender)   &  2.346754e-04 &  7.893245e-03 \\
(CentroidVector, AmarDoubleSource)        &  7.653400e-03 &  8.725340e-03 \\
(LinearPredictor, ClassifierRecommender)  &  7.635400e-03 &  9.872356e-04 \\
(LinearPredictor, AmarDoubleSource)       &  8.653400e-03 &  2.348220e-03 \\
(IndexQuery, ClassifierRecommender)       &  1.023646e+00 &  2.863500e-04 \\
(IndexQuery, AmarDoubleSource)            &  9.864500e-01 &  9.735676e-03 \\
(ClassifierRecommender, AmarDoubleSource) &  7.655000e-01 &  1.238896e-01 \\
\bottomrule
\end{tabular}
}
\caption{relevance table - MSE, MAE}
\end{table}

\begin{table}[H]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{lrr}
\toprule
{} &           Gini &  R-Precision - macro \\
{} &         pvalue &               pvalue \\
sys\_pair                                  &                &                      \\
\midrule
(CentroidVector, LinearPredictor)         &  5.505701e-281 &            12.002248 \\
(CentroidVector, IndexQuery)              &  3.835562e-276 &             7.002316 \\
(LinearPredictor, IndexQuery)             &   1.695515e+00 &             3.994123 \\
(CentroidVector, ClassifierRecommender)   &   2.364000e-04 &             3.978640 \\
(CentroidVector, AmarDoubleSource)        &   7.623410e-03 &             1.872534 \\
(LinearPredictor, ClassifierRecommender)  &   9.888000e-01 &             0.665710 \\
(LinearPredictor, AmarDoubleSource)       &   1.232300e+00 &             0.222188 \\
(IndexQuery, ClassifierRecommender)       &   1.564200e+00 &             0.861540 \\
(IndexQuery, AmarDoubleSource)            &   3.261000e-02 &             0.815810 \\
(ClassifierRecommender, AmarDoubleSource) &   1.152672e-02 &             0.122311 \\
\bottomrule
\end{tabular}
}
\caption{relevance table - Gini,  R-Precision - macro}
\end{table}


\begin{table}[H]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{lr}
\toprule
\textbf{Metrics} & \textbf{(CentroidVector, IndexQuery)} \\
\midrule
  Precision - macro - statistic & -3.55 \times 10^{1} \\
  Precision - macro - pvalue & 2.84 \times 10^{-276} \\
  Recall@5 - macro - statistic & -3.05 \times 10^{0} \\
  Recall@5 - macro - pvalue & 2.32 \times 10^{-3} \\
  Precision@5 - macro - statistic & -2.09 \times 10^{1} \\
  Precision@5 - macro - pvalue & 1.51 \times 10^{-96} \\
  F1 - macro - statistic & -1.49 \times 10^{1} \\
  F1 - macro - pvalue & 2.42 \times 10^{-50} \\
  MSE - statistic & -2.99 \times 10^{1} \\
  MSE - pvalue & 1.45 \times 10^{-95} \\
  MAE - statistic & -2.49 \times 10^{1} \\
  MAE - pvalue & 2.42 \times 10^{-50} \\
  Gini - statistic & -1.55 \times 10^{1} \\
  Gini - pvalue & 3.84 \times 10^{-276} \\
   R-Precision - macro - statistic & -3.05 \times 10^{0} \\
   R-Precision - macro - pvalue & 7.00 \times 10^{0} \\
\bottomrule
\end{tabular}%
}
\caption{CentroidVector and IndexQuery}
\end{table}



\section{Conclusion on the experiment}\label{sec:conclution}
This part is for conclusion to be sum up as needed.
\hfill\break
\hfill\break

% ------------------- END OF THE REPORT COMPLETED ---------------
% closing the document
\end{document}


