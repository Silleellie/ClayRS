import os

from clayrs import content_analyzer as ca
from clayrs import recsys as rs
from clayrs import evaluation as eva
from clayrs import utils as ut

# setup output folders for experiments
output_folder = os.path.join("report_stuff", "report_all", "experiment_result")
output_folder_graph = os.path.join("report_stuff", "report_all", "graph_experiment_result")

# Item analyzer using extra small dataset
movies_ca_config = ca.ItemAnalyzerConfig(
    source=ca.JSONFile(os.path.join('datasets', 'ml-100k_extra_small', 'items_extra_small.json')),
    id='movielens_id',
    output_directory=os.path.join('report_stuff', 'report_all', 'movies_codified'),
)

# Plot config
movies_ca_config.add_multiple_config(
    'plot',
    [
        # Id-less representation
        ca.FieldConfig(ca.OriginalData()),

        # Tf-Idf computed using Whoosh + single preprocessing using Spacy
        ca.FieldConfig(ca.WhooshTfIdf(),
                       preprocessing=ca.Spacy(stopwords_removal=True, lemmatization=True),
                       id='tfidf_w'),

        # Tf-Idf computed using SkLearn + multiple preprocessing (EkPhrasis + NLTK)
        ca.FieldConfig(ca.SkLearnTfIdf(),
                       preprocessing=[ca.Ekphrasis(spell_correction=True, spell_correct_elong=True),
                                      ca.NLTK(stopwords_removal=True, lemmatization=True)],
                       id='tfidf_sk'),

        # Word Embedding technique
        ca.FieldConfig(ca.WordEmbeddingTechnique(ca.Gensim()),
                       id='word_emb'),

        # Sentence Embedding technique
        ca.FieldConfig(ca.SentenceEmbeddingTechnique(ca.Sbert()),
                       id='sent_emb'),

        # Document Embedding technique
        ca.FieldConfig(ca.DocumentEmbeddingTechnique(ca.Sbert()),
                       id='doc_emb'),

        # Technique to combine word embeddings into sentence ones with the support of a CombiningTechnique (Centroid here)
        ca.FieldConfig(ca.Word2SentenceEmbedding(ca.Gensim(), ca.Centroid()),
                       id='word2sent'),

        # Technique to combine word embeddings into document ones with the support of a CombiningTechnique (Centroid here)
        ca.FieldConfig(ca.Word2DocEmbedding(ca.Gensim(), ca.Centroid()),
                       id='word2doc'),

        # Technique to combine sentence embeddings into document ones with the support of a CombiningTechnique (Centroid here)
        ca.FieldConfig(ca.Sentence2DocEmbedding(ca.Sbert(), ca.Centroid()),
                       id='sent2doc'),

        # Synset Document Frequency
        ca.FieldConfig(ca.PyWSDSynsetDocumentFrequency(), id="synset"),

        # This keeps the original text
        ca.FieldConfig(ca.OriginalData(), id="original"),

        # This keeps the original text and stores it in an index (to be used by IndexQuery)
        ca.FieldConfig(ca.OriginalData(), memory_interface=ca.SearchIndex(os.path.join('report_stuff', 'report_all', 'movies_codified')), id="search_i"),

    ]
)

# This imports the embeddings generated by another framework
# The "report.npy" file contains a numpy 2D-array of shape (6, 512)
# Each row in the array is mapped to an item through the "idx" field
# So if an item has "idx" field 0, the 1D-array with index 0 will be retrieved and serialized
movies_ca_config.add_single_config(
    'idx',
    ca.FieldConfig(ca.FromNPY(os.path.join("report_stuff", "report.npy")), id="npy"),
)

# Some examples of PostProcessors
# If no PostProcessor is specified in the config, the original output is kept
# Useful if you want to apply postprocessing operations but also keep the original one
# This implementation is not optimal, I will probably change it in the near future with a ca.Identity() postprocessor
postprocessors = [
    ca.PostProcessorConfig([], id="original"),
    ca.PostProcessorConfig([ca.FVGMM(2, improved=True)], id="fv"),
    ca.PostProcessorConfig([ca.VLADGMM(2)], id="vlad"),
    ca.PostProcessorConfig([ca.SkLearnPCA(1)])
]

# Media techniques are of two different types:
#   - Low Level: apply an algorithm on each single item
#   - High Level: extract embeddings from intermediate layers of a neural network
#
# There is a further distinction based on the modality:
#   - Audio data: e.g. audio preprocessors such as TorchResample
#   - Visual data: e.g. images/video frames preprocessors such as TorchCenterCrop
#
# To point to media items, a path (either local or absolute) to the video is specified in the source file
# (The framework also supports urls if needed)
movies_ca_config.add_multiple_config(
    'video_path',
    [

        # Low level technique for images
        ca.FieldConfig(
            ca.SkImageHogDescriptor(),
            preprocessing=[
                # This extracts 8 frames uniformly sampled over the time dimension of the video
                ca.TorchUniformTemporalSubSampler(8),
            ],
            id='hog'
        ),

        # Low level technique for audios
        ca.FieldConfig(
            ca.MFCC(),
            preprocessing=[ca.TorchResample(16000), ca.ConvertToMono()],
            postprocessing=postprocessors,
            id='mfcc'
        ),

        # High level technique for audios
        ca.FieldConfig(
            ca.VGGISH(feature_layer=-1,
                      batch_size=1,
                      device="cuda:0"),
            preprocessing=[ca.TorchResample(16000), ca.ConvertToMono()],
            postprocessing=postprocessors,
            id='vggish'
        ),

        # High level technique for images/video frames (each frame processed separately)
        ca.FieldConfig(
            ca.PytorchImageModels(model_name='densenet161',
                                  feature_layer=-2,
                                  batch_size=4,
                                  device='cuda:0',
                                  time_tuple=(0, 10),
                                  flatten=True),
            preprocessing=[
                ca.TorchUniformTemporalSubSampler(8),
                ca.TorchResize((256, 256)),
                ca.TorchCenterCrop(224),
                ca.TorchLambda(lambda x: x / 255.0),
                ca.TorchNormalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
            ],
            postprocessing=postprocessors,
            id='densenet161'
        ),

        # High level technique for video frames (one output for each clip consisting of X frames)
        ca.FieldConfig(
            ca.TorchVisionVideoModels(model_name="r2plus1d_18",
                                      feature_layer=-2,
                                      batch_size=4,
                                      device="cuda:0",
                                      time_tuple=(0, 10),
                                      flatten=True,
                                      mini_batch_size=10),
            preprocessing=[ca.ClipSampler(16, 5, "sequential"), # this clip sampler divides the video into 5 clips of 16 frames selected sequentially from the start
                           ca.TorchResize((128, 171)),
                           ca.TorchCenterCrop((112, 112)),
                           ca.TorchLambda(lambda x: x / 255.0),
                           ca.TorchNormalize(mean=[0.43216, 0.394666, 0.37645], std=[0.22803, 0.22145, 0.216989])],
            postprocessing=postprocessors,
            id='r2p1d',
        ),

    ]
)

# Exogenous techniques use an external source to extract data
# In this case we are querying DBPedia to extract movies properties, using as value the "dbpedia_label" in the source file
movies_ca_config.add_single_exogenous(
    ca.ExogenousConfig(ca.DBPediaMappingTechnique('dbo:Film', 'dbpedia_label'),
                       id='dbpedia')
)

# fit the ContentAnalyzer
i_con = ca.ContentAnalyzer(config=movies_ca_config)
i_con.fit()

# User analyzer using extra small dataset
users_ca_config = ca.UserAnalyzerConfig(
    source=ca.CSVFile(os.path.join('datasets', 'ml-100k_extra_small', 'users_extra_small.csv')),
    id='user_id',
    output_directory=os.path.join('report_stuff', 'report_all', 'users_codified'),
)

# In this case the external source if the source file itself
# The properties gender and occupation are imported
users_ca_config.add_single_exogenous(
    ca.ExogenousConfig(ca.PropertiesFromDataset(field_name_list=['gender', 'occupation']))
)

u_con = ca.ContentAnalyzer(config=users_ca_config)
u_con.fit()

ratings = ca.Ratings(ca.CSVFile(os.path.join('datasets', 'ml-100k_extra_small', 'ratings_extra_small.csv')))
print(ratings)

# All available ContentBased algorithms

# there are different possible classifiers in the "clayrs/recsys/content_based_algorithm/classifier/classifiers.py" script
alg1 = rs.ClassifierRecommender({'plot': 'tfidf_sk'}, classifier=rs.SkKNN())

alg2 = rs.CentroidVector({'plot': 'tfidf_sk'}, rs.CosineSimilarity(), threshold=4)

# there are different possible regressors in the "clayrs/recsys/content_based_algorithm/regressor/regressors.py" script
alg3 = rs.LinearPredictor({'plot': 'tfidf_sk'}, regressor=rs.SkLinearRegression())

# In AMAR, different possible Recommenders can be instantiated depending on the number of sources (this also impacts the number of
# item fields that can be specified) and different neural networks can be used "clayrs/recsys/network_based_algorithm/amar/amar_network.py"
alg4 = rs.AmarSingleSource(rs.AmarNetworkBasic, {'plot': 'tfidf_sk'}, threshold=4, batch_size=512, epochs=5)
alg5 = rs.AmarDoubleSource(rs.AmarNetworkMerge, {'plot': 'tfidf_sk'}, {'plot': 'tfidf_sk'}, threshold=4, batch_size=512, epochs=5)
alg6 = rs.VBPR({'plot': 'sent2doc'}, gamma_dim=0, theta_dim=0, batch_size=512, epochs=1, threshold=1)
alg7 = rs.IndexQuery({'plot': 'search_i'}, threshold=4)

# Content-based experiment with KFoldPartitioning
rs.ContentBasedExperiment(
    ratings,
    items_directory=os.path.join("report_stuff", "report_all", "movies_codified"),
    partitioning_technique=rs.KFoldPartitioning(),
    algorithm_list=[alg1, alg2, alg3, alg4, alg5, alg6, alg7],
    metric_list=[
        eva.Precision(),
        eva.Recall(),
        eva.FMeasure(),
        eva.GiniIndex(),
        eva.NDCG(),
        eva.RPrecision(),
        eva.RMSE(),
        eva.MSE(),
        eva.MAE(),
        eva.MRR(),
        eva.MAP(),
        eva.CatalogCoverage(set(ratings.unique_item_id_column)),
        eva.PopRecsCorrelation(original_ratings=ratings),
        eva.PredictionCoverage(set(ratings.unique_item_id_column)),
        eva.LongTailDistr(),
        eva.Correlation(),
        eva.PrecisionAtK(k=5),
        eva.RecallAtK(k=5),
        eva.FMeasureAtK(k=5, sys_average='micro'),
        eva.MRRAtK(k=5),
        eva.NDCGAtK(k=5),
    ],
    overwrite_if_exists=True,
    report=True,
    output_folder=output_folder
).rank(methodology=rs.TestRatingsMethodology())

alg_graph_1 = rs.NXPageRank()

# Graph-based experiment with HoldOutPartitioning
rs.GraphBasedExperiment(
    ratings,
    items_directory=os.path.join("report_stuff", "report_all", "movies_codified"),
    users_directory=os.path.join("report_stuff", "report_all", "users_codified"),
    # specify the exogenous properties to use
    item_exo_properties={0},
    user_exo_properties={0},
    link_label="score",
    partitioning_technique=rs.HoldOutPartitioning(train_set_size=0.75, random_state=99),
    algorithm_list=[alg_graph_1],
    metric_list=[
        eva.Precision(),
        eva.Recall(),
        eva.FMeasure(),
        eva.GiniIndex(),
        eva.NDCG(),
        eva.RPrecision(),
        eva.RMSE(),
        eva.MSE(),
        eva.MAE(),
        eva.MRR(),
        eva.MAP(),
        eva.CatalogCoverage(set(ratings.unique_item_id_column)),
        eva.PopRecsCorrelation(original_ratings=ratings),
        eva.PredictionCoverage(set(ratings.unique_item_id_column)),
        eva.LongTailDistr(),
        eva.Correlation(),
        eva.PrecisionAtK(k=5),
        eva.RecallAtK(k=5),
        eva.FMeasureAtK(k=5, sys_average='micro'),
        eva.MRRAtK(k=5),
        eva.NDCGAtK(k=5),
    ],
    overwrite_if_exists=True,
    report=True,
    output_folder=output_folder_graph
).rank(methodology=rs.TestRatingsMethodology())

# report CA which is skipped by the Experiment class
ut.Report(output_dir=output_folder, ca_report_filename="item_ca_report").yaml(content_analyzer=i_con)
ut.Report(output_dir=output_folder, ca_report_filename="user_ca_report").yaml(content_analyzer=u_con)

ut.Report(output_dir=output_folder_graph, ca_report_filename="item_ca_report").yaml(content_analyzer=i_con)
ut.Report(output_dir=output_folder_graph, ca_report_filename="user_ca_report").yaml(content_analyzer=u_con)

# score processors in ratings file
output_folder_ratings = os.path.join("report_stuff", "report_all", "ratings_rep")
os.makedirs(output_folder_ratings, exist_ok=True)

ratings = ca.Ratings(ca.CSVFile(os.path.join('datasets', 'ml-100k_extra_small', 'ratings_extra_small.csv')), score_processor=ca.NumberNormalizer(scale=(1., 2.)))
ut.Report(output_dir=output_folder_ratings, rs_report_filename="ratings_report_norm").yaml(original_ratings=ratings)

ratings = ca.Ratings(ca.CSVFile(os.path.join('datasets', 'ml-100k_extra_small', 'ratings_extra_small.csv')), score_processor=ca.TextBlobSentimentAnalysis())
ut.Report(output_dir=output_folder_ratings, rs_report_filename="ratings_report_sa").yaml(original_ratings=ratings)

# MISSING:
#
# ContentAnalyzer:
# - BabelPyEntityLinking
#
# RecSys:
# - BootstrapPartitioning (same as HoldOut and KFold)
#
# EvalModule:
# - PopRatioProfileVsRecs
# - DeltaGap