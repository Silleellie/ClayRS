{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Warning Docs are still a WIP Welcome to ClayRS's documentation! ClayRS is a python framework for (mainly) content-based recommender systems which allows you to perform several operations, starting from a raw representation of users and items to building and evaluating a recommender system. It also supports graph-based recommendation with feature selection algorithms and graph manipulation methods. The framework has three main modules, which you can also use individually: Given a raw source, the Content Analyzer : Creates and serializes contents, Using the chosen configuration The RecSys module allows to: Instantiate a recommender system Using items and users serialized by the Content Analyzer Make score prediction or recommend items for the active user(s) The EvalModel has the task of evaluating a recommender system, using several state-of-the-art metrics Code examples for all three modules will follow in the Usage section","title":"Home"},{"location":"#welcome-to-clayrss-documentation","text":"ClayRS is a python framework for (mainly) content-based recommender systems which allows you to perform several operations, starting from a raw representation of users and items to building and evaluating a recommender system. It also supports graph-based recommendation with feature selection algorithms and graph manipulation methods. The framework has three main modules, which you can also use individually: Given a raw source, the Content Analyzer : Creates and serializes contents, Using the chosen configuration The RecSys module allows to: Instantiate a recommender system Using items and users serialized by the Content Analyzer Make score prediction or recommend items for the active user(s) The EvalModel has the task of evaluating a recommender system, using several state-of-the-art metrics Code examples for all three modules will follow in the Usage section","title":"Welcome to ClayRS's documentation!"},{"location":"content_analyzer/introduction/","text":"Introduction ClayRS is a python framework for (mainly) content-based recommender systems which allows you to perform several operations, starting from a raw representation of users and items to building and evaluating a recommender system. It also supports graph-based recommendation with feature selection algorithms and graph manipulation methods. The framework has three main modules, which you can also use individually: Given a raw source, the Content Analyzer : * Creates and serializes contents, * Using the chosen configuration The RecSys module allows to: * Instantiate a recommender system * Using items and users serialized by the Content Analyzer * Make score prediction or recommend items for the active user(s) The EvalModel has the task of evaluating a recommender system, using several state-of-the-art metrics Code examples for all three modules will follow in the Usage section","title":"Introduction"},{"location":"content_analyzer/introduction/#introduction","text":"ClayRS is a python framework for (mainly) content-based recommender systems which allows you to perform several operations, starting from a raw representation of users and items to building and evaluating a recommender system. It also supports graph-based recommendation with feature selection algorithms and graph manipulation methods. The framework has three main modules, which you can also use individually: Given a raw source, the Content Analyzer : * Creates and serializes contents, * Using the chosen configuration The RecSys module allows to: * Instantiate a recommender system * Using items and users serialized by the Content Analyzer * Make score prediction or recommend items for the active user(s) The EvalModel has the task of evaluating a recommender system, using several state-of-the-art metrics Code examples for all three modules will follow in the Usage section","title":"Introduction"},{"location":"content_analyzer/ratings/","text":"Ratings Ratings ( source , user_id_column = 0 , item_id_column = 1 , score_column = 2 , timestamp_column = None , score_processor = None ) Class that imports the ratings Parameters: Name Type Description Default source RawInformationSource Source from which the ratings will be imported required rating_configs list<RatingsFieldConfig> required from_id_column str Name of the field containing the reference to the person who gave the rating (for example, the user id) required to_id_column str Name of the field containing the reference to the item that a person rated required timestamp_column str Name of the field containing the timestamp None output_directory str Name of the directory where the acquired ratings will be stored required score_combiner str Metric to use to combine the scores required Source code in clayrs/content_analyzer/ratings_manager/ratings.py 79 80 81 82 83 84 85 86 87 def __init__ ( self , source : RawInformationSource , user_id_column : Union [ str , int ] = 0 , item_id_column : Union [ str , int ] = 1 , score_column : Union [ str , int ] = 2 , timestamp_column : Union [ str , int ] = None , score_processor : RatingProcessor = None ): self . _ratings_dict = self . _import_ratings ( source , user_id_column , item_id_column , score_column , timestamp_column , score_processor ) RatingsLowMemory ( source , user_id_column = 0 , item_id_column = 1 , score_column = 2 , timestamp_column = None , score_processor = None ) Source code in clayrs/content_analyzer/ratings_manager/ratings.py 302 303 304 305 306 307 308 309 310 311 def __init__ ( self , source : RawInformationSource , user_id_column : Union [ str , int ] = 0 , item_id_column : Union [ str , int ] = 1 , score_column : Union [ str , int ] = 2 , timestamp_column : Union [ str , int ] = None , score_processor : RatingProcessor = None ): rat = pd . DataFrame ( source , dtype = str ) self . _ratings_dict = self . _import_ratings ( rat , user_id_column , item_id_column , score_column , timestamp_column , score_processor )","title":"Ratings"},{"location":"content_analyzer/ratings/#ratings","text":"","title":"Ratings"},{"location":"content_analyzer/ratings/#clayrs.content_analyzer.ratings_manager.ratings.Ratings","text":"Class that imports the ratings Parameters: Name Type Description Default source RawInformationSource Source from which the ratings will be imported required rating_configs list<RatingsFieldConfig> required from_id_column str Name of the field containing the reference to the person who gave the rating (for example, the user id) required to_id_column str Name of the field containing the reference to the item that a person rated required timestamp_column str Name of the field containing the timestamp None output_directory str Name of the directory where the acquired ratings will be stored required score_combiner str Metric to use to combine the scores required Source code in clayrs/content_analyzer/ratings_manager/ratings.py 79 80 81 82 83 84 85 86 87 def __init__ ( self , source : RawInformationSource , user_id_column : Union [ str , int ] = 0 , item_id_column : Union [ str , int ] = 1 , score_column : Union [ str , int ] = 2 , timestamp_column : Union [ str , int ] = None , score_processor : RatingProcessor = None ): self . _ratings_dict = self . _import_ratings ( source , user_id_column , item_id_column , score_column , timestamp_column , score_processor )","title":"Ratings"},{"location":"content_analyzer/ratings/#clayrs.content_analyzer.ratings_manager.ratings.RatingsLowMemory","text":"Source code in clayrs/content_analyzer/ratings_manager/ratings.py 302 303 304 305 306 307 308 309 310 311 def __init__ ( self , source : RawInformationSource , user_id_column : Union [ str , int ] = 0 , item_id_column : Union [ str , int ] = 1 , score_column : Union [ str , int ] = 2 , timestamp_column : Union [ str , int ] = None , score_processor : RatingProcessor = None ): rat = pd . DataFrame ( source , dtype = str ) self . _ratings_dict = self . _import_ratings ( rat , user_id_column , item_id_column , score_column , timestamp_column , score_processor )","title":"RatingsLowMemory"},{"location":"evaluation/eval_model/","text":"Eval Model class EvalModel ( pred_list , truth_list , metric_list ) Class for evaluating a recommender system. The Evaluation module needs the following parameters: A list of computed rank/predictions (in case multiple splits must be evaluated) A list of truths (in case multiple splits must be evaluated) List of metrics to compute Obviously the list of computed rank/predictions and list of truths must have the same length, and the rank/prediction in position \\(i\\) will be compared with the truth at position \\(i\\) Examples: >>> import clayrs.evaluation as eva >>> >>> em = eva . EvalModel ( >>> pred_list = rank_list , >>> truth_list = truth_list , >>> metric_list = [ >>> eva . NDCG (), >>> eva . Precision () >>> eva . RecallAtK ( k = 5 , sys_average = 'micro' ) >>> ] >>> ) Then call the fit() method of the instantiated EvalModel to perform the actual evaluation Parameters: Name Type Description Default pred_list Union [ List [ Prediction ], List [ Rank ]] Recommendations list to evaluate. It's a list in case multiple splits must be evaluated. Both Rank objects (where items are ordered and the score is not relevant) or Prediction objects (where the score predicted is the predicted rating for the user regarding a certain item) can be evaluated required truth_list List [ Ratings ] Ground truths list used to compare recommendations. It's a list in case multiple splits must be evaluated. required metric_list List [ Metric ] List of metrics that will be used to evaluate recommendation list specified required Raises: Type Description ValueError ValueError is raised in case the pred_list and truth_list are empty or have different length Source code in clayrs/evaluation/eval_model.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 def __init__ ( self , pred_list : Union [ List [ Prediction ], List [ Rank ]], truth_list : List [ Ratings ], metric_list : List [ Metric ]): if len ( pred_list ) == 0 and len ( truth_list ) == 0 : raise ValueError ( \"List containing predictions and list containing ground truths are empty!\" ) elif len ( pred_list ) != len ( truth_list ): raise ValueError ( \"List containing predictions and list containing ground truths must have the same length!\" ) self . _pred_list = pred_list self . _truth_list = truth_list self . _metric_list = metric_list self . _yaml_report_result = None append_metric ( metric ) Append a metric to the metric list that will be used to evaluate recommendation lists Parameters: Name Type Description Default metric Metric Metric to append to the metric list required Source code in clayrs/evaluation/eval_model.py 97 98 99 100 101 102 103 104 def append_metric ( self , metric : Metric ): \"\"\" Append a metric to the metric list that will be used to evaluate recommendation lists Args: metric: Metric to append to the metric list \"\"\" self . _metric_list . append ( metric ) fit ( user_id_list = None ) This method performs the actual evaluation of the recommendation frames passed as input in the constructor of the class In case you want to perform evaluation for selected users, specify their ids parameter of this method. Otherwise, all users in the recommendation frames will be considered in the evaluation process Examples: >>> import clayrs.evaluation as eva >>> selected_users = [ 'u1' , 'u22' , 'u3' ] # (1) >>> em = eva . EvalModel ( >>> pred_list , >>> truth_list , >>> metric_list = [ eva . Precision (), eva . Recall ()] >>> ) >>> em . fit ( selected_users ) The method returns two pandas DataFrame: one containing system results for every metric in the metric list, one containing users results for every metric eligible Returns: Type Description pd . DataFrame The first DataFrame contains the system result for every metric inside the metric_list pd . DataFrame The second DataFrame contains every user results for every metric eligible inside the metric_list Source code in clayrs/evaluation/eval_model.py 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 def fit ( self , user_id_list : list = None ) -> Tuple [ pd . DataFrame , pd . DataFrame ]: \"\"\" This method performs the actual evaluation of the recommendation frames passed as input in the constructor of the class In case you want to perform evaluation for selected users, specify their ids parameter of this method. Otherwise, all users in the recommendation frames will be considered in the evaluation process Examples: >>> import clayrs.evaluation as eva >>> selected_users = ['u1', 'u22', 'u3'] # (1) >>> em = eva.EvalModel( >>> pred_list, >>> truth_list, >>> metric_list=[eva.Precision(), eva.Recall()] >>> ) >>> em.fit(selected_users) The method returns two pandas DataFrame: one containing ***system results*** for every metric in the metric list, one containing ***users results*** for every metric eligible Returns: The first DataFrame contains the **system result** for every metric inside the metric_list The second DataFrame contains every **user results** for every metric eligible inside the metric_list \"\"\" logger . info ( 'Performing evaluation on metrics chosen' ) pred_list = self . _pred_list truth_list = self . _truth_list if user_id_list is not None : user_id_list_set = set ([ str ( user_id ) for user_id in user_id_list ]) pred_list = [ pred . filter_ratings ( user_id_list_set ) for pred in self . _pred_list ] truth_list = [ truth . filter_ratings ( user_id_list_set ) for truth in self . _truth_list ] sys_result , users_result = MetricEvaluator ( pred_list , truth_list ) . eval_metrics ( self . metric_list ) # we save the sys result for report yaml self . _yaml_report_result = sys_result . to_dict ( orient = 'index' ) return sys_result , users_result metric_list () property List of metrics used to evaluate recommendation lists Returns: Type Description List [ Metric ] The list containing all metrics Source code in clayrs/evaluation/eval_model.py 87 88 89 90 91 92 93 94 95 @property def metric_list ( self ) -> List [ Metric ]: \"\"\" List of metrics used to evaluate recommendation lists Returns: The list containing all metrics \"\"\" return self . _metric_list pred_list () property List containing recommendations frame Returns: Type Description Union [ List [ Prediction ], List [ Rank ]] The list containing recommendations frame Source code in clayrs/evaluation/eval_model.py 67 68 69 70 71 72 73 74 75 @property def pred_list ( self ) -> Union [ List [ Prediction ], List [ Rank ]]: \"\"\" List containing recommendations frame Returns: The list containing recommendations frame \"\"\" return self . _pred_list truth_list () property List containing ground truths Returns: Type Description List [ Ratings ] The list containing ground truths Source code in clayrs/evaluation/eval_model.py 77 78 79 80 81 82 83 84 85 @property def truth_list ( self ) -> List [ Ratings ]: \"\"\" List containing ground truths Returns: The list containing ground truths \"\"\" return self . _truth_list","title":"EvalModel class"},{"location":"evaluation/eval_model/#eval-model-class","text":"","title":"Eval Model class"},{"location":"evaluation/eval_model/#clayrs.evaluation.eval_model.EvalModel","text":"Class for evaluating a recommender system. The Evaluation module needs the following parameters: A list of computed rank/predictions (in case multiple splits must be evaluated) A list of truths (in case multiple splits must be evaluated) List of metrics to compute Obviously the list of computed rank/predictions and list of truths must have the same length, and the rank/prediction in position \\(i\\) will be compared with the truth at position \\(i\\) Examples: >>> import clayrs.evaluation as eva >>> >>> em = eva . EvalModel ( >>> pred_list = rank_list , >>> truth_list = truth_list , >>> metric_list = [ >>> eva . NDCG (), >>> eva . Precision () >>> eva . RecallAtK ( k = 5 , sys_average = 'micro' ) >>> ] >>> ) Then call the fit() method of the instantiated EvalModel to perform the actual evaluation Parameters: Name Type Description Default pred_list Union [ List [ Prediction ], List [ Rank ]] Recommendations list to evaluate. It's a list in case multiple splits must be evaluated. Both Rank objects (where items are ordered and the score is not relevant) or Prediction objects (where the score predicted is the predicted rating for the user regarding a certain item) can be evaluated required truth_list List [ Ratings ] Ground truths list used to compare recommendations. It's a list in case multiple splits must be evaluated. required metric_list List [ Metric ] List of metrics that will be used to evaluate recommendation list specified required Raises: Type Description ValueError ValueError is raised in case the pred_list and truth_list are empty or have different length Source code in clayrs/evaluation/eval_model.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 def __init__ ( self , pred_list : Union [ List [ Prediction ], List [ Rank ]], truth_list : List [ Ratings ], metric_list : List [ Metric ]): if len ( pred_list ) == 0 and len ( truth_list ) == 0 : raise ValueError ( \"List containing predictions and list containing ground truths are empty!\" ) elif len ( pred_list ) != len ( truth_list ): raise ValueError ( \"List containing predictions and list containing ground truths must have the same length!\" ) self . _pred_list = pred_list self . _truth_list = truth_list self . _metric_list = metric_list self . _yaml_report_result = None","title":"EvalModel"},{"location":"evaluation/eval_model/#clayrs.evaluation.eval_model.EvalModel.append_metric","text":"Append a metric to the metric list that will be used to evaluate recommendation lists Parameters: Name Type Description Default metric Metric Metric to append to the metric list required Source code in clayrs/evaluation/eval_model.py 97 98 99 100 101 102 103 104 def append_metric ( self , metric : Metric ): \"\"\" Append a metric to the metric list that will be used to evaluate recommendation lists Args: metric: Metric to append to the metric list \"\"\" self . _metric_list . append ( metric )","title":"append_metric()"},{"location":"evaluation/eval_model/#clayrs.evaluation.eval_model.EvalModel.fit","text":"This method performs the actual evaluation of the recommendation frames passed as input in the constructor of the class In case you want to perform evaluation for selected users, specify their ids parameter of this method. Otherwise, all users in the recommendation frames will be considered in the evaluation process Examples: >>> import clayrs.evaluation as eva >>> selected_users = [ 'u1' , 'u22' , 'u3' ] # (1) >>> em = eva . EvalModel ( >>> pred_list , >>> truth_list , >>> metric_list = [ eva . Precision (), eva . Recall ()] >>> ) >>> em . fit ( selected_users ) The method returns two pandas DataFrame: one containing system results for every metric in the metric list, one containing users results for every metric eligible Returns: Type Description pd . DataFrame The first DataFrame contains the system result for every metric inside the metric_list pd . DataFrame The second DataFrame contains every user results for every metric eligible inside the metric_list Source code in clayrs/evaluation/eval_model.py 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 def fit ( self , user_id_list : list = None ) -> Tuple [ pd . DataFrame , pd . DataFrame ]: \"\"\" This method performs the actual evaluation of the recommendation frames passed as input in the constructor of the class In case you want to perform evaluation for selected users, specify their ids parameter of this method. Otherwise, all users in the recommendation frames will be considered in the evaluation process Examples: >>> import clayrs.evaluation as eva >>> selected_users = ['u1', 'u22', 'u3'] # (1) >>> em = eva.EvalModel( >>> pred_list, >>> truth_list, >>> metric_list=[eva.Precision(), eva.Recall()] >>> ) >>> em.fit(selected_users) The method returns two pandas DataFrame: one containing ***system results*** for every metric in the metric list, one containing ***users results*** for every metric eligible Returns: The first DataFrame contains the **system result** for every metric inside the metric_list The second DataFrame contains every **user results** for every metric eligible inside the metric_list \"\"\" logger . info ( 'Performing evaluation on metrics chosen' ) pred_list = self . _pred_list truth_list = self . _truth_list if user_id_list is not None : user_id_list_set = set ([ str ( user_id ) for user_id in user_id_list ]) pred_list = [ pred . filter_ratings ( user_id_list_set ) for pred in self . _pred_list ] truth_list = [ truth . filter_ratings ( user_id_list_set ) for truth in self . _truth_list ] sys_result , users_result = MetricEvaluator ( pred_list , truth_list ) . eval_metrics ( self . metric_list ) # we save the sys result for report yaml self . _yaml_report_result = sys_result . to_dict ( orient = 'index' ) return sys_result , users_result","title":"fit()"},{"location":"evaluation/eval_model/#clayrs.evaluation.eval_model.EvalModel.metric_list","text":"List of metrics used to evaluate recommendation lists Returns: Type Description List [ Metric ] The list containing all metrics Source code in clayrs/evaluation/eval_model.py 87 88 89 90 91 92 93 94 95 @property def metric_list ( self ) -> List [ Metric ]: \"\"\" List of metrics used to evaluate recommendation lists Returns: The list containing all metrics \"\"\" return self . _metric_list","title":"metric_list()"},{"location":"evaluation/eval_model/#clayrs.evaluation.eval_model.EvalModel.pred_list","text":"List containing recommendations frame Returns: Type Description Union [ List [ Prediction ], List [ Rank ]] The list containing recommendations frame Source code in clayrs/evaluation/eval_model.py 67 68 69 70 71 72 73 74 75 @property def pred_list ( self ) -> Union [ List [ Prediction ], List [ Rank ]]: \"\"\" List containing recommendations frame Returns: The list containing recommendations frame \"\"\" return self . _pred_list","title":"pred_list()"},{"location":"evaluation/eval_model/#clayrs.evaluation.eval_model.EvalModel.truth_list","text":"List containing ground truths Returns: Type Description List [ Ratings ] The list containing ground truths Source code in clayrs/evaluation/eval_model.py 77 78 79 80 81 82 83 84 85 @property def truth_list ( self ) -> List [ Ratings ]: \"\"\" List containing ground truths Returns: The list containing ground truths \"\"\" return self . _truth_list","title":"truth_list()"},{"location":"evaluation/introduction/","text":"Warning Docs are still a WIP Introduction The Evaluation module has the task of evaluating a recommender system, using several state-of-the-art metrics The usage pipeline it's pretty simple, all the work is done by the EvalModel class. Suppose you want to evaluate recommendation lists using NDCG , macro Precision , micro Precision@5 , you need to instantiate the EvalModel class with the following parameters: A list of computed rank/predictions (in case multiple splits must be evaluated) A list of truths (in case multiple splits must be evaluated) List of metrics to compute Obviously the list of computed rank/predictions and list of truths must have the same length, and the rank/prediction in position \\(i\\) will be compared with the truth at position \\(i\\) Usage example In this case rank_list and truth_list are results obtained from the RecSys module of the framework import clayrs.evaluation as eva em = eva . EvalModel ( pred_list = rank_list , truth_list = truth_list , metric_list = [ eva . NDCG (), eva . Precision (), # (1) eva . RecallAtK ( k = 5 , sys_average = 'micro' ) ] ) If not specified, by default system average is computed as macro Info Precision , Recall , and in general all classification metrics requires a threshold which separates relevant items from non-relevant. If a threshold is specified, then it is fixed for all users If no threshold is specified, the mean rating score of each user will be used Check documentation for more Then simply call the fit () method of the instantiated object It will return two pandas DataFrame: the first one contains the metrics aggregated for the system, while the second contains the metrics computed for each user (where possible) sys_result , users_result = em . fit () Evaluating external recommendation lists The evaluation module is completely independent from the Recsys and Content Analyzer module: that means that we can easily evaluate recommendation lists computed by other frameworks/tools! Let's suppose we have recommendations (and related truths) generated via other tools in a csv format. We first import them into the framework and then pass them to the EvalModel class import clayrs.content_analyzer as ca csv_rank_1 = ca . CSVFile ( 'rank_split_1.csv' ) csv_truth_1 = ca . CSVFile ( 'truth_split_1.csv' ) csv_rank_2 = ca . CSVFile ( 'rank_split_2.csv' ) csv_truth_2 = ca . CSVFile ( 'truth_split_2.csv' ) # Importing split 1 (1) rank_1 = ca . Rank ( csv_rank_1 ) truth_1 = ca . Ratings ( csv_truth_1 ) # Importing split 2 (2) rank_2 = ca . Rank ( csv_rank_2 ) truth_2 = ca . Ratings ( csv_truth_2 ) # since multiple splits, we wrap ranks and truths in lists imported_ranks = [ rank_1 , rank_2 ] imported_truths = [ truth_1 , truth_2 ] Remember that this instantiation to the Rank/Ratings class assumes a certain order of the columns of your raw source. Otherwise, you need to manually map columns. Check documentation for more Remember that this instantiation to the Rank/Ratings class assumes a certain order of the columns of your raw source. Otherwise, you need to manually map columns. Check documentation for more Then simply evaluate them exactly in the same way as shown before! import clayrs.evaluation as eva em = eva . EvalModel ( pred_list = imported_ranks , truth_list = imported_truths , metric_list = [ # ... Choose your own metrics ] ) sys_results_df , users_results_df = em . fit () Perform a statistical test ClayRS lets you also compare different learning schemas by performing statistical tests: Simply instantiate the desired test and call its perform () method. The parameter it expects is the list of user_results dataframe obtained in the evaluation step, one for each learning schema to compare. ttest = eva . Ttest () all_comb_df = ttest . perform ([ user_result1 , user_result2 , user_result3 ]) Info In this case since the Ttest it's a paired test, the final result is a pandas DataFrame which contains learning schemas compared in pair: (system1, system2) (system1, system3) (system2, system3)","title":"Introduction"},{"location":"evaluation/introduction/#introduction","text":"The Evaluation module has the task of evaluating a recommender system, using several state-of-the-art metrics The usage pipeline it's pretty simple, all the work is done by the EvalModel class. Suppose you want to evaluate recommendation lists using NDCG , macro Precision , micro Precision@5 , you need to instantiate the EvalModel class with the following parameters: A list of computed rank/predictions (in case multiple splits must be evaluated) A list of truths (in case multiple splits must be evaluated) List of metrics to compute Obviously the list of computed rank/predictions and list of truths must have the same length, and the rank/prediction in position \\(i\\) will be compared with the truth at position \\(i\\)","title":"Introduction"},{"location":"evaluation/introduction/#usage-example","text":"In this case rank_list and truth_list are results obtained from the RecSys module of the framework import clayrs.evaluation as eva em = eva . EvalModel ( pred_list = rank_list , truth_list = truth_list , metric_list = [ eva . NDCG (), eva . Precision (), # (1) eva . RecallAtK ( k = 5 , sys_average = 'micro' ) ] ) If not specified, by default system average is computed as macro Info Precision , Recall , and in general all classification metrics requires a threshold which separates relevant items from non-relevant. If a threshold is specified, then it is fixed for all users If no threshold is specified, the mean rating score of each user will be used Check documentation for more Then simply call the fit () method of the instantiated object It will return two pandas DataFrame: the first one contains the metrics aggregated for the system, while the second contains the metrics computed for each user (where possible) sys_result , users_result = em . fit ()","title":"Usage example"},{"location":"evaluation/introduction/#evaluating-external-recommendation-lists","text":"The evaluation module is completely independent from the Recsys and Content Analyzer module: that means that we can easily evaluate recommendation lists computed by other frameworks/tools! Let's suppose we have recommendations (and related truths) generated via other tools in a csv format. We first import them into the framework and then pass them to the EvalModel class import clayrs.content_analyzer as ca csv_rank_1 = ca . CSVFile ( 'rank_split_1.csv' ) csv_truth_1 = ca . CSVFile ( 'truth_split_1.csv' ) csv_rank_2 = ca . CSVFile ( 'rank_split_2.csv' ) csv_truth_2 = ca . CSVFile ( 'truth_split_2.csv' ) # Importing split 1 (1) rank_1 = ca . Rank ( csv_rank_1 ) truth_1 = ca . Ratings ( csv_truth_1 ) # Importing split 2 (2) rank_2 = ca . Rank ( csv_rank_2 ) truth_2 = ca . Ratings ( csv_truth_2 ) # since multiple splits, we wrap ranks and truths in lists imported_ranks = [ rank_1 , rank_2 ] imported_truths = [ truth_1 , truth_2 ] Remember that this instantiation to the Rank/Ratings class assumes a certain order of the columns of your raw source. Otherwise, you need to manually map columns. Check documentation for more Remember that this instantiation to the Rank/Ratings class assumes a certain order of the columns of your raw source. Otherwise, you need to manually map columns. Check documentation for more Then simply evaluate them exactly in the same way as shown before! import clayrs.evaluation as eva em = eva . EvalModel ( pred_list = imported_ranks , truth_list = imported_truths , metric_list = [ # ... Choose your own metrics ] ) sys_results_df , users_results_df = em . fit ()","title":"Evaluating external recommendation lists"},{"location":"evaluation/introduction/#perform-a-statistical-test","text":"ClayRS lets you also compare different learning schemas by performing statistical tests: Simply instantiate the desired test and call its perform () method. The parameter it expects is the list of user_results dataframe obtained in the evaluation step, one for each learning schema to compare. ttest = eva . Ttest () all_comb_df = ttest . perform ([ user_result1 , user_result2 , user_result3 ]) Info In this case since the Ttest it's a paired test, the final result is a pandas DataFrame which contains learning schemas compared in pair: (system1, system2) (system1, system3) (system2, system3)","title":"Perform a statistical test"},{"location":"evaluation/metrics/classification_metrics/","text":"Classification metrics A classification metric uses confusion matrix terminology (true positive, false positive, true negative, false negative) to classify each item predicted, and in general it needs a way to discern relevant items from non-relevant items for users FMeasure ( beta = 1 , relevant_threshold = None , sys_average = 'macro' , precision = np . float64 ) Bases: ClassificationMetric The FMeasure metric combines Precision and Recall into a single metric. It is calculated as such for the single user : \\[ FMeasure_u = (1 + \\beta^2) \\cdot \\frac{P_u \\cdot R_u}{(\\beta^2 \\cdot P_u) + R_u} \\] Where: \\(P_u\\) is the Precision calculated for the user u \\(R_u\\) is the Recall calculated for the user u \\(\\beta\\) is a real factor which could weight differently Recall or Precision based on its value: \\(\\beta = 1\\) : Equally weight Precision and Recall \\(\\beta > 1\\) : Weight Recall more \\(\\beta < 1\\) : Weight Precision more A famous FMeasure is the F1 Metric, where \\(\\beta = 1\\) , which basically is the harmonic mean of recall and precision: \\[ F1_u = \\frac{2 \\cdot P_u \\cdot R_u}{P_u + R_u} \\] The FMeasure metric is calculated as such for the entire system , depending if 'macro' average or 'micro' average has been chosen: \\[ FMeasure_{sys} - micro = (1 + \\beta^2) \\cdot \\frac{P_u \\cdot R_u}{(\\beta^2 \\cdot P_u) + R_u} \\] \\[ FMeasure_{sys} - macro = \\frac{\\sum_{u \\in U} FMeasure_u}{|U|} \\] Parameters: Name Type Description Default beta float real factor which could weight differently Recall or Precision based on its value. Default is 1 1 relevant_threshold float parameter needed to discern relevant items and non-relevant items for every user. If not specified, the mean rating score of every user will be used None sys_average str specify how the system average must be computed. Default is 'macro' 'macro' Source code in clayrs/evaluation/metrics/classification_metrics.py 461 462 463 464 def __init__ ( self , beta : float = 1 , relevant_threshold : float = None , sys_average : str = 'macro' , precision : [ Callable ] = np . float64 ): super () . __init__ ( relevant_threshold , sys_average , precision ) self . __beta = beta FMeasureAtK ( k , beta = 1 , relevant_threshold = None , sys_average = 'macro' ) Bases: FMeasure The FMeasure@K metric combines Precision@K and Recall@K into a single metric. It is calculated as such for the single user : \\[ FMeasure@K_u = (1 + \\beta^2) \\cdot \\frac{P@K_u \\cdot R@K_u}{(\\beta^2 \\cdot P@K_u) + R@K_u} \\] Where: \\(P@K_u\\) is the Precision at K calculated for the user u \\(R@K_u\\) is the Recall at K calculated for the user u \\(\\beta\\) is a real factor which could weight differently Recall or Precision based on its value: \\(\\beta = 1\\) : Equally weight Precision and Recall \\(\\beta > 1\\) : Weight Recall more \\(\\beta < 1\\) : Weight Precision more A famous FMeasure@K is the F1@K Metric, where :math: \\beta = 1 , which basically is the harmonic mean of recall and precision: \\[ F1@K_u = \\frac{2 \\cdot P@K_u \\cdot R@K_u}{P@K_u + R@K_u} \\] The FMeasure@K metric is calculated as such for the entire system , depending if 'macro' average or 'micro' average has been chosen: \\[ FMeasure@K_{sys} - micro = (1 + \\beta^2) \\cdot \\frac{P@K_u \\cdot R@K_u}{(\\beta^2 \\cdot P@K_u) + R@K_u} \\] \\[ FMeasure@K_{sys} - macro = \\frac{\\sum_{u \\in U} FMeasure@K_u}{|U|} \\] Parameters: Name Type Description Default k int cutoff parameter. Will be used for the computation of Precision@K and Recall@K required beta float real factor which could weight differently Recall or Precision based on its value. Default is 1 1 relevant_threshold float parameter needed to discern relevant items and non-relevant items for every user. If not specified, the mean rating score of every user will be used None sys_average str specify how the system average must be computed. Default is 'macro' 'macro' Source code in clayrs/evaluation/metrics/classification_metrics.py 547 548 549 550 551 def __init__ ( self , k : int , beta : int = 1 , relevant_threshold : float = None , sys_average : str = 'macro' ): super () . __init__ ( beta , relevant_threshold , sys_average ) if k < 1 : raise ValueError ( 'k= {} not valid! k must be >= 1!' . format ( k )) self . __k = k Precision ( relevant_threshold = None , sys_average = 'macro' , precision = np . float64 ) Bases: ClassificationMetric The Precision metric is calculated as such for the single user : \\[ Precision_u = \\frac{tp_u}{tp_u + fp_u} \\] Where: \\(tp_u\\) is the number of items which are in the recommendation list of the user and have a rating >= relevant_threshold in its 'ground truth' \\(fp_u\\) is the number of items which are in the recommendation list of the user and have a rating < relevant_threshold in its 'ground truth' And it is calculated as such for the entire system , depending if 'macro' average or 'micro' average has been chosen: \\[ Precision_{sys} - micro = \\frac{\\sum_{u \\in U} tp_u}{\\sum_{u \\in U} tp_u + \\sum_{u \\in U} fp_u} \\] \\[ Precision_{sys} - macro = \\frac{\\sum_{u \\in U} Precision_u}{|U|} \\] Parameters: Name Type Description Default relevant_threshold float parameter needed to discern relevant items and non-relevant items for every user. If not specified, the mean rating score of every user will be used None sys_average str specify how the system average must be computed. Default is 'macro' 'macro' Source code in clayrs/evaluation/metrics/classification_metrics.py 145 146 147 def __init__ ( self , relevant_threshold : float = None , sys_average : str = 'macro' , precision : [ Callable ] = np . float64 ): super ( Precision , self ) . __init__ ( relevant_threshold , sys_average , precision ) PrecisionAtK ( k , relevant_threshold = None , sys_average = 'macro' , precision = np . float64 ) Bases: Precision The Precision@K metric is calculated as such for the single user : \\[ Precision@K_u = \\frac{tp@K_u}{tp@K_u + fp@K_u} \\] Where: \\(tp@K_u\\) is the number of items which are in the recommendation list of the user cutoff to the first K items and have a rating >= relevant_threshold in its 'ground truth' \\(tp@K_u\\) is the number of items which are in the recommendation list of the user cutoff to the first K items and have a rating < relevant_threshold in its 'ground truth' And it is calculated as such for the entire system , depending if 'macro' average or 'micro' average has been chosen: \\[ Precision@K_{sys} - micro = \\frac{\\sum_{u \\in U} tp@K_u}{\\sum_{u \\in U} tp@K_u + \\sum_{u \\in U} fp@K_u} \\] \\[ Precision@K_{sys} - macro = \\frac{\\sum_{u \\in U} Precision@K_u}{|U|} \\] Parameters: Name Type Description Default k int cutoff parameter. Only the first k items of the recommendation list will be considered required relevant_threshold float parameter needed to discern relevant items and non-relevant items for every user. If not specified, the mean rating score of every user will be used None sys_average str specify how the system average must be computed. Default is 'macro' 'macro' Source code in clayrs/evaluation/metrics/classification_metrics.py 206 207 208 209 210 211 def __init__ ( self , k : int , relevant_threshold : float = None , sys_average : str = 'macro' , precision : [ Callable ] = np . float64 ): super () . __init__ ( relevant_threshold , sys_average , precision ) if k < 1 : raise ValueError ( 'k= {} not valid! k must be >= 1!' . format ( k )) self . __k = k RPrecision ( relevant_threshold = None , sys_average = 'macro' , precision = np . float64 ) Bases: Precision The R-Precision metric is calculated as such for the single user : \\[ R-Precision_u = \\frac{tp@R_u}{tp@R_u + fp@R_u} \\] Where: \\(R\\) it's the number of relevant items for the user u \\(tp@R_u\\) is the number of items which are in the recommendation list of the user cutoff to the first R items and have a rating >= relevant_threshold in its 'ground truth' \\(tp@R_u\\) is the number of items which are in the recommendation list of the user cutoff to the first R items and have a rating < relevant_threshold in its 'ground truth' And it is calculated as such for the entire system , depending if 'macro' average or 'micro' average has been chosen: \\[ Precision@R_{sys} - micro = \\frac{\\sum_{u \\in U} tp@R_u}{\\sum_{u \\in U} tp@R_u + \\sum_{u \\in U} fp@R_u} \\] \\[ Precision@R_{sys} - macro = \\frac{\\sum_{u \\in U} R-Precision_u}{|U|} \\] Parameters: Name Type Description Default relevant_threshold float parameter needed to discern relevant items and non-relevant items for every user. If not specified, the mean rating score of every user will be used None sys_average str specify how the system average must be computed. Default is 'macro' 'macro' Source code in clayrs/evaluation/metrics/classification_metrics.py 269 270 271 def __init__ ( self , relevant_threshold : float = None , sys_average : str = 'macro' , precision : [ Callable ] = np . float64 ): super () . __init__ ( relevant_threshold , sys_average , precision ) Recall ( relevant_threshold = None , sys_average = 'macro' , precision = np . float64 ) Bases: ClassificationMetric The Recall metric is calculated as such for the single user : \\[ Recall_u = \\frac{tp_u}{tp_u + fn_u} \\] Where: \\(tp_u\\) is the number of items which are in the recommendation list of the user and have a rating >= relevant_threshold in its 'ground truth' \\(fn_u\\) is the number of items which are NOT in the recommendation list of the user and have a rating >= relevant_threshold in its 'ground truth' And it is calculated as such for the entire system , depending if 'macro' average or 'micro' average has been chosen: \\[ Recall_{sys} - micro = \\frac{\\sum_{u \\in U} tp_u}{\\sum_{u \\in U} tp_u + \\sum_{u \\in U} fn_u} \\] \\[ Recall_{sys} - macro = \\frac{\\sum_{u \\in U} Recall_u}{|U|} \\] Parameters: Name Type Description Default relevant_threshold float parameter needed to discern relevant items and non-relevant items for every user. If not specified, the mean rating score of every user will be used None sys_average str specify how the system average must be computed. Default is 'macro' 'macro' Source code in clayrs/evaluation/metrics/classification_metrics.py 326 327 328 def __init__ ( self , relevant_threshold : float = None , sys_average : str = 'macro' , precision : [ Callable ] = np . float64 ): super () . __init__ ( relevant_threshold , sys_average , precision ) RecallAtK ( k , relevant_threshold = None , sys_average = 'macro' , precision = np . float64 ) Bases: Recall The Recall@K metric is calculated as such for the single user : \\[ Recall@K_u = \\frac{tp@K_u}{tp@K_u + fn@K_u} \\] Where: \\(tp@K_u\\) is the number of items which are in the recommendation list of the user cutoff to the first K items and have a rating >= relevant_threshold in its 'ground truth' \\(tp@K_u\\) is the number of items which are NOT in the recommendation list of the user cutoff to the first K items and have a rating >= relevant_threshold in its 'ground truth' And it is calculated as such for the entire system , depending if 'macro' average or 'micro' average has been chosen: \\[ Recall@K_{sys} - micro = \\frac{\\sum_{u \\in U} tp@K_u}{\\sum_{u \\in U} tp@K_u + \\sum_{u \\in U} fn@K_u} \\] \\[ Recall@K_{sys} - macro = \\frac{\\sum_{u \\in U} Recall@K_u}{|U|} \\] Parameters: Name Type Description Default k int cutoff parameter. Only the first k items of the recommendation list will be considered required relevant_threshold float parameter needed to discern relevant items and non-relevant items for every user. If not specified, the mean rating score of every user will be used None sys_average str specify how the system average must be computed. Default is 'macro' 'macro' Source code in clayrs/evaluation/metrics/classification_metrics.py 386 387 388 389 390 391 def __init__ ( self , k : int , relevant_threshold : float = None , sys_average : str = 'macro' , precision : [ Callable ] = np . float64 ): super () . __init__ ( relevant_threshold , sys_average , precision ) if k < 1 : raise ValueError ( 'k= {} not valid! k must be >= 1!' . format ( k )) self . __k = k","title":"Classification metrics"},{"location":"evaluation/metrics/classification_metrics/#classification-metrics","text":"A classification metric uses confusion matrix terminology (true positive, false positive, true negative, false negative) to classify each item predicted, and in general it needs a way to discern relevant items from non-relevant items for users","title":"Classification metrics"},{"location":"evaluation/metrics/classification_metrics/#clayrs.evaluation.metrics.classification_metrics.FMeasure","text":"Bases: ClassificationMetric The FMeasure metric combines Precision and Recall into a single metric. It is calculated as such for the single user : \\[ FMeasure_u = (1 + \\beta^2) \\cdot \\frac{P_u \\cdot R_u}{(\\beta^2 \\cdot P_u) + R_u} \\] Where: \\(P_u\\) is the Precision calculated for the user u \\(R_u\\) is the Recall calculated for the user u \\(\\beta\\) is a real factor which could weight differently Recall or Precision based on its value: \\(\\beta = 1\\) : Equally weight Precision and Recall \\(\\beta > 1\\) : Weight Recall more \\(\\beta < 1\\) : Weight Precision more A famous FMeasure is the F1 Metric, where \\(\\beta = 1\\) , which basically is the harmonic mean of recall and precision: \\[ F1_u = \\frac{2 \\cdot P_u \\cdot R_u}{P_u + R_u} \\] The FMeasure metric is calculated as such for the entire system , depending if 'macro' average or 'micro' average has been chosen: \\[ FMeasure_{sys} - micro = (1 + \\beta^2) \\cdot \\frac{P_u \\cdot R_u}{(\\beta^2 \\cdot P_u) + R_u} \\] \\[ FMeasure_{sys} - macro = \\frac{\\sum_{u \\in U} FMeasure_u}{|U|} \\] Parameters: Name Type Description Default beta float real factor which could weight differently Recall or Precision based on its value. Default is 1 1 relevant_threshold float parameter needed to discern relevant items and non-relevant items for every user. If not specified, the mean rating score of every user will be used None sys_average str specify how the system average must be computed. Default is 'macro' 'macro' Source code in clayrs/evaluation/metrics/classification_metrics.py 461 462 463 464 def __init__ ( self , beta : float = 1 , relevant_threshold : float = None , sys_average : str = 'macro' , precision : [ Callable ] = np . float64 ): super () . __init__ ( relevant_threshold , sys_average , precision ) self . __beta = beta","title":"FMeasure"},{"location":"evaluation/metrics/classification_metrics/#clayrs.evaluation.metrics.classification_metrics.FMeasureAtK","text":"Bases: FMeasure The FMeasure@K metric combines Precision@K and Recall@K into a single metric. It is calculated as such for the single user : \\[ FMeasure@K_u = (1 + \\beta^2) \\cdot \\frac{P@K_u \\cdot R@K_u}{(\\beta^2 \\cdot P@K_u) + R@K_u} \\] Where: \\(P@K_u\\) is the Precision at K calculated for the user u \\(R@K_u\\) is the Recall at K calculated for the user u \\(\\beta\\) is a real factor which could weight differently Recall or Precision based on its value: \\(\\beta = 1\\) : Equally weight Precision and Recall \\(\\beta > 1\\) : Weight Recall more \\(\\beta < 1\\) : Weight Precision more A famous FMeasure@K is the F1@K Metric, where :math: \\beta = 1 , which basically is the harmonic mean of recall and precision: \\[ F1@K_u = \\frac{2 \\cdot P@K_u \\cdot R@K_u}{P@K_u + R@K_u} \\] The FMeasure@K metric is calculated as such for the entire system , depending if 'macro' average or 'micro' average has been chosen: \\[ FMeasure@K_{sys} - micro = (1 + \\beta^2) \\cdot \\frac{P@K_u \\cdot R@K_u}{(\\beta^2 \\cdot P@K_u) + R@K_u} \\] \\[ FMeasure@K_{sys} - macro = \\frac{\\sum_{u \\in U} FMeasure@K_u}{|U|} \\] Parameters: Name Type Description Default k int cutoff parameter. Will be used for the computation of Precision@K and Recall@K required beta float real factor which could weight differently Recall or Precision based on its value. Default is 1 1 relevant_threshold float parameter needed to discern relevant items and non-relevant items for every user. If not specified, the mean rating score of every user will be used None sys_average str specify how the system average must be computed. Default is 'macro' 'macro' Source code in clayrs/evaluation/metrics/classification_metrics.py 547 548 549 550 551 def __init__ ( self , k : int , beta : int = 1 , relevant_threshold : float = None , sys_average : str = 'macro' ): super () . __init__ ( beta , relevant_threshold , sys_average ) if k < 1 : raise ValueError ( 'k= {} not valid! k must be >= 1!' . format ( k )) self . __k = k","title":"FMeasureAtK"},{"location":"evaluation/metrics/classification_metrics/#clayrs.evaluation.metrics.classification_metrics.Precision","text":"Bases: ClassificationMetric The Precision metric is calculated as such for the single user : \\[ Precision_u = \\frac{tp_u}{tp_u + fp_u} \\] Where: \\(tp_u\\) is the number of items which are in the recommendation list of the user and have a rating >= relevant_threshold in its 'ground truth' \\(fp_u\\) is the number of items which are in the recommendation list of the user and have a rating < relevant_threshold in its 'ground truth' And it is calculated as such for the entire system , depending if 'macro' average or 'micro' average has been chosen: \\[ Precision_{sys} - micro = \\frac{\\sum_{u \\in U} tp_u}{\\sum_{u \\in U} tp_u + \\sum_{u \\in U} fp_u} \\] \\[ Precision_{sys} - macro = \\frac{\\sum_{u \\in U} Precision_u}{|U|} \\] Parameters: Name Type Description Default relevant_threshold float parameter needed to discern relevant items and non-relevant items for every user. If not specified, the mean rating score of every user will be used None sys_average str specify how the system average must be computed. Default is 'macro' 'macro' Source code in clayrs/evaluation/metrics/classification_metrics.py 145 146 147 def __init__ ( self , relevant_threshold : float = None , sys_average : str = 'macro' , precision : [ Callable ] = np . float64 ): super ( Precision , self ) . __init__ ( relevant_threshold , sys_average , precision )","title":"Precision"},{"location":"evaluation/metrics/classification_metrics/#clayrs.evaluation.metrics.classification_metrics.PrecisionAtK","text":"Bases: Precision The Precision@K metric is calculated as such for the single user : \\[ Precision@K_u = \\frac{tp@K_u}{tp@K_u + fp@K_u} \\] Where: \\(tp@K_u\\) is the number of items which are in the recommendation list of the user cutoff to the first K items and have a rating >= relevant_threshold in its 'ground truth' \\(tp@K_u\\) is the number of items which are in the recommendation list of the user cutoff to the first K items and have a rating < relevant_threshold in its 'ground truth' And it is calculated as such for the entire system , depending if 'macro' average or 'micro' average has been chosen: \\[ Precision@K_{sys} - micro = \\frac{\\sum_{u \\in U} tp@K_u}{\\sum_{u \\in U} tp@K_u + \\sum_{u \\in U} fp@K_u} \\] \\[ Precision@K_{sys} - macro = \\frac{\\sum_{u \\in U} Precision@K_u}{|U|} \\] Parameters: Name Type Description Default k int cutoff parameter. Only the first k items of the recommendation list will be considered required relevant_threshold float parameter needed to discern relevant items and non-relevant items for every user. If not specified, the mean rating score of every user will be used None sys_average str specify how the system average must be computed. Default is 'macro' 'macro' Source code in clayrs/evaluation/metrics/classification_metrics.py 206 207 208 209 210 211 def __init__ ( self , k : int , relevant_threshold : float = None , sys_average : str = 'macro' , precision : [ Callable ] = np . float64 ): super () . __init__ ( relevant_threshold , sys_average , precision ) if k < 1 : raise ValueError ( 'k= {} not valid! k must be >= 1!' . format ( k )) self . __k = k","title":"PrecisionAtK"},{"location":"evaluation/metrics/classification_metrics/#clayrs.evaluation.metrics.classification_metrics.RPrecision","text":"Bases: Precision The R-Precision metric is calculated as such for the single user : \\[ R-Precision_u = \\frac{tp@R_u}{tp@R_u + fp@R_u} \\] Where: \\(R\\) it's the number of relevant items for the user u \\(tp@R_u\\) is the number of items which are in the recommendation list of the user cutoff to the first R items and have a rating >= relevant_threshold in its 'ground truth' \\(tp@R_u\\) is the number of items which are in the recommendation list of the user cutoff to the first R items and have a rating < relevant_threshold in its 'ground truth' And it is calculated as such for the entire system , depending if 'macro' average or 'micro' average has been chosen: \\[ Precision@R_{sys} - micro = \\frac{\\sum_{u \\in U} tp@R_u}{\\sum_{u \\in U} tp@R_u + \\sum_{u \\in U} fp@R_u} \\] \\[ Precision@R_{sys} - macro = \\frac{\\sum_{u \\in U} R-Precision_u}{|U|} \\] Parameters: Name Type Description Default relevant_threshold float parameter needed to discern relevant items and non-relevant items for every user. If not specified, the mean rating score of every user will be used None sys_average str specify how the system average must be computed. Default is 'macro' 'macro' Source code in clayrs/evaluation/metrics/classification_metrics.py 269 270 271 def __init__ ( self , relevant_threshold : float = None , sys_average : str = 'macro' , precision : [ Callable ] = np . float64 ): super () . __init__ ( relevant_threshold , sys_average , precision )","title":"RPrecision"},{"location":"evaluation/metrics/classification_metrics/#clayrs.evaluation.metrics.classification_metrics.Recall","text":"Bases: ClassificationMetric The Recall metric is calculated as such for the single user : \\[ Recall_u = \\frac{tp_u}{tp_u + fn_u} \\] Where: \\(tp_u\\) is the number of items which are in the recommendation list of the user and have a rating >= relevant_threshold in its 'ground truth' \\(fn_u\\) is the number of items which are NOT in the recommendation list of the user and have a rating >= relevant_threshold in its 'ground truth' And it is calculated as such for the entire system , depending if 'macro' average or 'micro' average has been chosen: \\[ Recall_{sys} - micro = \\frac{\\sum_{u \\in U} tp_u}{\\sum_{u \\in U} tp_u + \\sum_{u \\in U} fn_u} \\] \\[ Recall_{sys} - macro = \\frac{\\sum_{u \\in U} Recall_u}{|U|} \\] Parameters: Name Type Description Default relevant_threshold float parameter needed to discern relevant items and non-relevant items for every user. If not specified, the mean rating score of every user will be used None sys_average str specify how the system average must be computed. Default is 'macro' 'macro' Source code in clayrs/evaluation/metrics/classification_metrics.py 326 327 328 def __init__ ( self , relevant_threshold : float = None , sys_average : str = 'macro' , precision : [ Callable ] = np . float64 ): super () . __init__ ( relevant_threshold , sys_average , precision )","title":"Recall"},{"location":"evaluation/metrics/classification_metrics/#clayrs.evaluation.metrics.classification_metrics.RecallAtK","text":"Bases: Recall The Recall@K metric is calculated as such for the single user : \\[ Recall@K_u = \\frac{tp@K_u}{tp@K_u + fn@K_u} \\] Where: \\(tp@K_u\\) is the number of items which are in the recommendation list of the user cutoff to the first K items and have a rating >= relevant_threshold in its 'ground truth' \\(tp@K_u\\) is the number of items which are NOT in the recommendation list of the user cutoff to the first K items and have a rating >= relevant_threshold in its 'ground truth' And it is calculated as such for the entire system , depending if 'macro' average or 'micro' average has been chosen: \\[ Recall@K_{sys} - micro = \\frac{\\sum_{u \\in U} tp@K_u}{\\sum_{u \\in U} tp@K_u + \\sum_{u \\in U} fn@K_u} \\] \\[ Recall@K_{sys} - macro = \\frac{\\sum_{u \\in U} Recall@K_u}{|U|} \\] Parameters: Name Type Description Default k int cutoff parameter. Only the first k items of the recommendation list will be considered required relevant_threshold float parameter needed to discern relevant items and non-relevant items for every user. If not specified, the mean rating score of every user will be used None sys_average str specify how the system average must be computed. Default is 'macro' 'macro' Source code in clayrs/evaluation/metrics/classification_metrics.py 386 387 388 389 390 391 def __init__ ( self , k : int , relevant_threshold : float = None , sys_average : str = 'macro' , precision : [ Callable ] = np . float64 ): super () . __init__ ( relevant_threshold , sys_average , precision ) if k < 1 : raise ValueError ( 'k= {} not valid! k must be >= 1!' . format ( k )) self . __k = k","title":"RecallAtK"},{"location":"evaluation/metrics/error_metrics/","text":"Error metrics Error metrics evaluate 'how wrong' the recommender system was in predicting a rating MAE Bases: ErrorMetric The MAE (Mean Absolute Error) metric is calculated as such for the single user : \\[ MAE_u = \\sum_{i \\in T_u} \\frac{|r_{u,i} - \\hat{r}_{u,i}|}{|T_u|} \\] Where: \\(T_u\\) is the test set of the user \\(u\\) \\(r_{u, i}\\) is the actual score give by user \\(u\\) to item \\(i\\) \\(\\hat{r}_{u, i}\\) is the predicted score give by user \\(u\\) to item \\(i\\) And it is calculated as such for the entire system : \\[ MAE_{sys} = \\sum_{u \\in T} \\frac{MAE_u}{|T|} \\] Where: \\(T\\) is the test set \\(MAE_u\\) is the MAE calculated for user \\(u\\) There may be cases in which some items of the test set of the user could not be predicted (eg. A CBRS was chosen and items were not present locally, a methodology different than TestRatings was chosen). In those cases the \\(MAE_u\\) formula becomes \\[ MAE_u = \\sum_{i \\in T_u} \\frac{|r_{u,i} - \\hat{r}_{u,i}|}{|T_u| - unk} \\] Where: \\(unk\\) ( unknown ) is the number of items of the user test set that could not be predicted If no items of the user test set has been predicted ( \\(|T_u| - unk = 0\\) ), then: \\[ MAE_u = NaN \\] MSE Bases: ErrorMetric The MSE (Mean Squared Error) metric is calculated as such for the single user : \\[ MSE_u = \\sum_{i \\in T_u} \\frac{(r_{u,i} - \\hat{r}_{u,i})^2}{|T_u|} \\] Where: \\(T_u\\) is the test set of the user \\(u\\) \\(r_{u, i}\\) is the actual score give by user \\(u\\) to item \\(i\\) \\(\\hat{r}_{u, i}\\) is the predicted score give by user \\(u\\) to item \\(i\\) And it is calculated as such for the entire system : $$ MSE_{sys} = \\sum_{u \\in T} \\frac{MSE_u}{|T|} $$ Where: \\(T\\) is the test set \\(MSE_u\\) is the MSE calculated for user \\(u\\) There may be cases in which some items of the test set of the user could not be predicted (eg. A CBRS was chosen and items were not present locally) In those cases the \\(MSE_u\\) formula becomes \\[ MSE_u = \\sum_{i \\in T_u} \\frac{(r_{u,i} - \\hat{r}_{u,i})^2}{|T_u| - unk} \\] Where: \\(unk\\) ( unknown ) is the number of items of the user test set that could not be predicted If no items of the user test set has been predicted ( \\(|T_u| - unk = 0\\) ), then: \\[ MSE_u = NaN \\] RMSE Bases: ErrorMetric The RMSE (Root Mean Squared Error) metric is calculated as such for the single user : \\[ RMSE_u = \\sqrt{\\sum_{i \\in T_u} \\frac{(r_{u,i} - \\hat{r}_{u,i})^2}{|T_u|}} \\] Where: \\(T_u\\) is the test set of the user \\(u\\) \\(r_{u, i}\\) is the actual score give by user \\(u\\) to item \\(i\\) \\(\\hat{r}_{u, i}\\) is the predicted score give by user \\(u\\) to item \\(i\\) And it is calculated as such for the entire system : \\[ RMSE_{sys} = \\sum_{u \\in T} \\frac{RMSE_u}{|T|} \\] Where: \\(T\\) is the test set \\(RMSE_u\\) is the RMSE calculated for user \\(u\\) There may be cases in which some items of the test set of the user could not be predicted (eg. A CBRS was chosen and items were not present locally, a methodology different than TestRatings was chosen). In those cases the \\(RMSE_u\\) formula becomes \\[ RMSE_u = \\sqrt{\\sum_{i \\in T_u} \\frac{(r_{u,i} - \\hat{r}_{u,i})^2}{|T_u| - unk}} \\] Where: \\(unk\\) ( unknown ) is the number of items of the user test set that could not be predicted If no items of the user test set has been predicted ( \\(|T_u| - unk = 0\\) ), then: \\[ RMSE_u = NaN \\]","title":"Error metrics"},{"location":"evaluation/metrics/error_metrics/#error-metrics","text":"Error metrics evaluate 'how wrong' the recommender system was in predicting a rating","title":"Error metrics"},{"location":"evaluation/metrics/error_metrics/#clayrs.evaluation.metrics.error_metrics.MAE","text":"Bases: ErrorMetric The MAE (Mean Absolute Error) metric is calculated as such for the single user : \\[ MAE_u = \\sum_{i \\in T_u} \\frac{|r_{u,i} - \\hat{r}_{u,i}|}{|T_u|} \\] Where: \\(T_u\\) is the test set of the user \\(u\\) \\(r_{u, i}\\) is the actual score give by user \\(u\\) to item \\(i\\) \\(\\hat{r}_{u, i}\\) is the predicted score give by user \\(u\\) to item \\(i\\) And it is calculated as such for the entire system : \\[ MAE_{sys} = \\sum_{u \\in T} \\frac{MAE_u}{|T|} \\] Where: \\(T\\) is the test set \\(MAE_u\\) is the MAE calculated for user \\(u\\) There may be cases in which some items of the test set of the user could not be predicted (eg. A CBRS was chosen and items were not present locally, a methodology different than TestRatings was chosen). In those cases the \\(MAE_u\\) formula becomes \\[ MAE_u = \\sum_{i \\in T_u} \\frac{|r_{u,i} - \\hat{r}_{u,i}|}{|T_u| - unk} \\] Where: \\(unk\\) ( unknown ) is the number of items of the user test set that could not be predicted If no items of the user test set has been predicted ( \\(|T_u| - unk = 0\\) ), then: \\[ MAE_u = NaN \\]","title":"MAE"},{"location":"evaluation/metrics/error_metrics/#clayrs.evaluation.metrics.error_metrics.MSE","text":"Bases: ErrorMetric The MSE (Mean Squared Error) metric is calculated as such for the single user : \\[ MSE_u = \\sum_{i \\in T_u} \\frac{(r_{u,i} - \\hat{r}_{u,i})^2}{|T_u|} \\] Where: \\(T_u\\) is the test set of the user \\(u\\) \\(r_{u, i}\\) is the actual score give by user \\(u\\) to item \\(i\\) \\(\\hat{r}_{u, i}\\) is the predicted score give by user \\(u\\) to item \\(i\\) And it is calculated as such for the entire system : $$ MSE_{sys} = \\sum_{u \\in T} \\frac{MSE_u}{|T|} $$ Where: \\(T\\) is the test set \\(MSE_u\\) is the MSE calculated for user \\(u\\) There may be cases in which some items of the test set of the user could not be predicted (eg. A CBRS was chosen and items were not present locally) In those cases the \\(MSE_u\\) formula becomes \\[ MSE_u = \\sum_{i \\in T_u} \\frac{(r_{u,i} - \\hat{r}_{u,i})^2}{|T_u| - unk} \\] Where: \\(unk\\) ( unknown ) is the number of items of the user test set that could not be predicted If no items of the user test set has been predicted ( \\(|T_u| - unk = 0\\) ), then: \\[ MSE_u = NaN \\]","title":"MSE"},{"location":"evaluation/metrics/error_metrics/#clayrs.evaluation.metrics.error_metrics.RMSE","text":"Bases: ErrorMetric The RMSE (Root Mean Squared Error) metric is calculated as such for the single user : \\[ RMSE_u = \\sqrt{\\sum_{i \\in T_u} \\frac{(r_{u,i} - \\hat{r}_{u,i})^2}{|T_u|}} \\] Where: \\(T_u\\) is the test set of the user \\(u\\) \\(r_{u, i}\\) is the actual score give by user \\(u\\) to item \\(i\\) \\(\\hat{r}_{u, i}\\) is the predicted score give by user \\(u\\) to item \\(i\\) And it is calculated as such for the entire system : \\[ RMSE_{sys} = \\sum_{u \\in T} \\frac{RMSE_u}{|T|} \\] Where: \\(T\\) is the test set \\(RMSE_u\\) is the RMSE calculated for user \\(u\\) There may be cases in which some items of the test set of the user could not be predicted (eg. A CBRS was chosen and items were not present locally, a methodology different than TestRatings was chosen). In those cases the \\(RMSE_u\\) formula becomes \\[ RMSE_u = \\sqrt{\\sum_{i \\in T_u} \\frac{(r_{u,i} - \\hat{r}_{u,i})^2}{|T_u| - unk}} \\] Where: \\(unk\\) ( unknown ) is the number of items of the user test set that could not be predicted If no items of the user test set has been predicted ( \\(|T_u| - unk = 0\\) ), then: \\[ RMSE_u = NaN \\]","title":"RMSE"},{"location":"evaluation/metrics/fairness_metrics/","text":"Fairness metrics Fairness metrics evaluate how unbiased the recommendation lists are (e.g. unbiased towards popularity of the items) CatalogCoverage ( catalog , top_n = None , k = None ) Bases: PredictionCoverage The Catalog Coverage metric measures in percentage how many distinct items are being recommended in relation to all available items. It's a system wide metric, so only its result it will be returned and not those of every user. It differs from the Prediction Coverage since it allows for different parameters to come into play. If no parameter is passed then it's a simple Prediction Coverage. The metric is calculated as such: \\[ Catalog Coverage_{sys} = (\\frac{|\\bigcup_{j=1...N}reclist(u_j)|}{|I|})\\cdot100 \\] Where: \\(N\\) is the total number of users \\(reclist(u_j)\\) is the set of items contained in the recommendation list of user \\(j\\) \\(I\\) is the set of all available items The \\(I\\) must be specified through the 'catalog' parameter The recommendation list of every user ( \\(reclist(u_j)\\) ) can be reduced to the first n parameter with the top-n parameter, so that catalog coverage is measured considering only the most highest ranked items. With the 'k' parameter one could specify the number of users that will be used to calculate catalog coverage: k users will be randomly sampled and their recommendation lists will be used. The formula above becomes: \\[ Catalog Coverage_{sys} = (\\frac{|\\bigcup_{j=1...k}reclist(u_j)|}{|I|})\\cdot100 \\] Where: \\(k\\) is the parameter specified Obviously 'k' < N, else simply recommendation lists of all users will be used Check the 'Beyond Accuracy: Evaluating Recommender Systems by Coverage and Serendipity' paper and page 13 of the 'Comparison of group recommendation algorithms' paper for more Parameters: Name Type Description Default catalog Set [ str ] set of item id of the catalog on which the prediction coverage must be computed required top_n int it's a cutoff parameter, if specified the Catalog Coverage will be calculated considering only the first 'n' items of every recommendation list of all users. Default is None None k int number of users randomly sampled. If specified, k users will be randomly sampled across all users and only their recommendation lists will be used to compute the CatalogCoverage None Source code in clayrs/evaluation/metrics/fairness_metrics.py 331 332 333 334 def __init__ ( self , catalog : Set [ str ], top_n : int = None , k : int = None ): super () . __init__ ( catalog ) self . __top_n = top_n self . __k = k DeltaGap ( user_groups , top_n = None , pop_percentage = 0.2 ) Bases: GroupFairnessMetric The Delta GAP (Group Average popularity) metric lets you compare the average popularity \"requested\" by one or multiple groups of users and the average popularity \"obtained\" with the recommendation given by the recsys. It's a system wide metric and results of every group will be returned. It is calculated as such: \\[ \\Delta GAP = \\frac{recs_GAP - profile_GAP}{profile_GAP} \\] Users are splitted into groups based on the user_groups parameter, which contains names of the groups as keys, and percentage of how many user must contain a group as values. For example: user_groups = {'popular_users': 0.3, 'medium_popular_users': 0.2, 'low_popular_users': 0.5} Every user will be inserted in a group based on how many popular items the user has rated (in relation to the percentage of users we specified as value in the dictionary): users with many popular items will be inserted into the first group, users with niche items rated will be inserted into one of the last groups. In general users are grouped by popularity in a descending order. If the 'top_n' parameter is specified, then the \\(\\Delta GAP\\) will be calculated considering only the first n items of every recommendation list of all users Parameters: Name Type Description Default user_groups Dict [ str , float ] Dict containing group names as keys and percentage of users as value, used to split users in groups. Users with more popular items rated are grouped into the first group, users with slightly less popular items rated are grouped into the second one, etc. required top_n int it's a cutoff parameter, if specified the Gini index will be calculated considering only ther first 'n' items of every recommendation list of all users. Default is None None pop_percentage float How many (in percentage) 'most popular items' must be considered. Default is 0.2 0.2 Source code in clayrs/evaluation/metrics/fairness_metrics.py 410 411 412 413 414 415 416 def __init__ ( self , user_groups : Dict [ str , float ], top_n : int = None , pop_percentage : float = 0.2 ): if not 0 < pop_percentage <= 1 : raise ValueError ( 'Incorrect percentage! Valid percentage range: 0 < percentage <= 1' ) self . __pop_percentage = pop_percentage self . __top_n = top_n super () . __init__ ( user_groups ) calculate_delta_gap ( recs_gap , profile_gap ) staticmethod Compute the ratio between the recommendation gap and the user profiles gap Parameters: Name Type Description Default recs_gap float recommendation gap required profile_gap float user profiles gap required Returns: Name Type Description score float delta gap measure Source code in clayrs/evaluation/metrics/fairness_metrics.py 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 @staticmethod def calculate_delta_gap ( recs_gap : float , profile_gap : float ) -> float : \"\"\" Compute the ratio between the recommendation gap and the user profiles gap Args: recs_gap: recommendation gap profile_gap: user profiles gap Returns: score: delta gap measure \"\"\" result = 0 if profile_gap != 0.0 : result = ( recs_gap - profile_gap ) / profile_gap return result calculate_gap ( group , avg_pop_by_users ) staticmethod Compute the GAP (Group Average Popularity) formula \\[ GAP = \\frac{\\sum_{u \\in U}\\cdot \\frac{\\sum_{i \\in i_u} pop_i}{|iu|}}{|G|} \\] Where: \\(G\\) is the set of users \\(i_u\\) is the set of items rated by user u \\(pop_i\\) is the popularity of item i Parameters: Name Type Description Default group Set [ str ] the set of users (user_id) required avg_pop_by_users Dict [ str , object ] average popularity by user required Returns: Name Type Description score float gap score Source code in clayrs/evaluation/metrics/fairness_metrics.py 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 @staticmethod def calculate_gap ( group : Set [ str ], avg_pop_by_users : Dict [ str , object ]) -> float : r \"\"\" Compute the GAP (Group Average Popularity) formula $$ GAP = \\frac{\\sum_{u \\in U}\\cdot \\frac{\\sum_{i \\in i_u} pop_i}{|iu|}}{|G|} $$ Where: - $G$ is the set of users - $i_u$ is the set of items rated by user u - $pop_i$ is the popularity of item i Args: group: the set of users (user_id) avg_pop_by_users: average popularity by user Returns: score (float): gap score \"\"\" total_pop = 0 for element in group : if avg_pop_by_users . get ( element ): total_pop += avg_pop_by_users [ element ] return total_pop / len ( group ) GiniIndex ( top_n = None ) Bases: FairnessMetric The Gini Index metric measures inequality in recommendation lists. It's a system wide metric, so only its result it will be returned and not those of every user. The metric is calculated as such: \\[ Gini_{sys} = \\frac{\\sum_i(2i - n - 1)x_i}{n\\cdot\\sum_i x_i} \\] Where: \\(n\\) is the total number of distinct items that are being recommended \\(x_i\\) is the number of times that the item \\(i\\) has been recommended A perfectly equal recommender system should recommend every item the same number of times, in which case the Gini index would be equal to 0. The more the recsys is \"disegual\", the more the Gini Index is closer to 1 If the 'top_n' parameter is specified, then the Gini index will measure inequality considering only the first n items of every recommendation list of all users Parameters: Name Type Description Default top_n int it's a cutoff parameter, if specified the Gini index will be calculated considering only the first 'n' items of every recommendation list of all users. Default is None None Source code in clayrs/evaluation/metrics/fairness_metrics.py 168 169 def __init__ ( self , top_n : int = None ): self . __top_n = top_n GroupFairnessMetric ( user_groups ) Bases: FairnessMetric Abstract class for fairness metrics based on user groups It has some concrete methods useful for group divisions, since every subclass needs to split users into groups: Users are splitted into groups based on the user_groups parameter, which contains names of the groups as keys, and percentage of how many user must contain a group as values. For example: user_groups = {'popular_users': 0.3, 'medium_popular_users': 0.2, 'low_popular_users': 0.5} Every user will be inserted in a group based on how many popular items the user has rated (in relation to the percentage of users we specified as value in the dictionary): users with many popular items will be inserted into the first group, users with niche items rated will be inserted into one of the last groups. In general users are grouped by popularity in a descending order. Parameters: Name Type Description Default user_groups Dict [ str , float ] Dict containing group names as keys and percentage of users as value, used to split users in groups. Users with more popular items rated are grouped into the first group, users with slightly less popular items rated are grouped into the second one, etc. required Source code in clayrs/evaluation/metrics/fairness_metrics.py 51 52 def __init__ ( self , user_groups : Dict [ str , float ]): self . __user_groups = user_groups get_avg_pop_by_users ( data , pop_by_items , group = None ) staticmethod Get the average popularity for each user in the DataFrame Parameters: Name Type Description Default data Ratings a Ratings object required pop_by_items Counter popularity for each label ('label', 'popularity') required group Set [ str ] (optional) the set of users (user_id) None Returns: Name Type Description avg_pop_by_users Dict<str, float> average popularity by user Source code in clayrs/evaluation/metrics/fairness_metrics.py 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 @staticmethod def get_avg_pop_by_users ( data : Ratings , pop_by_items : Counter , group : Set [ str ] = None ) -> Dict [ str , float ]: \"\"\" Get the average popularity for each user in the DataFrame Args: data: a Ratings object pop_by_items: popularity for each label ('label', 'popularity') group: (optional) the set of users (user_id) Returns: avg_pop_by_users (Dict<str, float>): average popularity by user \"\"\" if group is None : group = set ( data . user_id_column ) list_by_user = { user : [ interaction . item_id for interaction in data . get_user_interactions ( user )] for user in group } avg_pop_by_users = { user : get_avg_pop ( list_by_user [ user ], pop_by_items ) for user in group } return avg_pop_by_users split_user_in_groups ( score_frame , groups , pop_items ) staticmethod Splits the DataFrames in 3 different Sets, based on the recommendation popularity of each user Parameters: Name Type Description Default score_frame Ratings the Ratings object required groups Dict [ str , float ] each key contains the name of the group and each value contains the percentage of the specified group. If the groups don't cover the entire user collection, the rest of the users are considered in a 'default_diverse' group required pop_items Set [ str ] set of most popular 'to_id' labels required Returns: Name Type Description groups_dict Dict [ str , Set [ str ]] key = group_name, value = Set of 'user_id' labels Source code in clayrs/evaluation/metrics/fairness_metrics.py 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 @staticmethod def split_user_in_groups ( score_frame : Ratings , groups : Dict [ str , float ], pop_items : Set [ str ]) -> Dict [ str , Set [ str ]]: \"\"\" Splits the DataFrames in 3 different Sets, based on the recommendation popularity of each user Args: score_frame: the Ratings object groups: each key contains the name of the group and each value contains the percentage of the specified group. If the groups don't cover the entire user collection, the rest of the users are considered in a 'default_diverse' group pop_items: set of most popular 'to_id' labels Returns: groups_dict: key = group_name, value = Set of 'user_id' labels \"\"\" num_of_users = len ( set ( score_frame . user_id_column )) if num_of_users < len ( groups ): raise NotEnoughUsers ( \"You can't split in {} groups {} users! \" \"Try reducing number of groups\" . format ( len ( groups ), num_of_users )) for percentage_chosen in groups . values (): if not 0 < percentage_chosen <= 1 : raise ValueError ( 'Incorrect percentage! Valid percentage range: 0 < percentage <= 1' ) total = sum ( groups . values ()) if total > 1 : raise ValueError ( \"Incorrect percentage! Sum of percentage is > than 1\" ) elif total < 1 : remaining = round ( 1 - total , 10 ) # rounded at the 10th digit logger . warning ( \"Sum of percentage is < than 1, \" f \"the { remaining } percentage of users will be inserted into the \" \"'default_diverse' group\" ) pop_ratio_by_users = pop_ratio_by_user ( score_frame , most_pop_items = pop_items ) pop_ratio_by_users = sorted ( pop_ratio_by_users , key = pop_ratio_by_users . get , reverse = True ) groups_dict : Dict [ str , Set [ str ]] = {} last_index = 0 percentage = 0.0 for group_name in groups : percentage += groups [ group_name ] group_index = round ( num_of_users * percentage ) if group_index == 0 : logger . warning ( 'Not enough rows for group {} ! It will be discarded' . format ( group_name )) else : groups_dict [ group_name ] = set ( pop_ratio_by_users [ last_index : group_index ]) last_index = group_index if percentage < 1 : group_index = round ( num_of_users ) groups_dict [ 'default_diverse' ] = set ( pop_ratio_by_users [ last_index : group_index ]) return groups_dict PredictionCoverage ( catalog ) Bases: FairnessMetric The Prediction Coverage metric measures in percentage how many distinct items are being recommended in relation to all available items. It's a system wide metric, so only its result it will be returned and not those of every user. The metric is calculated as such: \\[ Prediction Coverage_{sys} = (\\frac{|I_p|}{|I|})\\cdot100 \\] Where: \\(I\\) is the set of all available items \\(I_p\\) is the set of recommended items The \\(I\\) must be specified through the 'catalog' parameter Check the 'Beyond Accuracy: Evaluating Recommender Systems by Coverage and Serendipity' paper for more Parameters: Name Type Description Default catalog Set [ str ] set of item id of the catalog on which the prediction coverage must be computed required Source code in clayrs/evaluation/metrics/fairness_metrics.py 239 240 def __init__ ( self , catalog : Set [ str ]): self . __catalog = set ( str ( item_id ) for item_id in catalog )","title":"Fairness metrics"},{"location":"evaluation/metrics/fairness_metrics/#fairness-metrics","text":"Fairness metrics evaluate how unbiased the recommendation lists are (e.g. unbiased towards popularity of the items)","title":"Fairness metrics"},{"location":"evaluation/metrics/fairness_metrics/#clayrs.evaluation.metrics.fairness_metrics.CatalogCoverage","text":"Bases: PredictionCoverage The Catalog Coverage metric measures in percentage how many distinct items are being recommended in relation to all available items. It's a system wide metric, so only its result it will be returned and not those of every user. It differs from the Prediction Coverage since it allows for different parameters to come into play. If no parameter is passed then it's a simple Prediction Coverage. The metric is calculated as such: \\[ Catalog Coverage_{sys} = (\\frac{|\\bigcup_{j=1...N}reclist(u_j)|}{|I|})\\cdot100 \\] Where: \\(N\\) is the total number of users \\(reclist(u_j)\\) is the set of items contained in the recommendation list of user \\(j\\) \\(I\\) is the set of all available items The \\(I\\) must be specified through the 'catalog' parameter The recommendation list of every user ( \\(reclist(u_j)\\) ) can be reduced to the first n parameter with the top-n parameter, so that catalog coverage is measured considering only the most highest ranked items. With the 'k' parameter one could specify the number of users that will be used to calculate catalog coverage: k users will be randomly sampled and their recommendation lists will be used. The formula above becomes: \\[ Catalog Coverage_{sys} = (\\frac{|\\bigcup_{j=1...k}reclist(u_j)|}{|I|})\\cdot100 \\] Where: \\(k\\) is the parameter specified Obviously 'k' < N, else simply recommendation lists of all users will be used Check the 'Beyond Accuracy: Evaluating Recommender Systems by Coverage and Serendipity' paper and page 13 of the 'Comparison of group recommendation algorithms' paper for more Parameters: Name Type Description Default catalog Set [ str ] set of item id of the catalog on which the prediction coverage must be computed required top_n int it's a cutoff parameter, if specified the Catalog Coverage will be calculated considering only the first 'n' items of every recommendation list of all users. Default is None None k int number of users randomly sampled. If specified, k users will be randomly sampled across all users and only their recommendation lists will be used to compute the CatalogCoverage None Source code in clayrs/evaluation/metrics/fairness_metrics.py 331 332 333 334 def __init__ ( self , catalog : Set [ str ], top_n : int = None , k : int = None ): super () . __init__ ( catalog ) self . __top_n = top_n self . __k = k","title":"CatalogCoverage"},{"location":"evaluation/metrics/fairness_metrics/#clayrs.evaluation.metrics.fairness_metrics.DeltaGap","text":"Bases: GroupFairnessMetric The Delta GAP (Group Average popularity) metric lets you compare the average popularity \"requested\" by one or multiple groups of users and the average popularity \"obtained\" with the recommendation given by the recsys. It's a system wide metric and results of every group will be returned. It is calculated as such: \\[ \\Delta GAP = \\frac{recs_GAP - profile_GAP}{profile_GAP} \\] Users are splitted into groups based on the user_groups parameter, which contains names of the groups as keys, and percentage of how many user must contain a group as values. For example: user_groups = {'popular_users': 0.3, 'medium_popular_users': 0.2, 'low_popular_users': 0.5} Every user will be inserted in a group based on how many popular items the user has rated (in relation to the percentage of users we specified as value in the dictionary): users with many popular items will be inserted into the first group, users with niche items rated will be inserted into one of the last groups. In general users are grouped by popularity in a descending order. If the 'top_n' parameter is specified, then the \\(\\Delta GAP\\) will be calculated considering only the first n items of every recommendation list of all users Parameters: Name Type Description Default user_groups Dict [ str , float ] Dict containing group names as keys and percentage of users as value, used to split users in groups. Users with more popular items rated are grouped into the first group, users with slightly less popular items rated are grouped into the second one, etc. required top_n int it's a cutoff parameter, if specified the Gini index will be calculated considering only ther first 'n' items of every recommendation list of all users. Default is None None pop_percentage float How many (in percentage) 'most popular items' must be considered. Default is 0.2 0.2 Source code in clayrs/evaluation/metrics/fairness_metrics.py 410 411 412 413 414 415 416 def __init__ ( self , user_groups : Dict [ str , float ], top_n : int = None , pop_percentage : float = 0.2 ): if not 0 < pop_percentage <= 1 : raise ValueError ( 'Incorrect percentage! Valid percentage range: 0 < percentage <= 1' ) self . __pop_percentage = pop_percentage self . __top_n = top_n super () . __init__ ( user_groups )","title":"DeltaGap"},{"location":"evaluation/metrics/fairness_metrics/#clayrs.evaluation.metrics.fairness_metrics.DeltaGap.calculate_delta_gap","text":"Compute the ratio between the recommendation gap and the user profiles gap Parameters: Name Type Description Default recs_gap float recommendation gap required profile_gap float user profiles gap required Returns: Name Type Description score float delta gap measure Source code in clayrs/evaluation/metrics/fairness_metrics.py 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 @staticmethod def calculate_delta_gap ( recs_gap : float , profile_gap : float ) -> float : \"\"\" Compute the ratio between the recommendation gap and the user profiles gap Args: recs_gap: recommendation gap profile_gap: user profiles gap Returns: score: delta gap measure \"\"\" result = 0 if profile_gap != 0.0 : result = ( recs_gap - profile_gap ) / profile_gap return result","title":"calculate_delta_gap()"},{"location":"evaluation/metrics/fairness_metrics/#clayrs.evaluation.metrics.fairness_metrics.DeltaGap.calculate_gap","text":"Compute the GAP (Group Average Popularity) formula \\[ GAP = \\frac{\\sum_{u \\in U}\\cdot \\frac{\\sum_{i \\in i_u} pop_i}{|iu|}}{|G|} \\] Where: \\(G\\) is the set of users \\(i_u\\) is the set of items rated by user u \\(pop_i\\) is the popularity of item i Parameters: Name Type Description Default group Set [ str ] the set of users (user_id) required avg_pop_by_users Dict [ str , object ] average popularity by user required Returns: Name Type Description score float gap score Source code in clayrs/evaluation/metrics/fairness_metrics.py 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 @staticmethod def calculate_gap ( group : Set [ str ], avg_pop_by_users : Dict [ str , object ]) -> float : r \"\"\" Compute the GAP (Group Average Popularity) formula $$ GAP = \\frac{\\sum_{u \\in U}\\cdot \\frac{\\sum_{i \\in i_u} pop_i}{|iu|}}{|G|} $$ Where: - $G$ is the set of users - $i_u$ is the set of items rated by user u - $pop_i$ is the popularity of item i Args: group: the set of users (user_id) avg_pop_by_users: average popularity by user Returns: score (float): gap score \"\"\" total_pop = 0 for element in group : if avg_pop_by_users . get ( element ): total_pop += avg_pop_by_users [ element ] return total_pop / len ( group )","title":"calculate_gap()"},{"location":"evaluation/metrics/fairness_metrics/#clayrs.evaluation.metrics.fairness_metrics.GiniIndex","text":"Bases: FairnessMetric The Gini Index metric measures inequality in recommendation lists. It's a system wide metric, so only its result it will be returned and not those of every user. The metric is calculated as such: \\[ Gini_{sys} = \\frac{\\sum_i(2i - n - 1)x_i}{n\\cdot\\sum_i x_i} \\] Where: \\(n\\) is the total number of distinct items that are being recommended \\(x_i\\) is the number of times that the item \\(i\\) has been recommended A perfectly equal recommender system should recommend every item the same number of times, in which case the Gini index would be equal to 0. The more the recsys is \"disegual\", the more the Gini Index is closer to 1 If the 'top_n' parameter is specified, then the Gini index will measure inequality considering only the first n items of every recommendation list of all users Parameters: Name Type Description Default top_n int it's a cutoff parameter, if specified the Gini index will be calculated considering only the first 'n' items of every recommendation list of all users. Default is None None Source code in clayrs/evaluation/metrics/fairness_metrics.py 168 169 def __init__ ( self , top_n : int = None ): self . __top_n = top_n","title":"GiniIndex"},{"location":"evaluation/metrics/fairness_metrics/#clayrs.evaluation.metrics.fairness_metrics.GroupFairnessMetric","text":"Bases: FairnessMetric Abstract class for fairness metrics based on user groups It has some concrete methods useful for group divisions, since every subclass needs to split users into groups: Users are splitted into groups based on the user_groups parameter, which contains names of the groups as keys, and percentage of how many user must contain a group as values. For example: user_groups = {'popular_users': 0.3, 'medium_popular_users': 0.2, 'low_popular_users': 0.5} Every user will be inserted in a group based on how many popular items the user has rated (in relation to the percentage of users we specified as value in the dictionary): users with many popular items will be inserted into the first group, users with niche items rated will be inserted into one of the last groups. In general users are grouped by popularity in a descending order. Parameters: Name Type Description Default user_groups Dict [ str , float ] Dict containing group names as keys and percentage of users as value, used to split users in groups. Users with more popular items rated are grouped into the first group, users with slightly less popular items rated are grouped into the second one, etc. required Source code in clayrs/evaluation/metrics/fairness_metrics.py 51 52 def __init__ ( self , user_groups : Dict [ str , float ]): self . __user_groups = user_groups","title":"GroupFairnessMetric"},{"location":"evaluation/metrics/fairness_metrics/#clayrs.evaluation.metrics.fairness_metrics.GroupFairnessMetric.get_avg_pop_by_users","text":"Get the average popularity for each user in the DataFrame Parameters: Name Type Description Default data Ratings a Ratings object required pop_by_items Counter popularity for each label ('label', 'popularity') required group Set [ str ] (optional) the set of users (user_id) None Returns: Name Type Description avg_pop_by_users Dict<str, float> average popularity by user Source code in clayrs/evaluation/metrics/fairness_metrics.py 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 @staticmethod def get_avg_pop_by_users ( data : Ratings , pop_by_items : Counter , group : Set [ str ] = None ) -> Dict [ str , float ]: \"\"\" Get the average popularity for each user in the DataFrame Args: data: a Ratings object pop_by_items: popularity for each label ('label', 'popularity') group: (optional) the set of users (user_id) Returns: avg_pop_by_users (Dict<str, float>): average popularity by user \"\"\" if group is None : group = set ( data . user_id_column ) list_by_user = { user : [ interaction . item_id for interaction in data . get_user_interactions ( user )] for user in group } avg_pop_by_users = { user : get_avg_pop ( list_by_user [ user ], pop_by_items ) for user in group } return avg_pop_by_users","title":"get_avg_pop_by_users()"},{"location":"evaluation/metrics/fairness_metrics/#clayrs.evaluation.metrics.fairness_metrics.GroupFairnessMetric.split_user_in_groups","text":"Splits the DataFrames in 3 different Sets, based on the recommendation popularity of each user Parameters: Name Type Description Default score_frame Ratings the Ratings object required groups Dict [ str , float ] each key contains the name of the group and each value contains the percentage of the specified group. If the groups don't cover the entire user collection, the rest of the users are considered in a 'default_diverse' group required pop_items Set [ str ] set of most popular 'to_id' labels required Returns: Name Type Description groups_dict Dict [ str , Set [ str ]] key = group_name, value = Set of 'user_id' labels Source code in clayrs/evaluation/metrics/fairness_metrics.py 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 @staticmethod def split_user_in_groups ( score_frame : Ratings , groups : Dict [ str , float ], pop_items : Set [ str ]) -> Dict [ str , Set [ str ]]: \"\"\" Splits the DataFrames in 3 different Sets, based on the recommendation popularity of each user Args: score_frame: the Ratings object groups: each key contains the name of the group and each value contains the percentage of the specified group. If the groups don't cover the entire user collection, the rest of the users are considered in a 'default_diverse' group pop_items: set of most popular 'to_id' labels Returns: groups_dict: key = group_name, value = Set of 'user_id' labels \"\"\" num_of_users = len ( set ( score_frame . user_id_column )) if num_of_users < len ( groups ): raise NotEnoughUsers ( \"You can't split in {} groups {} users! \" \"Try reducing number of groups\" . format ( len ( groups ), num_of_users )) for percentage_chosen in groups . values (): if not 0 < percentage_chosen <= 1 : raise ValueError ( 'Incorrect percentage! Valid percentage range: 0 < percentage <= 1' ) total = sum ( groups . values ()) if total > 1 : raise ValueError ( \"Incorrect percentage! Sum of percentage is > than 1\" ) elif total < 1 : remaining = round ( 1 - total , 10 ) # rounded at the 10th digit logger . warning ( \"Sum of percentage is < than 1, \" f \"the { remaining } percentage of users will be inserted into the \" \"'default_diverse' group\" ) pop_ratio_by_users = pop_ratio_by_user ( score_frame , most_pop_items = pop_items ) pop_ratio_by_users = sorted ( pop_ratio_by_users , key = pop_ratio_by_users . get , reverse = True ) groups_dict : Dict [ str , Set [ str ]] = {} last_index = 0 percentage = 0.0 for group_name in groups : percentage += groups [ group_name ] group_index = round ( num_of_users * percentage ) if group_index == 0 : logger . warning ( 'Not enough rows for group {} ! It will be discarded' . format ( group_name )) else : groups_dict [ group_name ] = set ( pop_ratio_by_users [ last_index : group_index ]) last_index = group_index if percentage < 1 : group_index = round ( num_of_users ) groups_dict [ 'default_diverse' ] = set ( pop_ratio_by_users [ last_index : group_index ]) return groups_dict","title":"split_user_in_groups()"},{"location":"evaluation/metrics/fairness_metrics/#clayrs.evaluation.metrics.fairness_metrics.PredictionCoverage","text":"Bases: FairnessMetric The Prediction Coverage metric measures in percentage how many distinct items are being recommended in relation to all available items. It's a system wide metric, so only its result it will be returned and not those of every user. The metric is calculated as such: \\[ Prediction Coverage_{sys} = (\\frac{|I_p|}{|I|})\\cdot100 \\] Where: \\(I\\) is the set of all available items \\(I_p\\) is the set of recommended items The \\(I\\) must be specified through the 'catalog' parameter Check the 'Beyond Accuracy: Evaluating Recommender Systems by Coverage and Serendipity' paper for more Parameters: Name Type Description Default catalog Set [ str ] set of item id of the catalog on which the prediction coverage must be computed required Source code in clayrs/evaluation/metrics/fairness_metrics.py 239 240 def __init__ ( self , catalog : Set [ str ]): self . __catalog = set ( str ( item_id ) for item_id in catalog )","title":"PredictionCoverage"},{"location":"evaluation/metrics/plot_metrics/","text":"Plot metrics Plot metrics save a plot in the chosen output directory LongTailDistr ( out_dir = '.' , file_name = 'long_tail_distr' , on = 'truth' , format = 'png' , overwrite = False ) Bases: PlotMetric This metric generates the Long Tail Distribution plot and saves it in the output directory with the file name specified. The plot can be generated both for the truth set or the predictions set (based on the on parameter): on = 'truth' : in this case the long tail distribution is useful to see which are the most popular items (the most rated ones) on = 'pred' : in this case the long tail distribution is useful to see which are the most recommended items The plot file will be saved as out_dir/file_name.format Since multiple split could be evaluated at once, the overwrite parameter comes into play: if is set to False, file with the same name will be saved as file_name (1).format , file_name (2).format , etc. so that for every split a plot is generated without overwriting any file previously generated Parameters: Name Type Description Default out_dir str Directory where the plot will be saved. Default is '.', meaning that the plot will be saved in the same directory where the python script it's being executed '.' file_name str Name of the plot file. Default is 'long_tail_distr' 'long_tail_distr' on str Set on which the Long Tail Distribution plot will be generated. Values accepted are 'truth' or 'pred' 'truth' format str Format of the plot file. Could be 'jpg', 'svg', 'png'. Default is 'png' 'png' overwrite bool parameter which specifies if the plot saved must overwrite any file that as the same name ('file_name.format'). Default is False False Raises: Type Description ValueError exception raised when a invalid value for the 'on' parameter is specified Source code in clayrs/evaluation/metrics/plot_metrics.py 110 111 112 113 114 115 116 117 118 def __init__ ( self , out_dir : str = '.' , file_name : str = 'long_tail_distr' , on : str = 'truth' , format : str = 'png' , overwrite : bool = False ): valid = { 'truth' , 'pred' } self . __on = on . lower () if self . __on not in valid : raise ValueError ( \"on= {} is not supported! Long Tail can be calculated only on: \\n \" \" {} \" . format ( on , valid )) super () . __init__ ( out_dir , file_name , format , overwrite ) PopProfileVsRecs ( user_groups , out_dir = '.' , file_name = 'pop_ratio_profile_vs_recs' , pop_percentage = 0.2 , store_frame = False , format = 'png' , overwrite = False ) Bases: GroupFairnessMetric , PlotMetric This metric generates a plot where users are split into groups and, for every group, a boxplot comparing profile popularity and recommendations popularity is drawn Users are split into groups based on the user_groups parameter, which contains names of the groups as keys, and percentage of how many user must contain a group as values. For example: user_groups = {'popular_users': 0.3, 'medium_popular_users': 0.2, 'low_popular_users': 0.5} Every user will be inserted in a group based on how many popular items the user has rated (in relation to the percentage of users we specified as value in the dictionary): users with many popular items will be inserted into the first group, users with niche items rated will be inserted into one of the last groups. In general users are grouped by popularity in a descending order. You could also specify how many most popular items must be considered with the pop_percentage parameter. By default is set to 0.2 which means that the top 20% items are considered as most popular The plot file will be saved as out_dir/file_name.format Since multiple split could be evaluated at once, the overwrite parameter comes into play: if is set to False, file with the same name will be saved as file_name (1).format , file_name (2).format , etc. so that for every split a plot is generated without overwriting any file previously generated Thanks to the 'store_frame' parameter it's also possible to store a csv containing the calculations done in order to build every boxplot. Will be saved in the same directory and with the same file name as the plot itself (but with the .csv format): The csv will be saved as out_dir/file_name.csv Parameters: Name Type Description Default user_groups Dict<str, float> Dict containing group names as keys and percentage of users as value, used to split users in groups. Users with more popular items rated are grouped into the first group, users with slightly less popular items rated are grouped into the second one, etc. required out_dir str Directory where the plot will be saved. Default is '.', meaning that the plot will be saved in the same directory where the python script it's being executed '.' file_name str Name of the plot file. Default is 'pop_ratio_profile_vs_recs' 'pop_ratio_profile_vs_recs' pop_percentage float How many (in percentage) 'most popular items' must be considered. Default is 0.2 0.2 store_frame bool True if you want to store calculations done in order to build every boxplot in a csv file, False otherwise. Default is set to False False format str Format of the plot file. Could be 'jpg', 'svg', 'png'. Default is 'png' 'png' overwrite bool parameter which specifies if the plot saved must overwrite any file that as the same name ('file_name.format'). Default is False False Source code in clayrs/evaluation/metrics/plot_metrics.py 214 215 216 217 218 219 220 221 222 223 224 225 226 def __init__ ( self , user_groups : Dict [ str , float ], out_dir : str = '.' , file_name : str = 'pop_ratio_profile_vs_recs' , pop_percentage : float = 0.2 , store_frame : bool = False , format : str = 'png' , overwrite : bool = False ): PlotMetric . __init__ ( self , out_dir , file_name , format , overwrite ) GroupFairnessMetric . __init__ ( self , user_groups ) if not 0 < pop_percentage <= 1 : raise ValueError ( 'Incorrect percentage! Valid percentage range: 0 < percentage <= 1' ) self . __pop_percentage = pop_percentage self . __user_groups = user_groups self . __store_frame = store_frame PopRecsCorrelation ( out_dir = '.' , file_name = 'pop_recs_correlation' , mode = 'both' , format = 'png' , overwrite = False ) Bases: PlotMetric This metric generates a plot which has as the X-axis the popularity and as Y-axis number of recommendations, so that it can be easily seen the correlation between popular (niche) items and how many times are being recommended by the recsys The plot file will be saved as out_dir/file_name.format Since multiple split could be evaluated at once, the overwrite parameter comes into play: if is set to False, file with the same name will be saved as file_name (1).format , file_name (2).format , etc. so that for every split a plot is generated without overwriting any file previously generated There exists cases in which some items are not recommended even once, so in the graph could appear zero recommendations . One could change this behaviour thanks to the 'mode' parameter: mode='both' : two graphs will be created, the first one containing eventual zero recommendations , the second one where zero recommendations are excluded. This additional graph will be stored as out_dir/file_name_no_zeros.format (the string '_no_zeros' will be added to the file_name chosen automatically) mode='w_zeros' : only a graph containing eventual zero recommendations will be created mode='no_zeros' : only a graph excluding eventual zero recommendations will be created. The graph will be saved as out_dir/file_name_no_zeros.format (the string '_no_zeros' will be added to the file_name chosen automatically) Parameters: Name Type Description Default out_dir str Directory where the plot will be saved. Default is '.', meaning that the plot will be saved in the same directory where the python script it's being executed '.' file_name str Name of the plot file. Default is 'pop_recs_correlation' 'pop_recs_correlation' mode str Parameter which dictates which graph must be created. By default is 'both', so the graph with eventual zero recommendations as well as the graph excluding eventual zero recommendations will be created. Check the class documentation for more 'both' format str Format of the plot file. Could be 'jpg', 'svg', 'png'. Default is 'png' 'png' overwrite bool parameter which specifies if the plot saved must overwrite any file that as the same name ('file_name.format'). Default is False False Source code in clayrs/evaluation/metrics/plot_metrics.py 400 401 402 403 404 405 406 407 408 409 def __init__ ( self , out_dir : str = '.' , file_name : str = 'pop_recs_correlation' , mode : str = 'both' , format : str = 'png' , overwrite : bool = False ): valid = { 'both' , 'no_zeros' , 'w_zeros' } self . __mode = mode . lower () if self . __mode not in valid : raise ValueError ( \"Mode {} is not supported! Modes available: \\n \" \" {} \" . format ( mode , valid )) super () . __init__ ( out_dir , file_name , format , overwrite ) build_no_zeros_plot ( popularities , recommendations ) Method which builds and saves the plot excluding eventual zero recommendations It saves the plot as out_dir/filename_no_zeros.format , according to their value passed in the constructor. Note that the '_no_zeros' string is automatically added to the file_name chosen Parameters: Name Type Description Default popularities list x-axis values representing popularity of every item required recommendations list y-axis values representing number of times every item has been recommended required Source code in clayrs/evaluation/metrics/plot_metrics.py 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 def build_no_zeros_plot ( self , popularities : list , recommendations : list ): \"\"\" Method which builds and saves the plot **excluding** eventual *zero recommendations* It saves the plot as *out_dir/filename_no_zeros.format*, according to their value passed in the constructor. Note that the '_no_zeros' string is automatically added to the file_name chosen Args: popularities (list): x-axis values representing popularity of every item recommendations (list): y-axis values representing number of times every item has been recommended \"\"\" title = 'Popularity-Recommendations Correlation (No zeros)' fig = self . build_plot ( popularities , recommendations , title ) file_name = self . file_name + '_no_zeros' self . save_figure ( fig , file_name ) build_plot ( x , y , title ) Method which builds a matplotlib plot given x-axis values, y-axis values and the title of the plot. X-axis label and Y-axis label are hard-coded as 'Popularity' and 'Recommendation frequency' respectively. Parameters: Name Type Description Default x list List containing x-axis values required y list List containing y-axis values required title str title of the plot required Returns: Type Description matplotlib . figure . Figure The matplotlib figure Source code in clayrs/evaluation/metrics/plot_metrics.py 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 def build_plot ( self , x : list , y : list , title : str ) -> matplotlib . figure . Figure : \"\"\" Method which builds a matplotlib plot given x-axis values, y-axis values and the title of the plot. X-axis label and Y-axis label are hard-coded as 'Popularity' and 'Recommendation frequency' respectively. Args: x (list): List containing x-axis values y (list): List containing y-axis values title (str): title of the plot Returns: The matplotlib figure \"\"\" fig = plt . figure () ax = fig . add_subplot () ax . set ( xlabel = 'Popularity' , ylabel = 'Recommendation frequency' , title = title ) ax . scatter ( x , y , marker = 'o' , s = 20 , c = 'orange' , edgecolors = 'black' , linewidths = 0.05 ) return fig build_w_zeros_plot ( popularities , recommendations ) Method which builds and saves the plot containing eventual zero recommendations It saves the plot as out_dir/filename.format , according to their value passed in the constructor Parameters: Name Type Description Default popularities list x-axis values representing popularity of every item required recommendations list y-axis values representing number of times every item has been recommended required Source code in clayrs/evaluation/metrics/plot_metrics.py 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 def build_w_zeros_plot ( self , popularities : list , recommendations : list ): \"\"\" Method which builds and saves the plot containing eventual *zero recommendations* It saves the plot as *out_dir/filename.format*, according to their value passed in the constructor Args: popularities (list): x-axis values representing popularity of every item recommendations (list): y-axis values representing number of times every item has been recommended \"\"\" title = 'Popularity-Recommendations Correlation' fig = self . build_plot ( popularities , recommendations , title ) file_name = self . file_name self . save_figure ( fig , file_name )","title":"Plot metrics"},{"location":"evaluation/metrics/plot_metrics/#plot-metrics","text":"Plot metrics save a plot in the chosen output directory","title":"Plot metrics"},{"location":"evaluation/metrics/plot_metrics/#clayrs.evaluation.metrics.plot_metrics.LongTailDistr","text":"Bases: PlotMetric This metric generates the Long Tail Distribution plot and saves it in the output directory with the file name specified. The plot can be generated both for the truth set or the predictions set (based on the on parameter): on = 'truth' : in this case the long tail distribution is useful to see which are the most popular items (the most rated ones) on = 'pred' : in this case the long tail distribution is useful to see which are the most recommended items The plot file will be saved as out_dir/file_name.format Since multiple split could be evaluated at once, the overwrite parameter comes into play: if is set to False, file with the same name will be saved as file_name (1).format , file_name (2).format , etc. so that for every split a plot is generated without overwriting any file previously generated Parameters: Name Type Description Default out_dir str Directory where the plot will be saved. Default is '.', meaning that the plot will be saved in the same directory where the python script it's being executed '.' file_name str Name of the plot file. Default is 'long_tail_distr' 'long_tail_distr' on str Set on which the Long Tail Distribution plot will be generated. Values accepted are 'truth' or 'pred' 'truth' format str Format of the plot file. Could be 'jpg', 'svg', 'png'. Default is 'png' 'png' overwrite bool parameter which specifies if the plot saved must overwrite any file that as the same name ('file_name.format'). Default is False False Raises: Type Description ValueError exception raised when a invalid value for the 'on' parameter is specified Source code in clayrs/evaluation/metrics/plot_metrics.py 110 111 112 113 114 115 116 117 118 def __init__ ( self , out_dir : str = '.' , file_name : str = 'long_tail_distr' , on : str = 'truth' , format : str = 'png' , overwrite : bool = False ): valid = { 'truth' , 'pred' } self . __on = on . lower () if self . __on not in valid : raise ValueError ( \"on= {} is not supported! Long Tail can be calculated only on: \\n \" \" {} \" . format ( on , valid )) super () . __init__ ( out_dir , file_name , format , overwrite )","title":"LongTailDistr"},{"location":"evaluation/metrics/plot_metrics/#clayrs.evaluation.metrics.plot_metrics.PopProfileVsRecs","text":"Bases: GroupFairnessMetric , PlotMetric This metric generates a plot where users are split into groups and, for every group, a boxplot comparing profile popularity and recommendations popularity is drawn Users are split into groups based on the user_groups parameter, which contains names of the groups as keys, and percentage of how many user must contain a group as values. For example: user_groups = {'popular_users': 0.3, 'medium_popular_users': 0.2, 'low_popular_users': 0.5} Every user will be inserted in a group based on how many popular items the user has rated (in relation to the percentage of users we specified as value in the dictionary): users with many popular items will be inserted into the first group, users with niche items rated will be inserted into one of the last groups. In general users are grouped by popularity in a descending order. You could also specify how many most popular items must be considered with the pop_percentage parameter. By default is set to 0.2 which means that the top 20% items are considered as most popular The plot file will be saved as out_dir/file_name.format Since multiple split could be evaluated at once, the overwrite parameter comes into play: if is set to False, file with the same name will be saved as file_name (1).format , file_name (2).format , etc. so that for every split a plot is generated without overwriting any file previously generated Thanks to the 'store_frame' parameter it's also possible to store a csv containing the calculations done in order to build every boxplot. Will be saved in the same directory and with the same file name as the plot itself (but with the .csv format): The csv will be saved as out_dir/file_name.csv Parameters: Name Type Description Default user_groups Dict<str, float> Dict containing group names as keys and percentage of users as value, used to split users in groups. Users with more popular items rated are grouped into the first group, users with slightly less popular items rated are grouped into the second one, etc. required out_dir str Directory where the plot will be saved. Default is '.', meaning that the plot will be saved in the same directory where the python script it's being executed '.' file_name str Name of the plot file. Default is 'pop_ratio_profile_vs_recs' 'pop_ratio_profile_vs_recs' pop_percentage float How many (in percentage) 'most popular items' must be considered. Default is 0.2 0.2 store_frame bool True if you want to store calculations done in order to build every boxplot in a csv file, False otherwise. Default is set to False False format str Format of the plot file. Could be 'jpg', 'svg', 'png'. Default is 'png' 'png' overwrite bool parameter which specifies if the plot saved must overwrite any file that as the same name ('file_name.format'). Default is False False Source code in clayrs/evaluation/metrics/plot_metrics.py 214 215 216 217 218 219 220 221 222 223 224 225 226 def __init__ ( self , user_groups : Dict [ str , float ], out_dir : str = '.' , file_name : str = 'pop_ratio_profile_vs_recs' , pop_percentage : float = 0.2 , store_frame : bool = False , format : str = 'png' , overwrite : bool = False ): PlotMetric . __init__ ( self , out_dir , file_name , format , overwrite ) GroupFairnessMetric . __init__ ( self , user_groups ) if not 0 < pop_percentage <= 1 : raise ValueError ( 'Incorrect percentage! Valid percentage range: 0 < percentage <= 1' ) self . __pop_percentage = pop_percentage self . __user_groups = user_groups self . __store_frame = store_frame","title":"PopProfileVsRecs"},{"location":"evaluation/metrics/plot_metrics/#clayrs.evaluation.metrics.plot_metrics.PopRecsCorrelation","text":"Bases: PlotMetric This metric generates a plot which has as the X-axis the popularity and as Y-axis number of recommendations, so that it can be easily seen the correlation between popular (niche) items and how many times are being recommended by the recsys The plot file will be saved as out_dir/file_name.format Since multiple split could be evaluated at once, the overwrite parameter comes into play: if is set to False, file with the same name will be saved as file_name (1).format , file_name (2).format , etc. so that for every split a plot is generated without overwriting any file previously generated There exists cases in which some items are not recommended even once, so in the graph could appear zero recommendations . One could change this behaviour thanks to the 'mode' parameter: mode='both' : two graphs will be created, the first one containing eventual zero recommendations , the second one where zero recommendations are excluded. This additional graph will be stored as out_dir/file_name_no_zeros.format (the string '_no_zeros' will be added to the file_name chosen automatically) mode='w_zeros' : only a graph containing eventual zero recommendations will be created mode='no_zeros' : only a graph excluding eventual zero recommendations will be created. The graph will be saved as out_dir/file_name_no_zeros.format (the string '_no_zeros' will be added to the file_name chosen automatically) Parameters: Name Type Description Default out_dir str Directory where the plot will be saved. Default is '.', meaning that the plot will be saved in the same directory where the python script it's being executed '.' file_name str Name of the plot file. Default is 'pop_recs_correlation' 'pop_recs_correlation' mode str Parameter which dictates which graph must be created. By default is 'both', so the graph with eventual zero recommendations as well as the graph excluding eventual zero recommendations will be created. Check the class documentation for more 'both' format str Format of the plot file. Could be 'jpg', 'svg', 'png'. Default is 'png' 'png' overwrite bool parameter which specifies if the plot saved must overwrite any file that as the same name ('file_name.format'). Default is False False Source code in clayrs/evaluation/metrics/plot_metrics.py 400 401 402 403 404 405 406 407 408 409 def __init__ ( self , out_dir : str = '.' , file_name : str = 'pop_recs_correlation' , mode : str = 'both' , format : str = 'png' , overwrite : bool = False ): valid = { 'both' , 'no_zeros' , 'w_zeros' } self . __mode = mode . lower () if self . __mode not in valid : raise ValueError ( \"Mode {} is not supported! Modes available: \\n \" \" {} \" . format ( mode , valid )) super () . __init__ ( out_dir , file_name , format , overwrite )","title":"PopRecsCorrelation"},{"location":"evaluation/metrics/plot_metrics/#clayrs.evaluation.metrics.plot_metrics.PopRecsCorrelation.build_no_zeros_plot","text":"Method which builds and saves the plot excluding eventual zero recommendations It saves the plot as out_dir/filename_no_zeros.format , according to their value passed in the constructor. Note that the '_no_zeros' string is automatically added to the file_name chosen Parameters: Name Type Description Default popularities list x-axis values representing popularity of every item required recommendations list y-axis values representing number of times every item has been recommended required Source code in clayrs/evaluation/metrics/plot_metrics.py 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 def build_no_zeros_plot ( self , popularities : list , recommendations : list ): \"\"\" Method which builds and saves the plot **excluding** eventual *zero recommendations* It saves the plot as *out_dir/filename_no_zeros.format*, according to their value passed in the constructor. Note that the '_no_zeros' string is automatically added to the file_name chosen Args: popularities (list): x-axis values representing popularity of every item recommendations (list): y-axis values representing number of times every item has been recommended \"\"\" title = 'Popularity-Recommendations Correlation (No zeros)' fig = self . build_plot ( popularities , recommendations , title ) file_name = self . file_name + '_no_zeros' self . save_figure ( fig , file_name )","title":"build_no_zeros_plot()"},{"location":"evaluation/metrics/plot_metrics/#clayrs.evaluation.metrics.plot_metrics.PopRecsCorrelation.build_plot","text":"Method which builds a matplotlib plot given x-axis values, y-axis values and the title of the plot. X-axis label and Y-axis label are hard-coded as 'Popularity' and 'Recommendation frequency' respectively. Parameters: Name Type Description Default x list List containing x-axis values required y list List containing y-axis values required title str title of the plot required Returns: Type Description matplotlib . figure . Figure The matplotlib figure Source code in clayrs/evaluation/metrics/plot_metrics.py 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 def build_plot ( self , x : list , y : list , title : str ) -> matplotlib . figure . Figure : \"\"\" Method which builds a matplotlib plot given x-axis values, y-axis values and the title of the plot. X-axis label and Y-axis label are hard-coded as 'Popularity' and 'Recommendation frequency' respectively. Args: x (list): List containing x-axis values y (list): List containing y-axis values title (str): title of the plot Returns: The matplotlib figure \"\"\" fig = plt . figure () ax = fig . add_subplot () ax . set ( xlabel = 'Popularity' , ylabel = 'Recommendation frequency' , title = title ) ax . scatter ( x , y , marker = 'o' , s = 20 , c = 'orange' , edgecolors = 'black' , linewidths = 0.05 ) return fig","title":"build_plot()"},{"location":"evaluation/metrics/plot_metrics/#clayrs.evaluation.metrics.plot_metrics.PopRecsCorrelation.build_w_zeros_plot","text":"Method which builds and saves the plot containing eventual zero recommendations It saves the plot as out_dir/filename.format , according to their value passed in the constructor Parameters: Name Type Description Default popularities list x-axis values representing popularity of every item required recommendations list y-axis values representing number of times every item has been recommended required Source code in clayrs/evaluation/metrics/plot_metrics.py 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 def build_w_zeros_plot ( self , popularities : list , recommendations : list ): \"\"\" Method which builds and saves the plot containing eventual *zero recommendations* It saves the plot as *out_dir/filename.format*, according to their value passed in the constructor Args: popularities (list): x-axis values representing popularity of every item recommendations (list): y-axis values representing number of times every item has been recommended \"\"\" title = 'Popularity-Recommendations Correlation' fig = self . build_plot ( popularities , recommendations , title ) file_name = self . file_name self . save_figure ( fig , file_name )","title":"build_w_zeros_plot()"},{"location":"evaluation/metrics/ranking_metrics/","text":"Ranking metrics Ranking metrics evaluate the quality of the recommendation lists Correlation ( method = 'pearson' , top_n = None ) Bases: RankingMetric The Correlation metric calculates the correlation between the ranking of a user and its ideal ranking. The currently correlation methods implemented are: pearson kendall spearman Every correlation method is implemented by the pandas library, so read its documentation for more The correlation metric is calculated as such for the single user : \\[ Corr_u = Corr(ranking_u, ideal\\_ranking_u) \\] Where: \\(ranking_u\\) is ranking of the user \\(ideal\\_ranking_u\\) is the ideal ranking for the user The ideal ranking is calculated based on the rating inside the ground truth of the user The Correlation metric calculated for the entire system is simply the average of every \\(Corr\\) : \\[ Corr_{sys} = \\frac{\\sum_{u} Corr_u}{|U|} \\] Where: \\(Corr_u\\) is the correlation of the user \\(u\\) \\(U\\) is the set of all users The system average excludes NaN values. It's also possible to specify a cutoff parameter thanks to the 'top_n' parameter: if specified, only the first \\(n\\) results of the recommendation list will be used in order to calculate the correlation Parameters: Name Type Description Default method str The correlation method to use. It must be 'pearson', 'kendall' or 'spearman', otherwise a ValueError exception is raised. By default is 'pearson' 'pearson' top_n int Cutoff parameter, if specified only the first n items of the recommendation list will be used in order to calculate the correlation None Raises: Type Description ValueError if an invalid method parameter is passed Source code in clayrs/evaluation/metrics/ranking_metrics.py 349 350 351 352 353 354 355 356 357 def __init__ ( self , method : str = 'pearson' , top_n : int = None ): valid = { 'pearson' , 'kendall' , 'spearman' } self . __method = method . lower () if self . __method not in valid : raise ValueError ( \"Method {} is not supported! Methods available: \\n \" \" {} \" . format ( method , valid )) self . __top_n = top_n MRR ( relevant_threshold = None ) Bases: RankingMetric The MRR (Mean Reciprocal Rank) metric is a system wide metric, so only its result it will be returned and not those of every user. MRR is calculated as such: \\[ MRR_{sys} = \\frac{1}{|Q|}\\cdot\\sum_{i=1}^{|Q|}\\frac{1}{rank(i)} \\] Where: \\(Q\\) is the set of recommendation lists \\(rank(i)\\) is the position of the first relevant item in the i-th recommendation list The MRR metric needs to discern relevant items from the not relevant ones: in order to do that, one could pass a custom relevant_threshold parameter that will be applied to every user, so that if a rating of an item is >= relevant_threshold, then it's relevant, otherwise it's not. If no relevant_threshold parameter is passed then, for every user, its mean rating score will be used Parameters: Name Type Description Default relevant_threshold float parameter needed to discern relevant items and non-relevant items for every user. If not specified, the mean rating score of every user will be used None Source code in clayrs/evaluation/metrics/ranking_metrics.py 172 173 def __init__ ( self , relevant_threshold : float = None ): self . __relevant_threshold = relevant_threshold calc_reciprocal_rank ( user_predictions , user_truth_relevant_items ) Method which calculates the RR (Reciprocal Rank) for a single user Parameters: Name Type Description Default user_predictions List [ Interaction ] list of Interactions object of the recommendation list for the user required user_truth_relevant_items Set [ Interaction ] list of relevant Interactions object of the truth set for the user required Source code in clayrs/evaluation/metrics/ranking_metrics.py 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 def calc_reciprocal_rank ( self , user_predictions : List [ Interaction ], user_truth_relevant_items : Set [ Interaction ]): \"\"\" Method which calculates the RR (Reciprocal Rank) for a single user Args: user_predictions: list of Interactions object of the recommendation list for the user user_truth_relevant_items: list of relevant Interactions object of the truth set for the user \"\"\" reciprocal_rank = 0 i = 1 for interaction_pred in user_predictions : if interaction_pred . item_id in user_truth_relevant_items : reciprocal_rank = 1 / i break # We only need the first relevant item position in the rank i += 1 return reciprocal_rank MRRAtK ( k , relevant_threshold = None ) Bases: MRR The MRR@K (Mean Reciprocal Rank at K) metric is a system wide metric, so only its result will be returned and not those of every user. MRR@K is calculated as such \\[ MRR@K_{sys} = \\frac{1}{|Q|}\\cdot\\sum_{i=1}^{K}\\frac{1}{rank(i)} \\] Where: \\(K\\) is the cutoff parameter \\(Q\\) is the set of recommendation lists \\(rank(i)\\) is the position of the first relevant item in the i-th recommendation list Parameters: Name Type Description Default k int the cutoff parameter. It must be >= 1, otherwise a ValueError exception is raised required relevant_threshold float parameter needed to discern relevant items and non-relevant items for every user. If not specified, the mean rating score of every user will be used None Raises: Type Description ValueError if an invalid cutoff parameter is passed (0 or negative) Source code in clayrs/evaluation/metrics/ranking_metrics.py 268 269 270 271 272 def __init__ ( self , k : int , relevant_threshold : float = None ): if k < 1 : raise ValueError ( 'k= {} not valid! k must be >= 1!' . format ( k )) self . __k = k super () . __init__ ( relevant_threshold ) calc_reciprocal_rank ( user_predictions , user_truth_relevant_items ) Method which calculates the RR (Reciprocal Rank) for a single user Parameters: Name Type Description Default user_predictions List [ Interaction ] list of Interactions object of the recommendation list for the user required user_truth_relevant_items Set [ Interaction ] list of relevant Interactions object of the truth set for the user required Source code in clayrs/evaluation/metrics/ranking_metrics.py 284 285 286 287 288 289 290 291 292 293 294 def calc_reciprocal_rank ( self , user_predictions : List [ Interaction ], user_truth_relevant_items : Set [ Interaction ]): \"\"\" Method which calculates the RR (Reciprocal Rank) for a single user Args: user_predictions: list of Interactions object of the recommendation list for the user user_truth_relevant_items: list of relevant Interactions object of the truth set for the user \"\"\" user_predictions_cut = user_predictions [: self . k ] return super () . calc_reciprocal_rank ( user_predictions_cut , user_truth_relevant_items ) NDCG Bases: RankingMetric The NDCG (Normalized Discounted Cumulative Gain) metric is calculated for the single user by using the sklearn implementation, so be sure to check its documentation . The NDCG of the entire system is calculated instead as such: \\[ NDCG_{sys} = \\frac{\\sum_{u} NDCG_u}{|U|} \\] Where: \\(NDCG_u\\) is the NDCG calculated for user :math: u \\(U\\) is the set of all users The system average excludes NaN values. NDCGAtK ( k ) Bases: NDCG The NDCG@K (Normalized Discounted Cumulative Gain at K) metric is calculated for the single user by using the sklearn implementation, so be sure to check its documentation . The NDCG@K of the entire system is calculated instead as such: \\[ NDCG@K_{sys} = \\frac{\\sum_{u} NDCG@K_u}{|U|} \\] Where: \\(NDCG@K_u\\) is the NDCG@K calculated for user \\(u\\) \\(U\\) is the set of all users The system average excludes NaN values. Parameters: Name Type Description Default k int the cutoff parameter required Source code in clayrs/evaluation/metrics/ranking_metrics.py 133 134 def __init__ ( self , k : int ): self . __k = k","title":"Ranking metrics"},{"location":"evaluation/metrics/ranking_metrics/#ranking-metrics","text":"Ranking metrics evaluate the quality of the recommendation lists","title":"Ranking metrics"},{"location":"evaluation/metrics/ranking_metrics/#clayrs.evaluation.metrics.ranking_metrics.Correlation","text":"Bases: RankingMetric The Correlation metric calculates the correlation between the ranking of a user and its ideal ranking. The currently correlation methods implemented are: pearson kendall spearman Every correlation method is implemented by the pandas library, so read its documentation for more The correlation metric is calculated as such for the single user : \\[ Corr_u = Corr(ranking_u, ideal\\_ranking_u) \\] Where: \\(ranking_u\\) is ranking of the user \\(ideal\\_ranking_u\\) is the ideal ranking for the user The ideal ranking is calculated based on the rating inside the ground truth of the user The Correlation metric calculated for the entire system is simply the average of every \\(Corr\\) : \\[ Corr_{sys} = \\frac{\\sum_{u} Corr_u}{|U|} \\] Where: \\(Corr_u\\) is the correlation of the user \\(u\\) \\(U\\) is the set of all users The system average excludes NaN values. It's also possible to specify a cutoff parameter thanks to the 'top_n' parameter: if specified, only the first \\(n\\) results of the recommendation list will be used in order to calculate the correlation Parameters: Name Type Description Default method str The correlation method to use. It must be 'pearson', 'kendall' or 'spearman', otherwise a ValueError exception is raised. By default is 'pearson' 'pearson' top_n int Cutoff parameter, if specified only the first n items of the recommendation list will be used in order to calculate the correlation None Raises: Type Description ValueError if an invalid method parameter is passed Source code in clayrs/evaluation/metrics/ranking_metrics.py 349 350 351 352 353 354 355 356 357 def __init__ ( self , method : str = 'pearson' , top_n : int = None ): valid = { 'pearson' , 'kendall' , 'spearman' } self . __method = method . lower () if self . __method not in valid : raise ValueError ( \"Method {} is not supported! Methods available: \\n \" \" {} \" . format ( method , valid )) self . __top_n = top_n","title":"Correlation"},{"location":"evaluation/metrics/ranking_metrics/#clayrs.evaluation.metrics.ranking_metrics.MRR","text":"Bases: RankingMetric The MRR (Mean Reciprocal Rank) metric is a system wide metric, so only its result it will be returned and not those of every user. MRR is calculated as such: \\[ MRR_{sys} = \\frac{1}{|Q|}\\cdot\\sum_{i=1}^{|Q|}\\frac{1}{rank(i)} \\] Where: \\(Q\\) is the set of recommendation lists \\(rank(i)\\) is the position of the first relevant item in the i-th recommendation list The MRR metric needs to discern relevant items from the not relevant ones: in order to do that, one could pass a custom relevant_threshold parameter that will be applied to every user, so that if a rating of an item is >= relevant_threshold, then it's relevant, otherwise it's not. If no relevant_threshold parameter is passed then, for every user, its mean rating score will be used Parameters: Name Type Description Default relevant_threshold float parameter needed to discern relevant items and non-relevant items for every user. If not specified, the mean rating score of every user will be used None Source code in clayrs/evaluation/metrics/ranking_metrics.py 172 173 def __init__ ( self , relevant_threshold : float = None ): self . __relevant_threshold = relevant_threshold","title":"MRR"},{"location":"evaluation/metrics/ranking_metrics/#clayrs.evaluation.metrics.ranking_metrics.MRR.calc_reciprocal_rank","text":"Method which calculates the RR (Reciprocal Rank) for a single user Parameters: Name Type Description Default user_predictions List [ Interaction ] list of Interactions object of the recommendation list for the user required user_truth_relevant_items Set [ Interaction ] list of relevant Interactions object of the truth set for the user required Source code in clayrs/evaluation/metrics/ranking_metrics.py 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 def calc_reciprocal_rank ( self , user_predictions : List [ Interaction ], user_truth_relevant_items : Set [ Interaction ]): \"\"\" Method which calculates the RR (Reciprocal Rank) for a single user Args: user_predictions: list of Interactions object of the recommendation list for the user user_truth_relevant_items: list of relevant Interactions object of the truth set for the user \"\"\" reciprocal_rank = 0 i = 1 for interaction_pred in user_predictions : if interaction_pred . item_id in user_truth_relevant_items : reciprocal_rank = 1 / i break # We only need the first relevant item position in the rank i += 1 return reciprocal_rank","title":"calc_reciprocal_rank()"},{"location":"evaluation/metrics/ranking_metrics/#clayrs.evaluation.metrics.ranking_metrics.MRRAtK","text":"Bases: MRR The MRR@K (Mean Reciprocal Rank at K) metric is a system wide metric, so only its result will be returned and not those of every user. MRR@K is calculated as such \\[ MRR@K_{sys} = \\frac{1}{|Q|}\\cdot\\sum_{i=1}^{K}\\frac{1}{rank(i)} \\] Where: \\(K\\) is the cutoff parameter \\(Q\\) is the set of recommendation lists \\(rank(i)\\) is the position of the first relevant item in the i-th recommendation list Parameters: Name Type Description Default k int the cutoff parameter. It must be >= 1, otherwise a ValueError exception is raised required relevant_threshold float parameter needed to discern relevant items and non-relevant items for every user. If not specified, the mean rating score of every user will be used None Raises: Type Description ValueError if an invalid cutoff parameter is passed (0 or negative) Source code in clayrs/evaluation/metrics/ranking_metrics.py 268 269 270 271 272 def __init__ ( self , k : int , relevant_threshold : float = None ): if k < 1 : raise ValueError ( 'k= {} not valid! k must be >= 1!' . format ( k )) self . __k = k super () . __init__ ( relevant_threshold )","title":"MRRAtK"},{"location":"evaluation/metrics/ranking_metrics/#clayrs.evaluation.metrics.ranking_metrics.MRRAtK.calc_reciprocal_rank","text":"Method which calculates the RR (Reciprocal Rank) for a single user Parameters: Name Type Description Default user_predictions List [ Interaction ] list of Interactions object of the recommendation list for the user required user_truth_relevant_items Set [ Interaction ] list of relevant Interactions object of the truth set for the user required Source code in clayrs/evaluation/metrics/ranking_metrics.py 284 285 286 287 288 289 290 291 292 293 294 def calc_reciprocal_rank ( self , user_predictions : List [ Interaction ], user_truth_relevant_items : Set [ Interaction ]): \"\"\" Method which calculates the RR (Reciprocal Rank) for a single user Args: user_predictions: list of Interactions object of the recommendation list for the user user_truth_relevant_items: list of relevant Interactions object of the truth set for the user \"\"\" user_predictions_cut = user_predictions [: self . k ] return super () . calc_reciprocal_rank ( user_predictions_cut , user_truth_relevant_items )","title":"calc_reciprocal_rank()"},{"location":"evaluation/metrics/ranking_metrics/#clayrs.evaluation.metrics.ranking_metrics.NDCG","text":"Bases: RankingMetric The NDCG (Normalized Discounted Cumulative Gain) metric is calculated for the single user by using the sklearn implementation, so be sure to check its documentation . The NDCG of the entire system is calculated instead as such: \\[ NDCG_{sys} = \\frac{\\sum_{u} NDCG_u}{|U|} \\] Where: \\(NDCG_u\\) is the NDCG calculated for user :math: u \\(U\\) is the set of all users The system average excludes NaN values.","title":"NDCG"},{"location":"evaluation/metrics/ranking_metrics/#clayrs.evaluation.metrics.ranking_metrics.NDCGAtK","text":"Bases: NDCG The NDCG@K (Normalized Discounted Cumulative Gain at K) metric is calculated for the single user by using the sklearn implementation, so be sure to check its documentation . The NDCG@K of the entire system is calculated instead as such: \\[ NDCG@K_{sys} = \\frac{\\sum_{u} NDCG@K_u}{|U|} \\] Where: \\(NDCG@K_u\\) is the NDCG@K calculated for user \\(u\\) \\(U\\) is the set of all users The system average excludes NaN values. Parameters: Name Type Description Default k int the cutoff parameter required Source code in clayrs/evaluation/metrics/ranking_metrics.py 133 134 def __init__ ( self , k : int ): self . __k = k","title":"NDCGAtK"},{"location":"evaluation/statistical_tests/paired/","text":"Paired statistical tests PairedTest Bases: StatisticalTest perform ( df_list ) Method which performs the chosen paired statistical test. Since it's a paired test, the final result is a pandas DataFrame which contains learning schemas compared in pair. For example if you call the perform() method by passing a list containing three different DataFrames, one for each learning schema to compare: # Ttest as example since it's a Paired Test Ttest () . perform ([ user_df1 , user_df2 , user_df3 ]) You will obtain a DataFrame comparing all different combinations: (system1, system2) (system1, system3) (system2, system3) The first value of each cell is the statistic , the second is the p-value Parameters: Name Type Description Default df_list List [ pd . DataFrame ] List containing DataFrames with several metrics to compare, preferably metrics computed for each user. One DataFrame corresponds to one learning schema required Returns: Type Description pd . DataFrame A Pandas DataFrame where each combination of learning schemas are compared in pair. The first value of each cell is the statistic , the second is the p-value Source code in clayrs/evaluation/statistical_test.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 def perform ( self , df_list : List [ pd . DataFrame ]) -> pd . DataFrame : \"\"\" Method which performs the chosen paired statistical test. Since it's a paired test, the final result is a pandas DataFrame which contains learning schemas compared in pair. For example if you call the `perform()` method by passing a list containing three different DataFrames, one for each learning schema to compare: ```python # Ttest as example since it's a Paired Test Ttest().perform([user_df1, user_df2, user_df3]) ``` You will obtain a DataFrame comparing all different combinations: * (system1, system2) * (system1, system3) * (system2, system3) The first value of each cell is the ***statistic***, the second is the ***p-value*** Args: df_list: List containing DataFrames with several metrics to compare, preferably metrics computed for each user. One DataFrame corresponds to one learning schema Returns: A Pandas DataFrame where each combination of learning schemas are compared in pair. The first value of each cell is the ***statistic***, the second is the ***p-value*** \"\"\" final_result = defaultdict ( list ) n_system_evaluated = 1 while len ( df_list ) != 0 : df1 = df_list . pop ( 0 ) for i , other_df in enumerate ( df_list , start = n_system_evaluated + 1 ): common_metrics = [ column for column in df1 . columns if column != 'user_id' and column in other_df . columns ] common_rows = self . _common_users ( df1 , other_df , list ( common_metrics )) final_result [ \"Systems evaluated\" ] . append (( f \"system_ { n_system_evaluated } \" , f \"system_ { i } \" )) for metric in common_metrics : # drop nan values since otherwise test may behave unexpectedly metric_rows = common_rows [[ f \" { metric } _x\" , f \" { metric } _y\" ]] . dropna () score_system1 = metric_rows [ f \" { metric } _x\" ] score_system2 = metric_rows [ f \" { metric } _y\" ] single_metric_result = self . _perform_test ( score_system1 , score_system2 ) final_result [ metric ] . append ( single_metric_result ) n_system_evaluated += 1 return pd . DataFrame ( final_result ) . set_index ( \"Systems evaluated\" ) Ttest Bases: PairedTest Calculate the T-test for the means of two independent samples of scores. This is a two-sided test for the null hypothesis that 2 independent samples have identical average (expected) values. This test assumes that the populations have identical variances by default. Wilcoxon Bases: PairedTest Compute the Wilcoxon rank-sum statistic for two samples. The Wilcoxon rank-sum test tests the null hypothesis that two sets of measurements are drawn from the same distribution. The alternative hypothesis is that values in one sample are more likely to be larger than the values in the other sample.","title":"Paired"},{"location":"evaluation/statistical_tests/paired/#paired-statistical-tests","text":"","title":"Paired statistical tests"},{"location":"evaluation/statistical_tests/paired/#clayrs.evaluation.statistical_test.PairedTest","text":"Bases: StatisticalTest","title":"PairedTest"},{"location":"evaluation/statistical_tests/paired/#clayrs.evaluation.statistical_test.PairedTest.perform","text":"Method which performs the chosen paired statistical test. Since it's a paired test, the final result is a pandas DataFrame which contains learning schemas compared in pair. For example if you call the perform() method by passing a list containing three different DataFrames, one for each learning schema to compare: # Ttest as example since it's a Paired Test Ttest () . perform ([ user_df1 , user_df2 , user_df3 ]) You will obtain a DataFrame comparing all different combinations: (system1, system2) (system1, system3) (system2, system3) The first value of each cell is the statistic , the second is the p-value Parameters: Name Type Description Default df_list List [ pd . DataFrame ] List containing DataFrames with several metrics to compare, preferably metrics computed for each user. One DataFrame corresponds to one learning schema required Returns: Type Description pd . DataFrame A Pandas DataFrame where each combination of learning schemas are compared in pair. The first value of each cell is the statistic , the second is the p-value Source code in clayrs/evaluation/statistical_test.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 def perform ( self , df_list : List [ pd . DataFrame ]) -> pd . DataFrame : \"\"\" Method which performs the chosen paired statistical test. Since it's a paired test, the final result is a pandas DataFrame which contains learning schemas compared in pair. For example if you call the `perform()` method by passing a list containing three different DataFrames, one for each learning schema to compare: ```python # Ttest as example since it's a Paired Test Ttest().perform([user_df1, user_df2, user_df3]) ``` You will obtain a DataFrame comparing all different combinations: * (system1, system2) * (system1, system3) * (system2, system3) The first value of each cell is the ***statistic***, the second is the ***p-value*** Args: df_list: List containing DataFrames with several metrics to compare, preferably metrics computed for each user. One DataFrame corresponds to one learning schema Returns: A Pandas DataFrame where each combination of learning schemas are compared in pair. The first value of each cell is the ***statistic***, the second is the ***p-value*** \"\"\" final_result = defaultdict ( list ) n_system_evaluated = 1 while len ( df_list ) != 0 : df1 = df_list . pop ( 0 ) for i , other_df in enumerate ( df_list , start = n_system_evaluated + 1 ): common_metrics = [ column for column in df1 . columns if column != 'user_id' and column in other_df . columns ] common_rows = self . _common_users ( df1 , other_df , list ( common_metrics )) final_result [ \"Systems evaluated\" ] . append (( f \"system_ { n_system_evaluated } \" , f \"system_ { i } \" )) for metric in common_metrics : # drop nan values since otherwise test may behave unexpectedly metric_rows = common_rows [[ f \" { metric } _x\" , f \" { metric } _y\" ]] . dropna () score_system1 = metric_rows [ f \" { metric } _x\" ] score_system2 = metric_rows [ f \" { metric } _y\" ] single_metric_result = self . _perform_test ( score_system1 , score_system2 ) final_result [ metric ] . append ( single_metric_result ) n_system_evaluated += 1 return pd . DataFrame ( final_result ) . set_index ( \"Systems evaluated\" )","title":"perform()"},{"location":"evaluation/statistical_tests/paired/#clayrs.evaluation.statistical_test.Ttest","text":"Bases: PairedTest Calculate the T-test for the means of two independent samples of scores. This is a two-sided test for the null hypothesis that 2 independent samples have identical average (expected) values. This test assumes that the populations have identical variances by default.","title":"Ttest"},{"location":"evaluation/statistical_tests/paired/#clayrs.evaluation.statistical_test.Wilcoxon","text":"Bases: PairedTest Compute the Wilcoxon rank-sum statistic for two samples. The Wilcoxon rank-sum test tests the null hypothesis that two sets of measurements are drawn from the same distribution. The alternative hypothesis is that values in one sample are more likely to be larger than the values in the other sample.","title":"Wilcoxon"},{"location":"first_steps/installation/","text":"Installation Via PIP recommended ClayRS requires Python 3.7 or later, while package dependencies are in requirements.txt and are all installable via pip , as ClayRS itself. To install it execute the following command: Latest pip install git+https://github.com/SwapUniba/clayrs.git This will automatically install compatible versions of all dependencies. Tip : We suggest to install ClayRS (or any python package, for that matters) in a virtual environment Virtual environments are special isolated environments where all the packages and versions you install only apply to that specific environment. It\u2019s like a private island! \u2014 but for code. Read this Medium article for understanding all the advantages and the official python guide on how to setup one","title":"Installation"},{"location":"first_steps/installation/#installation","text":"","title":"Installation"},{"location":"first_steps/installation/#via-pip-recommended","text":"ClayRS requires Python 3.7 or later, while package dependencies are in requirements.txt and are all installable via pip , as ClayRS itself. To install it execute the following command: Latest pip install git+https://github.com/SwapUniba/clayrs.git This will automatically install compatible versions of all dependencies. Tip : We suggest to install ClayRS (or any python package, for that matters) in a virtual environment Virtual environments are special isolated environments where all the packages and versions you install only apply to that specific environment. It\u2019s like a private island! \u2014 but for code. Read this Medium article for understanding all the advantages and the official python guide on how to setup one","title":"Via PIP"},{"location":"first_steps/quickstart/","text":"Quickstart Content Analyzer The first thing to do is to import the Content Analyzer module * We will access its methods and classes via dot notation import clayrs.content_analyzer as ca First let's point to the source containing raw information to process raw_source = ca . JSONFile ( 'items_info.json' ) Then let's start building the configuration for the items Info Note that same operations that can be specified for items could be also specified for users via the ca . UserAnalyzerConfig class # Configuration of item representation movies_ca_config = ca . ItemAnalyzerConfig ( source = raw_source , id = 'movielens_id' , # (1) output_directory = 'movies_codified/' # (2) ) The id in the raw source which uniquely identifies each item Directory which will contain items complexly represented Let's represent the plot field of each content with a TfIdf representation * * Since the preprocessing parameter has been specified, then each field is first preprocessed with the specified operations movies_ca_config . add_single_config ( 'plot' , ca . FieldConfig ( ca . SkLearnTfIdf (), preprocessing = ca . NLTK ( stopwords_removal = True , lemmatization = True ), id = 'tfidf' ) # (1) ) User defined id for the representation To finalize the Content Analyzer part, let's instantiate the ContentAnalyzer class by passing the built configuration and by calling its fit() method ca . ContentAnalyzer ( movies_ca_config ) . fit () The items will be created with the specified representations and serialized RecSys Similarly above, we must first import the RecSys module import clayrs.recsys as rs Then we load the rating frame from a TSV file Info In this case in our file the first three columns are user_id, item_id, score in this order If your file has a different structure you must specify how to map the column via parameters, check documentation for more ratings = ca . Ratings ( ca . CSVFile ( 'ratings.tsv' , separator = ' \\t ' )) Let's split with the KFold technique the loaded rating frame into train set and test set since n_splits=2 , train_list will contain two train_sets and test_list will contain two test_sets train_list , test_list = rs . KFoldPartitioning ( n_splits = 2 ) . split_all ( ratings ) In order to recommend items to users, we must choose an algorithm to use In this case we are using the CentroidVector algorithm which will work by using the first representation specified for the plot field You can freely choose which representation to use among all representation codified for the fields in the Content Analyzer phase centroid_vec = rs . CentroidVector ( { 'plot' : 'tfidf' }, # (1) similarity = rs . CosineSimilarity () ) We can reference the representation specified for the 'plot' field with the assigned custom id in the Content Analyzer phase Let's now compute the top-10 ranking for each user of the train set By default the candidate items are those in the test set of the user, but you can change this behaviour with the methodology parameter Since we used the kfold technique, we iterate over all train sets and test sets result_list = [] for train_set , test_set in zip ( train_list , test_list ): cbrs = rs . ContentBasedRS ( centroid_vec , train_set , 'movies_codified/' ) rank = cbrs . fit_rank ( test_set , n_recs = 10 ) result_list . append ( rank ) result_list will contain two Rank objects in this case, one for each split Evaluation module Similarly to the Content Analyzer and RecSys module, we must first import the evaluation module import clayrs.evaluation as eva The class responsible for evaluating recommendation lists is the EvalModel class. It needs the following parameters: A list of computed rank/predictions (in case multiple splits must be evaluated) A list of truths (in case multiple splits must be evaluated) List of metrics to compute Obviously the list of computed rank/predictions and list of truths must have the same length, and the rank/prediction in position \\(i\\) will be compared with the truth at position \\(i\\) em = eva . EvalModel ( pred_list = result_list , truth_list = test_list , metric_list = [ eva . NDCG (), eva . Precision (), eva . RecallAtK ( k = 5 ) ] ) Then simply call the fit () method of the instantiated object It will return two pandas DataFrame: the first one contains the metrics aggregated for the system, while the second contains the metrics computed for each user (where possible) sys_result , users_result = em . fit () Note Note that the EvalModel is able to compute evaluation of recommendations generated by other tools/frameworks, check documentation for more","title":"Quickstart"},{"location":"first_steps/quickstart/#quickstart","text":"","title":"Quickstart"},{"location":"first_steps/quickstart/#content-analyzer","text":"The first thing to do is to import the Content Analyzer module * We will access its methods and classes via dot notation import clayrs.content_analyzer as ca First let's point to the source containing raw information to process raw_source = ca . JSONFile ( 'items_info.json' ) Then let's start building the configuration for the items Info Note that same operations that can be specified for items could be also specified for users via the ca . UserAnalyzerConfig class # Configuration of item representation movies_ca_config = ca . ItemAnalyzerConfig ( source = raw_source , id = 'movielens_id' , # (1) output_directory = 'movies_codified/' # (2) ) The id in the raw source which uniquely identifies each item Directory which will contain items complexly represented Let's represent the plot field of each content with a TfIdf representation * * Since the preprocessing parameter has been specified, then each field is first preprocessed with the specified operations movies_ca_config . add_single_config ( 'plot' , ca . FieldConfig ( ca . SkLearnTfIdf (), preprocessing = ca . NLTK ( stopwords_removal = True , lemmatization = True ), id = 'tfidf' ) # (1) ) User defined id for the representation To finalize the Content Analyzer part, let's instantiate the ContentAnalyzer class by passing the built configuration and by calling its fit() method ca . ContentAnalyzer ( movies_ca_config ) . fit () The items will be created with the specified representations and serialized","title":"Content Analyzer"},{"location":"first_steps/quickstart/#recsys","text":"Similarly above, we must first import the RecSys module import clayrs.recsys as rs Then we load the rating frame from a TSV file Info In this case in our file the first three columns are user_id, item_id, score in this order If your file has a different structure you must specify how to map the column via parameters, check documentation for more ratings = ca . Ratings ( ca . CSVFile ( 'ratings.tsv' , separator = ' \\t ' )) Let's split with the KFold technique the loaded rating frame into train set and test set since n_splits=2 , train_list will contain two train_sets and test_list will contain two test_sets train_list , test_list = rs . KFoldPartitioning ( n_splits = 2 ) . split_all ( ratings ) In order to recommend items to users, we must choose an algorithm to use In this case we are using the CentroidVector algorithm which will work by using the first representation specified for the plot field You can freely choose which representation to use among all representation codified for the fields in the Content Analyzer phase centroid_vec = rs . CentroidVector ( { 'plot' : 'tfidf' }, # (1) similarity = rs . CosineSimilarity () ) We can reference the representation specified for the 'plot' field with the assigned custom id in the Content Analyzer phase Let's now compute the top-10 ranking for each user of the train set By default the candidate items are those in the test set of the user, but you can change this behaviour with the methodology parameter Since we used the kfold technique, we iterate over all train sets and test sets result_list = [] for train_set , test_set in zip ( train_list , test_list ): cbrs = rs . ContentBasedRS ( centroid_vec , train_set , 'movies_codified/' ) rank = cbrs . fit_rank ( test_set , n_recs = 10 ) result_list . append ( rank ) result_list will contain two Rank objects in this case, one for each split","title":"RecSys"},{"location":"first_steps/quickstart/#evaluation-module","text":"Similarly to the Content Analyzer and RecSys module, we must first import the evaluation module import clayrs.evaluation as eva The class responsible for evaluating recommendation lists is the EvalModel class. It needs the following parameters: A list of computed rank/predictions (in case multiple splits must be evaluated) A list of truths (in case multiple splits must be evaluated) List of metrics to compute Obviously the list of computed rank/predictions and list of truths must have the same length, and the rank/prediction in position \\(i\\) will be compared with the truth at position \\(i\\) em = eva . EvalModel ( pred_list = result_list , truth_list = test_list , metric_list = [ eva . NDCG (), eva . Precision (), eva . RecallAtK ( k = 5 ) ] ) Then simply call the fit () method of the instantiated object It will return two pandas DataFrame: the first one contains the metrics aggregated for the system, while the second contains the metrics computed for each user (where possible) sys_result , users_result = em . fit () Note Note that the EvalModel is able to compute evaluation of recommendations generated by other tools/frameworks, check documentation for more","title":"Evaluation module"},{"location":"recsys/introduction/","text":"Warning Docs are still a WIP Introduction The Recommender System module lets you easily build a Content Based Recommender System ( CBRS ) or a Graph Based Recommender system ( GBRS ) with various algorithms. Info The Recsys module is grounded on contents created with the Content Analyzer The following will introduce you to the standard usage pipeline for this module, starting from importing the dataset to generating recommendation lists. Importing the dataset The Ratings class allows you to import rating from a source file (or also from an existent dataframe) into a custom object. If the source file contains users (U), items (I) and ratings (R) in this order, no additional parameters are needed, otherwise the mapping must be explictly specified using: 'user_id' column, 'item_id' column, 'score' column In this case the dataset we want to import is a CSV file with the following header: user_id,item_id,rating,timestamp As you can see the user id column, item id column and score column are the first three column and are already in sequential order, so no additional parameter is required to the Ratings class: import clayrs.content_analyzer as ca ratings_raw_source = ca . CSVFile ( 'ratings.csv' ) # (1) ratings = ca . Ratings ( ratings_raw_source ) In this case our raw source is a CSV file, but ClayRS can also read from JSON files, DAT files and more Splitting the dataset Once you imported the dataset, the first thing you may want to do is to split it with a Partitioning technique The output of any partitioning technique are two lists. One containing the two train set (in this case), the other containing the two test set (in this case) import clayrs.recsys as rs # kfold partitioning technique kf = rs . KFoldPartitioning ( n_splits = 2 ) train_list , test_list = kf . split_all ( ratings ) # (1) You can pass to the split_all() method a specific user_id_list in case you only want to perform the splitting operation for a specific subset of users (e.g. select only users with more than x ratings) Defining a Content Based Recommender System A Content Based Recommender System needs an algorithm for ranking or predicting items to users. There are many available, in the following example we use the CentroidVector algorithm: It computes the centroid vector of the features of items liked by the user It computes the similarity between the centroid vector and unrated items The items liked by a user are those having a rating higher or equal than a specific threshold . If the threshold is not specified, the average score of all items liked by the user is used. As already said, the Recommender System leverages the representations defined by the Content Analyzer. Suppose you have complexly represented the 'plot' with a simple TfIdf technique and assigned to this representation the tfidf id: import clayrs.recsys as rs centroid_vec = rs . CentroidVector ( { 'plot' : 'tfidf' }, similarity = rs . CosineSimilarity () ) You can reference representation for a field also with an integer, in case you didn't assign any custom id during Content Analyzer phase. centroid_vec = rs . CentroidVector ( { 'plot' : 0 }, # (1) similarity = rs . CosineSimilarity () ) This means that you want to use the first representation with which the 'plot' field was complexly represented Please note that multiple representations could be adopted for a single field, and also multiple representations for multiple fields can be combined together! Simply specify them in the item_field dict that must be passed to any Content Based algorithm: centroid_vec = rs . CentroidVector ( { 'plot' : [ 0 , 'glove-50' , 'glove-100' ], 'genre' : [ 'tfidf' , 'fasttext' ]}, similarity = rs . CosineSimilarity () ) After choosing the algorithm, you are ready to instantiate the ContentBasedRS class. A CBRS needs the following parameters: The recommendation algorithm The train set The path of the items serialized by the Content Analyzer train_set = test_list [ 0 ] # (1) cbrs = rs . ContentBasedRS ( random_forests , train_set , 'movies_codified/' ) Since every partitioning technique returns a list of train sets ( here ), in this way we are using only the first train set produced. Just below there's an example on how to produce recommendation for more than one split In case you perform a splitting of the dataset which returns a multiple train and test sets (KFold technique): original_rat = ca . Ratings ( ca . CSVFile ( ratings_path )) train_list , test_list = rs . KFoldPartitioning ( n_splits = 5 ) . split_all ( original_rat ) alg = rs . CentroidVector () # any cb algorithm for train_set , test_set in zip ( train_list , test_list ): cbrs = rs . ContentBasedRS ( alg , train_set , items_path ) rank_to_append = cbrs . fit_rank ( test_set ) result_list . append ( rank_to_append ) result_list will contain recommendation lists for each split Defining a Graph Based Recommender System A Graph Based Recommender System ( GBRS ) requires to first define a graph Ratings imported are used to create a Full Graph where property nodes (e.g. gender for users, budget for movies) can be linked to every node without any restriction The framework also allows to create a Bipartite Graph (a graph without property node) and a Tripartite Graph (where property nodes are only linked to item nodes) In order to load properties in the graph, we must specify where users and items are serialized and which properties to add (the following is the same for item_exo_properties ): If user_exo_properties is specified as a set , then the graph will try to load all properties from said exogenous representation # example { 'my_exo_id' } If user_exo_properties is specified as a dict , then the graph will try to load said properties from said exogenous representation # example { 'my_exo_id' : [ 'my_prop1' , 'my_prop2' ]]} Let's now create the graph loading all properties: full_graph = rs . NXFullGraph ( ratings , user_contents_dir = 'users_codified/' , # (1) item_contents_dir = 'movies_codified/' , # (2) user_exo_properties = { 0 }, # (3) item_exo_properties = { 'dbpedia' }, # (4) link_label = 'score' ) Where users complexly represented have been serialized during Content Analyzer phase Where items complexly represented have been serialized during Content Analyzer phase This means that you want to use the first exogenous representation with which the each user has been expanded You can also access exogenous representation with custom id, if specified during Content Analyzer phase The last step to perform before defining the GBRS is to instantiate an algorithm for ranking or predicting items to users. In the following example we use the Personalized PageRank algorithm: pr = rs . NXPageRank ( personalized = True ) Finally we can instantiate the GBRS! gbrs = rs . GraphBasedRS ( pr , full_graph ) Generating recommendations Info The following procedure works both for CBRS and GBRS . In the following we will consider a cbrs as an example Now the cbrs must be fit before we can compute the rank: We could do this in two separate steps, by first calling the fit(..) method and then the rank(...) method Or by calling directly the fit_rank(...) method, which performs both in one step In this case we choose the first method: cbrs . fit () test_set = test_list [ 0 ] # (1) rank = cbrs . rank ( test_set , n_recs = 10 ) # top-10 recommendation for each user Since every partitioning technique returns a list of test sets ( here ), in this way we are using only the first train set produced. Just below there's an example on how to produce recommendation for more than one split In case you perform a splitting of the dataset which returns a multiple train and test sets (KFold technique): original_rat = ca . Ratings ( ca . CSVFile ( ratings_path )) train_list , test_list = rs . KFoldPartitioning ( n_splits = 5 ) . split_all ( original_rat ) alg = rs . NXPageRank () # any gb algorithm for train_set , test_set in zip ( train_list , test_list ): full_graph = rs . NXFullGraph ( train_set , user_contents_dir = 'users_codified/' , item_contents_dir = 'movies_codified/' , user_exo_properties = { 0 }, item_exo_properties = { 'dbpedia' }, link_label = 'score' ) gbrs = rs . GraphBasedRS ( alg , full_graph ) rank_to_append = gbrs . rank ( test_set ) result_list . append ( rank_to_append ) result_list will contain recommendation lists for each split Customizing the ranking process You can customize the ranking process by changing the parameters of the rank(...) method You can choice for which users to produce recommendations: rank = cbrs . rank ( test_set , user_id_list = [ 'u1' , 'u23' , 'u56' ]) If a cut rank list for each user must be produced: rank = cbrs . rank ( test_set , n_recs = 10 ) If a different methodology must be used: Info A methodology lets you customize which items must be ranked for each user. For each target user u , the following 4 different methodologies are available for defining those lists: TestRatings (default): the list of items to be evaluated consists of items rated by u in the test set TestItems : every item in the test set of every user except those in the training set of the target user will be predicted TrainingItems : every item in the training set of every user will be predicted except those in the training set of the target user AllItems : the whole set of items, except those in the training set of the target user, will be predicted More information on this paper . By default the methodology used is the TestRatings methodology rank = cbrs . rank ( test_set , methodology = rs . TrainingItems ()) Generating score predictions Some algorithm (e.g. LinearPredictor algorithm) are able to predict the numeric rating that a user would give to unseen items. The usage is exactly the same of generating recommendations and customizing the ranking process , the only thing that changes is the method to call: score_prediction = cbrs . fit_predict ( test_set ) or: cbrs . fit () score_prediction = cbrs . predict ( test_set ) Note : if the predict() or the fit_predict() method is called for an algorithm that is not able to perform score prediction, the NotPredictionAlg exception is raised","title":"Introduction"},{"location":"recsys/introduction/#introduction","text":"The Recommender System module lets you easily build a Content Based Recommender System ( CBRS ) or a Graph Based Recommender system ( GBRS ) with various algorithms. Info The Recsys module is grounded on contents created with the Content Analyzer The following will introduce you to the standard usage pipeline for this module, starting from importing the dataset to generating recommendation lists.","title":"Introduction"},{"location":"recsys/introduction/#importing-the-dataset","text":"The Ratings class allows you to import rating from a source file (or also from an existent dataframe) into a custom object. If the source file contains users (U), items (I) and ratings (R) in this order, no additional parameters are needed, otherwise the mapping must be explictly specified using: 'user_id' column, 'item_id' column, 'score' column In this case the dataset we want to import is a CSV file with the following header: user_id,item_id,rating,timestamp As you can see the user id column, item id column and score column are the first three column and are already in sequential order, so no additional parameter is required to the Ratings class: import clayrs.content_analyzer as ca ratings_raw_source = ca . CSVFile ( 'ratings.csv' ) # (1) ratings = ca . Ratings ( ratings_raw_source ) In this case our raw source is a CSV file, but ClayRS can also read from JSON files, DAT files and more","title":"Importing the dataset"},{"location":"recsys/introduction/#splitting-the-dataset","text":"Once you imported the dataset, the first thing you may want to do is to split it with a Partitioning technique The output of any partitioning technique are two lists. One containing the two train set (in this case), the other containing the two test set (in this case) import clayrs.recsys as rs # kfold partitioning technique kf = rs . KFoldPartitioning ( n_splits = 2 ) train_list , test_list = kf . split_all ( ratings ) # (1) You can pass to the split_all() method a specific user_id_list in case you only want to perform the splitting operation for a specific subset of users (e.g. select only users with more than x ratings)","title":"Splitting the dataset"},{"location":"recsys/introduction/#defining-a-content-based-recommender-system","text":"A Content Based Recommender System needs an algorithm for ranking or predicting items to users. There are many available, in the following example we use the CentroidVector algorithm: It computes the centroid vector of the features of items liked by the user It computes the similarity between the centroid vector and unrated items The items liked by a user are those having a rating higher or equal than a specific threshold . If the threshold is not specified, the average score of all items liked by the user is used. As already said, the Recommender System leverages the representations defined by the Content Analyzer. Suppose you have complexly represented the 'plot' with a simple TfIdf technique and assigned to this representation the tfidf id: import clayrs.recsys as rs centroid_vec = rs . CentroidVector ( { 'plot' : 'tfidf' }, similarity = rs . CosineSimilarity () ) You can reference representation for a field also with an integer, in case you didn't assign any custom id during Content Analyzer phase. centroid_vec = rs . CentroidVector ( { 'plot' : 0 }, # (1) similarity = rs . CosineSimilarity () ) This means that you want to use the first representation with which the 'plot' field was complexly represented Please note that multiple representations could be adopted for a single field, and also multiple representations for multiple fields can be combined together! Simply specify them in the item_field dict that must be passed to any Content Based algorithm: centroid_vec = rs . CentroidVector ( { 'plot' : [ 0 , 'glove-50' , 'glove-100' ], 'genre' : [ 'tfidf' , 'fasttext' ]}, similarity = rs . CosineSimilarity () ) After choosing the algorithm, you are ready to instantiate the ContentBasedRS class. A CBRS needs the following parameters: The recommendation algorithm The train set The path of the items serialized by the Content Analyzer train_set = test_list [ 0 ] # (1) cbrs = rs . ContentBasedRS ( random_forests , train_set , 'movies_codified/' ) Since every partitioning technique returns a list of train sets ( here ), in this way we are using only the first train set produced. Just below there's an example on how to produce recommendation for more than one split In case you perform a splitting of the dataset which returns a multiple train and test sets (KFold technique): original_rat = ca . Ratings ( ca . CSVFile ( ratings_path )) train_list , test_list = rs . KFoldPartitioning ( n_splits = 5 ) . split_all ( original_rat ) alg = rs . CentroidVector () # any cb algorithm for train_set , test_set in zip ( train_list , test_list ): cbrs = rs . ContentBasedRS ( alg , train_set , items_path ) rank_to_append = cbrs . fit_rank ( test_set ) result_list . append ( rank_to_append ) result_list will contain recommendation lists for each split","title":"Defining a Content Based Recommender System"},{"location":"recsys/introduction/#defining-a-graph-based-recommender-system","text":"A Graph Based Recommender System ( GBRS ) requires to first define a graph Ratings imported are used to create a Full Graph where property nodes (e.g. gender for users, budget for movies) can be linked to every node without any restriction The framework also allows to create a Bipartite Graph (a graph without property node) and a Tripartite Graph (where property nodes are only linked to item nodes) In order to load properties in the graph, we must specify where users and items are serialized and which properties to add (the following is the same for item_exo_properties ): If user_exo_properties is specified as a set , then the graph will try to load all properties from said exogenous representation # example { 'my_exo_id' } If user_exo_properties is specified as a dict , then the graph will try to load said properties from said exogenous representation # example { 'my_exo_id' : [ 'my_prop1' , 'my_prop2' ]]} Let's now create the graph loading all properties: full_graph = rs . NXFullGraph ( ratings , user_contents_dir = 'users_codified/' , # (1) item_contents_dir = 'movies_codified/' , # (2) user_exo_properties = { 0 }, # (3) item_exo_properties = { 'dbpedia' }, # (4) link_label = 'score' ) Where users complexly represented have been serialized during Content Analyzer phase Where items complexly represented have been serialized during Content Analyzer phase This means that you want to use the first exogenous representation with which the each user has been expanded You can also access exogenous representation with custom id, if specified during Content Analyzer phase The last step to perform before defining the GBRS is to instantiate an algorithm for ranking or predicting items to users. In the following example we use the Personalized PageRank algorithm: pr = rs . NXPageRank ( personalized = True ) Finally we can instantiate the GBRS! gbrs = rs . GraphBasedRS ( pr , full_graph )","title":"Defining a Graph Based Recommender System"},{"location":"recsys/introduction/#generating-recommendations","text":"Info The following procedure works both for CBRS and GBRS . In the following we will consider a cbrs as an example Now the cbrs must be fit before we can compute the rank: We could do this in two separate steps, by first calling the fit(..) method and then the rank(...) method Or by calling directly the fit_rank(...) method, which performs both in one step In this case we choose the first method: cbrs . fit () test_set = test_list [ 0 ] # (1) rank = cbrs . rank ( test_set , n_recs = 10 ) # top-10 recommendation for each user Since every partitioning technique returns a list of test sets ( here ), in this way we are using only the first train set produced. Just below there's an example on how to produce recommendation for more than one split In case you perform a splitting of the dataset which returns a multiple train and test sets (KFold technique): original_rat = ca . Ratings ( ca . CSVFile ( ratings_path )) train_list , test_list = rs . KFoldPartitioning ( n_splits = 5 ) . split_all ( original_rat ) alg = rs . NXPageRank () # any gb algorithm for train_set , test_set in zip ( train_list , test_list ): full_graph = rs . NXFullGraph ( train_set , user_contents_dir = 'users_codified/' , item_contents_dir = 'movies_codified/' , user_exo_properties = { 0 }, item_exo_properties = { 'dbpedia' }, link_label = 'score' ) gbrs = rs . GraphBasedRS ( alg , full_graph ) rank_to_append = gbrs . rank ( test_set ) result_list . append ( rank_to_append ) result_list will contain recommendation lists for each split","title":"Generating recommendations"},{"location":"recsys/introduction/#customizing-the-ranking-process","text":"You can customize the ranking process by changing the parameters of the rank(...) method You can choice for which users to produce recommendations: rank = cbrs . rank ( test_set , user_id_list = [ 'u1' , 'u23' , 'u56' ]) If a cut rank list for each user must be produced: rank = cbrs . rank ( test_set , n_recs = 10 ) If a different methodology must be used: Info A methodology lets you customize which items must be ranked for each user. For each target user u , the following 4 different methodologies are available for defining those lists: TestRatings (default): the list of items to be evaluated consists of items rated by u in the test set TestItems : every item in the test set of every user except those in the training set of the target user will be predicted TrainingItems : every item in the training set of every user will be predicted except those in the training set of the target user AllItems : the whole set of items, except those in the training set of the target user, will be predicted More information on this paper . By default the methodology used is the TestRatings methodology rank = cbrs . rank ( test_set , methodology = rs . TrainingItems ())","title":"Customizing the ranking process"},{"location":"recsys/introduction/#generating-score-predictions","text":"Some algorithm (e.g. LinearPredictor algorithm) are able to predict the numeric rating that a user would give to unseen items. The usage is exactly the same of generating recommendations and customizing the ranking process , the only thing that changes is the method to call: score_prediction = cbrs . fit_predict ( test_set ) or: cbrs . fit () score_prediction = cbrs . predict ( test_set ) Note : if the predict() or the fit_predict() method is called for an algorithm that is not able to perform score prediction, the NotPredictionAlg exception is raised","title":"Generating score predictions"},{"location":"recsys/content_based/content_based_recsys/","text":"Content Based RecSys ContentBasedRS ( algorithm , train_set , items_directory , users_directory = None ) Bases: RecSys Class for recommender systems which use the items' content in order to make predictions, some algorithms may also use users' content, so it's an optional parameter. Every CBRS differ from each other based the algorithm used. Examples: In case you perform a splitting of the dataset which returns a single train and test set (e.g. HoldOut technique): Single split train from clayrs import recsys as rs from clayrs import content_analyzer as ca original_rat = ca . Ratings ( ca . CSVFile ( ratings_path )) [ train ], [ test ] = rs . HoldOutPartitioning () . split_all ( original_rat ) alg = rs . CentroidVector () # any cb algorithm cbrs = rs . ContentBasedRS ( alg , train , items_path ) rank = cbrs . fit_rank ( test , n_recs = 10 ) In case you perform a splitting of the dataset which returns a multiple train and test sets (KFold technique): Multiple split train from clayrs import recsys as rs from clayrs import content_analyzer as ca original_rat = ca . Ratings ( ca . CSVFile ( ratings_path )) train_list , test_list = rs . KFoldPartitioning ( n_splits = 5 ) . split_all ( original_rat ) alg = rs . CentroidVector () # any cb algorithm for train_set , test_set in zip ( train_list , test_list ): cbrs = rs . ContentBasedRS ( alg , train_set , items_path ) rank_to_append = cbrs . fit_rank ( test_set ) result_list . append ( rank_to_append ) result_list will contain recommendation lists for each split Parameters: Name Type Description Default algorithm ContentBasedAlgorithm the content based algorithm that will be used in order to rank or make score prediction required train_set Ratings a Ratings object containing interactions between users and items required items_directory str the path of the items serialized by the Content Analyzer required users_directory str the path of the users serialized by the Content Analyzer None Source code in clayrs/recsys/recsys.py 109 110 111 112 113 114 115 116 117 118 119 def __init__ ( self , algorithm : ContentBasedAlgorithm , train_set : Ratings , items_directory : str , users_directory : str = None ): super () . __init__ ( algorithm ) self . __train_set = train_set self . __items_directory = items_directory self . __users_directory = users_directory self . _user_fit_dic = {} algorithm () property The content based algorithm chosen Source code in clayrs/recsys/recsys.py 121 122 123 124 125 126 127 @property def algorithm ( self ): \"\"\" The content based algorithm chosen \"\"\" alg : ContentBasedAlgorithm = super () . algorithm return alg fit () Method which will fit the algorithm chosen for each user in the train set passed in the constructor If the algorithm can't be fit for some users, a warning message is printed Source code in clayrs/recsys/recsys.py 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 def fit ( self ): \"\"\" Method which will fit the algorithm chosen for each user in the train set passed in the constructor If the algorithm can't be fit for some users, a warning message is printed \"\"\" items_to_load = set ( self . train_set . item_id_column ) loaded_items_interface = self . algorithm . _load_available_contents ( self . items_directory , items_to_load ) with get_progbar ( set ( self . train_set . user_id_column )) as pbar : pbar . set_description ( \"Fitting algorithm\" ) for user_id in pbar : user_train = self . train_set . get_user_interactions ( user_id ) try : user_alg = deepcopy ( self . algorithm ) user_alg . process_rated ( user_train , loaded_items_interface ) user_alg . fit () self . _user_fit_dic [ user_id ] = user_alg except UserSkipAlgFit as e : warning_message = str ( e ) + f \" \\n No algorithm will be fitted for the user { user_id } \" logger . warning ( warning_message ) self . _user_fit_dic [ user_id ] = None # we force the garbage collector after freeing loaded items del loaded_items_interface gc . collect () return self fit_predict ( test_set , user_id_list = None , methodology = TestRatingsMethodology (), save_fit = False ) Method used to both fit and calculate score prediction for all users in test set or all users in user_id_list parameter. The Recommender System will first be fit for each user in the train set passed in the constructor. If the algorithm can't be fit for some users, a warning message is printed BE CAREFUL : not all algorithms are able to perform score prediction Via the methodology parameter you can perform different candidate item selection. By default the TestRatingsMethodology() is used, so for each user items in its test set only will be considered for score prediction If the algorithm was not fit for some users, they will be skipped and a warning is printed With the save_fit parameter you can decide if you want that you recommender system remains fit even after the complete execution of this method, in case you want to compute ranking/score prediction with other methodologies, or with a different n_recs parameter. Be mindful since it can be memory-expensive, thus by default this behaviour is disabled Parameters: Name Type Description Default test_set Ratings Ratings object which represents the ground truth of the split considered required user_id_list List List of users for which you want to compute score prediction. If None, the ranking will be computed for all users of the test_set None methodology Union [ Methodology , None] Methodology object which governs the candidate item selection. Default is TestRatingsMethodology TestRatingsMethodology() save_fit bool Boolean value which let you choose if the Recommender System should remain fit even after the complete execution of this method. Default is False False Returns: Type Description Prediction Rank object containing recommendation lists for all users of the test set or for all users in user_id_list Source code in clayrs/recsys/recsys.py 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 def fit_predict ( self , test_set : Ratings , user_id_list : List = None , methodology : Union [ Methodology , None ] = TestRatingsMethodology (), save_fit : bool = False ) -> Prediction : \"\"\" Method used to both fit and calculate score prediction for all users in test set or all users in `user_id_list` parameter. The Recommender System will first be fit for each user in the train set passed in the constructor. If the algorithm can't be fit for some users, a warning message is printed **BE CAREFUL**: not all algorithms are able to perform *score prediction* Via the `methodology` parameter you can perform different candidate item selection. By default the `TestRatingsMethodology()` is used, so for each user items in its test set only will be considered for score prediction If the algorithm was not fit for some users, they will be skipped and a warning is printed With the `save_fit` parameter you can decide if you want that you recommender system remains *fit* even after the complete execution of this method, in case you want to compute ranking/score prediction with other methodologies, or with a different `n_recs` parameter. Be mindful since it can be memory-expensive, thus by default this behaviour is disabled Args: test_set: Ratings object which represents the ground truth of the split considered user_id_list: List of users for which you want to compute score prediction. If None, the ranking will be computed for all users of the `test_set` methodology: `Methodology` object which governs the candidate item selection. Default is `TestRatingsMethodology` save_fit: Boolean value which let you choose if the Recommender System should remain fit even after the complete execution of this method. Default is False Returns: Rank object containing recommendation lists for all users of the test set or for all users in `user_id_list` \"\"\" all_users = set ( test_set . user_id_column ) if user_id_list is not None : all_users = set ( user_id_list ) loaded_items_interface = self . algorithm . _load_available_contents ( self . items_directory , set ()) pred = [] logger . info ( \"Don't worry if it looks stuck at first\" ) logger . info ( \"First iterations will stabilize the estimated remaining time\" ) with get_progbar ( all_users ) as pbar : pbar . set_description ( f \"Loading first items from memory...\" ) for user_id in pbar : user_id = str ( user_id ) user_train = self . train_set . get_user_interactions ( user_id ) try : if save_fit : user_alg = deepcopy ( self . algorithm ) self . _user_fit_dic [ user_id ] = user_alg alg = user_alg else : alg = self . algorithm alg . process_rated ( user_train , loaded_items_interface ) alg . fit () except UserSkipAlgFit as e : warning_message = str ( e ) + f \" \\n The algorithm can't be fitted for the user { user_id } \" logger . warning ( warning_message ) if save_fit : self . _user_fit_dic [ user_id ] = None continue filter_list = None if methodology is not None : filter_list = set ( methodology . filter_single ( user_id , self . train_set , test_set )) user_pred = alg . predict ( user_train , loaded_items_interface , filter_list = filter_list ) pred . extend ( user_pred ) pbar . set_description ( f \"Computing fit_rank for user { user_id } \" ) pred = Prediction . from_list ( pred ) # we force the garbage collector after freeing loaded items del loaded_items_interface gc . collect () self . _yaml_report = { 'mode' : 'score_prediction' , 'methodology' : repr ( methodology )} return pred fit_rank ( test_set , n_recs = None , user_id_list = None , methodology = TestRatingsMethodology (), save_fit = False ) Method used to both fit and calculate ranking for all users in test set or all users in user_id_list parameter. The Recommender System will first be fit for each user in the train set passed in the constructor. If the algorithm can't be fit for some users, a warning message is printed If the n_recs is specified, then the rank will contain the top-n items for the users. Otherwise the rank will contain all unrated items of the particular users Via the methodology parameter you can perform different candidate item selection. By default the TestRatingsMethodology() is used, so for each user items in its test set only will be ranked If the algorithm was not fit for some users, they will be skipped and a warning is printed With the save_fit parameter you can decide if you want that you recommender system remains fit even after the complete execution of this method, in case you want to compute ranking with other methodologies, or with a different n_recs parameter. Be mindful since it can be memory-expensive, thus by default this behaviour is disabled Parameters: Name Type Description Default test_set Ratings Ratings object which represents the ground truth of the split considered required n_recs int Number of the top items that will be present in the ranking. If None all candidate items will be returned for the user None user_id_list List List of users for which you want to compute the ranking. If None, the ranking will be computed for all users of the test_set None methodology Union [ Methodology , None] Methodology object which governs the candidate item selection. Default is TestRatingsMethodology TestRatingsMethodology() save_fit bool Boolean value which let you choose if the Recommender System should remain fit even after the complete execution of this method. Default is False False Raises: Type Description NotFittedAlg Exception raised when this method is called without first calling the fit method Returns: Type Description Rank Rank object containing recommendation lists for all users of the test set or for all users in user_id_list Source code in clayrs/recsys/recsys.py 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 def fit_rank ( self , test_set : Ratings , n_recs : int = None , user_id_list : List = None , methodology : Union [ Methodology , None ] = TestRatingsMethodology (), save_fit : bool = False ) -> Rank : \"\"\" Method used to both fit and calculate ranking for all users in test set or all users in `user_id_list` parameter. The Recommender System will first be fit for each user in the train set passed in the constructor. If the algorithm can't be fit for some users, a warning message is printed If the `n_recs` is specified, then the rank will contain the top-n items for the users. Otherwise the rank will contain all unrated items of the particular users Via the `methodology` parameter you can perform different candidate item selection. By default the `TestRatingsMethodology()` is used, so for each user items in its test set only will be ranked If the algorithm was not fit for some users, they will be skipped and a warning is printed With the `save_fit` parameter you can decide if you want that you recommender system remains *fit* even after the complete execution of this method, in case you want to compute ranking with other methodologies, or with a different `n_recs` parameter. Be mindful since it can be memory-expensive, thus by default this behaviour is disabled Args: test_set: Ratings object which represents the ground truth of the split considered n_recs: Number of the top items that will be present in the ranking. If None all candidate items will be returned for the user user_id_list: List of users for which you want to compute the ranking. If None, the ranking will be computed for all users of the `test_set` methodology: `Methodology` object which governs the candidate item selection. Default is `TestRatingsMethodology` save_fit: Boolean value which let you choose if the Recommender System should remain fit even after the complete execution of this method. Default is False Raises: NotFittedAlg: Exception raised when this method is called without first calling the `fit` method Returns: Rank object containing recommendation lists for all users of the test set or for all users in `user_id_list` \"\"\" all_users = set ( test_set . user_id_column ) if user_id_list is not None : all_users = set ( user_id_list ) loaded_items_interface = self . algorithm . _load_available_contents ( self . items_directory , set ()) rank = [] logger . info ( \"Don't worry if it looks stuck at first\" ) logger . info ( \"First iterations will stabilize the estimated remaining time\" ) with get_progbar ( all_users ) as pbar : pbar . set_description ( f \"Loading first items from memory...\" ) for user_id in pbar : user_id = str ( user_id ) user_train = self . train_set . get_user_interactions ( user_id ) try : if save_fit : user_alg = deepcopy ( self . algorithm ) self . _user_fit_dic [ user_id ] = user_alg alg = user_alg else : alg = self . algorithm alg . process_rated ( user_train , loaded_items_interface ) alg . fit () except UserSkipAlgFit as e : warning_message = str ( e ) + f \" \\n The algorithm can't be fitted for the user { user_id } \" logger . warning ( warning_message ) if save_fit : self . _user_fit_dic [ user_id ] = None continue filter_list = None if methodology is not None : filter_list = set ( methodology . filter_single ( user_id , self . train_set , test_set )) user_rank = alg . rank ( user_train , loaded_items_interface , n_recs , filter_list = filter_list ) rank . extend ( user_rank ) pbar . set_description ( f \"Computing fit_rank for user { user_id } \" ) rank = Rank . from_list ( rank ) # we force the garbage collector after freeing loaded items del loaded_items_interface gc . collect () self . _yaml_report = { 'mode' : 'rank' , 'n_recs' : repr ( n_recs ), 'methodology' : repr ( methodology )} return rank items_directory () property Path of the serialized items by the Content Analyzer Source code in clayrs/recsys/recsys.py 136 137 138 139 140 141 @property def items_directory ( self ): \"\"\" Path of the serialized items by the Content Analyzer \"\"\" return self . __items_directory predict ( test_set , user_id_list = None , methodology = TestRatingsMethodology ()) Method used to calculate score predictions for all users in test set or all users in user_id_list parameter. You must first call the fit() method before you can compute score predictions. BE CAREFUL : not all algorithms are able to perform score prediction Via the methodology parameter you can perform different candidate item selection. By default the TestRatingsMethodology() is used, so for each user items in its test set only will be considered for score prediction If the algorithm was not fit for some users, they will be skipped and a warning is printed Parameters: Name Type Description Default test_set Ratings Ratings object which represents the ground truth of the split considered required user_id_list List List of users for which you want to compute score prediction. If None, the ranking will be computed for all users of the test_set None methodology Union [ Methodology , None] Methodology object which governs the candidate item selection. Default is TestRatingsMethodology TestRatingsMethodology() Raises: Type Description NotFittedAlg Exception raised when this method is called without first calling the fit method Returns: Type Description Prediction Prediction object containing score prediction lists for all users of the test set or for all users in user_id_list Source code in clayrs/recsys/recsys.py 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 def predict ( self , test_set : Ratings , user_id_list : List = None , methodology : Union [ Methodology , None ] = TestRatingsMethodology ()) -> Prediction : \"\"\" Method used to calculate score predictions for all users in test set or all users in `user_id_list` parameter. You must first call the `fit()` method before you can compute score predictions. **BE CAREFUL**: not all algorithms are able to perform *score prediction* Via the `methodology` parameter you can perform different candidate item selection. By default the `TestRatingsMethodology()` is used, so for each user items in its test set only will be considered for score prediction If the algorithm was not fit for some users, they will be skipped and a warning is printed Args: test_set: Ratings object which represents the ground truth of the split considered user_id_list: List of users for which you want to compute score prediction. If None, the ranking will be computed for all users of the `test_set` methodology: `Methodology` object which governs the candidate item selection. Default is `TestRatingsMethodology` Raises: NotFittedAlg: Exception raised when this method is called without first calling the `fit` method Returns: Prediction object containing score prediction lists for all users of the test set or for all users in `user_id_list` \"\"\" if len ( self . _user_fit_dic ) == 0 : raise NotFittedAlg ( \"Algorithm not fit! You must call the fit() method first, or fit_rank().\" ) all_users = set ( test_set . user_id_column ) if user_id_list is not None : all_users = set ( user_id_list ) loaded_items_interface = self . algorithm . _load_available_contents ( self . items_directory , set ()) pred = [] logger . info ( \"Don't worry if it looks stuck at first\" ) logger . info ( \"First iterations will stabilize the estimated remaining time\" ) with get_progbar ( all_users ) as pbar : pbar . set_description ( f \"Loading first items from memory...\" ) for user_id in pbar : user_id = str ( user_id ) user_train = self . train_set . get_user_interactions ( user_id ) filter_list = None if methodology is not None : filter_list = set ( methodology . filter_single ( user_id , self . train_set , test_set )) user_fitted_alg = self . _user_fit_dic . get ( user_id ) if user_fitted_alg is not None : user_pred = user_fitted_alg . predict ( user_train , loaded_items_interface , filter_list = filter_list ) else : user_pred = [] logger . warning ( f \"No algorithm fitted for user { user_id } ! It will be skipped\" ) pred . extend ( user_pred ) pbar . set_description ( f \"Computing predictions for user { user_id } \" ) pred = Prediction . from_list ( pred ) # we force the garbage collector after freeing loaded items del loaded_items_interface gc . collect () self . _yaml_report = { 'mode' : 'score_prediction' , 'methodology' : repr ( methodology )} return pred rank ( test_set , n_recs = None , user_id_list = None , methodology = TestRatingsMethodology ()) Method used to calculate ranking for all users in test set or all users in user_id_list parameter. You must first call the fit() method before you can compute the ranking. If the n_recs is specified, then the rank will contain the top-n items for the users. Otherwise the rank will contain all unrated items of the particular users Via the methodology parameter you can perform different candidate item selection. By default the TestRatingsMethodology() is used, so for each user items in its test set only will be ranked If the algorithm was not fit for some users, they will be skipped and a warning is printed Parameters: Name Type Description Default test_set Ratings Ratings object which represents the ground truth of the split considered required n_recs int Number of the top items that will be present in the ranking. If None all candidate items will be returned for the user None user_id_list List List of users for which you want to compute the ranking. If None, the ranking will be computed for all users of the test_set None methodology Union [ Methodology , None] Methodology object which governs the candidate item selection. Default is TestRatingsMethodology TestRatingsMethodology() Raises: Type Description NotFittedAlg Exception raised when this method is called without first calling the fit method Returns: Type Description Rank Rank object containing recommendation lists for all users of the test set or for all users in user_id_list Source code in clayrs/recsys/recsys.py 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 def rank ( self , test_set : Ratings , n_recs : int = None , user_id_list : List = None , methodology : Union [ Methodology , None ] = TestRatingsMethodology ()) -> Rank : \"\"\" Method used to calculate ranking for all users in test set or all users in `user_id_list` parameter. You must first call the `fit()` method before you can compute the ranking. If the `n_recs` is specified, then the rank will contain the top-n items for the users. Otherwise the rank will contain all unrated items of the particular users Via the `methodology` parameter you can perform different candidate item selection. By default the `TestRatingsMethodology()` is used, so for each user items in its test set only will be ranked If the algorithm was not fit for some users, they will be skipped and a warning is printed Args: test_set: Ratings object which represents the ground truth of the split considered n_recs: Number of the top items that will be present in the ranking. If None all candidate items will be returned for the user user_id_list: List of users for which you want to compute the ranking. If None, the ranking will be computed for all users of the `test_set` methodology: `Methodology` object which governs the candidate item selection. Default is `TestRatingsMethodology` Raises: NotFittedAlg: Exception raised when this method is called without first calling the `fit` method Returns: Rank object containing recommendation lists for all users of the test set or for all users in `user_id_list` \"\"\" if len ( self . _user_fit_dic ) == 0 : raise NotFittedAlg ( \"Algorithm not fit! You must call the fit() method first, or fit_rank().\" ) all_users = set ( test_set . user_id_column ) if user_id_list is not None : all_users = set ( user_id_list ) loaded_items_interface = self . algorithm . _load_available_contents ( self . items_directory , set ()) rank = [] logger . info ( \"Don't worry if it looks stuck at first\" ) logger . info ( \"First iterations will stabilize the estimated remaining time\" ) with get_progbar ( all_users ) as pbar : pbar . set_description ( f \"Loading first items from memory...\" ) for user_id in pbar : user_id = str ( user_id ) user_train = self . train_set . get_user_interactions ( user_id ) filter_list = None if methodology is not None : filter_list = set ( methodology . filter_single ( user_id , self . train_set , test_set )) user_fitted_alg = self . _user_fit_dic . get ( user_id ) if user_fitted_alg is not None : user_rank = user_fitted_alg . rank ( user_train , loaded_items_interface , n_recs , filter_list = filter_list ) else : user_rank = [] logger . warning ( f \"No algorithm fitted for user { user_id } ! It will be skipped\" ) rank . extend ( user_rank ) pbar . set_description ( f \"Computing rank for { user_id } \" ) rank = Rank . from_list ( rank ) # we force the garbage collector after freeing loaded items del loaded_items_interface gc . collect () self . _yaml_report = { 'mode' : 'rank' , 'n_recs' : repr ( n_recs ), 'methodology' : repr ( methodology )} return rank train_set () property The train set of the Content Based RecSys Source code in clayrs/recsys/recsys.py 129 130 131 132 133 134 @property def train_set ( self ): \"\"\" The train set of the Content Based RecSys \"\"\" return self . __train_set users_directory () property Path of the serialized users by the Content Analyzer Source code in clayrs/recsys/recsys.py 143 144 145 146 147 148 @property def users_directory ( self ): \"\"\" Path of the serialized users by the Content Analyzer \"\"\" return self . __users_directory","title":"Content Based recsys"},{"location":"recsys/content_based/content_based_recsys/#content-based-recsys","text":"","title":"Content Based RecSys"},{"location":"recsys/content_based/content_based_recsys/#clayrs.recsys.recsys.ContentBasedRS","text":"Bases: RecSys Class for recommender systems which use the items' content in order to make predictions, some algorithms may also use users' content, so it's an optional parameter. Every CBRS differ from each other based the algorithm used. Examples: In case you perform a splitting of the dataset which returns a single train and test set (e.g. HoldOut technique): Single split train from clayrs import recsys as rs from clayrs import content_analyzer as ca original_rat = ca . Ratings ( ca . CSVFile ( ratings_path )) [ train ], [ test ] = rs . HoldOutPartitioning () . split_all ( original_rat ) alg = rs . CentroidVector () # any cb algorithm cbrs = rs . ContentBasedRS ( alg , train , items_path ) rank = cbrs . fit_rank ( test , n_recs = 10 ) In case you perform a splitting of the dataset which returns a multiple train and test sets (KFold technique): Multiple split train from clayrs import recsys as rs from clayrs import content_analyzer as ca original_rat = ca . Ratings ( ca . CSVFile ( ratings_path )) train_list , test_list = rs . KFoldPartitioning ( n_splits = 5 ) . split_all ( original_rat ) alg = rs . CentroidVector () # any cb algorithm for train_set , test_set in zip ( train_list , test_list ): cbrs = rs . ContentBasedRS ( alg , train_set , items_path ) rank_to_append = cbrs . fit_rank ( test_set ) result_list . append ( rank_to_append ) result_list will contain recommendation lists for each split Parameters: Name Type Description Default algorithm ContentBasedAlgorithm the content based algorithm that will be used in order to rank or make score prediction required train_set Ratings a Ratings object containing interactions between users and items required items_directory str the path of the items serialized by the Content Analyzer required users_directory str the path of the users serialized by the Content Analyzer None Source code in clayrs/recsys/recsys.py 109 110 111 112 113 114 115 116 117 118 119 def __init__ ( self , algorithm : ContentBasedAlgorithm , train_set : Ratings , items_directory : str , users_directory : str = None ): super () . __init__ ( algorithm ) self . __train_set = train_set self . __items_directory = items_directory self . __users_directory = users_directory self . _user_fit_dic = {}","title":"ContentBasedRS"},{"location":"recsys/content_based/content_based_recsys/#clayrs.recsys.recsys.ContentBasedRS.algorithm","text":"The content based algorithm chosen Source code in clayrs/recsys/recsys.py 121 122 123 124 125 126 127 @property def algorithm ( self ): \"\"\" The content based algorithm chosen \"\"\" alg : ContentBasedAlgorithm = super () . algorithm return alg","title":"algorithm()"},{"location":"recsys/content_based/content_based_recsys/#clayrs.recsys.recsys.ContentBasedRS.fit","text":"Method which will fit the algorithm chosen for each user in the train set passed in the constructor If the algorithm can't be fit for some users, a warning message is printed Source code in clayrs/recsys/recsys.py 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 def fit ( self ): \"\"\" Method which will fit the algorithm chosen for each user in the train set passed in the constructor If the algorithm can't be fit for some users, a warning message is printed \"\"\" items_to_load = set ( self . train_set . item_id_column ) loaded_items_interface = self . algorithm . _load_available_contents ( self . items_directory , items_to_load ) with get_progbar ( set ( self . train_set . user_id_column )) as pbar : pbar . set_description ( \"Fitting algorithm\" ) for user_id in pbar : user_train = self . train_set . get_user_interactions ( user_id ) try : user_alg = deepcopy ( self . algorithm ) user_alg . process_rated ( user_train , loaded_items_interface ) user_alg . fit () self . _user_fit_dic [ user_id ] = user_alg except UserSkipAlgFit as e : warning_message = str ( e ) + f \" \\n No algorithm will be fitted for the user { user_id } \" logger . warning ( warning_message ) self . _user_fit_dic [ user_id ] = None # we force the garbage collector after freeing loaded items del loaded_items_interface gc . collect () return self","title":"fit()"},{"location":"recsys/content_based/content_based_recsys/#clayrs.recsys.recsys.ContentBasedRS.fit_predict","text":"Method used to both fit and calculate score prediction for all users in test set or all users in user_id_list parameter. The Recommender System will first be fit for each user in the train set passed in the constructor. If the algorithm can't be fit for some users, a warning message is printed BE CAREFUL : not all algorithms are able to perform score prediction Via the methodology parameter you can perform different candidate item selection. By default the TestRatingsMethodology() is used, so for each user items in its test set only will be considered for score prediction If the algorithm was not fit for some users, they will be skipped and a warning is printed With the save_fit parameter you can decide if you want that you recommender system remains fit even after the complete execution of this method, in case you want to compute ranking/score prediction with other methodologies, or with a different n_recs parameter. Be mindful since it can be memory-expensive, thus by default this behaviour is disabled Parameters: Name Type Description Default test_set Ratings Ratings object which represents the ground truth of the split considered required user_id_list List List of users for which you want to compute score prediction. If None, the ranking will be computed for all users of the test_set None methodology Union [ Methodology , None] Methodology object which governs the candidate item selection. Default is TestRatingsMethodology TestRatingsMethodology() save_fit bool Boolean value which let you choose if the Recommender System should remain fit even after the complete execution of this method. Default is False False Returns: Type Description Prediction Rank object containing recommendation lists for all users of the test set or for all users in user_id_list Source code in clayrs/recsys/recsys.py 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 def fit_predict ( self , test_set : Ratings , user_id_list : List = None , methodology : Union [ Methodology , None ] = TestRatingsMethodology (), save_fit : bool = False ) -> Prediction : \"\"\" Method used to both fit and calculate score prediction for all users in test set or all users in `user_id_list` parameter. The Recommender System will first be fit for each user in the train set passed in the constructor. If the algorithm can't be fit for some users, a warning message is printed **BE CAREFUL**: not all algorithms are able to perform *score prediction* Via the `methodology` parameter you can perform different candidate item selection. By default the `TestRatingsMethodology()` is used, so for each user items in its test set only will be considered for score prediction If the algorithm was not fit for some users, they will be skipped and a warning is printed With the `save_fit` parameter you can decide if you want that you recommender system remains *fit* even after the complete execution of this method, in case you want to compute ranking/score prediction with other methodologies, or with a different `n_recs` parameter. Be mindful since it can be memory-expensive, thus by default this behaviour is disabled Args: test_set: Ratings object which represents the ground truth of the split considered user_id_list: List of users for which you want to compute score prediction. If None, the ranking will be computed for all users of the `test_set` methodology: `Methodology` object which governs the candidate item selection. Default is `TestRatingsMethodology` save_fit: Boolean value which let you choose if the Recommender System should remain fit even after the complete execution of this method. Default is False Returns: Rank object containing recommendation lists for all users of the test set or for all users in `user_id_list` \"\"\" all_users = set ( test_set . user_id_column ) if user_id_list is not None : all_users = set ( user_id_list ) loaded_items_interface = self . algorithm . _load_available_contents ( self . items_directory , set ()) pred = [] logger . info ( \"Don't worry if it looks stuck at first\" ) logger . info ( \"First iterations will stabilize the estimated remaining time\" ) with get_progbar ( all_users ) as pbar : pbar . set_description ( f \"Loading first items from memory...\" ) for user_id in pbar : user_id = str ( user_id ) user_train = self . train_set . get_user_interactions ( user_id ) try : if save_fit : user_alg = deepcopy ( self . algorithm ) self . _user_fit_dic [ user_id ] = user_alg alg = user_alg else : alg = self . algorithm alg . process_rated ( user_train , loaded_items_interface ) alg . fit () except UserSkipAlgFit as e : warning_message = str ( e ) + f \" \\n The algorithm can't be fitted for the user { user_id } \" logger . warning ( warning_message ) if save_fit : self . _user_fit_dic [ user_id ] = None continue filter_list = None if methodology is not None : filter_list = set ( methodology . filter_single ( user_id , self . train_set , test_set )) user_pred = alg . predict ( user_train , loaded_items_interface , filter_list = filter_list ) pred . extend ( user_pred ) pbar . set_description ( f \"Computing fit_rank for user { user_id } \" ) pred = Prediction . from_list ( pred ) # we force the garbage collector after freeing loaded items del loaded_items_interface gc . collect () self . _yaml_report = { 'mode' : 'score_prediction' , 'methodology' : repr ( methodology )} return pred","title":"fit_predict()"},{"location":"recsys/content_based/content_based_recsys/#clayrs.recsys.recsys.ContentBasedRS.fit_rank","text":"Method used to both fit and calculate ranking for all users in test set or all users in user_id_list parameter. The Recommender System will first be fit for each user in the train set passed in the constructor. If the algorithm can't be fit for some users, a warning message is printed If the n_recs is specified, then the rank will contain the top-n items for the users. Otherwise the rank will contain all unrated items of the particular users Via the methodology parameter you can perform different candidate item selection. By default the TestRatingsMethodology() is used, so for each user items in its test set only will be ranked If the algorithm was not fit for some users, they will be skipped and a warning is printed With the save_fit parameter you can decide if you want that you recommender system remains fit even after the complete execution of this method, in case you want to compute ranking with other methodologies, or with a different n_recs parameter. Be mindful since it can be memory-expensive, thus by default this behaviour is disabled Parameters: Name Type Description Default test_set Ratings Ratings object which represents the ground truth of the split considered required n_recs int Number of the top items that will be present in the ranking. If None all candidate items will be returned for the user None user_id_list List List of users for which you want to compute the ranking. If None, the ranking will be computed for all users of the test_set None methodology Union [ Methodology , None] Methodology object which governs the candidate item selection. Default is TestRatingsMethodology TestRatingsMethodology() save_fit bool Boolean value which let you choose if the Recommender System should remain fit even after the complete execution of this method. Default is False False Raises: Type Description NotFittedAlg Exception raised when this method is called without first calling the fit method Returns: Type Description Rank Rank object containing recommendation lists for all users of the test set or for all users in user_id_list Source code in clayrs/recsys/recsys.py 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 def fit_rank ( self , test_set : Ratings , n_recs : int = None , user_id_list : List = None , methodology : Union [ Methodology , None ] = TestRatingsMethodology (), save_fit : bool = False ) -> Rank : \"\"\" Method used to both fit and calculate ranking for all users in test set or all users in `user_id_list` parameter. The Recommender System will first be fit for each user in the train set passed in the constructor. If the algorithm can't be fit for some users, a warning message is printed If the `n_recs` is specified, then the rank will contain the top-n items for the users. Otherwise the rank will contain all unrated items of the particular users Via the `methodology` parameter you can perform different candidate item selection. By default the `TestRatingsMethodology()` is used, so for each user items in its test set only will be ranked If the algorithm was not fit for some users, they will be skipped and a warning is printed With the `save_fit` parameter you can decide if you want that you recommender system remains *fit* even after the complete execution of this method, in case you want to compute ranking with other methodologies, or with a different `n_recs` parameter. Be mindful since it can be memory-expensive, thus by default this behaviour is disabled Args: test_set: Ratings object which represents the ground truth of the split considered n_recs: Number of the top items that will be present in the ranking. If None all candidate items will be returned for the user user_id_list: List of users for which you want to compute the ranking. If None, the ranking will be computed for all users of the `test_set` methodology: `Methodology` object which governs the candidate item selection. Default is `TestRatingsMethodology` save_fit: Boolean value which let you choose if the Recommender System should remain fit even after the complete execution of this method. Default is False Raises: NotFittedAlg: Exception raised when this method is called without first calling the `fit` method Returns: Rank object containing recommendation lists for all users of the test set or for all users in `user_id_list` \"\"\" all_users = set ( test_set . user_id_column ) if user_id_list is not None : all_users = set ( user_id_list ) loaded_items_interface = self . algorithm . _load_available_contents ( self . items_directory , set ()) rank = [] logger . info ( \"Don't worry if it looks stuck at first\" ) logger . info ( \"First iterations will stabilize the estimated remaining time\" ) with get_progbar ( all_users ) as pbar : pbar . set_description ( f \"Loading first items from memory...\" ) for user_id in pbar : user_id = str ( user_id ) user_train = self . train_set . get_user_interactions ( user_id ) try : if save_fit : user_alg = deepcopy ( self . algorithm ) self . _user_fit_dic [ user_id ] = user_alg alg = user_alg else : alg = self . algorithm alg . process_rated ( user_train , loaded_items_interface ) alg . fit () except UserSkipAlgFit as e : warning_message = str ( e ) + f \" \\n The algorithm can't be fitted for the user { user_id } \" logger . warning ( warning_message ) if save_fit : self . _user_fit_dic [ user_id ] = None continue filter_list = None if methodology is not None : filter_list = set ( methodology . filter_single ( user_id , self . train_set , test_set )) user_rank = alg . rank ( user_train , loaded_items_interface , n_recs , filter_list = filter_list ) rank . extend ( user_rank ) pbar . set_description ( f \"Computing fit_rank for user { user_id } \" ) rank = Rank . from_list ( rank ) # we force the garbage collector after freeing loaded items del loaded_items_interface gc . collect () self . _yaml_report = { 'mode' : 'rank' , 'n_recs' : repr ( n_recs ), 'methodology' : repr ( methodology )} return rank","title":"fit_rank()"},{"location":"recsys/content_based/content_based_recsys/#clayrs.recsys.recsys.ContentBasedRS.items_directory","text":"Path of the serialized items by the Content Analyzer Source code in clayrs/recsys/recsys.py 136 137 138 139 140 141 @property def items_directory ( self ): \"\"\" Path of the serialized items by the Content Analyzer \"\"\" return self . __items_directory","title":"items_directory()"},{"location":"recsys/content_based/content_based_recsys/#clayrs.recsys.recsys.ContentBasedRS.predict","text":"Method used to calculate score predictions for all users in test set or all users in user_id_list parameter. You must first call the fit() method before you can compute score predictions. BE CAREFUL : not all algorithms are able to perform score prediction Via the methodology parameter you can perform different candidate item selection. By default the TestRatingsMethodology() is used, so for each user items in its test set only will be considered for score prediction If the algorithm was not fit for some users, they will be skipped and a warning is printed Parameters: Name Type Description Default test_set Ratings Ratings object which represents the ground truth of the split considered required user_id_list List List of users for which you want to compute score prediction. If None, the ranking will be computed for all users of the test_set None methodology Union [ Methodology , None] Methodology object which governs the candidate item selection. Default is TestRatingsMethodology TestRatingsMethodology() Raises: Type Description NotFittedAlg Exception raised when this method is called without first calling the fit method Returns: Type Description Prediction Prediction object containing score prediction lists for all users of the test set or for all users in user_id_list Source code in clayrs/recsys/recsys.py 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 def predict ( self , test_set : Ratings , user_id_list : List = None , methodology : Union [ Methodology , None ] = TestRatingsMethodology ()) -> Prediction : \"\"\" Method used to calculate score predictions for all users in test set or all users in `user_id_list` parameter. You must first call the `fit()` method before you can compute score predictions. **BE CAREFUL**: not all algorithms are able to perform *score prediction* Via the `methodology` parameter you can perform different candidate item selection. By default the `TestRatingsMethodology()` is used, so for each user items in its test set only will be considered for score prediction If the algorithm was not fit for some users, they will be skipped and a warning is printed Args: test_set: Ratings object which represents the ground truth of the split considered user_id_list: List of users for which you want to compute score prediction. If None, the ranking will be computed for all users of the `test_set` methodology: `Methodology` object which governs the candidate item selection. Default is `TestRatingsMethodology` Raises: NotFittedAlg: Exception raised when this method is called without first calling the `fit` method Returns: Prediction object containing score prediction lists for all users of the test set or for all users in `user_id_list` \"\"\" if len ( self . _user_fit_dic ) == 0 : raise NotFittedAlg ( \"Algorithm not fit! You must call the fit() method first, or fit_rank().\" ) all_users = set ( test_set . user_id_column ) if user_id_list is not None : all_users = set ( user_id_list ) loaded_items_interface = self . algorithm . _load_available_contents ( self . items_directory , set ()) pred = [] logger . info ( \"Don't worry if it looks stuck at first\" ) logger . info ( \"First iterations will stabilize the estimated remaining time\" ) with get_progbar ( all_users ) as pbar : pbar . set_description ( f \"Loading first items from memory...\" ) for user_id in pbar : user_id = str ( user_id ) user_train = self . train_set . get_user_interactions ( user_id ) filter_list = None if methodology is not None : filter_list = set ( methodology . filter_single ( user_id , self . train_set , test_set )) user_fitted_alg = self . _user_fit_dic . get ( user_id ) if user_fitted_alg is not None : user_pred = user_fitted_alg . predict ( user_train , loaded_items_interface , filter_list = filter_list ) else : user_pred = [] logger . warning ( f \"No algorithm fitted for user { user_id } ! It will be skipped\" ) pred . extend ( user_pred ) pbar . set_description ( f \"Computing predictions for user { user_id } \" ) pred = Prediction . from_list ( pred ) # we force the garbage collector after freeing loaded items del loaded_items_interface gc . collect () self . _yaml_report = { 'mode' : 'score_prediction' , 'methodology' : repr ( methodology )} return pred","title":"predict()"},{"location":"recsys/content_based/content_based_recsys/#clayrs.recsys.recsys.ContentBasedRS.rank","text":"Method used to calculate ranking for all users in test set or all users in user_id_list parameter. You must first call the fit() method before you can compute the ranking. If the n_recs is specified, then the rank will contain the top-n items for the users. Otherwise the rank will contain all unrated items of the particular users Via the methodology parameter you can perform different candidate item selection. By default the TestRatingsMethodology() is used, so for each user items in its test set only will be ranked If the algorithm was not fit for some users, they will be skipped and a warning is printed Parameters: Name Type Description Default test_set Ratings Ratings object which represents the ground truth of the split considered required n_recs int Number of the top items that will be present in the ranking. If None all candidate items will be returned for the user None user_id_list List List of users for which you want to compute the ranking. If None, the ranking will be computed for all users of the test_set None methodology Union [ Methodology , None] Methodology object which governs the candidate item selection. Default is TestRatingsMethodology TestRatingsMethodology() Raises: Type Description NotFittedAlg Exception raised when this method is called without first calling the fit method Returns: Type Description Rank Rank object containing recommendation lists for all users of the test set or for all users in user_id_list Source code in clayrs/recsys/recsys.py 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 def rank ( self , test_set : Ratings , n_recs : int = None , user_id_list : List = None , methodology : Union [ Methodology , None ] = TestRatingsMethodology ()) -> Rank : \"\"\" Method used to calculate ranking for all users in test set or all users in `user_id_list` parameter. You must first call the `fit()` method before you can compute the ranking. If the `n_recs` is specified, then the rank will contain the top-n items for the users. Otherwise the rank will contain all unrated items of the particular users Via the `methodology` parameter you can perform different candidate item selection. By default the `TestRatingsMethodology()` is used, so for each user items in its test set only will be ranked If the algorithm was not fit for some users, they will be skipped and a warning is printed Args: test_set: Ratings object which represents the ground truth of the split considered n_recs: Number of the top items that will be present in the ranking. If None all candidate items will be returned for the user user_id_list: List of users for which you want to compute the ranking. If None, the ranking will be computed for all users of the `test_set` methodology: `Methodology` object which governs the candidate item selection. Default is `TestRatingsMethodology` Raises: NotFittedAlg: Exception raised when this method is called without first calling the `fit` method Returns: Rank object containing recommendation lists for all users of the test set or for all users in `user_id_list` \"\"\" if len ( self . _user_fit_dic ) == 0 : raise NotFittedAlg ( \"Algorithm not fit! You must call the fit() method first, or fit_rank().\" ) all_users = set ( test_set . user_id_column ) if user_id_list is not None : all_users = set ( user_id_list ) loaded_items_interface = self . algorithm . _load_available_contents ( self . items_directory , set ()) rank = [] logger . info ( \"Don't worry if it looks stuck at first\" ) logger . info ( \"First iterations will stabilize the estimated remaining time\" ) with get_progbar ( all_users ) as pbar : pbar . set_description ( f \"Loading first items from memory...\" ) for user_id in pbar : user_id = str ( user_id ) user_train = self . train_set . get_user_interactions ( user_id ) filter_list = None if methodology is not None : filter_list = set ( methodology . filter_single ( user_id , self . train_set , test_set )) user_fitted_alg = self . _user_fit_dic . get ( user_id ) if user_fitted_alg is not None : user_rank = user_fitted_alg . rank ( user_train , loaded_items_interface , n_recs , filter_list = filter_list ) else : user_rank = [] logger . warning ( f \"No algorithm fitted for user { user_id } ! It will be skipped\" ) rank . extend ( user_rank ) pbar . set_description ( f \"Computing rank for { user_id } \" ) rank = Rank . from_list ( rank ) # we force the garbage collector after freeing loaded items del loaded_items_interface gc . collect () self . _yaml_report = { 'mode' : 'rank' , 'n_recs' : repr ( n_recs ), 'methodology' : repr ( methodology )} return rank","title":"rank()"},{"location":"recsys/content_based/content_based_recsys/#clayrs.recsys.recsys.ContentBasedRS.train_set","text":"The train set of the Content Based RecSys Source code in clayrs/recsys/recsys.py 129 130 131 132 133 134 @property def train_set ( self ): \"\"\" The train set of the Content Based RecSys \"\"\" return self . __train_set","title":"train_set()"},{"location":"recsys/content_based/content_based_recsys/#clayrs.recsys.recsys.ContentBasedRS.users_directory","text":"Path of the serialized users by the Content Analyzer Source code in clayrs/recsys/recsys.py 143 144 145 146 147 148 @property def users_directory ( self ): \"\"\" Path of the serialized users by the Content Analyzer \"\"\" return self . __users_directory","title":"users_directory()"},{"location":"recsys/content_based/content_based_algorithms/centroid_vector/","text":"Centroid Vector CentroidVector ( item_field , similarity , threshold = None , embedding_combiner = Centroid ()) Bases: ContentBasedAlgorithm Class that implements a centroid-like recommender. It first gets the centroid of the items that the user liked. Then computes the similarity between the centroid and the item of which the ranking score must be predicted. It's a ranking algorithm, so it can't do score prediction It computes the centroid vector of the features of items liked by the user It computes the similarity between the centroid vector and the items of which the ranking score must be predicted The items liked by a user are those having a rating higher or equal than a specific threshold . If the threshold is not specified, the average score of all items liked by the user is used. Examples: Interested in only a field representation, CosineSimilarity as similarity, \\(threshold = 3\\) (Every item with rating \\(>= 3\\) will be considered as positive) >>> from clayrs import recsys as rs >>> alg = rs . CentroidVector ({ \"Plot\" : 0 }, rs . CosineSimilarity (), 3 ) Interested in multiple field representations of the items, CosineSimilarity as similarity, \\(threshold = None\\) (Every item with rating \\(>=\\) mean rating of the user will be considered as positive) >>> alg = rs . CentroidVector ( >>> item_field = { \"Plot\" : [ 0 , \"tfidf\" ], >>> \"Genre\" : [ 0 , 1 ], >>> \"Director\" : \"doc2vec\" }, >>> similarity = rs . CosineSimilarity (), >>> threshold = None ) Info After instantiating the CentroidVector algorithm, pass it in the initialization of a CBRS and the use its method to calculate ranking for single user or multiple users: Examples: >>> cbrs = rs.ContentBasedRS(algorithm=alg, ...) >>> cbrs.fit_rank(...) >>> # ... Parameters: Name Type Description Default item_field dict dict where the key is the name of the field that contains the content to use, value is the representation(s) id(s) that will be used for the said item. The value of a field can be a string or a list, use a list if you want to use multiple representations for a particular field. required similarity Similarity Kind of similarity to use required threshold float Threshold for the ratings. If the rating is greater than the threshold, it will be considered as positive. If the threshold is not specified, the average score of all items liked by the user is used. None embedding_combiner CombiningTechnique CombiningTechnique used when embeddings representation must be used but they are in a matrix form instead of a single vector (e.g. when WordEmbedding representations must be used you have one vector for each word). By default the Centroid of the rows of the matrix is computed Centroid() Source code in clayrs/recsys/content_based_algorithm/centroid_vector/centroid_vector.py 70 71 72 73 74 75 76 77 def __init__ ( self , item_field : dict , similarity : Similarity , threshold : float = None , embedding_combiner : CombiningTechnique = Centroid ()): super () . __init__ ( item_field , threshold ) self . _similarity = similarity self . _emb_combiner = embedding_combiner self . _centroid : Optional [ np . ndarray ] = None self . _positive_rated_dict : Optional [ Dict ] = None fit () The fit process for the CentroidVector consists in calculating the centroid of the features of the positive items ONLY. This method uses extracted features of the positive items stored in a private attribute, so process_rated() must be called before this method. The built centroid will also be stored in a private attribute. Source code in clayrs/recsys/content_based_algorithm/centroid_vector/centroid_vector.py 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 def fit ( self ): \"\"\" The fit process for the CentroidVector consists in calculating the centroid of the features of the positive items ONLY. This method uses extracted features of the positive items stored in a private attribute, so `process_rated()` must be called before this method. The built centroid will also be stored in a private attribute. \"\"\" positive_rated_features = list ( self . _positive_rated_dict . values ()) positive_rated_features_fused = self . fuse_representations ( positive_rated_features , self . _emb_combiner , as_array = True ) self . _centroid = positive_rated_features_fused . mean ( axis = 0 ) # we delete variable used to fit since will no longer be used del self . _positive_rated_dict predict ( user_ratings , available_loaded_items , filter_list = None ) CentroidVector is not a score prediction algorithm, calling this method will raise the NotPredictionAlg exception! Raises: Type Description NotPredictionAlg exception raised since the CentroidVector algorithm is not a score prediction algorithm Source code in clayrs/recsys/content_based_algorithm/centroid_vector/centroid_vector.py 141 142 143 144 145 146 147 148 149 150 def predict ( self , user_ratings : List [ Interaction ], available_loaded_items : LoadedContentsDict , filter_list : List [ str ] = None ) -> List [ Interaction ]: \"\"\" CentroidVector is not a score prediction algorithm, calling this method will raise the `NotPredictionAlg` exception! Raises: NotPredictionAlg: exception raised since the CentroidVector algorithm is not a score prediction algorithm \"\"\" raise NotPredictionAlg ( \"CentroidVector is not a Score Prediction Algorithm!\" ) process_rated ( user_ratings , available_loaded_items ) Function that extracts features from positive rated items ONLY! The extracted features will be used to fit the algorithm (build the centroid). Features extracted will be stored in a private attributes of the class. Parameters: Name Type Description Default user_ratings List [ Interaction ] List of Interaction objects for a single user required available_loaded_items LoadedContentsDict The LoadedContents interface which contains loaded contents required Source code in clayrs/recsys/content_based_algorithm/centroid_vector/centroid_vector.py 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 def process_rated ( self , user_ratings : List [ Interaction ], available_loaded_items : LoadedContentsDict ): \"\"\" Function that extracts features from positive rated items ONLY! The extracted features will be used to fit the algorithm (build the centroid). Features extracted will be stored in a private attributes of the class. Args: user_ratings: List of Interaction objects for a single user available_loaded_items: The LoadedContents interface which contains loaded contents \"\"\" items_scores_dict = { interaction . item_id : interaction . score for interaction in user_ratings } # Load rated items from the path loaded_rated_items : List [ Union [ Content , None ]] = available_loaded_items . get_list ([ item_id for item_id in items_scores_dict . keys ()]) # If threshold wasn't passed in the constructor, then we take the mean rating # given by the user as its threshold threshold = self . threshold if threshold is None : threshold = self . _calc_mean_user_threshold ( user_ratings ) # Calculates labels and extract features from the positive rated items positive_rated_dict = {} for item in loaded_rated_items : if item is not None : score_assigned = float ( items_scores_dict [ item . content_id ]) if score_assigned >= threshold : positive_rated_dict [ item ] = self . extract_features_item ( item ) if len ( user_ratings ) == 0 : raise EmptyUserRatings ( \"The user selected doesn't have any ratings!\" ) user_id = user_ratings [ 0 ] . user_id if len ( loaded_rated_items ) == 0 or ( loaded_rated_items . count ( None ) == len ( loaded_rated_items )): raise NoRatedItems ( \"User {} - No rated items available locally!\" . format ( user_id )) if len ( positive_rated_dict ) == 0 : raise OnlyNegativeItems ( \"User {} - There are only negative items available locally!\" ) self . _positive_rated_dict = positive_rated_dict rank ( user_ratings , available_loaded_items , recs_number = None , filter_list = None ) Rank the top-n recommended items for the user. If the recs_number parameter isn't specified, All unrated items for the user will be ranked (or only items in the filter list, if specified). One can specify which items must be ranked with the filter_list parameter, in this case ONLY items in the filter_list parameter will be ranked. One can also pass items already seen by the user with the filter_list parameter. Otherwise, ALL unrated items will be ranked. Parameters: Name Type Description Default user_ratings List [ Interaction ] List of Interaction objects for a single user required available_loaded_items LoadedContentsDict The LoadedContents interface which contains loaded contents required recs_number int number of the top ranked items to return, if None all ranked items will be returned None filter_list list list of the items to rank, if None all unrated items for the user will be ranked None Returns: Type Description List [ Interaction ] List of Interactions object in a descending order w.r.t the 'score' attribute, representing the ranking for a single user Source code in clayrs/recsys/content_based_algorithm/centroid_vector/centroid_vector.py 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 def rank ( self , user_ratings : List [ Interaction ], available_loaded_items : LoadedContentsDict , recs_number : int = None , filter_list : List [ str ] = None ) -> List [ Interaction ]: \"\"\" Rank the top-n recommended items for the user. If the recs_number parameter isn't specified, All unrated items for the user will be ranked (or only items in the filter list, if specified). One can specify which items must be ranked with the `filter_list` parameter, in this case ONLY items in the `filter_list` parameter will be ranked. One can also pass items already seen by the user with the filter_list parameter. Otherwise, **ALL** unrated items will be ranked. Args: user_ratings: List of Interaction objects for a single user available_loaded_items: The LoadedContents interface which contains loaded contents recs_number: number of the top ranked items to return, if None all ranked items will be returned filter_list (list): list of the items to rank, if None all unrated items for the user will be ranked Returns: List of Interactions object in a descending order w.r.t the 'score' attribute, representing the ranking for a single user \"\"\" try : user_id = user_ratings [ 0 ] . user_id except IndexError : raise EmptyUserRatings ( \"The user selected doesn't have any ratings!\" ) user_seen_items = set ([ interaction . item_id for interaction in user_ratings ]) # Load items to predict if filter_list is None : items_to_predict = available_loaded_items . get_list ([ item_id for item_id in available_loaded_items if item_id not in user_seen_items ]) else : items_to_predict = available_loaded_items . get_list ( filter_list ) # Extract features of the items to predict id_items_to_predict = [] features_items_to_predict = [] for item in items_to_predict : if item is not None : id_items_to_predict . append ( item . content_id ) features_items_to_predict . append ( self . extract_features_item ( item )) if len ( id_items_to_predict ) > 0 : # Calculate predictions, they are the similarity of the new items with the centroid vector features_fused = self . fuse_representations ( features_items_to_predict , self . _emb_combiner , as_array = True ) similarities = [ self . _similarity . perform ( self . _centroid , item ) for item in features_fused ] else : similarities = [] # Build the item_score dict (key is item_id, value is rank score predicted) # and order the keys in descending order item_score_dict = dict ( zip ( id_items_to_predict , similarities )) ordered_item_ids = sorted ( item_score_dict , key = item_score_dict . get , reverse = True ) # we only save the top-n items_ids corresponding to top-n recommendations # (if recs_number is None ordered_item_ids will contain all item_ids as the original list) ordered_item_ids = ordered_item_ids [: recs_number ] # we construct the output data rank_interaction_list = [ Interaction ( user_id , item_id , item_score_dict [ item_id ]) for item_id in ordered_item_ids ] return rank_interaction_list Similarities implemented The following are similarities you can use in the similarity parameter of the CentroidVector class CosineSimilarity () Bases: Similarity Computes cosine similarity Source code in clayrs/recsys/content_based_algorithm/centroid_vector/similarities.py 25 26 def __init__ ( self ): super () . __init__ () perform ( v1 , v2 ) Calculates the cosine similarity between v1 and v2 Parameters: Name Type Description Default v1 np . ndarray first numpy array required v2 np . ndarray second numpy array required Source code in clayrs/recsys/content_based_algorithm/centroid_vector/similarities.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 def perform ( self , v1 : np . ndarray , v2 : np . ndarray ): \"\"\" Calculates the cosine similarity between v1 and v2 Args: v1: first numpy array v2: second numpy array \"\"\" if not v1 . any () or not v2 . any (): return 0 else : # Cosine_distance is defined in the scipy library as 1 - cosine_similarity, so: # 1 - cosine_distance = 1 - (1 - cosine_similarity) = cosine_similarity return 1 - cosine_distance ( v1 , v2 )","title":"Centroid Vector"},{"location":"recsys/content_based/content_based_algorithms/centroid_vector/#centroid-vector","text":"","title":"Centroid Vector"},{"location":"recsys/content_based/content_based_algorithms/centroid_vector/#clayrs.recsys.content_based_algorithm.centroid_vector.CentroidVector","text":"Bases: ContentBasedAlgorithm Class that implements a centroid-like recommender. It first gets the centroid of the items that the user liked. Then computes the similarity between the centroid and the item of which the ranking score must be predicted. It's a ranking algorithm, so it can't do score prediction It computes the centroid vector of the features of items liked by the user It computes the similarity between the centroid vector and the items of which the ranking score must be predicted The items liked by a user are those having a rating higher or equal than a specific threshold . If the threshold is not specified, the average score of all items liked by the user is used. Examples: Interested in only a field representation, CosineSimilarity as similarity, \\(threshold = 3\\) (Every item with rating \\(>= 3\\) will be considered as positive) >>> from clayrs import recsys as rs >>> alg = rs . CentroidVector ({ \"Plot\" : 0 }, rs . CosineSimilarity (), 3 ) Interested in multiple field representations of the items, CosineSimilarity as similarity, \\(threshold = None\\) (Every item with rating \\(>=\\) mean rating of the user will be considered as positive) >>> alg = rs . CentroidVector ( >>> item_field = { \"Plot\" : [ 0 , \"tfidf\" ], >>> \"Genre\" : [ 0 , 1 ], >>> \"Director\" : \"doc2vec\" }, >>> similarity = rs . CosineSimilarity (), >>> threshold = None ) Info After instantiating the CentroidVector algorithm, pass it in the initialization of a CBRS and the use its method to calculate ranking for single user or multiple users: Examples: >>> cbrs = rs.ContentBasedRS(algorithm=alg, ...) >>> cbrs.fit_rank(...) >>> # ... Parameters: Name Type Description Default item_field dict dict where the key is the name of the field that contains the content to use, value is the representation(s) id(s) that will be used for the said item. The value of a field can be a string or a list, use a list if you want to use multiple representations for a particular field. required similarity Similarity Kind of similarity to use required threshold float Threshold for the ratings. If the rating is greater than the threshold, it will be considered as positive. If the threshold is not specified, the average score of all items liked by the user is used. None embedding_combiner CombiningTechnique CombiningTechnique used when embeddings representation must be used but they are in a matrix form instead of a single vector (e.g. when WordEmbedding representations must be used you have one vector for each word). By default the Centroid of the rows of the matrix is computed Centroid() Source code in clayrs/recsys/content_based_algorithm/centroid_vector/centroid_vector.py 70 71 72 73 74 75 76 77 def __init__ ( self , item_field : dict , similarity : Similarity , threshold : float = None , embedding_combiner : CombiningTechnique = Centroid ()): super () . __init__ ( item_field , threshold ) self . _similarity = similarity self . _emb_combiner = embedding_combiner self . _centroid : Optional [ np . ndarray ] = None self . _positive_rated_dict : Optional [ Dict ] = None","title":"CentroidVector"},{"location":"recsys/content_based/content_based_algorithms/centroid_vector/#clayrs.recsys.content_based_algorithm.centroid_vector.centroid_vector.CentroidVector.fit","text":"The fit process for the CentroidVector consists in calculating the centroid of the features of the positive items ONLY. This method uses extracted features of the positive items stored in a private attribute, so process_rated() must be called before this method. The built centroid will also be stored in a private attribute. Source code in clayrs/recsys/content_based_algorithm/centroid_vector/centroid_vector.py 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 def fit ( self ): \"\"\" The fit process for the CentroidVector consists in calculating the centroid of the features of the positive items ONLY. This method uses extracted features of the positive items stored in a private attribute, so `process_rated()` must be called before this method. The built centroid will also be stored in a private attribute. \"\"\" positive_rated_features = list ( self . _positive_rated_dict . values ()) positive_rated_features_fused = self . fuse_representations ( positive_rated_features , self . _emb_combiner , as_array = True ) self . _centroid = positive_rated_features_fused . mean ( axis = 0 ) # we delete variable used to fit since will no longer be used del self . _positive_rated_dict","title":"fit()"},{"location":"recsys/content_based/content_based_algorithms/centroid_vector/#clayrs.recsys.content_based_algorithm.centroid_vector.centroid_vector.CentroidVector.predict","text":"CentroidVector is not a score prediction algorithm, calling this method will raise the NotPredictionAlg exception! Raises: Type Description NotPredictionAlg exception raised since the CentroidVector algorithm is not a score prediction algorithm Source code in clayrs/recsys/content_based_algorithm/centroid_vector/centroid_vector.py 141 142 143 144 145 146 147 148 149 150 def predict ( self , user_ratings : List [ Interaction ], available_loaded_items : LoadedContentsDict , filter_list : List [ str ] = None ) -> List [ Interaction ]: \"\"\" CentroidVector is not a score prediction algorithm, calling this method will raise the `NotPredictionAlg` exception! Raises: NotPredictionAlg: exception raised since the CentroidVector algorithm is not a score prediction algorithm \"\"\" raise NotPredictionAlg ( \"CentroidVector is not a Score Prediction Algorithm!\" )","title":"predict()"},{"location":"recsys/content_based/content_based_algorithms/centroid_vector/#clayrs.recsys.content_based_algorithm.centroid_vector.centroid_vector.CentroidVector.process_rated","text":"Function that extracts features from positive rated items ONLY! The extracted features will be used to fit the algorithm (build the centroid). Features extracted will be stored in a private attributes of the class. Parameters: Name Type Description Default user_ratings List [ Interaction ] List of Interaction objects for a single user required available_loaded_items LoadedContentsDict The LoadedContents interface which contains loaded contents required Source code in clayrs/recsys/content_based_algorithm/centroid_vector/centroid_vector.py 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 def process_rated ( self , user_ratings : List [ Interaction ], available_loaded_items : LoadedContentsDict ): \"\"\" Function that extracts features from positive rated items ONLY! The extracted features will be used to fit the algorithm (build the centroid). Features extracted will be stored in a private attributes of the class. Args: user_ratings: List of Interaction objects for a single user available_loaded_items: The LoadedContents interface which contains loaded contents \"\"\" items_scores_dict = { interaction . item_id : interaction . score for interaction in user_ratings } # Load rated items from the path loaded_rated_items : List [ Union [ Content , None ]] = available_loaded_items . get_list ([ item_id for item_id in items_scores_dict . keys ()]) # If threshold wasn't passed in the constructor, then we take the mean rating # given by the user as its threshold threshold = self . threshold if threshold is None : threshold = self . _calc_mean_user_threshold ( user_ratings ) # Calculates labels and extract features from the positive rated items positive_rated_dict = {} for item in loaded_rated_items : if item is not None : score_assigned = float ( items_scores_dict [ item . content_id ]) if score_assigned >= threshold : positive_rated_dict [ item ] = self . extract_features_item ( item ) if len ( user_ratings ) == 0 : raise EmptyUserRatings ( \"The user selected doesn't have any ratings!\" ) user_id = user_ratings [ 0 ] . user_id if len ( loaded_rated_items ) == 0 or ( loaded_rated_items . count ( None ) == len ( loaded_rated_items )): raise NoRatedItems ( \"User {} - No rated items available locally!\" . format ( user_id )) if len ( positive_rated_dict ) == 0 : raise OnlyNegativeItems ( \"User {} - There are only negative items available locally!\" ) self . _positive_rated_dict = positive_rated_dict","title":"process_rated()"},{"location":"recsys/content_based/content_based_algorithms/centroid_vector/#clayrs.recsys.content_based_algorithm.centroid_vector.centroid_vector.CentroidVector.rank","text":"Rank the top-n recommended items for the user. If the recs_number parameter isn't specified, All unrated items for the user will be ranked (or only items in the filter list, if specified). One can specify which items must be ranked with the filter_list parameter, in this case ONLY items in the filter_list parameter will be ranked. One can also pass items already seen by the user with the filter_list parameter. Otherwise, ALL unrated items will be ranked. Parameters: Name Type Description Default user_ratings List [ Interaction ] List of Interaction objects for a single user required available_loaded_items LoadedContentsDict The LoadedContents interface which contains loaded contents required recs_number int number of the top ranked items to return, if None all ranked items will be returned None filter_list list list of the items to rank, if None all unrated items for the user will be ranked None Returns: Type Description List [ Interaction ] List of Interactions object in a descending order w.r.t the 'score' attribute, representing the ranking for a single user Source code in clayrs/recsys/content_based_algorithm/centroid_vector/centroid_vector.py 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 def rank ( self , user_ratings : List [ Interaction ], available_loaded_items : LoadedContentsDict , recs_number : int = None , filter_list : List [ str ] = None ) -> List [ Interaction ]: \"\"\" Rank the top-n recommended items for the user. If the recs_number parameter isn't specified, All unrated items for the user will be ranked (or only items in the filter list, if specified). One can specify which items must be ranked with the `filter_list` parameter, in this case ONLY items in the `filter_list` parameter will be ranked. One can also pass items already seen by the user with the filter_list parameter. Otherwise, **ALL** unrated items will be ranked. Args: user_ratings: List of Interaction objects for a single user available_loaded_items: The LoadedContents interface which contains loaded contents recs_number: number of the top ranked items to return, if None all ranked items will be returned filter_list (list): list of the items to rank, if None all unrated items for the user will be ranked Returns: List of Interactions object in a descending order w.r.t the 'score' attribute, representing the ranking for a single user \"\"\" try : user_id = user_ratings [ 0 ] . user_id except IndexError : raise EmptyUserRatings ( \"The user selected doesn't have any ratings!\" ) user_seen_items = set ([ interaction . item_id for interaction in user_ratings ]) # Load items to predict if filter_list is None : items_to_predict = available_loaded_items . get_list ([ item_id for item_id in available_loaded_items if item_id not in user_seen_items ]) else : items_to_predict = available_loaded_items . get_list ( filter_list ) # Extract features of the items to predict id_items_to_predict = [] features_items_to_predict = [] for item in items_to_predict : if item is not None : id_items_to_predict . append ( item . content_id ) features_items_to_predict . append ( self . extract_features_item ( item )) if len ( id_items_to_predict ) > 0 : # Calculate predictions, they are the similarity of the new items with the centroid vector features_fused = self . fuse_representations ( features_items_to_predict , self . _emb_combiner , as_array = True ) similarities = [ self . _similarity . perform ( self . _centroid , item ) for item in features_fused ] else : similarities = [] # Build the item_score dict (key is item_id, value is rank score predicted) # and order the keys in descending order item_score_dict = dict ( zip ( id_items_to_predict , similarities )) ordered_item_ids = sorted ( item_score_dict , key = item_score_dict . get , reverse = True ) # we only save the top-n items_ids corresponding to top-n recommendations # (if recs_number is None ordered_item_ids will contain all item_ids as the original list) ordered_item_ids = ordered_item_ids [: recs_number ] # we construct the output data rank_interaction_list = [ Interaction ( user_id , item_id , item_score_dict [ item_id ]) for item_id in ordered_item_ids ] return rank_interaction_list","title":"rank()"},{"location":"recsys/content_based/content_based_algorithms/centroid_vector/#similarities-implemented","text":"The following are similarities you can use in the similarity parameter of the CentroidVector class","title":"Similarities implemented"},{"location":"recsys/content_based/content_based_algorithms/centroid_vector/#clayrs.recsys.content_based_algorithm.centroid_vector.similarities.CosineSimilarity","text":"Bases: Similarity Computes cosine similarity Source code in clayrs/recsys/content_based_algorithm/centroid_vector/similarities.py 25 26 def __init__ ( self ): super () . __init__ ()","title":"CosineSimilarity"},{"location":"recsys/content_based/content_based_algorithms/centroid_vector/#clayrs.recsys.content_based_algorithm.centroid_vector.similarities.CosineSimilarity.perform","text":"Calculates the cosine similarity between v1 and v2 Parameters: Name Type Description Default v1 np . ndarray first numpy array required v2 np . ndarray second numpy array required Source code in clayrs/recsys/content_based_algorithm/centroid_vector/similarities.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 def perform ( self , v1 : np . ndarray , v2 : np . ndarray ): \"\"\" Calculates the cosine similarity between v1 and v2 Args: v1: first numpy array v2: second numpy array \"\"\" if not v1 . any () or not v2 . any (): return 0 else : # Cosine_distance is defined in the scipy library as 1 - cosine_similarity, so: # 1 - cosine_distance = 1 - (1 - cosine_similarity) = cosine_similarity return 1 - cosine_distance ( v1 , v2 )","title":"perform()"},{"location":"recsys/content_based/content_based_algorithms/classifier_recommender/","text":"Classifier Recommender ClassifierRecommender ( item_field , classifier , threshold = None , embedding_combiner = Centroid ()) Bases: ContentBasedAlgorithm Class that implements recommendation through a specified Classifier . It's a ranking algorithm so it can't do score prediction. Examples: Interested in only a field representation, DecisionTree classifier from sklearn, \\(threshold = 3\\) (Every item with rating score \\(>= 3\\) will be considered as positive ) >>> from clayrs import recsys as rs >>> alg = rs . ClassifierRecommender ({ \"Plot\" : 0 }, rs . SkDecisionTree (), 3 ) Interested in only a field representation, KNN classifier with custom parameters from sklearn, \\(threshold = 3\\) (Every item with rating score \\(>= 3\\) will be considered as positive) >>> alg = rs . ClassifierRecommender ({ \"Plot\" : 0 }, rs . SkKNN ( n_neighbors = 3 ), 0 ) Interested in multiple field representations of the items, KNN classifier with custom parameters from sklearn, \\(threshold = None\\) (Every item with rating \\(>=\\) mean rating of the user will be considered as positive) >>> alg = ClassifierRecommender ( >>> item_field = { \"Plot\" : [ 0 , \"tfidf\" ], >>> \"Genre\" : [ 0 , 1 ], >>> \"Director\" : \"doc2vec\" }, >>> classifier = rs . SkKNN ( n_neighbors = 3 ), >>> threshold = None ) Info After instantiating the ClassifierRecommender algorithm, pass it in the initialization of a CBRS and the use its method to calculate ranking for single user or multiple users: Examples: >>> cbrs = rs.ContentBasedRS(algorithm=alg, ...) >>> cbrs.fit_rank(...) >>> # ... Parameters: Name Type Description Default item_field dict dict where the key is the name of the field that contains the content to use, value is the representation(s) id(s) that will be used for the said item. The value of a field can be a string or a list, use a list if you want to use multiple representations for a particular field. required classifier Classifier classifier that will be used. Can be one object of the Classifier class. required threshold float Threshold for the ratings. If the rating is greater than the threshold, it will be considered as positive. If the threshold is not specified, the average score of all items liked by the user is used. None embedding_combiner CombiningTechnique CombiningTechnique used when embeddings representation must be used but they are in a matrix form instead of a single vector (e.g. when WordEmbedding representations must be used you have one vector for each word). By default the Centroid of the rows of the matrix is computed Centroid() Source code in clayrs/recsys/content_based_algorithm/classifier/classifier_recommender.py 67 68 69 70 71 72 73 def __init__ ( self , item_field : dict , classifier : Classifier , threshold : float = None , embedding_combiner : CombiningTechnique = Centroid ()): super () . __init__ ( item_field , threshold ) self . _classifier = classifier self . _embedding_combiner = embedding_combiner self . _labels : Optional [ list ] = None self . _rated_dict : Optional [ dict ] = None fit () Fit the classifier specified in the constructor with the features and labels extracted with the process_rated() method. It uses private attributes to fit the classifier, so process_rated() must be called before this method. Source code in clayrs/recsys/content_based_algorithm/classifier/classifier_recommender.py 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 def fit ( self ): \"\"\" Fit the classifier specified in the constructor with the features and labels extracted with the `process_rated()` method. It uses private attributes to fit the classifier, so `process_rated()` must be called before this method. \"\"\" rated_features = list ( self . _rated_dict . values ()) # Fuse the input if there are dicts, multiple representation, etc. fused_features = self . fuse_representations ( rated_features , self . _embedding_combiner ) self . _classifier . fit ( fused_features , self . _labels ) # we delete variables used to fit since will no longer be used del self . _rated_dict del self . _labels predict ( user_ratings , available_loaded_items , filter_list = None ) ClassifierRecommender is not a score prediction algorithm, calling this method will raise the NotPredictionAlg exception! Raises: Type Description NotPredictionAlg exception raised since the CentroidVector algorithm is not a score prediction algorithm Source code in clayrs/recsys/content_based_algorithm/classifier/classifier_recommender.py 157 158 159 160 161 162 163 164 165 166 def predict ( self , user_ratings : List [ Interaction ], available_loaded_items : LoadedContentsDict , filter_list : List [ str ] = None ) -> List [ Interaction ]: \"\"\" ClassifierRecommender is not a score prediction algorithm, calling this method will raise the `NotPredictionAlg` exception! Raises: NotPredictionAlg: exception raised since the CentroidVector algorithm is not a score prediction algorithm \"\"\" raise NotPredictionAlg ( \"ClassifierRecommender is not a Score Prediction Algorithm!\" ) process_rated ( user_ratings , available_loaded_items ) Function that extracts features from rated item and labels them. The extracted features will be later used to fit the classifier. Features and labels will be stored in private attributes of the class. IF there are no rated_items available locally or if there are only positive/negative items, an exception is thrown. Parameters: Name Type Description Default user_ratings List [ Interaction ] List of Interaction objects for a single user required available_loaded_items LoadedContentsDict The LoadedContents interface which contains loaded contents required Raises: Type Description NoRatedItems Exception raised when there isn't any item available locally rated by the user OnlyPositiveItems Exception raised when there are only positive items available locally for the user (Items that the user liked) OnlyNegativeitems Exception raised when there are only negative items available locally for the user (Items that the user disliked) Source code in clayrs/recsys/content_based_algorithm/classifier/classifier_recommender.py 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 def process_rated ( self , user_ratings : List [ Interaction ], available_loaded_items : LoadedContentsDict ): \"\"\" Function that extracts features from rated item and labels them. The extracted features will be later used to fit the classifier. Features and labels will be stored in private attributes of the class. IF there are no rated_items available locally or if there are only positive/negative items, an exception is thrown. Args: user_ratings: List of Interaction objects for a single user available_loaded_items: The LoadedContents interface which contains loaded contents Raises: NoRatedItems: Exception raised when there isn't any item available locally rated by the user OnlyPositiveItems: Exception raised when there are only positive items available locally for the user (Items that the user liked) OnlyNegativeitems: Exception raised when there are only negative items available locally for the user (Items that the user disliked) \"\"\" # Load rated items from the path items_scores_dict = { interaction . item_id : interaction . score for interaction in user_ratings } # Load rated items from the path loaded_rated_items : List [ Union [ Content , None ]] = available_loaded_items . get_list ([ item_id for item_id in items_scores_dict . keys ()]) threshold = self . threshold if threshold is None : threshold = self . _calc_mean_user_threshold ( user_ratings ) # Assign label and extract features from the rated items labels = [] rated_dict = {} for item in loaded_rated_items : if item is not None : rated_dict [ item ] = self . extract_features_item ( item ) # This conversion raises Exception when there are multiple same to_id for the user score_assigned = float ( items_scores_dict [ item . content_id ]) if score_assigned >= threshold : labels . append ( 1 ) else : labels . append ( 0 ) if len ( user_ratings ) == 0 : raise EmptyUserRatings ( \"The user selected doesn't have any ratings!\" ) user_id = user_ratings [ 0 ] . user_id if len ( rated_dict ) == 0 : raise NoRatedItems ( \"User {} - No rated item available locally!\" . format ( user_id )) if 0 not in labels : raise OnlyPositiveItems ( \"User {} - There are only positive items available locally!\" . format ( user_id )) elif 1 not in labels : raise OnlyNegativeItems ( \"User {} - There are only negative items available locally!\" . format ( user_id )) self . _labels = labels self . _rated_dict = rated_dict rank ( user_ratings , available_loaded_items , recs_number = None , filter_list = None ) Rank the top-n recommended items for the user. If the recs_number parameter isn't specified, All unrated items for the user will be ranked (or only items in the filter list, if specified). One can specify which items must be ranked with the filter_list parameter, in this case ONLY items in the filter_list parameter will be ranked. One can also pass items already seen by the user with the filter_list parameter. Otherwise, ALL unrated items will be ranked. Parameters: Name Type Description Default user_ratings List [ Interaction ] List of Interaction objects for a single user required available_loaded_items LoadedContentsDict The LoadedContents interface which contains loaded contents required recs_number int number of the top ranked items to return, if None all ranked items will be returned None filter_list list list of the items to rank, if None all unrated items for the user will be ranked None Returns: Type Description List [ Interaction ] List of Interactions object in a descending order w.r.t the 'score' attribute, representing the ranking for a single user Source code in clayrs/recsys/content_based_algorithm/classifier/classifier_recommender.py 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 def rank ( self , user_ratings : List [ Interaction ], available_loaded_items : LoadedContentsDict , recs_number : int = None , filter_list : List [ str ] = None ) -> List [ Interaction ]: \"\"\" Rank the top-n recommended items for the user. If the recs_number parameter isn't specified, All unrated items for the user will be ranked (or only items in the filter list, if specified). One can specify which items must be ranked with the `filter_list` parameter, in this case ONLY items in the `filter_list` parameter will be ranked. One can also pass items already seen by the user with the filter_list parameter. Otherwise, **ALL** unrated items will be ranked. Args: user_ratings: List of Interaction objects for a single user available_loaded_items: The LoadedContents interface which contains loaded contents recs_number: number of the top ranked items to return, if None all ranked items will be returned filter_list (list): list of the items to rank, if None all unrated items for the user will be ranked Returns: List of Interactions object in a descending order w.r.t the 'score' attribute, representing the ranking for a single user \"\"\" try : user_id = user_ratings [ 0 ] . user_id except IndexError : raise EmptyUserRatings ( \"The user selected doesn't have any ratings!\" ) user_seen_items = set ([ interaction . item_id for interaction in user_ratings ]) # Load items to predict if filter_list is None : items_to_predict = available_loaded_items . get_list ([ item_id for item_id in available_loaded_items if item_id not in user_seen_items ]) else : items_to_predict = available_loaded_items . get_list ( filter_list ) # Extract features of the items to predict id_items_to_predict = [] features_items_to_predict = [] for item in items_to_predict : if item is not None : id_items_to_predict . append ( item . content_id ) features_items_to_predict . append ( self . extract_features_item ( item )) if len ( id_items_to_predict ) > 0 : # Fuse the input if there are dicts, multiple representation, etc. fused_features_items_to_pred = self . fuse_representations ( features_items_to_predict , self . _embedding_combiner ) class_prob = self . _classifier . predict_proba ( fused_features_items_to_pred ) else : class_prob = [] # for each item we extract the probability that the item is liked (class 1) score_labels = ( prob [ 1 ] for prob in class_prob ) # Build the item_score dict (key is item_id, value is rank score predicted) # and order the keys in descending order item_score_dict = dict ( zip ( id_items_to_predict , score_labels )) ordered_item_ids = sorted ( item_score_dict , key = item_score_dict . get , reverse = True ) # we only save the top-n items_ids corresponding to top-n recommendations # (if recs_number is None ordered_item_ids will contain all item_ids as the original list) ordered_item_ids = ordered_item_ids [: recs_number ] # we construct the output data rank_interaction_list = [ Interaction ( user_id , item_id , item_score_dict [ item_id ]) for item_id in ordered_item_ids ] return rank_interaction_list Classifiers Implemented The following are the classifiers you can use in the classifier parameter of the ClassifierRecommender class SkDecisionTree ( * , criterion = 'gini' , splitter = 'best' , max_depth = None , min_samples_split = 2 , min_samples_leaf = 1 , min_weight_fraction_leaf = 0.0 , max_features = None , random_state = None , max_leaf_nodes = None , min_impurity_decrease = 0.0 , class_weight = None , ccp_alpha = 0.0 ) Bases: Classifier Class that implements the Decision Tree Classifier from sklearn. The parameters one could pass are the same ones you would pass instantiating the classifier directly from sklearn Sklearn documentation: here Source code in clayrs/recsys/content_based_algorithm/classifier/classifiers.py 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 def __init__ ( self , * , criterion : Any = \"gini\" , splitter : Any = \"best\" , max_depth : Any = None , min_samples_split : Any = 2 , min_samples_leaf : Any = 1 , min_weight_fraction_leaf : Any = 0.0 , max_features : Any = None , random_state : Any = None , max_leaf_nodes : Any = None , min_impurity_decrease : Any = 0.0 , class_weight : Any = None , ccp_alpha : Any = 0.0 ): clf = DecisionTreeClassifier ( criterion = criterion , splitter = splitter , max_depth = max_depth , min_samples_split = min_samples_split , min_samples_leaf = min_samples_leaf , min_weight_fraction_leaf = min_weight_fraction_leaf , max_features = max_features , random_state = random_state , max_leaf_nodes = max_leaf_nodes , min_impurity_decrease = min_impurity_decrease , class_weight = class_weight , ccp_alpha = ccp_alpha ) super () . __init__ ( clf , inspect . currentframe ()) SkGaussianProcess ( kernel = None , * , optimizer = 'fmin_l_bfgs_b' , n_restarts_optimizer = 0 , max_iter_predict = 100 , warm_start = False , copy_X_train = True , random_state = None , multi_class = 'one_vs_rest' , n_jobs = None ) Bases: Classifier Class that implements the Gaussian Process Classifier from sklearn. The parameters one could pass are the same ones you would pass instantiating the classifier directly from sklearn Sklearn documentation: here Source code in clayrs/recsys/content_based_algorithm/classifier/classifiers.py 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 def __init__ ( self , kernel : Any = None , * , optimizer : Any = \"fmin_l_bfgs_b\" , n_restarts_optimizer : Any = 0 , max_iter_predict : Any = 100 , warm_start : Any = False , copy_X_train : Any = True , random_state : Any = None , multi_class : Any = \"one_vs_rest\" , n_jobs : Any = None ): clf = GaussianProcessClassifier ( kernel = kernel , optimizer = optimizer , n_restarts_optimizer = n_restarts_optimizer , max_iter_predict = max_iter_predict , warm_start = warm_start , copy_X_train = copy_X_train , random_state = random_state , multi_class = multi_class , n_jobs = n_jobs ) super () . __init__ ( clf , inspect . currentframe ()) SkKNN ( n_neighbors = 5 , * , weights = 'uniform' , algorithm = 'auto' , leaf_size = 30 , p = 2 , metric = 'minkowski' , metric_params = None , n_jobs = None ) Bases: Classifier Class that implements the KNN Classifier from sklearn. The parameters one could pass are the same ones you would pass instantiating the classifier KNN directly from sklearn. Sklearn documentation: here Since KNN implementation of sklearn has n_neighbors = 5 as default, it can throw an exception if less sample in the training data are provided, so we change dynamically the n_neighbors parameter according to the number of samples if the dataset is too small and if no manual n_neighbors is set Source code in clayrs/recsys/content_based_algorithm/classifier/classifiers.py 129 130 131 132 133 134 135 136 137 138 139 140 141 def __init__ ( self , n_neighbors : Any = 5 , * , weights : Any = \"uniform\" , algorithm : Any = \"auto\" , leaf_size : Any = 30 , p : Any = 2 , metric : Any = \"minkowski\" , metric_params : Any = None , n_jobs : Any = None ): clf = KNeighborsClassifier ( n_neighbors = n_neighbors , weights = weights , algorithm = algorithm , leaf_size = leaf_size , p = p , metric = metric , metric_params = metric_params , n_jobs = n_jobs ) super () . __init__ ( clf , inspect . currentframe ()) SkLogisticRegression ( penalty = 'l2' , * , dual = False , tol = 0.0001 , C = 1.0 , fit_intercept = True , intercept_scaling = 1 , class_weight = None , random_state = None , solver = 'lbfgs' , max_iter = 100 , multi_class = 'auto' , verbose = 0 , warm_start = False , n_jobs = None , l1_ratio = None ) Bases: Classifier Class that implements the Logistic Regression Classifier from sklearn. The parameters one could pass are the same ones you would pass instantiating the classifier directly from sklearn Sklearn documentation: here Source code in clayrs/recsys/content_based_algorithm/classifier/classifiers.py 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 def __init__ ( self , penalty : Any = \"l2\" , * , dual : Any = False , tol : Any = 1e-4 , C : Any = 1.0 , fit_intercept : Any = True , intercept_scaling : Any = 1 , class_weight : Any = None , random_state : Any = None , solver : Any = \"lbfgs\" , max_iter : Any = 100 , multi_class : Any = \"auto\" , verbose : Any = 0 , warm_start : Any = False , n_jobs : Any = None , l1_ratio : Any = None ): clf = LogisticRegression ( penalty = penalty , dual = dual , tol = tol , C = C , fit_intercept = fit_intercept , intercept_scaling = intercept_scaling , class_weight = class_weight , random_state = random_state , solver = solver , max_iter = max_iter , multi_class = multi_class , verbose = verbose , warm_start = warm_start , n_jobs = n_jobs , l1_ratio = l1_ratio ) super () . __init__ ( clf , inspect . currentframe ()) SkRandomForest ( n_estimators = 100 , * , criterion = 'gini' , max_depth = None , min_samples_split = 2 , min_samples_leaf = 1 , min_weight_fraction_leaf = 0.0 , max_features = 'auto' , max_leaf_nodes = None , min_impurity_decrease = 0.0 , bootstrap = True , oob_score = False , n_jobs = None , random_state = None , verbose = 0 , warm_start = False , class_weight = None , ccp_alpha = 0.0 , max_samples = None ) Bases: Classifier Class that implements the Random Forest Classifier from sklearn. The parameters one could pass are the same ones you would pass instantiating the classifier directly from sklearn Sklearn documentation: here Source code in clayrs/recsys/content_based_algorithm/classifier/classifiers.py 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 def __init__ ( self , n_estimators : Any = 100 , * , criterion : Any = \"gini\" , max_depth : Any = None , min_samples_split : Any = 2 , min_samples_leaf : Any = 1 , min_weight_fraction_leaf : Any = 0.0 , max_features : Any = \"auto\" , max_leaf_nodes : Any = None , min_impurity_decrease : Any = 0.0 , bootstrap : Any = True , oob_score : Any = False , n_jobs : Any = None , random_state : Any = None , verbose : Any = 0 , warm_start : Any = False , class_weight : Any = None , ccp_alpha : Any = 0.0 , max_samples : Any = None ): clf = RandomForestClassifier ( n_estimators = n_estimators , criterion = criterion , max_depth = max_depth , min_samples_split = min_samples_split , min_samples_leaf = min_samples_leaf , min_weight_fraction_leaf = min_weight_fraction_leaf , max_features = max_features , max_leaf_nodes = max_leaf_nodes , min_impurity_decrease = min_impurity_decrease , bootstrap = bootstrap , oob_score = oob_score , n_jobs = n_jobs , random_state = random_state , verbose = verbose , warm_start = warm_start , class_weight = class_weight , ccp_alpha = ccp_alpha , max_samples = max_samples ) super () . __init__ ( clf , inspect . currentframe ()) SkSVC ( * , C = 1.0 , kernel = 'rbf' , degree = 3 , gamma = 'scale' , coef0 = 0.0 , shrinking = True , tol = 0.001 , cache_size = 200 , class_weight = None , verbose = False , max_iter =- 1 , decision_function_shape = 'ovr' , break_ties = False , random_state = None ) Bases: Classifier Class that implements the SVC Classifier from sklearn. The parameters one could pass are the same ones you would pass instantiating the classifier SVC directly from sklearn. Sklearn documentation: here The only parameter from sklearn that cannot be passed is the 'probability' parameter: it is set to True and cannot be changed Source code in clayrs/recsys/content_based_algorithm/classifier/classifiers.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 def __init__ ( self , * , C : Any = 1.0 , kernel : Any = \"rbf\" , degree : Any = 3 , gamma : Any = \"scale\" , coef0 : Any = 0.0 , shrinking : Any = True , tol : Any = 1e-3 , cache_size : Any = 200 , class_weight : Any = None , verbose : Any = False , max_iter : Any = - 1 , decision_function_shape : Any = \"ovr\" , break_ties : Any = False , random_state : Any = None ): # Force the probability parameter at True, otherwise SVC won't predict_proba clf = SVC ( C = C , kernel = kernel , degree = degree , gamma = gamma , coef0 = coef0 , shrinking = shrinking , tol = tol , cache_size = cache_size , class_weight = class_weight , verbose = verbose , max_iter = max_iter , decision_function_shape = decision_function_shape , break_ties = break_ties , random_state = random_state , probability = True ) super () . __init__ ( clf , inspect . currentframe ())","title":"Classifier Recommender"},{"location":"recsys/content_based/content_based_algorithms/classifier_recommender/#classifier-recommender","text":"","title":"Classifier Recommender"},{"location":"recsys/content_based/content_based_algorithms/classifier_recommender/#clayrs.recsys.content_based_algorithm.classifier.classifier_recommender.ClassifierRecommender","text":"Bases: ContentBasedAlgorithm Class that implements recommendation through a specified Classifier . It's a ranking algorithm so it can't do score prediction. Examples: Interested in only a field representation, DecisionTree classifier from sklearn, \\(threshold = 3\\) (Every item with rating score \\(>= 3\\) will be considered as positive ) >>> from clayrs import recsys as rs >>> alg = rs . ClassifierRecommender ({ \"Plot\" : 0 }, rs . SkDecisionTree (), 3 ) Interested in only a field representation, KNN classifier with custom parameters from sklearn, \\(threshold = 3\\) (Every item with rating score \\(>= 3\\) will be considered as positive) >>> alg = rs . ClassifierRecommender ({ \"Plot\" : 0 }, rs . SkKNN ( n_neighbors = 3 ), 0 ) Interested in multiple field representations of the items, KNN classifier with custom parameters from sklearn, \\(threshold = None\\) (Every item with rating \\(>=\\) mean rating of the user will be considered as positive) >>> alg = ClassifierRecommender ( >>> item_field = { \"Plot\" : [ 0 , \"tfidf\" ], >>> \"Genre\" : [ 0 , 1 ], >>> \"Director\" : \"doc2vec\" }, >>> classifier = rs . SkKNN ( n_neighbors = 3 ), >>> threshold = None ) Info After instantiating the ClassifierRecommender algorithm, pass it in the initialization of a CBRS and the use its method to calculate ranking for single user or multiple users: Examples: >>> cbrs = rs.ContentBasedRS(algorithm=alg, ...) >>> cbrs.fit_rank(...) >>> # ... Parameters: Name Type Description Default item_field dict dict where the key is the name of the field that contains the content to use, value is the representation(s) id(s) that will be used for the said item. The value of a field can be a string or a list, use a list if you want to use multiple representations for a particular field. required classifier Classifier classifier that will be used. Can be one object of the Classifier class. required threshold float Threshold for the ratings. If the rating is greater than the threshold, it will be considered as positive. If the threshold is not specified, the average score of all items liked by the user is used. None embedding_combiner CombiningTechnique CombiningTechnique used when embeddings representation must be used but they are in a matrix form instead of a single vector (e.g. when WordEmbedding representations must be used you have one vector for each word). By default the Centroid of the rows of the matrix is computed Centroid() Source code in clayrs/recsys/content_based_algorithm/classifier/classifier_recommender.py 67 68 69 70 71 72 73 def __init__ ( self , item_field : dict , classifier : Classifier , threshold : float = None , embedding_combiner : CombiningTechnique = Centroid ()): super () . __init__ ( item_field , threshold ) self . _classifier = classifier self . _embedding_combiner = embedding_combiner self . _labels : Optional [ list ] = None self . _rated_dict : Optional [ dict ] = None","title":"ClassifierRecommender"},{"location":"recsys/content_based/content_based_algorithms/classifier_recommender/#clayrs.recsys.content_based_algorithm.classifier.classifier_recommender.ClassifierRecommender.fit","text":"Fit the classifier specified in the constructor with the features and labels extracted with the process_rated() method. It uses private attributes to fit the classifier, so process_rated() must be called before this method. Source code in clayrs/recsys/content_based_algorithm/classifier/classifier_recommender.py 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 def fit ( self ): \"\"\" Fit the classifier specified in the constructor with the features and labels extracted with the `process_rated()` method. It uses private attributes to fit the classifier, so `process_rated()` must be called before this method. \"\"\" rated_features = list ( self . _rated_dict . values ()) # Fuse the input if there are dicts, multiple representation, etc. fused_features = self . fuse_representations ( rated_features , self . _embedding_combiner ) self . _classifier . fit ( fused_features , self . _labels ) # we delete variables used to fit since will no longer be used del self . _rated_dict del self . _labels","title":"fit()"},{"location":"recsys/content_based/content_based_algorithms/classifier_recommender/#clayrs.recsys.content_based_algorithm.classifier.classifier_recommender.ClassifierRecommender.predict","text":"ClassifierRecommender is not a score prediction algorithm, calling this method will raise the NotPredictionAlg exception! Raises: Type Description NotPredictionAlg exception raised since the CentroidVector algorithm is not a score prediction algorithm Source code in clayrs/recsys/content_based_algorithm/classifier/classifier_recommender.py 157 158 159 160 161 162 163 164 165 166 def predict ( self , user_ratings : List [ Interaction ], available_loaded_items : LoadedContentsDict , filter_list : List [ str ] = None ) -> List [ Interaction ]: \"\"\" ClassifierRecommender is not a score prediction algorithm, calling this method will raise the `NotPredictionAlg` exception! Raises: NotPredictionAlg: exception raised since the CentroidVector algorithm is not a score prediction algorithm \"\"\" raise NotPredictionAlg ( \"ClassifierRecommender is not a Score Prediction Algorithm!\" )","title":"predict()"},{"location":"recsys/content_based/content_based_algorithms/classifier_recommender/#clayrs.recsys.content_based_algorithm.classifier.classifier_recommender.ClassifierRecommender.process_rated","text":"Function that extracts features from rated item and labels them. The extracted features will be later used to fit the classifier. Features and labels will be stored in private attributes of the class. IF there are no rated_items available locally or if there are only positive/negative items, an exception is thrown. Parameters: Name Type Description Default user_ratings List [ Interaction ] List of Interaction objects for a single user required available_loaded_items LoadedContentsDict The LoadedContents interface which contains loaded contents required Raises: Type Description NoRatedItems Exception raised when there isn't any item available locally rated by the user OnlyPositiveItems Exception raised when there are only positive items available locally for the user (Items that the user liked) OnlyNegativeitems Exception raised when there are only negative items available locally for the user (Items that the user disliked) Source code in clayrs/recsys/content_based_algorithm/classifier/classifier_recommender.py 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 def process_rated ( self , user_ratings : List [ Interaction ], available_loaded_items : LoadedContentsDict ): \"\"\" Function that extracts features from rated item and labels them. The extracted features will be later used to fit the classifier. Features and labels will be stored in private attributes of the class. IF there are no rated_items available locally or if there are only positive/negative items, an exception is thrown. Args: user_ratings: List of Interaction objects for a single user available_loaded_items: The LoadedContents interface which contains loaded contents Raises: NoRatedItems: Exception raised when there isn't any item available locally rated by the user OnlyPositiveItems: Exception raised when there are only positive items available locally for the user (Items that the user liked) OnlyNegativeitems: Exception raised when there are only negative items available locally for the user (Items that the user disliked) \"\"\" # Load rated items from the path items_scores_dict = { interaction . item_id : interaction . score for interaction in user_ratings } # Load rated items from the path loaded_rated_items : List [ Union [ Content , None ]] = available_loaded_items . get_list ([ item_id for item_id in items_scores_dict . keys ()]) threshold = self . threshold if threshold is None : threshold = self . _calc_mean_user_threshold ( user_ratings ) # Assign label and extract features from the rated items labels = [] rated_dict = {} for item in loaded_rated_items : if item is not None : rated_dict [ item ] = self . extract_features_item ( item ) # This conversion raises Exception when there are multiple same to_id for the user score_assigned = float ( items_scores_dict [ item . content_id ]) if score_assigned >= threshold : labels . append ( 1 ) else : labels . append ( 0 ) if len ( user_ratings ) == 0 : raise EmptyUserRatings ( \"The user selected doesn't have any ratings!\" ) user_id = user_ratings [ 0 ] . user_id if len ( rated_dict ) == 0 : raise NoRatedItems ( \"User {} - No rated item available locally!\" . format ( user_id )) if 0 not in labels : raise OnlyPositiveItems ( \"User {} - There are only positive items available locally!\" . format ( user_id )) elif 1 not in labels : raise OnlyNegativeItems ( \"User {} - There are only negative items available locally!\" . format ( user_id )) self . _labels = labels self . _rated_dict = rated_dict","title":"process_rated()"},{"location":"recsys/content_based/content_based_algorithms/classifier_recommender/#clayrs.recsys.content_based_algorithm.classifier.classifier_recommender.ClassifierRecommender.rank","text":"Rank the top-n recommended items for the user. If the recs_number parameter isn't specified, All unrated items for the user will be ranked (or only items in the filter list, if specified). One can specify which items must be ranked with the filter_list parameter, in this case ONLY items in the filter_list parameter will be ranked. One can also pass items already seen by the user with the filter_list parameter. Otherwise, ALL unrated items will be ranked. Parameters: Name Type Description Default user_ratings List [ Interaction ] List of Interaction objects for a single user required available_loaded_items LoadedContentsDict The LoadedContents interface which contains loaded contents required recs_number int number of the top ranked items to return, if None all ranked items will be returned None filter_list list list of the items to rank, if None all unrated items for the user will be ranked None Returns: Type Description List [ Interaction ] List of Interactions object in a descending order w.r.t the 'score' attribute, representing the ranking for a single user Source code in clayrs/recsys/content_based_algorithm/classifier/classifier_recommender.py 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 def rank ( self , user_ratings : List [ Interaction ], available_loaded_items : LoadedContentsDict , recs_number : int = None , filter_list : List [ str ] = None ) -> List [ Interaction ]: \"\"\" Rank the top-n recommended items for the user. If the recs_number parameter isn't specified, All unrated items for the user will be ranked (or only items in the filter list, if specified). One can specify which items must be ranked with the `filter_list` parameter, in this case ONLY items in the `filter_list` parameter will be ranked. One can also pass items already seen by the user with the filter_list parameter. Otherwise, **ALL** unrated items will be ranked. Args: user_ratings: List of Interaction objects for a single user available_loaded_items: The LoadedContents interface which contains loaded contents recs_number: number of the top ranked items to return, if None all ranked items will be returned filter_list (list): list of the items to rank, if None all unrated items for the user will be ranked Returns: List of Interactions object in a descending order w.r.t the 'score' attribute, representing the ranking for a single user \"\"\" try : user_id = user_ratings [ 0 ] . user_id except IndexError : raise EmptyUserRatings ( \"The user selected doesn't have any ratings!\" ) user_seen_items = set ([ interaction . item_id for interaction in user_ratings ]) # Load items to predict if filter_list is None : items_to_predict = available_loaded_items . get_list ([ item_id for item_id in available_loaded_items if item_id not in user_seen_items ]) else : items_to_predict = available_loaded_items . get_list ( filter_list ) # Extract features of the items to predict id_items_to_predict = [] features_items_to_predict = [] for item in items_to_predict : if item is not None : id_items_to_predict . append ( item . content_id ) features_items_to_predict . append ( self . extract_features_item ( item )) if len ( id_items_to_predict ) > 0 : # Fuse the input if there are dicts, multiple representation, etc. fused_features_items_to_pred = self . fuse_representations ( features_items_to_predict , self . _embedding_combiner ) class_prob = self . _classifier . predict_proba ( fused_features_items_to_pred ) else : class_prob = [] # for each item we extract the probability that the item is liked (class 1) score_labels = ( prob [ 1 ] for prob in class_prob ) # Build the item_score dict (key is item_id, value is rank score predicted) # and order the keys in descending order item_score_dict = dict ( zip ( id_items_to_predict , score_labels )) ordered_item_ids = sorted ( item_score_dict , key = item_score_dict . get , reverse = True ) # we only save the top-n items_ids corresponding to top-n recommendations # (if recs_number is None ordered_item_ids will contain all item_ids as the original list) ordered_item_ids = ordered_item_ids [: recs_number ] # we construct the output data rank_interaction_list = [ Interaction ( user_id , item_id , item_score_dict [ item_id ]) for item_id in ordered_item_ids ] return rank_interaction_list","title":"rank()"},{"location":"recsys/content_based/content_based_algorithms/classifier_recommender/#classifiers-implemented","text":"The following are the classifiers you can use in the classifier parameter of the ClassifierRecommender class","title":"Classifiers Implemented"},{"location":"recsys/content_based/content_based_algorithms/classifier_recommender/#clayrs.recsys.content_based_algorithm.classifier.classifiers.SkDecisionTree","text":"Bases: Classifier Class that implements the Decision Tree Classifier from sklearn. The parameters one could pass are the same ones you would pass instantiating the classifier directly from sklearn Sklearn documentation: here Source code in clayrs/recsys/content_based_algorithm/classifier/classifiers.py 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 def __init__ ( self , * , criterion : Any = \"gini\" , splitter : Any = \"best\" , max_depth : Any = None , min_samples_split : Any = 2 , min_samples_leaf : Any = 1 , min_weight_fraction_leaf : Any = 0.0 , max_features : Any = None , random_state : Any = None , max_leaf_nodes : Any = None , min_impurity_decrease : Any = 0.0 , class_weight : Any = None , ccp_alpha : Any = 0.0 ): clf = DecisionTreeClassifier ( criterion = criterion , splitter = splitter , max_depth = max_depth , min_samples_split = min_samples_split , min_samples_leaf = min_samples_leaf , min_weight_fraction_leaf = min_weight_fraction_leaf , max_features = max_features , random_state = random_state , max_leaf_nodes = max_leaf_nodes , min_impurity_decrease = min_impurity_decrease , class_weight = class_weight , ccp_alpha = ccp_alpha ) super () . __init__ ( clf , inspect . currentframe ())","title":"SkDecisionTree"},{"location":"recsys/content_based/content_based_algorithms/classifier_recommender/#clayrs.recsys.content_based_algorithm.classifier.classifiers.SkGaussianProcess","text":"Bases: Classifier Class that implements the Gaussian Process Classifier from sklearn. The parameters one could pass are the same ones you would pass instantiating the classifier directly from sklearn Sklearn documentation: here Source code in clayrs/recsys/content_based_algorithm/classifier/classifiers.py 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 def __init__ ( self , kernel : Any = None , * , optimizer : Any = \"fmin_l_bfgs_b\" , n_restarts_optimizer : Any = 0 , max_iter_predict : Any = 100 , warm_start : Any = False , copy_X_train : Any = True , random_state : Any = None , multi_class : Any = \"one_vs_rest\" , n_jobs : Any = None ): clf = GaussianProcessClassifier ( kernel = kernel , optimizer = optimizer , n_restarts_optimizer = n_restarts_optimizer , max_iter_predict = max_iter_predict , warm_start = warm_start , copy_X_train = copy_X_train , random_state = random_state , multi_class = multi_class , n_jobs = n_jobs ) super () . __init__ ( clf , inspect . currentframe ())","title":"SkGaussianProcess"},{"location":"recsys/content_based/content_based_algorithms/classifier_recommender/#clayrs.recsys.content_based_algorithm.classifier.classifiers.SkKNN","text":"Bases: Classifier Class that implements the KNN Classifier from sklearn. The parameters one could pass are the same ones you would pass instantiating the classifier KNN directly from sklearn. Sklearn documentation: here Since KNN implementation of sklearn has n_neighbors = 5 as default, it can throw an exception if less sample in the training data are provided, so we change dynamically the n_neighbors parameter according to the number of samples if the dataset is too small and if no manual n_neighbors is set Source code in clayrs/recsys/content_based_algorithm/classifier/classifiers.py 129 130 131 132 133 134 135 136 137 138 139 140 141 def __init__ ( self , n_neighbors : Any = 5 , * , weights : Any = \"uniform\" , algorithm : Any = \"auto\" , leaf_size : Any = 30 , p : Any = 2 , metric : Any = \"minkowski\" , metric_params : Any = None , n_jobs : Any = None ): clf = KNeighborsClassifier ( n_neighbors = n_neighbors , weights = weights , algorithm = algorithm , leaf_size = leaf_size , p = p , metric = metric , metric_params = metric_params , n_jobs = n_jobs ) super () . __init__ ( clf , inspect . currentframe ())","title":"SkKNN"},{"location":"recsys/content_based/content_based_algorithms/classifier_recommender/#clayrs.recsys.content_based_algorithm.classifier.classifiers.SkLogisticRegression","text":"Bases: Classifier Class that implements the Logistic Regression Classifier from sklearn. The parameters one could pass are the same ones you would pass instantiating the classifier directly from sklearn Sklearn documentation: here Source code in clayrs/recsys/content_based_algorithm/classifier/classifiers.py 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 def __init__ ( self , penalty : Any = \"l2\" , * , dual : Any = False , tol : Any = 1e-4 , C : Any = 1.0 , fit_intercept : Any = True , intercept_scaling : Any = 1 , class_weight : Any = None , random_state : Any = None , solver : Any = \"lbfgs\" , max_iter : Any = 100 , multi_class : Any = \"auto\" , verbose : Any = 0 , warm_start : Any = False , n_jobs : Any = None , l1_ratio : Any = None ): clf = LogisticRegression ( penalty = penalty , dual = dual , tol = tol , C = C , fit_intercept = fit_intercept , intercept_scaling = intercept_scaling , class_weight = class_weight , random_state = random_state , solver = solver , max_iter = max_iter , multi_class = multi_class , verbose = verbose , warm_start = warm_start , n_jobs = n_jobs , l1_ratio = l1_ratio ) super () . __init__ ( clf , inspect . currentframe ())","title":"SkLogisticRegression"},{"location":"recsys/content_based/content_based_algorithms/classifier_recommender/#clayrs.recsys.content_based_algorithm.classifier.classifiers.SkRandomForest","text":"Bases: Classifier Class that implements the Random Forest Classifier from sklearn. The parameters one could pass are the same ones you would pass instantiating the classifier directly from sklearn Sklearn documentation: here Source code in clayrs/recsys/content_based_algorithm/classifier/classifiers.py 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 def __init__ ( self , n_estimators : Any = 100 , * , criterion : Any = \"gini\" , max_depth : Any = None , min_samples_split : Any = 2 , min_samples_leaf : Any = 1 , min_weight_fraction_leaf : Any = 0.0 , max_features : Any = \"auto\" , max_leaf_nodes : Any = None , min_impurity_decrease : Any = 0.0 , bootstrap : Any = True , oob_score : Any = False , n_jobs : Any = None , random_state : Any = None , verbose : Any = 0 , warm_start : Any = False , class_weight : Any = None , ccp_alpha : Any = 0.0 , max_samples : Any = None ): clf = RandomForestClassifier ( n_estimators = n_estimators , criterion = criterion , max_depth = max_depth , min_samples_split = min_samples_split , min_samples_leaf = min_samples_leaf , min_weight_fraction_leaf = min_weight_fraction_leaf , max_features = max_features , max_leaf_nodes = max_leaf_nodes , min_impurity_decrease = min_impurity_decrease , bootstrap = bootstrap , oob_score = oob_score , n_jobs = n_jobs , random_state = random_state , verbose = verbose , warm_start = warm_start , class_weight = class_weight , ccp_alpha = ccp_alpha , max_samples = max_samples ) super () . __init__ ( clf , inspect . currentframe ())","title":"SkRandomForest"},{"location":"recsys/content_based/content_based_algorithms/classifier_recommender/#clayrs.recsys.content_based_algorithm.classifier.classifiers.SkSVC","text":"Bases: Classifier Class that implements the SVC Classifier from sklearn. The parameters one could pass are the same ones you would pass instantiating the classifier SVC directly from sklearn. Sklearn documentation: here The only parameter from sklearn that cannot be passed is the 'probability' parameter: it is set to True and cannot be changed Source code in clayrs/recsys/content_based_algorithm/classifier/classifiers.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 def __init__ ( self , * , C : Any = 1.0 , kernel : Any = \"rbf\" , degree : Any = 3 , gamma : Any = \"scale\" , coef0 : Any = 0.0 , shrinking : Any = True , tol : Any = 1e-3 , cache_size : Any = 200 , class_weight : Any = None , verbose : Any = False , max_iter : Any = - 1 , decision_function_shape : Any = \"ovr\" , break_ties : Any = False , random_state : Any = None ): # Force the probability parameter at True, otherwise SVC won't predict_proba clf = SVC ( C = C , kernel = kernel , degree = degree , gamma = gamma , coef0 = coef0 , shrinking = shrinking , tol = tol , cache_size = cache_size , class_weight = class_weight , verbose = verbose , max_iter = max_iter , decision_function_shape = decision_function_shape , break_ties = break_ties , random_state = random_state , probability = True ) super () . __init__ ( clf , inspect . currentframe ())","title":"SkSVC"},{"location":"recsys/content_based/content_based_algorithms/index_query/","text":"IndexQuery ( item_field , classic_similarity = True , threshold = None ) Bases: ContentBasedAlgorithm Class for the search engine recommender using an index. It firsts builds a query using the representation(s) specified of the positive items, then uses the mentioned query to do an actual search inside the index: every items will have a score of \"closeness\" in relation to the query, we use this score to rank every item. Just be sure to use textual representation(s) to build a significant query and to make a significant search! Examples: Interested in only a field representation, classic tfidf similarity, \\(threshold = 3\\) (Every item with rating \\(>= 3\\) will be considered as positive) >>> from clayrs import recsys as rs >>> alg = rs . IndexQuery ({ \"Plot\" : 0 }, threshold = 3 ) Interested in multiple field representations of the items, BM25 similarity, \\(threshold = None\\) (Every item with rating \\(>=\\) mean rating of the user will be considered as positive) >>> alg = rs . IndexQuery ( >>> item_field = { \"Plot\" : [ 0 , \"original_text\" ], >>> \"Genre\" : [ 0 , 1 ], >>> \"Director\" : \"preprocessed_text\" }, >>> classic_similarity = False , >>> threshold = 3 ) Info After instantiating the IndexQuery algorithm, pass it in the initialization of a CBRS and the use its method to calculate ranking for single user or multiple users: Examples: >>> cbrs = rs.ContentBasedRS(algorithm=alg, ...) >>> cbrs.fit_rank(...) >>> # ... Parameters: Name Type Description Default item_field dict dict where the key is the name of the field that contains the content to use, value is the representation(s) id(s) that will be used for the said item, just BE SURE to use textual representation(s). The value of a field can be a string or a list, use a list if you want to use multiple representations for a particular field. required classic_similarity bool True if you want to use the classic implementation of tfidf in Whoosh, False if you want BM25F True threshold float Threshold for the ratings. If the rating is greater than the threshold, it will be considered as positive. If the threshold is not specified, the average score of all items liked by the user is used. None Source code in clayrs/recsys/content_based_algorithm/index_query/index_query.py 61 62 63 64 65 66 def __init__ ( self , item_field : dict , classic_similarity : bool = True , threshold : float = None ): super () . __init__ ( item_field , threshold ) self . _string_query : Optional [ str ] = None self . _scores : Optional [ list ] = None self . _positive_user_docs : Optional [ dict ] = None self . _classic_similarity : bool = classic_similarity fit () The fit process for the IndexQuery consists in building a query using the features of the positive items ONLY (items that the user liked). The terms relative to these 'positive' items are boosted by the rating he/she gave. This method uses extracted features of the positive items stored in a private attribute, so process_rated() must be called before this method. The built query will also be stored in a private attribute. Source code in clayrs/recsys/content_based_algorithm/index_query/index_query.py 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 def fit ( self ): \"\"\" The fit process for the IndexQuery consists in building a query using the features of the positive items ONLY (items that the user liked). The terms relative to these 'positive' items are boosted by the rating he/she gave. This method uses extracted features of the positive items stored in a private attribute, so process_rated() must be called before this method. The built query will also be stored in a private attribute. \"\"\" # For each field of each document one string (containing the name of the field and the data in it) # is created and added to the query. # Also each part of the query that refers to a document # is boosted by the score given by the user to said document string_query = \"(\" for doc , score in zip ( self . _positive_user_docs . keys (), self . _scores ): string_query += \"(\" for field_name in self . _positive_user_docs [ doc ]: if field_name == 'content_id' : continue word_list = self . _positive_user_docs [ doc ][ field_name ] . split () string_query += field_name + \":(\" for term in word_list : string_query += term + \" \" string_query += \") \" string_query += \")^\" + str ( score ) + \" \" string_query += \") \" self . _string_query = string_query predict ( user_ratings , available_loaded_items , filter_list = None ) IndexQuery is not a Prediction Score Algorithm, so if this method is called, a NotPredictionAlg exception is raised Raises: Type Description NotPredictionAlg exception raised since the CentroidVector algorithm is not a score prediction algorithm Source code in clayrs/recsys/content_based_algorithm/index_query/index_query.py 204 205 206 207 208 209 210 211 212 213 def predict ( self , user_ratings : List [ Interaction ], available_loaded_items : LoadedContentsIndex , filter_list : List [ str ] = None ) -> List [ Interaction ]: \"\"\" IndexQuery is not a Prediction Score Algorithm, so if this method is called, a NotPredictionAlg exception is raised Raises: NotPredictionAlg: exception raised since the CentroidVector algorithm is not a score prediction algorithm \"\"\" raise NotPredictionAlg ( \"IndexQuery is not a Score Prediction Algorithm!\" ) process_rated ( user_ratings , available_loaded_items ) Function that extracts features from positive rated items ONLY! The extracted features will be used to fit the algorithm (build the query). Features extracted will be stored in private attributes of the class. Parameters: Name Type Description Default user_ratings List [ Interaction ] List of Interaction objects for a single user required available_loaded_items LoadedContentsIndex The LoadedContents interface which contains loaded contents required Source code in clayrs/recsys/content_based_algorithm/index_query/index_query.py 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 def process_rated ( self , user_ratings : List [ Interaction ], available_loaded_items : LoadedContentsIndex ): \"\"\" Function that extracts features from positive rated items ONLY! The extracted features will be used to fit the algorithm (build the query). Features extracted will be stored in private attributes of the class. Args: user_ratings: List of Interaction objects for a single user available_loaded_items: The LoadedContents interface which contains loaded contents \"\"\" items_scores_dict = { interaction . item_id : interaction . score for interaction in user_ratings } threshold = self . threshold if threshold is None : threshold = self . _calc_mean_user_threshold ( user_ratings ) # Initializes positive_user_docs which is a dictionary that has the document_id as key and # another dictionary as value. The dictionary value has the name of the field as key # and its contents as value. By doing so we obtain the data of the fields while # also storing information regarding the field and the document where it was scores = [] positive_user_docs = {} ix = available_loaded_items . get_contents_interface () for item_id , score in zip ( items_scores_dict . keys (), items_scores_dict . values ()): if score >= threshold : # {item_id: {\"item\": item_dictionary, \"score\": item_score}} item_query = ix . query ( item_id , 1 , classic_similarity = self . _classic_similarity ) if len ( item_query ) != 0 : item = item_query . pop ( item_id ) . get ( 'item' ) scores . append ( score ) positive_user_docs [ item_id ] = self . _get_representations ( item ) if len ( user_ratings ) == 0 : raise EmptyUserRatings ( \"The user selected doesn't have any ratings!\" ) user_id = user_ratings [ 0 ] . user_id if len ( positive_user_docs ) == 0 : raise OnlyNegativeItems ( f \"User { user_id } - There are no rated items available locally or there are only \" f \"negative items available locally!\" ) self . _positive_user_docs = positive_user_docs self . _scores = scores rank ( user_ratings , available_loaded_items , recs_number = None , filter_list = None ) Rank the top-n recommended items for the user. If the recs_number parameter isn't specified, All unrated items for the user will be ranked (or only items in the filter list, if specified). One can specify which items must be ranked with the filter_list parameter, in this case ONLY items in the filter_list parameter will be ranked. One can also pass items already seen by the user with the filter_list parameter. Otherwise, ALL unrated items will be ranked. Parameters: Name Type Description Default user_ratings List [ Interaction ] List of Interaction objects for a single user required available_loaded_items LoadedContentsIndex The LoadedContents interface which contains loaded contents. In this case it will be a LoadedContentsIndex required recs_number int number of the top ranked items to return, if None all ranked items will be returned None filter_list list list of the items to rank, if None all unrated items for the user will be ranked None Returns: Type Description List [ Interaction ] List of Interactions object in a descending order w.r.t the 'score' attribute, representing the ranking for a single user Source code in clayrs/recsys/content_based_algorithm/index_query/index_query.py 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 def rank ( self , user_ratings : List [ Interaction ], available_loaded_items : LoadedContentsIndex , recs_number : int = None , filter_list : List [ str ] = None ) -> List [ Interaction ]: \"\"\" Rank the top-n recommended items for the user. If the recs_number parameter isn't specified, All unrated items for the user will be ranked (or only items in the filter list, if specified). One can specify which items must be ranked with the `filter_list` parameter, in this case ONLY items in the `filter_list` parameter will be ranked. One can also pass items already seen by the user with the filter_list parameter. Otherwise, **ALL** unrated items will be ranked. Args: user_ratings: List of Interaction objects for a single user available_loaded_items: The LoadedContents interface which contains loaded contents. In this case it will be a `LoadedContentsIndex` recs_number: number of the top ranked items to return, if None all ranked items will be returned filter_list (list): list of the items to rank, if None all unrated items for the user will be ranked Returns: List of Interactions object in a descending order w.r.t the 'score' attribute, representing the ranking for a single user \"\"\" try : user_id = user_ratings [ 0 ] . user_id except IndexError : raise EmptyUserRatings ( \"The user selected doesn't have any ratings!\" ) user_seen_items = set ([ interaction . item_id for interaction in user_ratings ]) mask_list = self . _build_mask_list ( user_seen_items , filter_list ) ix = available_loaded_items . get_contents_interface () score_docs = ix . query ( self . _string_query , recs_number , mask_list , filter_list , self . _classic_similarity ) # we construct the output data rank_interaction_list = [ Interaction ( user_id , item_id , score_docs [ item_id ][ 'score' ]) for item_id in score_docs ] return rank_interaction_list","title":"Index Query"},{"location":"recsys/content_based/content_based_algorithms/index_query/#clayrs.recsys.content_based_algorithm.index_query.index_query.IndexQuery","text":"Bases: ContentBasedAlgorithm Class for the search engine recommender using an index. It firsts builds a query using the representation(s) specified of the positive items, then uses the mentioned query to do an actual search inside the index: every items will have a score of \"closeness\" in relation to the query, we use this score to rank every item. Just be sure to use textual representation(s) to build a significant query and to make a significant search! Examples: Interested in only a field representation, classic tfidf similarity, \\(threshold = 3\\) (Every item with rating \\(>= 3\\) will be considered as positive) >>> from clayrs import recsys as rs >>> alg = rs . IndexQuery ({ \"Plot\" : 0 }, threshold = 3 ) Interested in multiple field representations of the items, BM25 similarity, \\(threshold = None\\) (Every item with rating \\(>=\\) mean rating of the user will be considered as positive) >>> alg = rs . IndexQuery ( >>> item_field = { \"Plot\" : [ 0 , \"original_text\" ], >>> \"Genre\" : [ 0 , 1 ], >>> \"Director\" : \"preprocessed_text\" }, >>> classic_similarity = False , >>> threshold = 3 ) Info After instantiating the IndexQuery algorithm, pass it in the initialization of a CBRS and the use its method to calculate ranking for single user or multiple users: Examples: >>> cbrs = rs.ContentBasedRS(algorithm=alg, ...) >>> cbrs.fit_rank(...) >>> # ... Parameters: Name Type Description Default item_field dict dict where the key is the name of the field that contains the content to use, value is the representation(s) id(s) that will be used for the said item, just BE SURE to use textual representation(s). The value of a field can be a string or a list, use a list if you want to use multiple representations for a particular field. required classic_similarity bool True if you want to use the classic implementation of tfidf in Whoosh, False if you want BM25F True threshold float Threshold for the ratings. If the rating is greater than the threshold, it will be considered as positive. If the threshold is not specified, the average score of all items liked by the user is used. None Source code in clayrs/recsys/content_based_algorithm/index_query/index_query.py 61 62 63 64 65 66 def __init__ ( self , item_field : dict , classic_similarity : bool = True , threshold : float = None ): super () . __init__ ( item_field , threshold ) self . _string_query : Optional [ str ] = None self . _scores : Optional [ list ] = None self . _positive_user_docs : Optional [ dict ] = None self . _classic_similarity : bool = classic_similarity","title":"IndexQuery"},{"location":"recsys/content_based/content_based_algorithms/index_query/#clayrs.recsys.content_based_algorithm.index_query.index_query.IndexQuery.fit","text":"The fit process for the IndexQuery consists in building a query using the features of the positive items ONLY (items that the user liked). The terms relative to these 'positive' items are boosted by the rating he/she gave. This method uses extracted features of the positive items stored in a private attribute, so process_rated() must be called before this method. The built query will also be stored in a private attribute. Source code in clayrs/recsys/content_based_algorithm/index_query/index_query.py 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 def fit ( self ): \"\"\" The fit process for the IndexQuery consists in building a query using the features of the positive items ONLY (items that the user liked). The terms relative to these 'positive' items are boosted by the rating he/she gave. This method uses extracted features of the positive items stored in a private attribute, so process_rated() must be called before this method. The built query will also be stored in a private attribute. \"\"\" # For each field of each document one string (containing the name of the field and the data in it) # is created and added to the query. # Also each part of the query that refers to a document # is boosted by the score given by the user to said document string_query = \"(\" for doc , score in zip ( self . _positive_user_docs . keys (), self . _scores ): string_query += \"(\" for field_name in self . _positive_user_docs [ doc ]: if field_name == 'content_id' : continue word_list = self . _positive_user_docs [ doc ][ field_name ] . split () string_query += field_name + \":(\" for term in word_list : string_query += term + \" \" string_query += \") \" string_query += \")^\" + str ( score ) + \" \" string_query += \") \" self . _string_query = string_query","title":"fit()"},{"location":"recsys/content_based/content_based_algorithms/index_query/#clayrs.recsys.content_based_algorithm.index_query.index_query.IndexQuery.predict","text":"IndexQuery is not a Prediction Score Algorithm, so if this method is called, a NotPredictionAlg exception is raised Raises: Type Description NotPredictionAlg exception raised since the CentroidVector algorithm is not a score prediction algorithm Source code in clayrs/recsys/content_based_algorithm/index_query/index_query.py 204 205 206 207 208 209 210 211 212 213 def predict ( self , user_ratings : List [ Interaction ], available_loaded_items : LoadedContentsIndex , filter_list : List [ str ] = None ) -> List [ Interaction ]: \"\"\" IndexQuery is not a Prediction Score Algorithm, so if this method is called, a NotPredictionAlg exception is raised Raises: NotPredictionAlg: exception raised since the CentroidVector algorithm is not a score prediction algorithm \"\"\" raise NotPredictionAlg ( \"IndexQuery is not a Score Prediction Algorithm!\" )","title":"predict()"},{"location":"recsys/content_based/content_based_algorithms/index_query/#clayrs.recsys.content_based_algorithm.index_query.index_query.IndexQuery.process_rated","text":"Function that extracts features from positive rated items ONLY! The extracted features will be used to fit the algorithm (build the query). Features extracted will be stored in private attributes of the class. Parameters: Name Type Description Default user_ratings List [ Interaction ] List of Interaction objects for a single user required available_loaded_items LoadedContentsIndex The LoadedContents interface which contains loaded contents required Source code in clayrs/recsys/content_based_algorithm/index_query/index_query.py 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 def process_rated ( self , user_ratings : List [ Interaction ], available_loaded_items : LoadedContentsIndex ): \"\"\" Function that extracts features from positive rated items ONLY! The extracted features will be used to fit the algorithm (build the query). Features extracted will be stored in private attributes of the class. Args: user_ratings: List of Interaction objects for a single user available_loaded_items: The LoadedContents interface which contains loaded contents \"\"\" items_scores_dict = { interaction . item_id : interaction . score for interaction in user_ratings } threshold = self . threshold if threshold is None : threshold = self . _calc_mean_user_threshold ( user_ratings ) # Initializes positive_user_docs which is a dictionary that has the document_id as key and # another dictionary as value. The dictionary value has the name of the field as key # and its contents as value. By doing so we obtain the data of the fields while # also storing information regarding the field and the document where it was scores = [] positive_user_docs = {} ix = available_loaded_items . get_contents_interface () for item_id , score in zip ( items_scores_dict . keys (), items_scores_dict . values ()): if score >= threshold : # {item_id: {\"item\": item_dictionary, \"score\": item_score}} item_query = ix . query ( item_id , 1 , classic_similarity = self . _classic_similarity ) if len ( item_query ) != 0 : item = item_query . pop ( item_id ) . get ( 'item' ) scores . append ( score ) positive_user_docs [ item_id ] = self . _get_representations ( item ) if len ( user_ratings ) == 0 : raise EmptyUserRatings ( \"The user selected doesn't have any ratings!\" ) user_id = user_ratings [ 0 ] . user_id if len ( positive_user_docs ) == 0 : raise OnlyNegativeItems ( f \"User { user_id } - There are no rated items available locally or there are only \" f \"negative items available locally!\" ) self . _positive_user_docs = positive_user_docs self . _scores = scores","title":"process_rated()"},{"location":"recsys/content_based/content_based_algorithms/index_query/#clayrs.recsys.content_based_algorithm.index_query.index_query.IndexQuery.rank","text":"Rank the top-n recommended items for the user. If the recs_number parameter isn't specified, All unrated items for the user will be ranked (or only items in the filter list, if specified). One can specify which items must be ranked with the filter_list parameter, in this case ONLY items in the filter_list parameter will be ranked. One can also pass items already seen by the user with the filter_list parameter. Otherwise, ALL unrated items will be ranked. Parameters: Name Type Description Default user_ratings List [ Interaction ] List of Interaction objects for a single user required available_loaded_items LoadedContentsIndex The LoadedContents interface which contains loaded contents. In this case it will be a LoadedContentsIndex required recs_number int number of the top ranked items to return, if None all ranked items will be returned None filter_list list list of the items to rank, if None all unrated items for the user will be ranked None Returns: Type Description List [ Interaction ] List of Interactions object in a descending order w.r.t the 'score' attribute, representing the ranking for a single user Source code in clayrs/recsys/content_based_algorithm/index_query/index_query.py 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 def rank ( self , user_ratings : List [ Interaction ], available_loaded_items : LoadedContentsIndex , recs_number : int = None , filter_list : List [ str ] = None ) -> List [ Interaction ]: \"\"\" Rank the top-n recommended items for the user. If the recs_number parameter isn't specified, All unrated items for the user will be ranked (or only items in the filter list, if specified). One can specify which items must be ranked with the `filter_list` parameter, in this case ONLY items in the `filter_list` parameter will be ranked. One can also pass items already seen by the user with the filter_list parameter. Otherwise, **ALL** unrated items will be ranked. Args: user_ratings: List of Interaction objects for a single user available_loaded_items: The LoadedContents interface which contains loaded contents. In this case it will be a `LoadedContentsIndex` recs_number: number of the top ranked items to return, if None all ranked items will be returned filter_list (list): list of the items to rank, if None all unrated items for the user will be ranked Returns: List of Interactions object in a descending order w.r.t the 'score' attribute, representing the ranking for a single user \"\"\" try : user_id = user_ratings [ 0 ] . user_id except IndexError : raise EmptyUserRatings ( \"The user selected doesn't have any ratings!\" ) user_seen_items = set ([ interaction . item_id for interaction in user_ratings ]) mask_list = self . _build_mask_list ( user_seen_items , filter_list ) ix = available_loaded_items . get_contents_interface () score_docs = ix . query ( self . _string_query , recs_number , mask_list , filter_list , self . _classic_similarity ) # we construct the output data rank_interaction_list = [ Interaction ( user_id , item_id , score_docs [ item_id ][ 'score' ]) for item_id in score_docs ] return rank_interaction_list","title":"rank()"},{"location":"recsys/content_based/content_based_algorithms/linear_predictor/","text":"Linear Predictor LinearPredictor ( item_field , regressor , only_greater_eq = None , embedding_combiner = Centroid ()) Bases: ContentBasedAlgorithm Class that implements recommendation through a specified linear predictor. It's a score prediction algorithm, so it can predict what rating a user would give to an unseen item. As such, it's also a ranking algorithm (it simply ranks in descending order unseen items by the predicted rating) Examples: Interested in only a field representation, LinearRegression regressor from sklearn >>> from clayrs import recsys as rs >>> alg = rs . LinearPredictor ({ \"Plot\" : 0 }, rs . SkLinearRegression ()) Interested in only a field representation, Ridge regressor from sklearn with custom parameters >>> alg = rs . LinearPredictor ({ \"Plot\" : 0 }, rs . SkRidge ( alpha = 0.8 )) Interested in multiple field representations of the items, Ridge regressor from sklearn with custom parameters, \\(only_greater_eq = 2\\) (Every item with rating \\(>= 2\\) will be discarded and not considered in the ranking/score prediction task) >>> alg = rs . LinearPredictor ( >>> item_field = { \"Plot\" : [ 0 , \"tfidf\" ], >>> \"Genre\" : [ 0 , 1 ], >>> \"Director\" : \"doc2vec\" }, >>> regressor = rs . SkRidge ( alpha = 0.8 ), >>> only_greater_eq = 2 ) Info After instantiating the LinearPredictor algorithm, pass it in the initialization of a CBRS and the use its method to predict ratings or calculate ranking for a single user or multiple users: Examples: >>> cbrs = rs.ContentBasedRS(algorithm=alg, ...) >>> cbrs.fit_predict(...) >>> cbrs.fit_rank(...) >>> # ... Parameters: Name Type Description Default item_field dict dict where the key is the name of the field that contains the content to use, value is the representation(s) id(s) that will be used for the said item. The value of a field can be a string or a list, use a list if you want to use multiple representations for a particular field. required regressor Regressor regressor that will be used. Can be one object of the Regressor class. required only_greater_eq float Threshold for the ratings. Only items with rating greater or equal than the threshold will be considered, items with lower rating will be discarded. If None, no item will be filter out None embedding_combiner CombiningTechnique CombiningTechnique used when embeddings representation must be used but they are in a matrix form instead of a single vector (e.g. when WordEmbedding representations must be used you have one vector for each word). By default the Centroid of the rows of the matrix is computed Centroid() Source code in clayrs/recsys/content_based_algorithm/regressor/linear_predictor.py 68 69 70 71 72 73 74 def __init__ ( self , item_field : dict , regressor : Regressor , only_greater_eq : float = None , embedding_combiner : CombiningTechnique = Centroid ()): super () . __init__ ( item_field , only_greater_eq ) self . _regressor = regressor self . _labels : Optional [ list ] = None self . _rated_dict : Optional [ dict ] = None self . _embedding_combiner = embedding_combiner fit () Fit the regressor specified in the constructor with the features and labels (rating scores) extracted with the process_rated() method. It uses private attributes to fit the classifier, so process_rated() must be called before this method. Source code in clayrs/recsys/content_based_algorithm/regressor/linear_predictor.py 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 def fit ( self ): \"\"\" Fit the regressor specified in the constructor with the features and labels (rating scores) extracted with the process_rated() method. It uses private attributes to fit the classifier, so process_rated() must be called before this method. \"\"\" rated_features = list ( self . _rated_dict . values ()) # Fuse the input if there are dicts, multiple representation, etc. fused_features = self . fuse_representations ( rated_features , self . _embedding_combiner ) self . _regressor . fit ( fused_features , self . _labels ) # we delete variables used to fit since will no longer be used del self . _labels del self . _rated_dict predict ( user_ratings , available_loaded_items , filter_list = None ) Predicts how much a user will like unrated items. One can specify which items must be predicted with the filter_list parameter, in this case ONLY items in the filter_list will be predicted. One can also pass items already seen by the user with the filter_list parameter. Otherwise, ALL unrated items will be predicted. Parameters: Name Type Description Default user_ratings List [ Interaction ] List of Interaction objects for a single user required available_loaded_items LoadedContentsDict The LoadedContents interface which contains loaded contents required filter_list List [ str ] List of the items to predict, if None all unrated items for the user will be predicted None Returns: Type Description List [ Interaction ] List of Interactions object where the 'score' attribute is the rating predicted by the algorithm Source code in clayrs/recsys/content_based_algorithm/regressor/linear_predictor.py 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 def predict ( self , user_ratings : List [ Interaction ], available_loaded_items : LoadedContentsDict , filter_list : List [ str ] = None ) -> List [ Interaction ]: \"\"\" Predicts how much a user will like unrated items. One can specify which items must be predicted with the filter_list parameter, in this case ONLY items in the filter_list will be predicted. One can also pass items already seen by the user with the filter_list parameter. Otherwise, ALL unrated items will be predicted. Args: user_ratings: List of Interaction objects for a single user available_loaded_items: The LoadedContents interface which contains loaded contents filter_list: List of the items to predict, if None all unrated items for the user will be predicted Returns: List of Interactions object where the 'score' attribute is the rating predicted by the algorithm \"\"\" try : user_id = user_ratings [ 0 ] . user_id except IndexError : raise EmptyUserRatings ( \"The user selected doesn't have any ratings!\" ) id_items_to_predict , score_labels = self . _common_prediction_process ( user_ratings , available_loaded_items , filter_list ) # Build the output data pred_interaction_list = [ Interaction ( user_id , item_id , score ) for item_id , score in zip ( id_items_to_predict , score_labels )] return pred_interaction_list process_rated ( user_ratings , available_loaded_items ) Function that extracts features from rated item and labels them. The extracted features will be later used to fit the classifier. Features and labels (in this case the rating score) will be stored in private attributes of the class. IF there are no rated items available locally, an exception is thrown. Parameters: Name Type Description Default user_ratings List [ Interaction ] List of Interaction objects for a single user required available_loaded_items LoadedContentsDict The LoadedContents interface which contains loaded contents required Raises: Type Description NoRatedItems Exception raised when there isn't any item available locally rated by the user Source code in clayrs/recsys/content_based_algorithm/regressor/linear_predictor.py 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 def process_rated ( self , user_ratings : List [ Interaction ], available_loaded_items : LoadedContentsDict ): \"\"\" Function that extracts features from rated item and labels them. The extracted features will be later used to fit the classifier. Features and labels (in this case the rating score) will be stored in private attributes of the class. IF there are no rated items available locally, an exception is thrown. Args: user_ratings: List of Interaction objects for a single user available_loaded_items: The LoadedContents interface which contains loaded contents Raises: NoRatedItems: Exception raised when there isn't any item available locally rated by the user \"\"\" items_scores_dict = { interaction . item_id : interaction . score for interaction in user_ratings } # Create list of all the available items that are useful for the user loaded_rated_items : List [ Union [ Content , None ]] = available_loaded_items . get_list ([ item_id for item_id in set ( items_scores_dict . keys ())]) # Assign label and extract features from the rated items labels = [] rated_dict = {} for item in loaded_rated_items : if item is not None : # This conversion raises Exception when there are multiple equals 'to_id' for the user score_assigned = float ( items_scores_dict [ item . content_id ]) if self . threshold is None : rated_dict [ item ] = self . extract_features_item ( item ) labels . append ( score_assigned ) elif score_assigned >= self . threshold : rated_dict [ item ] = self . extract_features_item ( item ) labels . append ( score_assigned ) if len ( user_ratings ) == 0 : raise EmptyUserRatings ( \"The user selected doesn't have any ratings!\" ) user_id = user_ratings [ 0 ] . user_id if len ( rated_dict ) == 0 : raise NoRatedItems ( \"User {} - No rated item available locally!\" . format ( user_id )) self . _labels = labels self . _rated_dict = rated_dict rank ( user_ratings , available_loaded_items , recs_number = None , filter_list = None ) Rank the top-n recommended items for the user. If the recs_number parameter isn't specified, All unrated items for the user will be ranked (or only items in the filter list, if specified). One can specify which items must be ranked with the filter_list parameter, in this case ONLY items in the filter_list parameter will be ranked. One can also pass items already seen by the user with the filter_list parameter. Otherwise, ALL unrated items will be ranked. Parameters: Name Type Description Default user_ratings List [ Interaction ] List of Interaction objects for a single user required available_loaded_items LoadedContentsDict The LoadedContents interface which contains loaded contents required recs_number int number of the top ranked items to return, if None all ranked items will be returned None filter_list list list of the items to rank, if None all unrated items for the user will be ranked None Returns: Type Description List [ Interaction ] List of Interactions object in a descending order w.r.t the 'score' attribute, representing the ranking for a single user Source code in clayrs/recsys/content_based_algorithm/regressor/linear_predictor.py 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 def rank ( self , user_ratings : List [ Interaction ], available_loaded_items : LoadedContentsDict , recs_number : int = None , filter_list : List [ str ] = None ) -> List [ Interaction ]: \"\"\" Rank the top-n recommended items for the user. If the recs_number parameter isn't specified, All unrated items for the user will be ranked (or only items in the filter list, if specified). One can specify which items must be ranked with the `filter_list` parameter, in this case ONLY items in the `filter_list` parameter will be ranked. One can also pass items already seen by the user with the filter_list parameter. Otherwise, **ALL** unrated items will be ranked. Args: user_ratings: List of Interaction objects for a single user available_loaded_items: The LoadedContents interface which contains loaded contents recs_number: number of the top ranked items to return, if None all ranked items will be returned filter_list (list): list of the items to rank, if None all unrated items for the user will be ranked Returns: List of Interactions object in a descending order w.r.t the 'score' attribute, representing the ranking for a single user \"\"\" try : user_id = user_ratings [ 0 ] . user_id except IndexError : raise EmptyUserRatings ( \"The user selected doesn't have any ratings!\" ) # Predict the rating for the items and sort them in descending order id_items_to_predict , score_labels = self . _common_prediction_process ( user_ratings , available_loaded_items , filter_list ) # Build the item_score dict (key is item_id, value is rank score predicted) # and order the keys in descending order item_score_dict = dict ( zip ( id_items_to_predict , score_labels )) ordered_item_ids = sorted ( item_score_dict , key = item_score_dict . get , reverse = True ) # we only save the top-n items_ids corresponding to top-n recommendations # (if recs_number is None ordered_item_ids will contain all item_ids as the original list) ordered_item_ids = ordered_item_ids [: recs_number ] # we construct the output data rank_interaction_list = [ Interaction ( user_id , item_id , item_score_dict [ item_id ]) for item_id in ordered_item_ids ] return rank_interaction_list Regressors Implemented The following are the regressors you can use in the regressor parameter of the LinearPredictor class SkARDRegression ( * , n_iter = 300 , tol = 0.001 , alpha_1 = 1e-06 , alpha_2 = 1e-06 , lambda_1 = 1e-06 , lambda_2 = 1e-06 , compute_score = False , threshold_lambda = 10000.0 , fit_intercept = True , normalize = 'deprecated' , copy_X = True , verbose = False ) Bases: Regressor Class that implements the ARD regressor from sklearn. The parameters one could pass are the same ones you would pass instantiating the regressor ARD directly from sklearn. Sklearn documentation: here Source code in clayrs/recsys/content_based_algorithm/regressor/regressors.py 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 def __init__ ( self , * , n_iter : Any = 300 , tol : Any = 1.0e-3 , alpha_1 : Any = 1.0e-6 , alpha_2 : Any = 1.0e-6 , lambda_1 : Any = 1.0e-6 , lambda_2 : Any = 1.0e-6 , compute_score : Any = False , threshold_lambda : Any = 1.0e4 , fit_intercept : Any = True , normalize : Any = \"deprecated\" , copy_X : Any = True , verbose : Any = False ): model = ARDRegression ( n_iter = n_iter , tol = tol , alpha_1 = alpha_1 , alpha_2 = alpha_2 , lambda_1 = lambda_1 , lambda_2 = lambda_2 , compute_score = compute_score , threshold_lambda = threshold_lambda , fit_intercept = fit_intercept , normalize = normalize , copy_X = copy_X , verbose = verbose ) super () . __init__ ( model , inspect . currentframe ()) SkBayesianRidge ( * , n_iter = 300 , tol = 0.001 , alpha_1 = 1e-06 , alpha_2 = 1e-06 , lambda_1 = 1e-06 , lambda_2 = 1e-06 , alpha_init = None , lambda_init = None , compute_score = False , fit_intercept = True , normalize = 'deprecated' , copy_X = True , verbose = False ) Bases: Regressor Class that implements the BayesianRidge regressor from sklearn. The parameters one could pass are the same ones you would pass instantiating the regressor BayesianRidge directly from sklearn. Sklearn documentation: here Source code in clayrs/recsys/content_based_algorithm/regressor/regressors.py 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 def __init__ ( self , * , n_iter : Any = 300 , tol : Any = 1.0e-3 , alpha_1 : Any = 1.0e-6 , alpha_2 : Any = 1.0e-6 , lambda_1 : Any = 1.0e-6 , lambda_2 : Any = 1.0e-6 , alpha_init : Any = None , lambda_init : Any = None , compute_score : Any = False , fit_intercept : Any = True , normalize : Any = \"deprecated\" , copy_X : Any = True , verbose : Any = False ): model = BayesianRidge ( n_iter = n_iter , tol = tol , alpha_1 = alpha_1 , alpha_2 = alpha_2 , lambda_1 = lambda_1 , lambda_2 = lambda_2 , alpha_init = alpha_init , lambda_init = lambda_init , compute_score = compute_score , fit_intercept = fit_intercept , normalize = normalize , copy_X = copy_X , verbose = verbose ) super () . __init__ ( model , inspect . currentframe ()) SkHuberRegressor ( * , epsilon = 1.35 , max_iter = 100 , alpha = 0.0001 , warm_start = False , fit_intercept = True , tol = 1e-05 ) Bases: Regressor Class that implements the Huber regressor from sklearn. The parameters one could pass are the same ones you would pass instantiating the regressor Huber directly from sklearn. Sklearn documentation: here Source code in clayrs/recsys/content_based_algorithm/regressor/regressors.py 256 257 258 259 260 261 262 263 264 265 266 def __init__ ( self , * , epsilon : Any = 1.35 , max_iter : Any = 100 , alpha : Any = 0.0001 , warm_start : Any = False , fit_intercept : Any = True , tol : Any = 1e-05 ): model = HuberRegressor ( epsilon = epsilon , max_iter = max_iter , alpha = alpha , warm_start = warm_start , fit_intercept = fit_intercept , tol = tol ) super () . __init__ ( model , inspect . currentframe ()) SkLinearRegression ( * , fit_intercept = True , normalize = 'deprecated' , copy_X = True , n_jobs = None , positive = False ) Bases: Regressor Class that implements the LinearRegression regressor from sklearn. The parameters one could pass are the same ones you would pass instantiating the regressor LinearRegression directly from sklearn. Sklearn documentation: here Source code in clayrs/recsys/content_based_algorithm/regressor/regressors.py 80 81 82 83 84 85 86 87 88 def __init__ ( self , * , fit_intercept : Any = True , normalize : Any = \"deprecated\" , copy_X : Any = True , n_jobs : Any = None , positive : Any = False ): model = LinearRegression ( fit_intercept = fit_intercept , normalize = normalize , copy_X = copy_X , n_jobs = n_jobs , positive = positive ) super () . __init__ ( model , inspect . currentframe ()) SkPassiveAggressiveRegressor ( * , C = 1.0 , fit_intercept = True , max_iter = 1000 , tol = 0.001 , early_stopping = False , validation_fraction = 0.1 , n_iter_no_change = 5 , shuffle = True , verbose = 0 , loss = 'epsilon_insensitive' , epsilon = DEFAULT_EPSILON , random_state = None , warm_start = False , average = False ) Bases: Regressor Class that implements the PassiveAggressive regressor from sklearn. The parameters one could pass are the same ones you would pass instantiating the regressor PassiveAggressive directly from sklearn. Sklearn documentation: here Source code in clayrs/recsys/content_based_algorithm/regressor/regressors.py 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 def __init__ ( self , * , C : Any = 1.0 , fit_intercept : Any = True , max_iter : Any = 1000 , tol : Any = 1e-3 , early_stopping : Any = False , validation_fraction : Any = 0.1 , n_iter_no_change : Any = 5 , shuffle : Any = True , verbose : Any = 0 , loss : Any = \"epsilon_insensitive\" , epsilon : Any = DEFAULT_EPSILON , random_state : Any = None , warm_start : Any = False , average : Any = False ): model = PassiveAggressiveRegressor ( C = C , fit_intercept = fit_intercept , max_iter = max_iter , tol = tol , early_stopping = early_stopping , validation_fraction = validation_fraction , n_iter_no_change = n_iter_no_change , shuffle = shuffle , verbose = verbose , loss = loss , epsilon = epsilon , random_state = random_state , warm_start = warm_start , average = average ) super () . __init__ ( model , inspect . currentframe ()) SkRidge ( alpha = 1.0 , * , fit_intercept = True , normalize = 'deprecated' , copy_X = True , max_iter = None , tol = 0.001 , solver = 'auto' , positive = False , random_state = None ) Bases: Regressor Class that implements the Ridge regressor from sklearn. The parameters one could pass are the same ones you would pass instantiating the regressor Ridge directly from sklearn. Sklearn documentation: here Source code in clayrs/recsys/content_based_algorithm/regressor/regressors.py 104 105 106 107 108 109 110 111 112 113 114 115 116 117 def __init__ ( self , alpha : Any = 1.0 , * , fit_intercept : Any = True , normalize : Any = \"deprecated\" , copy_X : Any = True , max_iter : Any = None , tol : Any = 1e-3 , solver : Any = \"auto\" , positive : Any = False , random_state : Any = None ): model = Ridge ( alpha = alpha , fit_intercept = fit_intercept , normalize = normalize , copy_X = copy_X , max_iter = max_iter , tol = tol , solver = solver , positive = positive , random_state = random_state ) super () . __init__ ( model , inspect . currentframe ()) SkSGDRegressor ( loss = 'squared_error' , * , penalty = 'l2' , alpha = 0.0001 , l1_ratio = 0.15 , fit_intercept = True , max_iter = 1000 , tol = 0.001 , shuffle = True , verbose = 0 , epsilon = DEFAULT_EPSILON , random_state = None , learning_rate = 'invscaling' , eta0 = 0.01 , power_t = 0.25 , early_stopping = False , validation_fraction = 0.1 , n_iter_no_change = 5 , warm_start = False , average = False ) Bases: Regressor Class that implements the SGD regressor from sklearn. The parameters one could pass are the same ones you would pass instantiating the regressor SGD directly from sklearn. Sklearn documentation: here Source code in clayrs/recsys/content_based_algorithm/regressor/regressors.py 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 def __init__ ( self , loss : Any = \"squared_error\" , * , penalty : Any = \"l2\" , alpha : Any = 0.0001 , l1_ratio : Any = 0.15 , fit_intercept : Any = True , max_iter : Any = 1000 , tol : Any = 1e-3 , shuffle : Any = True , verbose : Any = 0 , epsilon : Any = DEFAULT_EPSILON , random_state : Any = None , learning_rate : Any = \"invscaling\" , eta0 : Any = 0.01 , power_t : Any = 0.25 , early_stopping : Any = False , validation_fraction : Any = 0.1 , n_iter_no_change : Any = 5 , warm_start : Any = False , average : Any = False ): model = SGDRegressor ( loss = loss , penalty = penalty , alpha = alpha , l1_ratio = l1_ratio , fit_intercept = fit_intercept , max_iter = max_iter , tol = tol , shuffle = shuffle , verbose = verbose , epsilon = epsilon , random_state = random_state , learning_rate = learning_rate , eta0 = eta0 , power_t = power_t , early_stopping = early_stopping , validation_fraction = validation_fraction , n_iter_no_change = n_iter_no_change , warm_start = warm_start , average = average ) super () . __init__ ( model , inspect . currentframe ())","title":"Linear Predictor"},{"location":"recsys/content_based/content_based_algorithms/linear_predictor/#linear-predictor","text":"","title":"Linear Predictor"},{"location":"recsys/content_based/content_based_algorithms/linear_predictor/#clayrs.recsys.content_based_algorithm.regressor.linear_predictor.LinearPredictor","text":"Bases: ContentBasedAlgorithm Class that implements recommendation through a specified linear predictor. It's a score prediction algorithm, so it can predict what rating a user would give to an unseen item. As such, it's also a ranking algorithm (it simply ranks in descending order unseen items by the predicted rating) Examples: Interested in only a field representation, LinearRegression regressor from sklearn >>> from clayrs import recsys as rs >>> alg = rs . LinearPredictor ({ \"Plot\" : 0 }, rs . SkLinearRegression ()) Interested in only a field representation, Ridge regressor from sklearn with custom parameters >>> alg = rs . LinearPredictor ({ \"Plot\" : 0 }, rs . SkRidge ( alpha = 0.8 )) Interested in multiple field representations of the items, Ridge regressor from sklearn with custom parameters, \\(only_greater_eq = 2\\) (Every item with rating \\(>= 2\\) will be discarded and not considered in the ranking/score prediction task) >>> alg = rs . LinearPredictor ( >>> item_field = { \"Plot\" : [ 0 , \"tfidf\" ], >>> \"Genre\" : [ 0 , 1 ], >>> \"Director\" : \"doc2vec\" }, >>> regressor = rs . SkRidge ( alpha = 0.8 ), >>> only_greater_eq = 2 ) Info After instantiating the LinearPredictor algorithm, pass it in the initialization of a CBRS and the use its method to predict ratings or calculate ranking for a single user or multiple users: Examples: >>> cbrs = rs.ContentBasedRS(algorithm=alg, ...) >>> cbrs.fit_predict(...) >>> cbrs.fit_rank(...) >>> # ... Parameters: Name Type Description Default item_field dict dict where the key is the name of the field that contains the content to use, value is the representation(s) id(s) that will be used for the said item. The value of a field can be a string or a list, use a list if you want to use multiple representations for a particular field. required regressor Regressor regressor that will be used. Can be one object of the Regressor class. required only_greater_eq float Threshold for the ratings. Only items with rating greater or equal than the threshold will be considered, items with lower rating will be discarded. If None, no item will be filter out None embedding_combiner CombiningTechnique CombiningTechnique used when embeddings representation must be used but they are in a matrix form instead of a single vector (e.g. when WordEmbedding representations must be used you have one vector for each word). By default the Centroid of the rows of the matrix is computed Centroid() Source code in clayrs/recsys/content_based_algorithm/regressor/linear_predictor.py 68 69 70 71 72 73 74 def __init__ ( self , item_field : dict , regressor : Regressor , only_greater_eq : float = None , embedding_combiner : CombiningTechnique = Centroid ()): super () . __init__ ( item_field , only_greater_eq ) self . _regressor = regressor self . _labels : Optional [ list ] = None self . _rated_dict : Optional [ dict ] = None self . _embedding_combiner = embedding_combiner","title":"LinearPredictor"},{"location":"recsys/content_based/content_based_algorithms/linear_predictor/#clayrs.recsys.content_based_algorithm.regressor.linear_predictor.LinearPredictor.fit","text":"Fit the regressor specified in the constructor with the features and labels (rating scores) extracted with the process_rated() method. It uses private attributes to fit the classifier, so process_rated() must be called before this method. Source code in clayrs/recsys/content_based_algorithm/regressor/linear_predictor.py 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 def fit ( self ): \"\"\" Fit the regressor specified in the constructor with the features and labels (rating scores) extracted with the process_rated() method. It uses private attributes to fit the classifier, so process_rated() must be called before this method. \"\"\" rated_features = list ( self . _rated_dict . values ()) # Fuse the input if there are dicts, multiple representation, etc. fused_features = self . fuse_representations ( rated_features , self . _embedding_combiner ) self . _regressor . fit ( fused_features , self . _labels ) # we delete variables used to fit since will no longer be used del self . _labels del self . _rated_dict","title":"fit()"},{"location":"recsys/content_based/content_based_algorithms/linear_predictor/#clayrs.recsys.content_based_algorithm.regressor.linear_predictor.LinearPredictor.predict","text":"Predicts how much a user will like unrated items. One can specify which items must be predicted with the filter_list parameter, in this case ONLY items in the filter_list will be predicted. One can also pass items already seen by the user with the filter_list parameter. Otherwise, ALL unrated items will be predicted. Parameters: Name Type Description Default user_ratings List [ Interaction ] List of Interaction objects for a single user required available_loaded_items LoadedContentsDict The LoadedContents interface which contains loaded contents required filter_list List [ str ] List of the items to predict, if None all unrated items for the user will be predicted None Returns: Type Description List [ Interaction ] List of Interactions object where the 'score' attribute is the rating predicted by the algorithm Source code in clayrs/recsys/content_based_algorithm/regressor/linear_predictor.py 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 def predict ( self , user_ratings : List [ Interaction ], available_loaded_items : LoadedContentsDict , filter_list : List [ str ] = None ) -> List [ Interaction ]: \"\"\" Predicts how much a user will like unrated items. One can specify which items must be predicted with the filter_list parameter, in this case ONLY items in the filter_list will be predicted. One can also pass items already seen by the user with the filter_list parameter. Otherwise, ALL unrated items will be predicted. Args: user_ratings: List of Interaction objects for a single user available_loaded_items: The LoadedContents interface which contains loaded contents filter_list: List of the items to predict, if None all unrated items for the user will be predicted Returns: List of Interactions object where the 'score' attribute is the rating predicted by the algorithm \"\"\" try : user_id = user_ratings [ 0 ] . user_id except IndexError : raise EmptyUserRatings ( \"The user selected doesn't have any ratings!\" ) id_items_to_predict , score_labels = self . _common_prediction_process ( user_ratings , available_loaded_items , filter_list ) # Build the output data pred_interaction_list = [ Interaction ( user_id , item_id , score ) for item_id , score in zip ( id_items_to_predict , score_labels )] return pred_interaction_list","title":"predict()"},{"location":"recsys/content_based/content_based_algorithms/linear_predictor/#clayrs.recsys.content_based_algorithm.regressor.linear_predictor.LinearPredictor.process_rated","text":"Function that extracts features from rated item and labels them. The extracted features will be later used to fit the classifier. Features and labels (in this case the rating score) will be stored in private attributes of the class. IF there are no rated items available locally, an exception is thrown. Parameters: Name Type Description Default user_ratings List [ Interaction ] List of Interaction objects for a single user required available_loaded_items LoadedContentsDict The LoadedContents interface which contains loaded contents required Raises: Type Description NoRatedItems Exception raised when there isn't any item available locally rated by the user Source code in clayrs/recsys/content_based_algorithm/regressor/linear_predictor.py 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 def process_rated ( self , user_ratings : List [ Interaction ], available_loaded_items : LoadedContentsDict ): \"\"\" Function that extracts features from rated item and labels them. The extracted features will be later used to fit the classifier. Features and labels (in this case the rating score) will be stored in private attributes of the class. IF there are no rated items available locally, an exception is thrown. Args: user_ratings: List of Interaction objects for a single user available_loaded_items: The LoadedContents interface which contains loaded contents Raises: NoRatedItems: Exception raised when there isn't any item available locally rated by the user \"\"\" items_scores_dict = { interaction . item_id : interaction . score for interaction in user_ratings } # Create list of all the available items that are useful for the user loaded_rated_items : List [ Union [ Content , None ]] = available_loaded_items . get_list ([ item_id for item_id in set ( items_scores_dict . keys ())]) # Assign label and extract features from the rated items labels = [] rated_dict = {} for item in loaded_rated_items : if item is not None : # This conversion raises Exception when there are multiple equals 'to_id' for the user score_assigned = float ( items_scores_dict [ item . content_id ]) if self . threshold is None : rated_dict [ item ] = self . extract_features_item ( item ) labels . append ( score_assigned ) elif score_assigned >= self . threshold : rated_dict [ item ] = self . extract_features_item ( item ) labels . append ( score_assigned ) if len ( user_ratings ) == 0 : raise EmptyUserRatings ( \"The user selected doesn't have any ratings!\" ) user_id = user_ratings [ 0 ] . user_id if len ( rated_dict ) == 0 : raise NoRatedItems ( \"User {} - No rated item available locally!\" . format ( user_id )) self . _labels = labels self . _rated_dict = rated_dict","title":"process_rated()"},{"location":"recsys/content_based/content_based_algorithms/linear_predictor/#clayrs.recsys.content_based_algorithm.regressor.linear_predictor.LinearPredictor.rank","text":"Rank the top-n recommended items for the user. If the recs_number parameter isn't specified, All unrated items for the user will be ranked (or only items in the filter list, if specified). One can specify which items must be ranked with the filter_list parameter, in this case ONLY items in the filter_list parameter will be ranked. One can also pass items already seen by the user with the filter_list parameter. Otherwise, ALL unrated items will be ranked. Parameters: Name Type Description Default user_ratings List [ Interaction ] List of Interaction objects for a single user required available_loaded_items LoadedContentsDict The LoadedContents interface which contains loaded contents required recs_number int number of the top ranked items to return, if None all ranked items will be returned None filter_list list list of the items to rank, if None all unrated items for the user will be ranked None Returns: Type Description List [ Interaction ] List of Interactions object in a descending order w.r.t the 'score' attribute, representing the ranking for a single user Source code in clayrs/recsys/content_based_algorithm/regressor/linear_predictor.py 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 def rank ( self , user_ratings : List [ Interaction ], available_loaded_items : LoadedContentsDict , recs_number : int = None , filter_list : List [ str ] = None ) -> List [ Interaction ]: \"\"\" Rank the top-n recommended items for the user. If the recs_number parameter isn't specified, All unrated items for the user will be ranked (or only items in the filter list, if specified). One can specify which items must be ranked with the `filter_list` parameter, in this case ONLY items in the `filter_list` parameter will be ranked. One can also pass items already seen by the user with the filter_list parameter. Otherwise, **ALL** unrated items will be ranked. Args: user_ratings: List of Interaction objects for a single user available_loaded_items: The LoadedContents interface which contains loaded contents recs_number: number of the top ranked items to return, if None all ranked items will be returned filter_list (list): list of the items to rank, if None all unrated items for the user will be ranked Returns: List of Interactions object in a descending order w.r.t the 'score' attribute, representing the ranking for a single user \"\"\" try : user_id = user_ratings [ 0 ] . user_id except IndexError : raise EmptyUserRatings ( \"The user selected doesn't have any ratings!\" ) # Predict the rating for the items and sort them in descending order id_items_to_predict , score_labels = self . _common_prediction_process ( user_ratings , available_loaded_items , filter_list ) # Build the item_score dict (key is item_id, value is rank score predicted) # and order the keys in descending order item_score_dict = dict ( zip ( id_items_to_predict , score_labels )) ordered_item_ids = sorted ( item_score_dict , key = item_score_dict . get , reverse = True ) # we only save the top-n items_ids corresponding to top-n recommendations # (if recs_number is None ordered_item_ids will contain all item_ids as the original list) ordered_item_ids = ordered_item_ids [: recs_number ] # we construct the output data rank_interaction_list = [ Interaction ( user_id , item_id , item_score_dict [ item_id ]) for item_id in ordered_item_ids ] return rank_interaction_list","title":"rank()"},{"location":"recsys/content_based/content_based_algorithms/linear_predictor/#regressors-implemented","text":"The following are the regressors you can use in the regressor parameter of the LinearPredictor class","title":"Regressors Implemented"},{"location":"recsys/content_based/content_based_algorithms/linear_predictor/#clayrs.recsys.content_based_algorithm.regressor.regressors.SkARDRegression","text":"Bases: Regressor Class that implements the ARD regressor from sklearn. The parameters one could pass are the same ones you would pass instantiating the regressor ARD directly from sklearn. Sklearn documentation: here Source code in clayrs/recsys/content_based_algorithm/regressor/regressors.py 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 def __init__ ( self , * , n_iter : Any = 300 , tol : Any = 1.0e-3 , alpha_1 : Any = 1.0e-6 , alpha_2 : Any = 1.0e-6 , lambda_1 : Any = 1.0e-6 , lambda_2 : Any = 1.0e-6 , compute_score : Any = False , threshold_lambda : Any = 1.0e4 , fit_intercept : Any = True , normalize : Any = \"deprecated\" , copy_X : Any = True , verbose : Any = False ): model = ARDRegression ( n_iter = n_iter , tol = tol , alpha_1 = alpha_1 , alpha_2 = alpha_2 , lambda_1 = lambda_1 , lambda_2 = lambda_2 , compute_score = compute_score , threshold_lambda = threshold_lambda , fit_intercept = fit_intercept , normalize = normalize , copy_X = copy_X , verbose = verbose ) super () . __init__ ( model , inspect . currentframe ())","title":"SkARDRegression"},{"location":"recsys/content_based/content_based_algorithms/linear_predictor/#clayrs.recsys.content_based_algorithm.regressor.regressors.SkBayesianRidge","text":"Bases: Regressor Class that implements the BayesianRidge regressor from sklearn. The parameters one could pass are the same ones you would pass instantiating the regressor BayesianRidge directly from sklearn. Sklearn documentation: here Source code in clayrs/recsys/content_based_algorithm/regressor/regressors.py 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 def __init__ ( self , * , n_iter : Any = 300 , tol : Any = 1.0e-3 , alpha_1 : Any = 1.0e-6 , alpha_2 : Any = 1.0e-6 , lambda_1 : Any = 1.0e-6 , lambda_2 : Any = 1.0e-6 , alpha_init : Any = None , lambda_init : Any = None , compute_score : Any = False , fit_intercept : Any = True , normalize : Any = \"deprecated\" , copy_X : Any = True , verbose : Any = False ): model = BayesianRidge ( n_iter = n_iter , tol = tol , alpha_1 = alpha_1 , alpha_2 = alpha_2 , lambda_1 = lambda_1 , lambda_2 = lambda_2 , alpha_init = alpha_init , lambda_init = lambda_init , compute_score = compute_score , fit_intercept = fit_intercept , normalize = normalize , copy_X = copy_X , verbose = verbose ) super () . __init__ ( model , inspect . currentframe ())","title":"SkBayesianRidge"},{"location":"recsys/content_based/content_based_algorithms/linear_predictor/#clayrs.recsys.content_based_algorithm.regressor.regressors.SkHuberRegressor","text":"Bases: Regressor Class that implements the Huber regressor from sklearn. The parameters one could pass are the same ones you would pass instantiating the regressor Huber directly from sklearn. Sklearn documentation: here Source code in clayrs/recsys/content_based_algorithm/regressor/regressors.py 256 257 258 259 260 261 262 263 264 265 266 def __init__ ( self , * , epsilon : Any = 1.35 , max_iter : Any = 100 , alpha : Any = 0.0001 , warm_start : Any = False , fit_intercept : Any = True , tol : Any = 1e-05 ): model = HuberRegressor ( epsilon = epsilon , max_iter = max_iter , alpha = alpha , warm_start = warm_start , fit_intercept = fit_intercept , tol = tol ) super () . __init__ ( model , inspect . currentframe ())","title":"SkHuberRegressor"},{"location":"recsys/content_based/content_based_algorithms/linear_predictor/#clayrs.recsys.content_based_algorithm.regressor.regressors.SkLinearRegression","text":"Bases: Regressor Class that implements the LinearRegression regressor from sklearn. The parameters one could pass are the same ones you would pass instantiating the regressor LinearRegression directly from sklearn. Sklearn documentation: here Source code in clayrs/recsys/content_based_algorithm/regressor/regressors.py 80 81 82 83 84 85 86 87 88 def __init__ ( self , * , fit_intercept : Any = True , normalize : Any = \"deprecated\" , copy_X : Any = True , n_jobs : Any = None , positive : Any = False ): model = LinearRegression ( fit_intercept = fit_intercept , normalize = normalize , copy_X = copy_X , n_jobs = n_jobs , positive = positive ) super () . __init__ ( model , inspect . currentframe ())","title":"SkLinearRegression"},{"location":"recsys/content_based/content_based_algorithms/linear_predictor/#clayrs.recsys.content_based_algorithm.regressor.regressors.SkPassiveAggressiveRegressor","text":"Bases: Regressor Class that implements the PassiveAggressive regressor from sklearn. The parameters one could pass are the same ones you would pass instantiating the regressor PassiveAggressive directly from sklearn. Sklearn documentation: here Source code in clayrs/recsys/content_based_algorithm/regressor/regressors.py 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 def __init__ ( self , * , C : Any = 1.0 , fit_intercept : Any = True , max_iter : Any = 1000 , tol : Any = 1e-3 , early_stopping : Any = False , validation_fraction : Any = 0.1 , n_iter_no_change : Any = 5 , shuffle : Any = True , verbose : Any = 0 , loss : Any = \"epsilon_insensitive\" , epsilon : Any = DEFAULT_EPSILON , random_state : Any = None , warm_start : Any = False , average : Any = False ): model = PassiveAggressiveRegressor ( C = C , fit_intercept = fit_intercept , max_iter = max_iter , tol = tol , early_stopping = early_stopping , validation_fraction = validation_fraction , n_iter_no_change = n_iter_no_change , shuffle = shuffle , verbose = verbose , loss = loss , epsilon = epsilon , random_state = random_state , warm_start = warm_start , average = average ) super () . __init__ ( model , inspect . currentframe ())","title":"SkPassiveAggressiveRegressor"},{"location":"recsys/content_based/content_based_algorithms/linear_predictor/#clayrs.recsys.content_based_algorithm.regressor.regressors.SkRidge","text":"Bases: Regressor Class that implements the Ridge regressor from sklearn. The parameters one could pass are the same ones you would pass instantiating the regressor Ridge directly from sklearn. Sklearn documentation: here Source code in clayrs/recsys/content_based_algorithm/regressor/regressors.py 104 105 106 107 108 109 110 111 112 113 114 115 116 117 def __init__ ( self , alpha : Any = 1.0 , * , fit_intercept : Any = True , normalize : Any = \"deprecated\" , copy_X : Any = True , max_iter : Any = None , tol : Any = 1e-3 , solver : Any = \"auto\" , positive : Any = False , random_state : Any = None ): model = Ridge ( alpha = alpha , fit_intercept = fit_intercept , normalize = normalize , copy_X = copy_X , max_iter = max_iter , tol = tol , solver = solver , positive = positive , random_state = random_state ) super () . __init__ ( model , inspect . currentframe ())","title":"SkRidge"},{"location":"recsys/content_based/content_based_algorithms/linear_predictor/#clayrs.recsys.content_based_algorithm.regressor.regressors.SkSGDRegressor","text":"Bases: Regressor Class that implements the SGD regressor from sklearn. The parameters one could pass are the same ones you would pass instantiating the regressor SGD directly from sklearn. Sklearn documentation: here Source code in clayrs/recsys/content_based_algorithm/regressor/regressors.py 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 def __init__ ( self , loss : Any = \"squared_error\" , * , penalty : Any = \"l2\" , alpha : Any = 0.0001 , l1_ratio : Any = 0.15 , fit_intercept : Any = True , max_iter : Any = 1000 , tol : Any = 1e-3 , shuffle : Any = True , verbose : Any = 0 , epsilon : Any = DEFAULT_EPSILON , random_state : Any = None , learning_rate : Any = \"invscaling\" , eta0 : Any = 0.01 , power_t : Any = 0.25 , early_stopping : Any = False , validation_fraction : Any = 0.1 , n_iter_no_change : Any = 5 , warm_start : Any = False , average : Any = False ): model = SGDRegressor ( loss = loss , penalty = penalty , alpha = alpha , l1_ratio = l1_ratio , fit_intercept = fit_intercept , max_iter = max_iter , tol = tol , shuffle = shuffle , verbose = verbose , epsilon = epsilon , random_state = random_state , learning_rate = learning_rate , eta0 = eta0 , power_t = power_t , early_stopping = early_stopping , validation_fraction = validation_fraction , n_iter_no_change = n_iter_no_change , warm_start = warm_start , average = average ) super () . __init__ ( model , inspect . currentframe ())","title":"SkSGDRegressor"},{"location":"recsys/graph_based/feature_selection/","text":"Feature Selection Via the feature_selecter function you are able to perform feature selection on a given graph, by keeping properties that are the most important according to a given feature selection algorithm . Check the documentation of the method for more and for a usage example feature_selector ( graph , fs_algorithm_user = None , fs_algorithm_item = None , user_target_nodes = None , item_target_nodes = None , inplace = False ) Given a FullGraph, this method performs feature selection on it and returns the \"reduced\" graph. You can choose to reduce only user properties ( evaluate the fs_algorithm_user parameter ), to reduce only item properties ( evaluate the fs_algorithm_item parameter ) or both ( evaluate the fs_algorithm_user parameter and the fs_algorithm_item parameter ). You can also choose different feature selection algorithms* for users and items. You can also define a custom list of user and item nodes: In this case only properties of those nodes will be considered during the feature selection process (instead of using properties of all users and items) This function changes a copy of the original graph by default, but you can change this behaviour with the inplace parameter. Examples: # create a full graph full_graph = rs . NXFullGraph ( ratings , user_contents_dir = 'users_codified/' , # (1) item_contents_dir = 'movies_codified/' , # (2) user_exo_properties = { 0 }, # (3) item_exo_properties = { 'dbpedia' }, # (4) link_label = 'score' ) # perform feature selection by keeping only top 5 property labels # according to page rank algorithm fs_graph = rs . feature_selector ( full_graph , fs_algorithm_item = rs . TopKPageRank ( k = 5 )) Parameters: Name Type Description Default graph FullDiGraph Original graph on which feature selection will be performed required fs_algorithm_user FeatureSelectionAlgorithm FeatureSelectionAlgorithm that will be performed on user properties. Can be different from fs_algorithm_item None fs_algorithm_item FeatureSelectionAlgorithm FeatureSelectionAlgorithm that will be performed on item properties. Can be different from fs_algorithm_user None user_target_nodes list List of user nodes to consider in the feature selection process: only properties of user nodes in this list will be \"reduced\" None item_target_nodes list List of item nodes to consider in the feature selection process: only properties of item nodes in this list will be \"reduced\" None inplace bool Boolean parameter that let you choose if changes must be performed on the original graph ( inplace=True ) or on its copy ( inplace=False ). Default is False False Returns: Type Description FullDiGraph Copy of the original graph from which the less important Property nodes (the ones having edges with less FullDiGraph important property labels) will be removed Source code in clayrs/recsys/graphs/feature_selection/feature_selection_fn.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 def feature_selector ( graph : FullDiGraph , fs_algorithm_user : FeatureSelectionAlgorithm = None , fs_algorithm_item : FeatureSelectionAlgorithm = None , user_target_nodes : Iterable [ UserNode ] = None , item_target_nodes : Iterable [ ItemNode ] = None , inplace : bool = False ) -> FullDiGraph : \"\"\" Given a FullGraph, this method performs feature selection on it and returns the \"reduced\" graph. You can choose to reduce only *user properties* (*evaluate the `fs_algorithm_user` parameter*), to reduce only *item properties* (*evaluate the `fs_algorithm_item` parameter*) or both (*evaluate the `fs_algorithm_user` parameter* and the `fs_algorithm_item` parameter*). You can also choose different *feature selection algorithms* for users and items. You can also define a custom list of user and item nodes: * In this case only properties of those nodes will be considered during the feature selection process (instead of using properties of all users and items) This function changes a *copy* of the original graph by default, but you can change this behaviour with the `inplace` parameter. Examples: ```python # create a full graph full_graph = rs.NXFullGraph(ratings, user_contents_dir='users_codified/', # (1) item_contents_dir='movies_codified/', # (2) user_exo_properties={0}, # (3) item_exo_properties={'dbpedia'}, # (4) link_label='score') # perform feature selection by keeping only top 5 property labels # according to page rank algorithm fs_graph = rs.feature_selector(full_graph, fs_algorithm_item=rs.TopKPageRank(k=5)) ``` Args: graph: Original graph on which feature selection will be performed fs_algorithm_user: FeatureSelectionAlgorithm that will be performed on user properties. Can be different from `fs_algorithm_item` fs_algorithm_item: FeatureSelectionAlgorithm that will be performed on item properties. Can be different from `fs_algorithm_user` user_target_nodes (list): List of user nodes to consider in the feature selection process: only properties of user nodes in this list will be \"reduced\" item_target_nodes (list): List of item nodes to consider in the feature selection process: only properties of item nodes in this list will be \"reduced\" inplace: Boolean parameter that let you choose if changes must be performed on the original graph (`inplace=True`) or on its copy (`inplace=False`). Default is False Returns: Copy of the original graph from which the less important Property nodes (the ones having edges with less important property labels) will be removed \"\"\" if fs_algorithm_user is not None and user_target_nodes is None : user_target_nodes = graph . user_nodes if fs_algorithm_item is not None and item_target_nodes is None : item_target_nodes = graph . item_nodes property_labels_to_remove = list () user_fs_failed = False item_fs_failed = False if fs_algorithm_user is not None : logger . info ( \"Performing Feature Selection on users\" ) try : user_props_to_remove = fs_algorithm_user . perform ( graph , list ( user_target_nodes ), mode = 'to_remove' ) property_labels_to_remove . extend ( user_props_to_remove ) except FeatureSelectionException as e : logger . warning ( str ( e ) + \"! \\n Users original properties will be kept\" ) user_fs_failed = True if fs_algorithm_item is not None : logger . info ( \"Performing Feature Selection on items\" ) try : item_props_to_remove = fs_algorithm_item . perform ( graph , list ( item_target_nodes ), mode = 'to_remove' ) property_labels_to_remove . extend ( item_props_to_remove ) except FeatureSelectionException as e : logger . warning ( str ( e ) + \"! \\n Items original properties will be kept\" ) item_fs_failed = True # in case user feature selection or item feature selection failed # if both failed the original graph is returned # if only one of them failed, the original properties (either for items or users) are retrieved if user_fs_failed and item_fs_failed : logger . warning ( \"Since both feature selection on items and feature selection on users failed or no fs algorithm\" \"has been defined, \\n the original graph will be returned\" ) if inplace is True : graph_fs = _delete_property_nodes ( graph , property_labels_to_remove ) else : graph_copy = graph . copy () graph_fs = _delete_property_nodes ( graph_copy , property_labels_to_remove ) return graph_fs Feature Selection algorithms The following are the feature selection algorithms you can use in the fs_algorithms_user and/or in the fs_algorithm_item TopKPageRank ( k = 10 , alpha = 0.85 , personalization = None , max_iter = 100 , tol = 1e-06 , nstart = None , weight = True , dangling = None ) Bases: TopKFeatureSelection Computes the PageRank as FeatureSelection algorithm. Property labels of the original graph will be scored with their page rank score and only the top-k labels will be kept in the feature selected graph , while discarding the others Parameters: Name Type Description Default k int Top-k property labels to keep in the feature selected graph 10 alpha Any Damping parameter for PageRank, default=0.85. 0.85 personalization Any The \"personalization vector\" consisting of a dictionary with a key some subset of graph nodes and personalization value each of those. At least one personalization value must be non-zero. If not specfiied, a nodes personalization value will be zero. By default, a uniform distribution is used. None max_iter Any Maximum number of iterations in power method eigenvalue solver. 100 tol Any Error tolerance used to check convergence in power method solver. 1e-06 nstart Any Starting value of PageRank iteration for each node. None weight bool Edge data key to use as weight. If None weights are set to 1. True dangling Any The outedges to be assigned to any \"dangling\" nodes, i.e., nodes without any outedges. The dict key is the node the outedge points to and the dict value is the weight of that outedge. By default, dangling nodes are given outedges according to the personalization vector (uniform if not specified). This must be selected to result in an irreducible transition matrix (see notes under google_matrix). It may be common to have the dangling dict to be the same as the personalization dict. None Source code in clayrs/recsys/graphs/feature_selection/feature_selection_alg.py 199 200 201 202 203 204 205 206 207 208 209 def __init__ ( self , k : int = 10 , alpha : Any = 0.85 , personalization : Any = None , max_iter : Any = 100 , tol : Any = 1.0e-6 , nstart : Any = None , weight : bool = True , dangling : Any = None ): super () . __init__ ( k ) self . alpha = alpha self . personalization = personalization self . max_iter = max_iter self . tol = tol self . nstart = nstart self . weight = 'weight' if weight is True else None self . dangling = dangling TopKEigenVectorCentrality ( k = 10 , max_iter = 100 , tol = 1e-06 , nstart = None , weight = False ) Bases: TopKFeatureSelection Computes the Eigen Vector Centrality as FeatureSelection algorithm. Property labels of the original graph will be scored with their eigen vector centrality score and only the top-k labels will be kept in the feature selected graph , while discarding the others Parameters: Name Type Description Default k int Top-k property labels to keep in the feature selected graph 10 max_iter Any Maximum number of iterations in power method. 100 tol Any Error tolerance used to check convergence in power method iteration. 1e-06 nstart Any Starting value of eigenvector iteration for each node. None weight bool Boolean value which tells the algorithm if weight of the edges must be considered or not. Default is True False Source code in clayrs/recsys/graphs/feature_selection/feature_selection_alg.py 232 233 234 235 236 237 238 def __init__ ( self , k : int = 10 , max_iter : Any = 100 , tol : Any = 1.0e-6 , nstart : Any = None , weight : bool = False ): super () . __init__ ( k ) self . max_iter = max_iter self . tol = tol self . nstart = nstart self . weight = 'weight' if weight is True else None TopKDegreeCentrality ( k = 10 ) Bases: TopKFeatureSelection Computes the Degree Centrality as FeatureSelection algorithm. Property labels of the original graph will be scored with their degree centrality score and only the top-k labels will be kept in the feature selected graph , while discarding the others Source code in clayrs/recsys/graphs/feature_selection/feature_selection_alg.py 252 253 def __init__ ( self , k : int = 10 ): super () . __init__ ( k )","title":"Feature Selection"},{"location":"recsys/graph_based/feature_selection/#feature-selection","text":"Via the feature_selecter function you are able to perform feature selection on a given graph, by keeping properties that are the most important according to a given feature selection algorithm . Check the documentation of the method for more and for a usage example","title":"Feature Selection"},{"location":"recsys/graph_based/feature_selection/#clayrs.recsys.graphs.feature_selection.feature_selection_fn.feature_selector","text":"Given a FullGraph, this method performs feature selection on it and returns the \"reduced\" graph. You can choose to reduce only user properties ( evaluate the fs_algorithm_user parameter ), to reduce only item properties ( evaluate the fs_algorithm_item parameter ) or both ( evaluate the fs_algorithm_user parameter and the fs_algorithm_item parameter ). You can also choose different feature selection algorithms* for users and items. You can also define a custom list of user and item nodes: In this case only properties of those nodes will be considered during the feature selection process (instead of using properties of all users and items) This function changes a copy of the original graph by default, but you can change this behaviour with the inplace parameter. Examples: # create a full graph full_graph = rs . NXFullGraph ( ratings , user_contents_dir = 'users_codified/' , # (1) item_contents_dir = 'movies_codified/' , # (2) user_exo_properties = { 0 }, # (3) item_exo_properties = { 'dbpedia' }, # (4) link_label = 'score' ) # perform feature selection by keeping only top 5 property labels # according to page rank algorithm fs_graph = rs . feature_selector ( full_graph , fs_algorithm_item = rs . TopKPageRank ( k = 5 )) Parameters: Name Type Description Default graph FullDiGraph Original graph on which feature selection will be performed required fs_algorithm_user FeatureSelectionAlgorithm FeatureSelectionAlgorithm that will be performed on user properties. Can be different from fs_algorithm_item None fs_algorithm_item FeatureSelectionAlgorithm FeatureSelectionAlgorithm that will be performed on item properties. Can be different from fs_algorithm_user None user_target_nodes list List of user nodes to consider in the feature selection process: only properties of user nodes in this list will be \"reduced\" None item_target_nodes list List of item nodes to consider in the feature selection process: only properties of item nodes in this list will be \"reduced\" None inplace bool Boolean parameter that let you choose if changes must be performed on the original graph ( inplace=True ) or on its copy ( inplace=False ). Default is False False Returns: Type Description FullDiGraph Copy of the original graph from which the less important Property nodes (the ones having edges with less FullDiGraph important property labels) will be removed Source code in clayrs/recsys/graphs/feature_selection/feature_selection_fn.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 def feature_selector ( graph : FullDiGraph , fs_algorithm_user : FeatureSelectionAlgorithm = None , fs_algorithm_item : FeatureSelectionAlgorithm = None , user_target_nodes : Iterable [ UserNode ] = None , item_target_nodes : Iterable [ ItemNode ] = None , inplace : bool = False ) -> FullDiGraph : \"\"\" Given a FullGraph, this method performs feature selection on it and returns the \"reduced\" graph. You can choose to reduce only *user properties* (*evaluate the `fs_algorithm_user` parameter*), to reduce only *item properties* (*evaluate the `fs_algorithm_item` parameter*) or both (*evaluate the `fs_algorithm_user` parameter* and the `fs_algorithm_item` parameter*). You can also choose different *feature selection algorithms* for users and items. You can also define a custom list of user and item nodes: * In this case only properties of those nodes will be considered during the feature selection process (instead of using properties of all users and items) This function changes a *copy* of the original graph by default, but you can change this behaviour with the `inplace` parameter. Examples: ```python # create a full graph full_graph = rs.NXFullGraph(ratings, user_contents_dir='users_codified/', # (1) item_contents_dir='movies_codified/', # (2) user_exo_properties={0}, # (3) item_exo_properties={'dbpedia'}, # (4) link_label='score') # perform feature selection by keeping only top 5 property labels # according to page rank algorithm fs_graph = rs.feature_selector(full_graph, fs_algorithm_item=rs.TopKPageRank(k=5)) ``` Args: graph: Original graph on which feature selection will be performed fs_algorithm_user: FeatureSelectionAlgorithm that will be performed on user properties. Can be different from `fs_algorithm_item` fs_algorithm_item: FeatureSelectionAlgorithm that will be performed on item properties. Can be different from `fs_algorithm_user` user_target_nodes (list): List of user nodes to consider in the feature selection process: only properties of user nodes in this list will be \"reduced\" item_target_nodes (list): List of item nodes to consider in the feature selection process: only properties of item nodes in this list will be \"reduced\" inplace: Boolean parameter that let you choose if changes must be performed on the original graph (`inplace=True`) or on its copy (`inplace=False`). Default is False Returns: Copy of the original graph from which the less important Property nodes (the ones having edges with less important property labels) will be removed \"\"\" if fs_algorithm_user is not None and user_target_nodes is None : user_target_nodes = graph . user_nodes if fs_algorithm_item is not None and item_target_nodes is None : item_target_nodes = graph . item_nodes property_labels_to_remove = list () user_fs_failed = False item_fs_failed = False if fs_algorithm_user is not None : logger . info ( \"Performing Feature Selection on users\" ) try : user_props_to_remove = fs_algorithm_user . perform ( graph , list ( user_target_nodes ), mode = 'to_remove' ) property_labels_to_remove . extend ( user_props_to_remove ) except FeatureSelectionException as e : logger . warning ( str ( e ) + \"! \\n Users original properties will be kept\" ) user_fs_failed = True if fs_algorithm_item is not None : logger . info ( \"Performing Feature Selection on items\" ) try : item_props_to_remove = fs_algorithm_item . perform ( graph , list ( item_target_nodes ), mode = 'to_remove' ) property_labels_to_remove . extend ( item_props_to_remove ) except FeatureSelectionException as e : logger . warning ( str ( e ) + \"! \\n Items original properties will be kept\" ) item_fs_failed = True # in case user feature selection or item feature selection failed # if both failed the original graph is returned # if only one of them failed, the original properties (either for items or users) are retrieved if user_fs_failed and item_fs_failed : logger . warning ( \"Since both feature selection on items and feature selection on users failed or no fs algorithm\" \"has been defined, \\n the original graph will be returned\" ) if inplace is True : graph_fs = _delete_property_nodes ( graph , property_labels_to_remove ) else : graph_copy = graph . copy () graph_fs = _delete_property_nodes ( graph_copy , property_labels_to_remove ) return graph_fs","title":"feature_selector()"},{"location":"recsys/graph_based/feature_selection/#feature-selection-algorithms","text":"The following are the feature selection algorithms you can use in the fs_algorithms_user and/or in the fs_algorithm_item","title":"Feature Selection algorithms"},{"location":"recsys/graph_based/feature_selection/#clayrs.recsys.graphs.feature_selection.feature_selection_alg.TopKPageRank","text":"Bases: TopKFeatureSelection Computes the PageRank as FeatureSelection algorithm. Property labels of the original graph will be scored with their page rank score and only the top-k labels will be kept in the feature selected graph , while discarding the others Parameters: Name Type Description Default k int Top-k property labels to keep in the feature selected graph 10 alpha Any Damping parameter for PageRank, default=0.85. 0.85 personalization Any The \"personalization vector\" consisting of a dictionary with a key some subset of graph nodes and personalization value each of those. At least one personalization value must be non-zero. If not specfiied, a nodes personalization value will be zero. By default, a uniform distribution is used. None max_iter Any Maximum number of iterations in power method eigenvalue solver. 100 tol Any Error tolerance used to check convergence in power method solver. 1e-06 nstart Any Starting value of PageRank iteration for each node. None weight bool Edge data key to use as weight. If None weights are set to 1. True dangling Any The outedges to be assigned to any \"dangling\" nodes, i.e., nodes without any outedges. The dict key is the node the outedge points to and the dict value is the weight of that outedge. By default, dangling nodes are given outedges according to the personalization vector (uniform if not specified). This must be selected to result in an irreducible transition matrix (see notes under google_matrix). It may be common to have the dangling dict to be the same as the personalization dict. None Source code in clayrs/recsys/graphs/feature_selection/feature_selection_alg.py 199 200 201 202 203 204 205 206 207 208 209 def __init__ ( self , k : int = 10 , alpha : Any = 0.85 , personalization : Any = None , max_iter : Any = 100 , tol : Any = 1.0e-6 , nstart : Any = None , weight : bool = True , dangling : Any = None ): super () . __init__ ( k ) self . alpha = alpha self . personalization = personalization self . max_iter = max_iter self . tol = tol self . nstart = nstart self . weight = 'weight' if weight is True else None self . dangling = dangling","title":"TopKPageRank"},{"location":"recsys/graph_based/feature_selection/#clayrs.recsys.graphs.feature_selection.feature_selection_alg.TopKEigenVectorCentrality","text":"Bases: TopKFeatureSelection Computes the Eigen Vector Centrality as FeatureSelection algorithm. Property labels of the original graph will be scored with their eigen vector centrality score and only the top-k labels will be kept in the feature selected graph , while discarding the others Parameters: Name Type Description Default k int Top-k property labels to keep in the feature selected graph 10 max_iter Any Maximum number of iterations in power method. 100 tol Any Error tolerance used to check convergence in power method iteration. 1e-06 nstart Any Starting value of eigenvector iteration for each node. None weight bool Boolean value which tells the algorithm if weight of the edges must be considered or not. Default is True False Source code in clayrs/recsys/graphs/feature_selection/feature_selection_alg.py 232 233 234 235 236 237 238 def __init__ ( self , k : int = 10 , max_iter : Any = 100 , tol : Any = 1.0e-6 , nstart : Any = None , weight : bool = False ): super () . __init__ ( k ) self . max_iter = max_iter self . tol = tol self . nstart = nstart self . weight = 'weight' if weight is True else None","title":"TopKEigenVectorCentrality"},{"location":"recsys/graph_based/feature_selection/#clayrs.recsys.graphs.feature_selection.feature_selection_alg.TopKDegreeCentrality","text":"Bases: TopKFeatureSelection Computes the Degree Centrality as FeatureSelection algorithm. Property labels of the original graph will be scored with their degree centrality score and only the top-k labels will be kept in the feature selected graph , while discarding the others Source code in clayrs/recsys/graphs/feature_selection/feature_selection_alg.py 252 253 def __init__ ( self , k : int = 10 ): super () . __init__ ( k )","title":"TopKDegreeCentrality"},{"location":"recsys/graph_based/graph_based_recsys/","text":"Graph Based RecSys GraphBasedRS ( algorithm , graph ) Bases: RecSys Class for recommender systems which use a graph in order to make predictions Every GBRS differ from each other based the algorithm used. Examples: In case you perform a splitting of the dataset which returns a single train and test set (e.g. HoldOut technique): Single split train from clayrs import recsys as rs from clayrs import content_analyzer as ca original_rat = ca . Ratings ( ca . CSVFile ( ratings_path )) [ train ], [ test ] = rs . HoldOutPartitioning () . split_all ( original_rat ) alg = rs . NXPageRank () # any gb algorithm graph = rs . NXBipartiteGraph ( train ) gbrs = rs . GraphBasedRS ( alg , graph ) rank = gbrs . rank ( test , n_recs = 10 ) In case you perform a splitting of the dataset which returns a multiple train and test sets (KFold technique): Multiple split train from clayrs import recsys as rs from clayrs import content_analyzer as ca original_rat = ca . Ratings ( ca . CSVFile ( ratings_path )) train_list , test_list = rs . KFoldPartitioning ( n_splits = 5 ) . split_all ( original_rat ) alg = rs . NXPageRank () # any gb algorithm for train_set , test_set in zip ( train_list , test_list ): graph = rs . NXBipartiteGraph ( train_set ) gbrs = rs . GraphBasedRS ( alg , graph ) rank_to_append = gbrs . rank ( test_set ) result_list . append ( rank_to_append ) result_list will contain recommendation lists for each split Parameters: Name Type Description Default algorithm GraphBasedAlgorithm the graph based algorithm that will be used in order to rank or make score prediction required graph FullDiGraph a Graph object containing interactions required Source code in clayrs/recsys/recsys.py 580 581 582 583 584 585 def __init__ ( self , algorithm : GraphBasedAlgorithm , graph : FullDiGraph ): self . __graph = graph super () . __init__ ( algorithm ) algorithm () property The graph based algorithm chosen Source code in clayrs/recsys/recsys.py 598 599 600 601 602 603 604 @property def algorithm ( self ): \"\"\" The graph based algorithm chosen \"\"\" alg : GraphBasedAlgorithm = super () . algorithm return alg graph () property The graph containing interactions Source code in clayrs/recsys/recsys.py 591 592 593 594 595 596 @property def graph ( self ): \"\"\" The graph containing interactions \"\"\" return self . __graph predict ( test_set , user_id_list = None , methodology = TestRatingsMethodology ()) Method used to calculate score predictions for all users in test set or all users in user_id_list parameter. BE CAREFUL : not all algorithms are able to perform score prediction Via the methodology parameter you can perform different candidate item selection. By default the TestRatingsMethodology() is used, so for each user items in its test set only will be considered for score prediction If the algorithm was not fit for some users, they will be skipped and a warning is printed Parameters: Name Type Description Default test_set Ratings Ratings object which represents the ground truth of the split considered required user_id_list List List of users for which you want to compute score prediction. If None, the ranking will be computed for all users of the test_set None methodology Union [ Methodology , None] Methodology object which governs the candidate item selection. Default is TestRatingsMethodology TestRatingsMethodology() Returns: Type Description Prediction Prediction object containing score prediction lists for all users of the test set or for all users in user_id_list Source code in clayrs/recsys/recsys.py 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 def predict ( self , test_set : Ratings , user_id_list : List = None , methodology : Union [ Methodology , None ] = TestRatingsMethodology ()) -> Prediction : \"\"\" Method used to calculate score predictions for all users in test set or all users in `user_id_list` parameter. **BE CAREFUL**: not all algorithms are able to perform *score prediction* Via the `methodology` parameter you can perform different candidate item selection. By default the `TestRatingsMethodology()` is used, so for each user items in its test set only will be considered for score prediction If the algorithm was not fit for some users, they will be skipped and a warning is printed Args: test_set: Ratings object which represents the ground truth of the split considered user_id_list: List of users for which you want to compute score prediction. If None, the ranking will be computed for all users of the `test_set` methodology: `Methodology` object which governs the candidate item selection. Default is `TestRatingsMethodology` Returns: Prediction object containing score prediction lists for all users of the test set or for all users in `user_id_list` \"\"\" all_users = set ( test_set . user_id_column ) if user_id_list is not None : all_users = set ( user_id_list ) filter_dict : Union [ Dict , None ] = None if methodology is not None : train_set = self . graph . to_ratings () filter_dict = methodology . filter_all ( train_set , test_set , result_as_iter_dict = True ) total_predict_list = self . algorithm . predict ( all_users , self . graph , filter_dict ) total_predict = Prediction . from_list ( total_predict_list ) self . _yaml_report = { 'graph' : repr ( self . graph ), 'mode' : 'score_prediction' , 'methodology' : repr ( methodology )} return total_predict rank ( test_set , n_recs = None , user_id_list = None , methodology = TestRatingsMethodology ()) Method used to calculate ranking for all users in test set or all users in user_id_list parameter. You must first call the fit() method before you can compute the ranking. If the n_recs is specified, then the rank will contain the top-n items for the users. Otherwise the rank will contain all unrated items of the particular users Via the methodology parameter you can perform different candidate item selection. By default the TestRatingsMethodology() is used, so for each user items in its test set only will be ranked If the algorithm was not fit for some users, they will be skipped and a warning is printed Parameters: Name Type Description Default test_set Ratings Ratings object which represents the ground truth of the split considered required n_recs int Number of the top items that will be present in the ranking. If None all candidate items will be returned for the user None user_id_list List List of users for which you want to compute the ranking. If None, the ranking will be computed for all users of the test_set None methodology Union [ Methodology , None] Methodology object which governs the candidate item selection. Default is TestRatingsMethodology TestRatingsMethodology() Raises: Type Description NotFittedAlg Exception raised when this method is called without first calling the fit method Returns: Type Description Rank Rank object containing recommendation lists for all users of the test set or for all users in user_id_list Source code in clayrs/recsys/recsys.py 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 def rank ( self , test_set : Ratings , n_recs : int = None , user_id_list : List = None , methodology : Union [ Methodology , None ] = TestRatingsMethodology ()) -> Rank : \"\"\" Method used to calculate ranking for all users in test set or all users in `user_id_list` parameter. You must first call the `fit()` method before you can compute the ranking. If the `n_recs` is specified, then the rank will contain the top-n items for the users. Otherwise the rank will contain all unrated items of the particular users Via the `methodology` parameter you can perform different candidate item selection. By default the `TestRatingsMethodology()` is used, so for each user items in its test set only will be ranked If the algorithm was not fit for some users, they will be skipped and a warning is printed Args: test_set: Ratings object which represents the ground truth of the split considered n_recs: Number of the top items that will be present in the ranking. If None all candidate items will be returned for the user user_id_list: List of users for which you want to compute the ranking. If None, the ranking will be computed for all users of the `test_set` methodology: `Methodology` object which governs the candidate item selection. Default is `TestRatingsMethodology` Raises: NotFittedAlg: Exception raised when this method is called without first calling the `fit` method Returns: Rank object containing recommendation lists for all users of the test set or for all users in `user_id_list` \"\"\" all_users = set ( test_set . user_id_column ) if user_id_list is not None : all_users = set ( user_id_list ) filter_dict : Union [ Dict , None ] = None if methodology is not None : train_set = self . graph . to_ratings () filter_dict = methodology . filter_all ( train_set , test_set , result_as_iter_dict = True ) total_rank_list = self . algorithm . rank ( all_users , self . graph , n_recs , filter_dict ) total_rank = Rank . from_list ( total_rank_list ) if len ( total_rank ) == 0 : logger . warning ( \"No items could be ranked for any users! Remember that items to rank must be present \" \"in the graph. \\n \" \"Try changing methodology!\" ) elif len ( set ( total_rank . user_id_column )) != len ( all_users ): logger . warning ( f \"No items could be ranked for users { all_users - set ( total_rank . user_id_column ) } \\n \" f \"No nodes to rank for them found in the graph. Try changing methodology! \" ) self . _yaml_report = { 'graph' : repr ( self . graph ), 'mode' : 'rank' , 'n_recs' : repr ( n_recs ), 'methodology' : repr ( methodology )} return total_rank","title":"Graph Based recsys"},{"location":"recsys/graph_based/graph_based_recsys/#graph-based-recsys","text":"","title":"Graph Based RecSys"},{"location":"recsys/graph_based/graph_based_recsys/#clayrs.recsys.recsys.GraphBasedRS","text":"Bases: RecSys Class for recommender systems which use a graph in order to make predictions Every GBRS differ from each other based the algorithm used. Examples: In case you perform a splitting of the dataset which returns a single train and test set (e.g. HoldOut technique): Single split train from clayrs import recsys as rs from clayrs import content_analyzer as ca original_rat = ca . Ratings ( ca . CSVFile ( ratings_path )) [ train ], [ test ] = rs . HoldOutPartitioning () . split_all ( original_rat ) alg = rs . NXPageRank () # any gb algorithm graph = rs . NXBipartiteGraph ( train ) gbrs = rs . GraphBasedRS ( alg , graph ) rank = gbrs . rank ( test , n_recs = 10 ) In case you perform a splitting of the dataset which returns a multiple train and test sets (KFold technique): Multiple split train from clayrs import recsys as rs from clayrs import content_analyzer as ca original_rat = ca . Ratings ( ca . CSVFile ( ratings_path )) train_list , test_list = rs . KFoldPartitioning ( n_splits = 5 ) . split_all ( original_rat ) alg = rs . NXPageRank () # any gb algorithm for train_set , test_set in zip ( train_list , test_list ): graph = rs . NXBipartiteGraph ( train_set ) gbrs = rs . GraphBasedRS ( alg , graph ) rank_to_append = gbrs . rank ( test_set ) result_list . append ( rank_to_append ) result_list will contain recommendation lists for each split Parameters: Name Type Description Default algorithm GraphBasedAlgorithm the graph based algorithm that will be used in order to rank or make score prediction required graph FullDiGraph a Graph object containing interactions required Source code in clayrs/recsys/recsys.py 580 581 582 583 584 585 def __init__ ( self , algorithm : GraphBasedAlgorithm , graph : FullDiGraph ): self . __graph = graph super () . __init__ ( algorithm )","title":"GraphBasedRS"},{"location":"recsys/graph_based/graph_based_recsys/#clayrs.recsys.recsys.GraphBasedRS.algorithm","text":"The graph based algorithm chosen Source code in clayrs/recsys/recsys.py 598 599 600 601 602 603 604 @property def algorithm ( self ): \"\"\" The graph based algorithm chosen \"\"\" alg : GraphBasedAlgorithm = super () . algorithm return alg","title":"algorithm()"},{"location":"recsys/graph_based/graph_based_recsys/#clayrs.recsys.recsys.GraphBasedRS.graph","text":"The graph containing interactions Source code in clayrs/recsys/recsys.py 591 592 593 594 595 596 @property def graph ( self ): \"\"\" The graph containing interactions \"\"\" return self . __graph","title":"graph()"},{"location":"recsys/graph_based/graph_based_recsys/#clayrs.recsys.recsys.GraphBasedRS.predict","text":"Method used to calculate score predictions for all users in test set or all users in user_id_list parameter. BE CAREFUL : not all algorithms are able to perform score prediction Via the methodology parameter you can perform different candidate item selection. By default the TestRatingsMethodology() is used, so for each user items in its test set only will be considered for score prediction If the algorithm was not fit for some users, they will be skipped and a warning is printed Parameters: Name Type Description Default test_set Ratings Ratings object which represents the ground truth of the split considered required user_id_list List List of users for which you want to compute score prediction. If None, the ranking will be computed for all users of the test_set None methodology Union [ Methodology , None] Methodology object which governs the candidate item selection. Default is TestRatingsMethodology TestRatingsMethodology() Returns: Type Description Prediction Prediction object containing score prediction lists for all users of the test set or for all users in user_id_list Source code in clayrs/recsys/recsys.py 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 def predict ( self , test_set : Ratings , user_id_list : List = None , methodology : Union [ Methodology , None ] = TestRatingsMethodology ()) -> Prediction : \"\"\" Method used to calculate score predictions for all users in test set or all users in `user_id_list` parameter. **BE CAREFUL**: not all algorithms are able to perform *score prediction* Via the `methodology` parameter you can perform different candidate item selection. By default the `TestRatingsMethodology()` is used, so for each user items in its test set only will be considered for score prediction If the algorithm was not fit for some users, they will be skipped and a warning is printed Args: test_set: Ratings object which represents the ground truth of the split considered user_id_list: List of users for which you want to compute score prediction. If None, the ranking will be computed for all users of the `test_set` methodology: `Methodology` object which governs the candidate item selection. Default is `TestRatingsMethodology` Returns: Prediction object containing score prediction lists for all users of the test set or for all users in `user_id_list` \"\"\" all_users = set ( test_set . user_id_column ) if user_id_list is not None : all_users = set ( user_id_list ) filter_dict : Union [ Dict , None ] = None if methodology is not None : train_set = self . graph . to_ratings () filter_dict = methodology . filter_all ( train_set , test_set , result_as_iter_dict = True ) total_predict_list = self . algorithm . predict ( all_users , self . graph , filter_dict ) total_predict = Prediction . from_list ( total_predict_list ) self . _yaml_report = { 'graph' : repr ( self . graph ), 'mode' : 'score_prediction' , 'methodology' : repr ( methodology )} return total_predict","title":"predict()"},{"location":"recsys/graph_based/graph_based_recsys/#clayrs.recsys.recsys.GraphBasedRS.rank","text":"Method used to calculate ranking for all users in test set or all users in user_id_list parameter. You must first call the fit() method before you can compute the ranking. If the n_recs is specified, then the rank will contain the top-n items for the users. Otherwise the rank will contain all unrated items of the particular users Via the methodology parameter you can perform different candidate item selection. By default the TestRatingsMethodology() is used, so for each user items in its test set only will be ranked If the algorithm was not fit for some users, they will be skipped and a warning is printed Parameters: Name Type Description Default test_set Ratings Ratings object which represents the ground truth of the split considered required n_recs int Number of the top items that will be present in the ranking. If None all candidate items will be returned for the user None user_id_list List List of users for which you want to compute the ranking. If None, the ranking will be computed for all users of the test_set None methodology Union [ Methodology , None] Methodology object which governs the candidate item selection. Default is TestRatingsMethodology TestRatingsMethodology() Raises: Type Description NotFittedAlg Exception raised when this method is called without first calling the fit method Returns: Type Description Rank Rank object containing recommendation lists for all users of the test set or for all users in user_id_list Source code in clayrs/recsys/recsys.py 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 def rank ( self , test_set : Ratings , n_recs : int = None , user_id_list : List = None , methodology : Union [ Methodology , None ] = TestRatingsMethodology ()) -> Rank : \"\"\" Method used to calculate ranking for all users in test set or all users in `user_id_list` parameter. You must first call the `fit()` method before you can compute the ranking. If the `n_recs` is specified, then the rank will contain the top-n items for the users. Otherwise the rank will contain all unrated items of the particular users Via the `methodology` parameter you can perform different candidate item selection. By default the `TestRatingsMethodology()` is used, so for each user items in its test set only will be ranked If the algorithm was not fit for some users, they will be skipped and a warning is printed Args: test_set: Ratings object which represents the ground truth of the split considered n_recs: Number of the top items that will be present in the ranking. If None all candidate items will be returned for the user user_id_list: List of users for which you want to compute the ranking. If None, the ranking will be computed for all users of the `test_set` methodology: `Methodology` object which governs the candidate item selection. Default is `TestRatingsMethodology` Raises: NotFittedAlg: Exception raised when this method is called without first calling the `fit` method Returns: Rank object containing recommendation lists for all users of the test set or for all users in `user_id_list` \"\"\" all_users = set ( test_set . user_id_column ) if user_id_list is not None : all_users = set ( user_id_list ) filter_dict : Union [ Dict , None ] = None if methodology is not None : train_set = self . graph . to_ratings () filter_dict = methodology . filter_all ( train_set , test_set , result_as_iter_dict = True ) total_rank_list = self . algorithm . rank ( all_users , self . graph , n_recs , filter_dict ) total_rank = Rank . from_list ( total_rank_list ) if len ( total_rank ) == 0 : logger . warning ( \"No items could be ranked for any users! Remember that items to rank must be present \" \"in the graph. \\n \" \"Try changing methodology!\" ) elif len ( set ( total_rank . user_id_column )) != len ( all_users ): logger . warning ( f \"No items could be ranked for users { all_users - set ( total_rank . user_id_column ) } \\n \" f \"No nodes to rank for them found in the graph. Try changing methodology! \" ) self . _yaml_report = { 'graph' : repr ( self . graph ), 'mode' : 'rank' , 'n_recs' : repr ( n_recs ), 'methodology' : repr ( methodology )} return total_rank","title":"rank()"},{"location":"recsys/graph_based/graph_based_algorithms/nx_pagerank/","text":"Page Rank NXPageRank ( alpha = 0.85 , personalized = False , max_iter = 100 , tol = 1e-06 , nstart = None , weight = True ) Bases: PageRank Page Rank algorithm based on the networkx implementation. Please note that it can only be used for instantiated NXGraphs The PageRank can be personalized , in this case the PageRank will be calculated with a personalization vector made by items in the user profile weighted by the score given to them. Parameters: Name Type Description Default alpha Any Damping parameter for PageRank, default=0.85. 0.85 personalized bool Boolean value that specifies if the page rank must be calculated considering the user profile as personalization vector. Default is False False max_iter Any Maximum number of iterations in power method eigenvalue solver. 100 tol Any Error tolerance used to check convergence in power method solver. 1e-06 nstart Any Starting value of PageRank iteration for each node. None weight bool Boolean value which tells the algorithm if weight of the edges must be considered or not. Default is True True Source code in clayrs/recsys/graph_based_algorithm/page_rank/nx_page_rank.py 30 31 32 33 34 35 36 37 38 def __init__ ( self , alpha : Any = 0.85 , personalized : bool = False , max_iter : Any = 100 , tol : Any = 1.0e-6 , nstart : Any = None , weight : bool = True ): self . alpha = alpha self . max_iter = max_iter self . tol = tol self . nstart = nstart self . weight = 'weight' if weight is True else None super () . __init__ ( personalized ) rank ( all_users , graph , recs_number = None , filter_dict = None ) Rank the top-n recommended items for the user. If the recs_number parameter isn't specified, All unrated items for the user will be ranked (or only items in the filter list, if specified). One can specify which items must be ranked for each user with the filter_dict parameter, in this case every user is mapped with a list of items for which a ranking score must be computed. Otherwise, ALL unrated items will be ranked for each user. Parameters: Name Type Description Default all_users Set [ str ] Set of user id for which a recommendation list must be generated required graph NXBipartiteGraph A NX graph previously instantiated required recs_number int number of the top ranked items to return, if None all ranked items will be returned None filter_dict Dict [ str , Set ] Dict containing filters list for each user. If None all unrated items for each user will be ranked None Returns: Type Description List [ Interaction ] List of Interactions object in a descending order w.r.t the 'score' attribute, representing the ranking for a single user Source code in clayrs/recsys/graph_based_algorithm/page_rank/nx_page_rank.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 def rank ( self , all_users : Set [ str ], graph : NXBipartiteGraph , recs_number : int = None , filter_dict : Dict [ str , Set ] = None ) -> List [ Interaction ]: \"\"\" Rank the top-n recommended items for the user. If the recs_number parameter isn't specified, All unrated items for the user will be ranked (or only items in the filter list, if specified). One can specify which items must be ranked for each user with the `filter_dict` parameter, in this case every user is mapped with a list of items for which a ranking score must be computed. Otherwise, **ALL** unrated items will be ranked for each user. Args: all_users: Set of user id for which a recommendation list must be generated graph: A NX graph previously instantiated recs_number: number of the top ranked items to return, if None all ranked items will be returned filter_dict: Dict containing filters list for each user. If None all unrated items for each user will be ranked Returns: List of Interactions object in a descending order w.r.t the 'score' attribute, representing the ranking for a single user \"\"\" # scores will contain pagerank scores scores = None all_rank_interaction_list = [] with get_progbar ( all_users ) as pbar : for user_id in pbar : pbar . set_description ( f \"Computing rank for { user_id } \" ) filter_list = None if filter_dict is not None : filter_list = [ ItemNode ( item_to_rank ) for item_to_rank in filter_dict . pop ( user_id )] user_node = UserNode ( user_id ) # run the pageRank if self . _personalized is True : # the personalization vector is formed by the nodes that the user voted with their weight # + all the other nodes in the graph with weight as the min weight given by the user # (This because if a node isn't specified in the personalization vector will have 0 score in page # rank) succ = graph . get_successors ( user_node ) profile = { scored_node : graph . get_link_data ( user_node , scored_node ) . get ( 'weight' ) for scored_node in succ if graph . get_link_data ( user_node , scored_node ) . get ( 'weight' ) is not None } pers = { node : profile [ node ] if node in profile else min ( set ( profile . values ())) for node in graph . to_networkx () . nodes } scores = nx . pagerank ( graph . to_networkx (), personalization = pers , alpha = self . alpha , max_iter = self . max_iter , tol = self . tol , nstart = self . nstart , weight = self . weight ) # if scores is None it means this is the first time we are running normal pagerank # for all the other users the pagerank won't be computed again elif scores is None : scores = nx . pagerank ( graph . to_networkx (), alpha = self . alpha , max_iter = self . max_iter , tol = self . tol , nstart = self . nstart , weight = self . weight ) # clean the results removing user nodes, selected user profile and eventually properties user_scores = self . filter_result ( graph , scores , filter_list , user_node ) # Build the item_score dict (key is item_id, value is rank score predicted) # and order the keys in descending order item_score_dict = dict ( zip ([ node . value for node in user_scores . keys ()], user_scores . values ())) ordered_item_ids = sorted ( item_score_dict , key = item_score_dict . get , reverse = True ) # we only save the top-n items_ids corresponding to top-n recommendations # (if recs_number is None ordered_item_ids will contain all item_ids as the original list) ordered_item_ids = ordered_item_ids [: recs_number ] # we construct the output data single_rank_interaction_list = [ Interaction ( user_id , item_id , item_score_dict [ item_id ]) for item_id in ordered_item_ids ] all_rank_interaction_list . extend ( single_rank_interaction_list ) return all_rank_interaction_list","title":"Page Rank"},{"location":"recsys/graph_based/graph_based_algorithms/nx_pagerank/#page-rank","text":"","title":"Page Rank"},{"location":"recsys/graph_based/graph_based_algorithms/nx_pagerank/#clayrs.recsys.graph_based_algorithm.page_rank.nx_page_rank.NXPageRank","text":"Bases: PageRank Page Rank algorithm based on the networkx implementation. Please note that it can only be used for instantiated NXGraphs The PageRank can be personalized , in this case the PageRank will be calculated with a personalization vector made by items in the user profile weighted by the score given to them. Parameters: Name Type Description Default alpha Any Damping parameter for PageRank, default=0.85. 0.85 personalized bool Boolean value that specifies if the page rank must be calculated considering the user profile as personalization vector. Default is False False max_iter Any Maximum number of iterations in power method eigenvalue solver. 100 tol Any Error tolerance used to check convergence in power method solver. 1e-06 nstart Any Starting value of PageRank iteration for each node. None weight bool Boolean value which tells the algorithm if weight of the edges must be considered or not. Default is True True Source code in clayrs/recsys/graph_based_algorithm/page_rank/nx_page_rank.py 30 31 32 33 34 35 36 37 38 def __init__ ( self , alpha : Any = 0.85 , personalized : bool = False , max_iter : Any = 100 , tol : Any = 1.0e-6 , nstart : Any = None , weight : bool = True ): self . alpha = alpha self . max_iter = max_iter self . tol = tol self . nstart = nstart self . weight = 'weight' if weight is True else None super () . __init__ ( personalized )","title":"NXPageRank"},{"location":"recsys/graph_based/graph_based_algorithms/nx_pagerank/#clayrs.recsys.graph_based_algorithm.page_rank.nx_page_rank.NXPageRank.rank","text":"Rank the top-n recommended items for the user. If the recs_number parameter isn't specified, All unrated items for the user will be ranked (or only items in the filter list, if specified). One can specify which items must be ranked for each user with the filter_dict parameter, in this case every user is mapped with a list of items for which a ranking score must be computed. Otherwise, ALL unrated items will be ranked for each user. Parameters: Name Type Description Default all_users Set [ str ] Set of user id for which a recommendation list must be generated required graph NXBipartiteGraph A NX graph previously instantiated required recs_number int number of the top ranked items to return, if None all ranked items will be returned None filter_dict Dict [ str , Set ] Dict containing filters list for each user. If None all unrated items for each user will be ranked None Returns: Type Description List [ Interaction ] List of Interactions object in a descending order w.r.t the 'score' attribute, representing the ranking for a single user Source code in clayrs/recsys/graph_based_algorithm/page_rank/nx_page_rank.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 def rank ( self , all_users : Set [ str ], graph : NXBipartiteGraph , recs_number : int = None , filter_dict : Dict [ str , Set ] = None ) -> List [ Interaction ]: \"\"\" Rank the top-n recommended items for the user. If the recs_number parameter isn't specified, All unrated items for the user will be ranked (or only items in the filter list, if specified). One can specify which items must be ranked for each user with the `filter_dict` parameter, in this case every user is mapped with a list of items for which a ranking score must be computed. Otherwise, **ALL** unrated items will be ranked for each user. Args: all_users: Set of user id for which a recommendation list must be generated graph: A NX graph previously instantiated recs_number: number of the top ranked items to return, if None all ranked items will be returned filter_dict: Dict containing filters list for each user. If None all unrated items for each user will be ranked Returns: List of Interactions object in a descending order w.r.t the 'score' attribute, representing the ranking for a single user \"\"\" # scores will contain pagerank scores scores = None all_rank_interaction_list = [] with get_progbar ( all_users ) as pbar : for user_id in pbar : pbar . set_description ( f \"Computing rank for { user_id } \" ) filter_list = None if filter_dict is not None : filter_list = [ ItemNode ( item_to_rank ) for item_to_rank in filter_dict . pop ( user_id )] user_node = UserNode ( user_id ) # run the pageRank if self . _personalized is True : # the personalization vector is formed by the nodes that the user voted with their weight # + all the other nodes in the graph with weight as the min weight given by the user # (This because if a node isn't specified in the personalization vector will have 0 score in page # rank) succ = graph . get_successors ( user_node ) profile = { scored_node : graph . get_link_data ( user_node , scored_node ) . get ( 'weight' ) for scored_node in succ if graph . get_link_data ( user_node , scored_node ) . get ( 'weight' ) is not None } pers = { node : profile [ node ] if node in profile else min ( set ( profile . values ())) for node in graph . to_networkx () . nodes } scores = nx . pagerank ( graph . to_networkx (), personalization = pers , alpha = self . alpha , max_iter = self . max_iter , tol = self . tol , nstart = self . nstart , weight = self . weight ) # if scores is None it means this is the first time we are running normal pagerank # for all the other users the pagerank won't be computed again elif scores is None : scores = nx . pagerank ( graph . to_networkx (), alpha = self . alpha , max_iter = self . max_iter , tol = self . tol , nstart = self . nstart , weight = self . weight ) # clean the results removing user nodes, selected user profile and eventually properties user_scores = self . filter_result ( graph , scores , filter_list , user_node ) # Build the item_score dict (key is item_id, value is rank score predicted) # and order the keys in descending order item_score_dict = dict ( zip ([ node . value for node in user_scores . keys ()], user_scores . values ())) ordered_item_ids = sorted ( item_score_dict , key = item_score_dict . get , reverse = True ) # we only save the top-n items_ids corresponding to top-n recommendations # (if recs_number is None ordered_item_ids will contain all item_ids as the original list) ordered_item_ids = ordered_item_ids [: recs_number ] # we construct the output data single_rank_interaction_list = [ Interaction ( user_id , item_id , item_score_dict [ item_id ]) for item_id in ordered_item_ids ] all_rank_interaction_list . extend ( single_rank_interaction_list ) return all_rank_interaction_list","title":"rank()"},{"location":"recsys/graph_based/graphs/nodes/","text":"Nodes categories The followings are all the various category of nodes that can be added to a graph. Info Please note that there exists Bipartite Graph , Tripartite Graph and Full Graph , all with their peculiarities and restrictions. Check their documentation for more! UserNode ( value ) Bases: Node Class that represents 'user' nodes Parameters: Name Type Description Default value object the value to store in the node required Source code in clayrs/recsys/graphs/graph.py 64 65 def __init__ ( self , value : str ): super () . __init__ ( value ) ItemNode ( value ) Bases: Node Class that represents 'item' nodes Parameters: Name Type Description Default value object the value to store in the node required Source code in clayrs/recsys/graphs/graph.py 82 83 def __init__ ( self , value : str ): super () . __init__ ( value ) PropertyNode ( value ) Bases: Node Class that represents 'property' nodes Parameters: Name Type Description Default value object the value to store in the node required Source code in clayrs/recsys/graphs/graph.py 100 101 def __init__ ( self , value : str ): super () . __init__ ( value )","title":"Nodes categories"},{"location":"recsys/graph_based/graphs/nodes/#nodes-categories","text":"The followings are all the various category of nodes that can be added to a graph. Info Please note that there exists Bipartite Graph , Tripartite Graph and Full Graph , all with their peculiarities and restrictions. Check their documentation for more!","title":"Nodes categories"},{"location":"recsys/graph_based/graphs/nodes/#clayrs.recsys.graphs.UserNode","text":"Bases: Node Class that represents 'user' nodes Parameters: Name Type Description Default value object the value to store in the node required Source code in clayrs/recsys/graphs/graph.py 64 65 def __init__ ( self , value : str ): super () . __init__ ( value )","title":"UserNode"},{"location":"recsys/graph_based/graphs/nodes/#clayrs.recsys.graphs.ItemNode","text":"Bases: Node Class that represents 'item' nodes Parameters: Name Type Description Default value object the value to store in the node required Source code in clayrs/recsys/graphs/graph.py 82 83 def __init__ ( self , value : str ): super () . __init__ ( value )","title":"ItemNode"},{"location":"recsys/graph_based/graphs/nodes/#clayrs.recsys.graphs.PropertyNode","text":"Bases: Node Class that represents 'property' nodes Parameters: Name Type Description Default value object the value to store in the node required Source code in clayrs/recsys/graphs/graph.py 100 101 def __init__ ( self , value : str ): super () . __init__ ( value )","title":"PropertyNode"},{"location":"recsys/graph_based/graphs/nx_bipartite/","text":"Bipartite Graph NXBipartiteGraph ( source_frame = None , link_label = None ) Bases: BipartiteDiGraph Class that implements a Bipartite graph through networkx library. Info A Bipartite Graph is a graph which supports only User nodes and Item nodes. If you need to model also other node categories, consider using a Tripartite Graph or a Full Graph It creates a graph from an initial Rating object. Consider the following matrix representation of the Rating object +------+-----------+-------+ | User | Item | Score | +------+-----------+-------+ | u1 | Tenet | 4 | | u2 | Inception | 5 | | ... | ... | ... | +------+-----------+-------+ The graph will be created with the following interactions: 4 u1 -----> Tenet 5 u2 -----> Inception where u1 and u2 become User nodes and Tenet and Inception become Item nodes , with the edge weighted depending on the score given If the link_label parameter is specified, then each link between users and items will be labeled with the label specified (e.g. link_label='score' ): (4, 'score') u1 -------------> Tenet (5, 'score') u2 -------------> Inception Parameters: Name Type Description Default source_frame Ratings the initial Ratings object needed to create the graph None link_label str If specified, each link will be labeled with the given label. Default is None None Source code in clayrs/recsys/graphs/nx_implementation/nx_bipartite_graphs.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 def __init__ ( self , source_frame : Ratings = None , link_label : str = None ): self . _graph = nx . DiGraph () if source_frame is not None : not_none_dict = {} if link_label is not None : not_none_dict [ 'label' ] = link_label with get_progbar ( source_frame ) as progbar : progbar . set_description ( \"Creating User->Item links\" ) if len ( source_frame . timestamp_column ) != 0 : edges_with_attributes_gen = (( UserNode ( interaction . user_id ), ItemNode ( interaction . item_id ), # {**x, **y} merges the dicts x and y { ** not_none_dict , ** { 'weight' : interaction . score , 'timestamp' : interaction . timestamp }} ) for interaction in progbar ) else : edges_with_attributes_gen = (( UserNode ( interaction . user_id ), ItemNode ( interaction . item_id ), # {**x, **y} merges the dicts x and y { ** not_none_dict , ** { 'weight' : interaction . score }}) for interaction in progbar ) self . _graph . add_edges_from ( edges_with_attributes_gen ) add_link ( start_node , final_node , weight = None , label = None , timestamp = None ) Creates a link connecting the start_node to the final_node . If two lists are passed, then the node in position \\(i\\) in the start_node list will be linked to the node in position \\(i\\) in the final_node list. If nodes to link do not exist, they will be added automatically to the graph. Please remember that since this is a Bipartite Graph, only User nodes and Item nodes can be added! A link can be weighted with the weight parameter and labeled with the label parameter. A timestamp can also be specified via timestamp parameter. All three are optional parameters, so they are not required Parameters: Name Type Description Default start_node Union [ Node , List [ Node ]] Single Node object or a list of Node objects. They will be the 'head' of the link, since it's a directed graph required final_node object Single Node object or a list Node objects. They will be the 'tail' of the link, since it's a directed graph required weight float weight of the link, default is None (no weight) None label str label of the link, default is None (no label) None timestamp str timestamp of the link, default is None (no timestamp) None Source code in clayrs/recsys/graphs/nx_implementation/nx_bipartite_graphs.py 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 def add_link ( self , start_node : Union [ Node , List [ Node ]], final_node : Union [ Node , List [ Node ]], weight : float = None , label : str = None , timestamp : str = None ): \"\"\" Creates a link connecting the `start_node` to the `final_node`. If two lists are passed, then the node in position $i$ in the `start_node` list will be linked to the node in position $i$ in the `final_node` list. If nodes to link do not exist, they will be added automatically to the graph. Please remember that since this is a Bipartite Graph, only *User nodes* and *Item nodes* can be added! A link can be weighted with the `weight` parameter and labeled with the `label` parameter. A timestamp can also be specified via `timestamp` parameter. All three are optional parameters, so they are not required Args: start_node: Single Node object or a list of Node objects. They will be the 'head' of the link, since it's a directed graph final_node (object): Single Node object or a list Node objects. They will be the 'tail' of the link, since it's a directed graph weight: weight of the link, default is None (no weight) label: label of the link, default is None (no label) timestamp: timestamp of the link, default is None (no timestamp) \"\"\" if not isinstance ( start_node , list ): start_node = [ start_node ] if not isinstance ( final_node , list ): final_node = [ final_node ] self . add_node ( start_node ) self . add_node ( final_node ) not_none_dict = {} if label is not None : not_none_dict [ 'label' ] = label if weight is not None : not_none_dict [ 'weight' ] = weight if timestamp is not None : not_none_dict [ 'timestamp' ] = timestamp self . _graph . add_edges_from ( zip ( start_node , final_node ), ** not_none_dict ) add_node ( node ) Adds one or multiple Node objects to the graph. Since this is a Bipartite Graph, only User Node and Item Node can be added! No duplicates are allowed, but different category nodes with same id are (e.g. ItemNode('1') and UserNode('1') ) Parameters: Name Type Description Default node Union [ Node , List [ Node ]] Node(s) object(s) that needs to be added to the graph required Raises: Type Description ValueError Exception raised when one of the node to add to the graph is not a User or Item node Source code in clayrs/recsys/graphs/nx_implementation/nx_bipartite_graphs.py 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 def add_node ( self , node : Union [ Node , List [ Node ]]): \"\"\" Adds one or multiple Node objects to the graph. Since this is a Bipartite Graph, only `User Node` and `Item Node` can be added! No duplicates are allowed, but different category nodes with same id are (e.g. `ItemNode('1')` and `UserNode('1')`) Args: node: Node(s) object(s) that needs to be added to the graph Raises: ValueError: Exception raised when one of the node to add to the graph is not a User or Item node \"\"\" if not isinstance ( node , list ): node = [ node ] if any ( not isinstance ( n , ( UserNode , ItemNode )) for n in node ): raise ValueError ( \"You can only add UserNodes or ItemNodes to a bipartite graph!\" ) self . _graph . add_nodes_from ( node ) closeness_centrality () Calculate the closeness centrality for every node in the graph Returns: Type Description Dict Dictionary containing the closeness centrality for each node in the graph Source code in clayrs/recsys/graphs/nx_implementation/nx_bipartite_graphs.py 291 292 293 294 295 296 297 298 def closeness_centrality ( self ) -> Dict : \"\"\" Calculate the closeness centrality for every node in the graph Returns: Dictionary containing the closeness centrality for each node in the graph \"\"\" return nx . closeness_centrality ( self . _graph ) degree_centrality () Calculate the degree centrality for every node in the graph Returns: Type Description Dict Dictionary containing the degree centrality for each node in the graph Source code in clayrs/recsys/graphs/nx_implementation/nx_bipartite_graphs.py 282 283 284 285 286 287 288 289 def degree_centrality ( self ) -> Dict : \"\"\" Calculate the degree centrality for every node in the graph Returns: Dictionary containing the degree centrality for each node in the graph \"\"\" return nx . degree_centrality ( self . _graph ) dispersion () Calculate the dispersion for every node in the graph Returns: Type Description Dict Dictionary containing the dispersion computed for each node in the graph Source code in clayrs/recsys/graphs/nx_implementation/nx_bipartite_graphs.py 300 301 302 303 304 305 306 307 def dispersion ( self ) -> Dict : \"\"\" Calculate the dispersion for every node in the graph Returns: Dictionary containing the dispersion computed for each node in the graph \"\"\" return nx . dispersion ( self . _graph ) get_link_data ( start_node , final_node ) Get link data such as weight, label, timestamp. between the start_node and the final_node . Returns None if said link doesn't exists Remember that this is a directed graph so the result differs if 'start_node' and 'final_node' are switched. Parameters: Name Type Description Default start_node Node Node object from where the link starts required final_node Node Node object to where the link ends required Source code in clayrs/recsys/graphs/nx_implementation/nx_bipartite_graphs.py 183 184 185 186 187 188 189 190 191 192 193 194 195 def get_link_data ( self , start_node : Node , final_node : Node ): \"\"\" Get link data such as weight, label, timestamp. between the `start_node` and the `final_node`. Returns None if said link doesn't exists Remember that this is a directed graph so the result differs if 'start_node' and 'final_node' are switched. Args: start_node: Node object from where the link starts final_node: Node object to where the link ends \"\"\" return self . _graph . get_edge_data ( start_node , final_node ) get_predecessors ( node ) Returns a list containing the predecessors of the node passed. Raises TypeError exception if the node doesn't exists in the graph. Taken from networkx library: A predecessor of n is a node m such that there exists a directed edge from m to n For example: # GRAPH: I1 <-- U1 \u2191 U2 >>> graph . get_predecessors ( ItemNode ( 'I1' )) [ User U1 , User U2 ] Parameters: Name Type Description Default node Node Node for which we want to know the predecessors required Raises: Type Description TypeError Exception raised when the node it's not in the graph Source code in clayrs/recsys/graphs/nx_implementation/nx_bipartite_graphs.py 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 def get_predecessors ( self , node : Node ) -> List [ Node ]: \"\"\" Returns a list containing the *predecessors* of the node passed. Raises TypeError exception if the node doesn't exists in the graph. Taken from networkx library: > A predecessor of n is a node m such that there exists a directed edge from m to n For example: ``` # GRAPH: I1 <-- U1 \u2191 U2 ``` ```python >>> graph.get_predecessors(ItemNode('I1')) [User U1, User U2] ``` Args: node: Node for which we want to know the predecessors Raises: TypeError: Exception raised when the node it's not in the graph \"\"\" try : return list ( self . _graph . predecessors ( node )) except nx . NetworkXError : raise TypeError ( \"The node specified is not in the graph!\" ) get_successors ( node ) Returns a list containing the successors of the node passed. Returns None if the node doesn't exists in the graph. Taken from networkx library: A successor of n is a node m such that there exists a directed edge from n to m For example: U1 --> I2 \u2193 I1 >>> graph . get_successors ( UserNode ( 'U1' )) [ Item I1 , Item I2 ] Parameters: Name Type Description Default node Node Node for which we want to know the successors required Raises: Type Description TypeError Exception raised when the node it's not in the graph Source code in clayrs/recsys/graphs/nx_implementation/nx_bipartite_graphs.py 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 def get_successors ( self , node : Node ) -> List [ Node ]: \"\"\" Returns a list containing the successors of the node passed. Returns None if the node doesn't exists in the graph. Taken from networkx library: > A successor of n is a node m such that there exists a directed edge from n to m For example: ``` U1 --> I2 \u2193 I1 ``` ```python >>> graph.get_successors(UserNode('U1')) [Item I1, Item I2] ``` Args: node: Node for which we want to know the successors Raises: TypeError: Exception raised when the node it's not in the graph \"\"\" try : return list ( self . _graph . successors ( node )) except nx . NetworkXError : raise TypeError ( \"The node specified is not in the graph!\" ) item_nodes () property Returns a set of all Item nodes in the graph Source code in clayrs/recsys/graphs/nx_implementation/nx_bipartite_graphs.py 97 98 99 100 101 102 @property def item_nodes ( self ) -> Set [ ItemNode ]: \"\"\" Returns a set of all *Item nodes* in the graph \"\"\" return set ([ node for node in self . _graph . nodes if isinstance ( node , ItemNode )]) node_exists ( node ) Returns True if the node passed exists in the graph, False otherwise Parameters: Name Type Description Default node Node Node to check whether it's present in the graph or not required Source code in clayrs/recsys/graphs/nx_implementation/nx_bipartite_graphs.py 266 267 268 269 270 271 272 273 274 def node_exists ( self , node : Node ) -> bool : \"\"\" Returns True if the node passed exists in the graph, False otherwise Args: node: Node to check whether it's present in the graph or not \"\"\" r = self . _graph . nodes . get ( node ) return r is not None remove_link ( start_node , final_node ) Removes the link connecting the start_node to the final_node . If there's no link between the two nodes, then a warning is printed Parameters: Name Type Description Default start_node Node head node of the link to remove required final_node Node tail node of the link to remove required Source code in clayrs/recsys/graphs/nx_implementation/nx_bipartite_graphs.py 168 169 170 171 172 173 174 175 176 177 178 179 180 181 def remove_link ( self , start_node : Node , final_node : Node ): \"\"\" Removes the link connecting the `start_node` to the `final_node`. If there's no link between the two nodes, then a warning is printed Args: start_node: *head* node of the link to remove final_node: *tail* node of the link to remove \"\"\" try : self . _graph . remove_edge ( start_node , final_node ) except nx . NetworkXError : logger . warning ( \"No link exists between the start node and the final node! \\n \" \"No link will be removed\" ) remove_node ( node_to_remove ) Removes one or multiple nodes from the graph. If one of the nodes to remove is not present in the graph, it is silently ignored Parameters: Name Type Description Default node_to_remove Union [ Node , List [ Node ]] Single Node object or a list of Node objects to remove from the graph required Source code in clayrs/recsys/graphs/nx_implementation/nx_bipartite_graphs.py 309 310 311 312 313 314 315 316 317 318 319 320 def remove_node ( self , node_to_remove : Union [ Node , List [ Node ]]): \"\"\" Removes one or multiple nodes from the graph. If one of the nodes to remove is not present in the graph, it is silently ignored Args: node_to_remove: Single Node object or a list of Node objects to remove from the graph \"\"\" if not isinstance ( node_to_remove , list ): node_to_remove = [ node_to_remove ] self . _graph . remove_nodes_from ( node_to_remove ) to_networkx () Returns underlying networkx implementation of the graph Source code in clayrs/recsys/graphs/nx_implementation/nx_bipartite_graphs.py 276 277 278 279 280 def to_networkx ( self ) -> nx . DiGraph : \"\"\" Returns underlying networkx implementation of the graph \"\"\" return self . _graph user_nodes () property Returns a set of all User nodes in the graph Source code in clayrs/recsys/graphs/nx_implementation/nx_bipartite_graphs.py 90 91 92 93 94 95 @property def user_nodes ( self ) -> Set [ UserNode ]: \"\"\" Returns a set of all *User nodes* in the graph \"\"\" return set ([ node for node in self . _graph . nodes if isinstance ( node , UserNode )])","title":"Bipartite Graph"},{"location":"recsys/graph_based/graphs/nx_bipartite/#bipartite-graph","text":"","title":"Bipartite Graph"},{"location":"recsys/graph_based/graphs/nx_bipartite/#clayrs.recsys.graphs.nx_implementation.nx_bipartite_graphs.NXBipartiteGraph","text":"Bases: BipartiteDiGraph Class that implements a Bipartite graph through networkx library. Info A Bipartite Graph is a graph which supports only User nodes and Item nodes. If you need to model also other node categories, consider using a Tripartite Graph or a Full Graph It creates a graph from an initial Rating object. Consider the following matrix representation of the Rating object +------+-----------+-------+ | User | Item | Score | +------+-----------+-------+ | u1 | Tenet | 4 | | u2 | Inception | 5 | | ... | ... | ... | +------+-----------+-------+ The graph will be created with the following interactions: 4 u1 -----> Tenet 5 u2 -----> Inception where u1 and u2 become User nodes and Tenet and Inception become Item nodes , with the edge weighted depending on the score given If the link_label parameter is specified, then each link between users and items will be labeled with the label specified (e.g. link_label='score' ): (4, 'score') u1 -------------> Tenet (5, 'score') u2 -------------> Inception Parameters: Name Type Description Default source_frame Ratings the initial Ratings object needed to create the graph None link_label str If specified, each link will be labeled with the given label. Default is None None Source code in clayrs/recsys/graphs/nx_implementation/nx_bipartite_graphs.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 def __init__ ( self , source_frame : Ratings = None , link_label : str = None ): self . _graph = nx . DiGraph () if source_frame is not None : not_none_dict = {} if link_label is not None : not_none_dict [ 'label' ] = link_label with get_progbar ( source_frame ) as progbar : progbar . set_description ( \"Creating User->Item links\" ) if len ( source_frame . timestamp_column ) != 0 : edges_with_attributes_gen = (( UserNode ( interaction . user_id ), ItemNode ( interaction . item_id ), # {**x, **y} merges the dicts x and y { ** not_none_dict , ** { 'weight' : interaction . score , 'timestamp' : interaction . timestamp }} ) for interaction in progbar ) else : edges_with_attributes_gen = (( UserNode ( interaction . user_id ), ItemNode ( interaction . item_id ), # {**x, **y} merges the dicts x and y { ** not_none_dict , ** { 'weight' : interaction . score }}) for interaction in progbar ) self . _graph . add_edges_from ( edges_with_attributes_gen )","title":"NXBipartiteGraph"},{"location":"recsys/graph_based/graphs/nx_bipartite/#clayrs.recsys.graphs.nx_implementation.nx_bipartite_graphs.NXBipartiteGraph.add_link","text":"Creates a link connecting the start_node to the final_node . If two lists are passed, then the node in position \\(i\\) in the start_node list will be linked to the node in position \\(i\\) in the final_node list. If nodes to link do not exist, they will be added automatically to the graph. Please remember that since this is a Bipartite Graph, only User nodes and Item nodes can be added! A link can be weighted with the weight parameter and labeled with the label parameter. A timestamp can also be specified via timestamp parameter. All three are optional parameters, so they are not required Parameters: Name Type Description Default start_node Union [ Node , List [ Node ]] Single Node object or a list of Node objects. They will be the 'head' of the link, since it's a directed graph required final_node object Single Node object or a list Node objects. They will be the 'tail' of the link, since it's a directed graph required weight float weight of the link, default is None (no weight) None label str label of the link, default is None (no label) None timestamp str timestamp of the link, default is None (no timestamp) None Source code in clayrs/recsys/graphs/nx_implementation/nx_bipartite_graphs.py 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 def add_link ( self , start_node : Union [ Node , List [ Node ]], final_node : Union [ Node , List [ Node ]], weight : float = None , label : str = None , timestamp : str = None ): \"\"\" Creates a link connecting the `start_node` to the `final_node`. If two lists are passed, then the node in position $i$ in the `start_node` list will be linked to the node in position $i$ in the `final_node` list. If nodes to link do not exist, they will be added automatically to the graph. Please remember that since this is a Bipartite Graph, only *User nodes* and *Item nodes* can be added! A link can be weighted with the `weight` parameter and labeled with the `label` parameter. A timestamp can also be specified via `timestamp` parameter. All three are optional parameters, so they are not required Args: start_node: Single Node object or a list of Node objects. They will be the 'head' of the link, since it's a directed graph final_node (object): Single Node object or a list Node objects. They will be the 'tail' of the link, since it's a directed graph weight: weight of the link, default is None (no weight) label: label of the link, default is None (no label) timestamp: timestamp of the link, default is None (no timestamp) \"\"\" if not isinstance ( start_node , list ): start_node = [ start_node ] if not isinstance ( final_node , list ): final_node = [ final_node ] self . add_node ( start_node ) self . add_node ( final_node ) not_none_dict = {} if label is not None : not_none_dict [ 'label' ] = label if weight is not None : not_none_dict [ 'weight' ] = weight if timestamp is not None : not_none_dict [ 'timestamp' ] = timestamp self . _graph . add_edges_from ( zip ( start_node , final_node ), ** not_none_dict )","title":"add_link()"},{"location":"recsys/graph_based/graphs/nx_bipartite/#clayrs.recsys.graphs.nx_implementation.nx_bipartite_graphs.NXBipartiteGraph.add_node","text":"Adds one or multiple Node objects to the graph. Since this is a Bipartite Graph, only User Node and Item Node can be added! No duplicates are allowed, but different category nodes with same id are (e.g. ItemNode('1') and UserNode('1') ) Parameters: Name Type Description Default node Union [ Node , List [ Node ]] Node(s) object(s) that needs to be added to the graph required Raises: Type Description ValueError Exception raised when one of the node to add to the graph is not a User or Item node Source code in clayrs/recsys/graphs/nx_implementation/nx_bipartite_graphs.py 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 def add_node ( self , node : Union [ Node , List [ Node ]]): \"\"\" Adds one or multiple Node objects to the graph. Since this is a Bipartite Graph, only `User Node` and `Item Node` can be added! No duplicates are allowed, but different category nodes with same id are (e.g. `ItemNode('1')` and `UserNode('1')`) Args: node: Node(s) object(s) that needs to be added to the graph Raises: ValueError: Exception raised when one of the node to add to the graph is not a User or Item node \"\"\" if not isinstance ( node , list ): node = [ node ] if any ( not isinstance ( n , ( UserNode , ItemNode )) for n in node ): raise ValueError ( \"You can only add UserNodes or ItemNodes to a bipartite graph!\" ) self . _graph . add_nodes_from ( node )","title":"add_node()"},{"location":"recsys/graph_based/graphs/nx_bipartite/#clayrs.recsys.graphs.nx_implementation.nx_bipartite_graphs.NXBipartiteGraph.closeness_centrality","text":"Calculate the closeness centrality for every node in the graph Returns: Type Description Dict Dictionary containing the closeness centrality for each node in the graph Source code in clayrs/recsys/graphs/nx_implementation/nx_bipartite_graphs.py 291 292 293 294 295 296 297 298 def closeness_centrality ( self ) -> Dict : \"\"\" Calculate the closeness centrality for every node in the graph Returns: Dictionary containing the closeness centrality for each node in the graph \"\"\" return nx . closeness_centrality ( self . _graph )","title":"closeness_centrality()"},{"location":"recsys/graph_based/graphs/nx_bipartite/#clayrs.recsys.graphs.nx_implementation.nx_bipartite_graphs.NXBipartiteGraph.degree_centrality","text":"Calculate the degree centrality for every node in the graph Returns: Type Description Dict Dictionary containing the degree centrality for each node in the graph Source code in clayrs/recsys/graphs/nx_implementation/nx_bipartite_graphs.py 282 283 284 285 286 287 288 289 def degree_centrality ( self ) -> Dict : \"\"\" Calculate the degree centrality for every node in the graph Returns: Dictionary containing the degree centrality for each node in the graph \"\"\" return nx . degree_centrality ( self . _graph )","title":"degree_centrality()"},{"location":"recsys/graph_based/graphs/nx_bipartite/#clayrs.recsys.graphs.nx_implementation.nx_bipartite_graphs.NXBipartiteGraph.dispersion","text":"Calculate the dispersion for every node in the graph Returns: Type Description Dict Dictionary containing the dispersion computed for each node in the graph Source code in clayrs/recsys/graphs/nx_implementation/nx_bipartite_graphs.py 300 301 302 303 304 305 306 307 def dispersion ( self ) -> Dict : \"\"\" Calculate the dispersion for every node in the graph Returns: Dictionary containing the dispersion computed for each node in the graph \"\"\" return nx . dispersion ( self . _graph )","title":"dispersion()"},{"location":"recsys/graph_based/graphs/nx_bipartite/#clayrs.recsys.graphs.nx_implementation.nx_bipartite_graphs.NXBipartiteGraph.get_link_data","text":"Get link data such as weight, label, timestamp. between the start_node and the final_node . Returns None if said link doesn't exists Remember that this is a directed graph so the result differs if 'start_node' and 'final_node' are switched. Parameters: Name Type Description Default start_node Node Node object from where the link starts required final_node Node Node object to where the link ends required Source code in clayrs/recsys/graphs/nx_implementation/nx_bipartite_graphs.py 183 184 185 186 187 188 189 190 191 192 193 194 195 def get_link_data ( self , start_node : Node , final_node : Node ): \"\"\" Get link data such as weight, label, timestamp. between the `start_node` and the `final_node`. Returns None if said link doesn't exists Remember that this is a directed graph so the result differs if 'start_node' and 'final_node' are switched. Args: start_node: Node object from where the link starts final_node: Node object to where the link ends \"\"\" return self . _graph . get_edge_data ( start_node , final_node )","title":"get_link_data()"},{"location":"recsys/graph_based/graphs/nx_bipartite/#clayrs.recsys.graphs.nx_implementation.nx_bipartite_graphs.NXBipartiteGraph.get_predecessors","text":"Returns a list containing the predecessors of the node passed. Raises TypeError exception if the node doesn't exists in the graph. Taken from networkx library: A predecessor of n is a node m such that there exists a directed edge from m to n For example: # GRAPH: I1 <-- U1 \u2191 U2 >>> graph . get_predecessors ( ItemNode ( 'I1' )) [ User U1 , User U2 ] Parameters: Name Type Description Default node Node Node for which we want to know the predecessors required Raises: Type Description TypeError Exception raised when the node it's not in the graph Source code in clayrs/recsys/graphs/nx_implementation/nx_bipartite_graphs.py 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 def get_predecessors ( self , node : Node ) -> List [ Node ]: \"\"\" Returns a list containing the *predecessors* of the node passed. Raises TypeError exception if the node doesn't exists in the graph. Taken from networkx library: > A predecessor of n is a node m such that there exists a directed edge from m to n For example: ``` # GRAPH: I1 <-- U1 \u2191 U2 ``` ```python >>> graph.get_predecessors(ItemNode('I1')) [User U1, User U2] ``` Args: node: Node for which we want to know the predecessors Raises: TypeError: Exception raised when the node it's not in the graph \"\"\" try : return list ( self . _graph . predecessors ( node )) except nx . NetworkXError : raise TypeError ( \"The node specified is not in the graph!\" )","title":"get_predecessors()"},{"location":"recsys/graph_based/graphs/nx_bipartite/#clayrs.recsys.graphs.nx_implementation.nx_bipartite_graphs.NXBipartiteGraph.get_successors","text":"Returns a list containing the successors of the node passed. Returns None if the node doesn't exists in the graph. Taken from networkx library: A successor of n is a node m such that there exists a directed edge from n to m For example: U1 --> I2 \u2193 I1 >>> graph . get_successors ( UserNode ( 'U1' )) [ Item I1 , Item I2 ] Parameters: Name Type Description Default node Node Node for which we want to know the successors required Raises: Type Description TypeError Exception raised when the node it's not in the graph Source code in clayrs/recsys/graphs/nx_implementation/nx_bipartite_graphs.py 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 def get_successors ( self , node : Node ) -> List [ Node ]: \"\"\" Returns a list containing the successors of the node passed. Returns None if the node doesn't exists in the graph. Taken from networkx library: > A successor of n is a node m such that there exists a directed edge from n to m For example: ``` U1 --> I2 \u2193 I1 ``` ```python >>> graph.get_successors(UserNode('U1')) [Item I1, Item I2] ``` Args: node: Node for which we want to know the successors Raises: TypeError: Exception raised when the node it's not in the graph \"\"\" try : return list ( self . _graph . successors ( node )) except nx . NetworkXError : raise TypeError ( \"The node specified is not in the graph!\" )","title":"get_successors()"},{"location":"recsys/graph_based/graphs/nx_bipartite/#clayrs.recsys.graphs.nx_implementation.nx_bipartite_graphs.NXBipartiteGraph.item_nodes","text":"Returns a set of all Item nodes in the graph Source code in clayrs/recsys/graphs/nx_implementation/nx_bipartite_graphs.py 97 98 99 100 101 102 @property def item_nodes ( self ) -> Set [ ItemNode ]: \"\"\" Returns a set of all *Item nodes* in the graph \"\"\" return set ([ node for node in self . _graph . nodes if isinstance ( node , ItemNode )])","title":"item_nodes()"},{"location":"recsys/graph_based/graphs/nx_bipartite/#clayrs.recsys.graphs.nx_implementation.nx_bipartite_graphs.NXBipartiteGraph.node_exists","text":"Returns True if the node passed exists in the graph, False otherwise Parameters: Name Type Description Default node Node Node to check whether it's present in the graph or not required Source code in clayrs/recsys/graphs/nx_implementation/nx_bipartite_graphs.py 266 267 268 269 270 271 272 273 274 def node_exists ( self , node : Node ) -> bool : \"\"\" Returns True if the node passed exists in the graph, False otherwise Args: node: Node to check whether it's present in the graph or not \"\"\" r = self . _graph . nodes . get ( node ) return r is not None","title":"node_exists()"},{"location":"recsys/graph_based/graphs/nx_bipartite/#clayrs.recsys.graphs.nx_implementation.nx_bipartite_graphs.NXBipartiteGraph.remove_link","text":"Removes the link connecting the start_node to the final_node . If there's no link between the two nodes, then a warning is printed Parameters: Name Type Description Default start_node Node head node of the link to remove required final_node Node tail node of the link to remove required Source code in clayrs/recsys/graphs/nx_implementation/nx_bipartite_graphs.py 168 169 170 171 172 173 174 175 176 177 178 179 180 181 def remove_link ( self , start_node : Node , final_node : Node ): \"\"\" Removes the link connecting the `start_node` to the `final_node`. If there's no link between the two nodes, then a warning is printed Args: start_node: *head* node of the link to remove final_node: *tail* node of the link to remove \"\"\" try : self . _graph . remove_edge ( start_node , final_node ) except nx . NetworkXError : logger . warning ( \"No link exists between the start node and the final node! \\n \" \"No link will be removed\" )","title":"remove_link()"},{"location":"recsys/graph_based/graphs/nx_bipartite/#clayrs.recsys.graphs.nx_implementation.nx_bipartite_graphs.NXBipartiteGraph.remove_node","text":"Removes one or multiple nodes from the graph. If one of the nodes to remove is not present in the graph, it is silently ignored Parameters: Name Type Description Default node_to_remove Union [ Node , List [ Node ]] Single Node object or a list of Node objects to remove from the graph required Source code in clayrs/recsys/graphs/nx_implementation/nx_bipartite_graphs.py 309 310 311 312 313 314 315 316 317 318 319 320 def remove_node ( self , node_to_remove : Union [ Node , List [ Node ]]): \"\"\" Removes one or multiple nodes from the graph. If one of the nodes to remove is not present in the graph, it is silently ignored Args: node_to_remove: Single Node object or a list of Node objects to remove from the graph \"\"\" if not isinstance ( node_to_remove , list ): node_to_remove = [ node_to_remove ] self . _graph . remove_nodes_from ( node_to_remove )","title":"remove_node()"},{"location":"recsys/graph_based/graphs/nx_bipartite/#clayrs.recsys.graphs.nx_implementation.nx_bipartite_graphs.NXBipartiteGraph.to_networkx","text":"Returns underlying networkx implementation of the graph Source code in clayrs/recsys/graphs/nx_implementation/nx_bipartite_graphs.py 276 277 278 279 280 def to_networkx ( self ) -> nx . DiGraph : \"\"\" Returns underlying networkx implementation of the graph \"\"\" return self . _graph","title":"to_networkx()"},{"location":"recsys/graph_based/graphs/nx_bipartite/#clayrs.recsys.graphs.nx_implementation.nx_bipartite_graphs.NXBipartiteGraph.user_nodes","text":"Returns a set of all User nodes in the graph Source code in clayrs/recsys/graphs/nx_implementation/nx_bipartite_graphs.py 90 91 92 93 94 95 @property def user_nodes ( self ) -> Set [ UserNode ]: \"\"\" Returns a set of all *User nodes* in the graph \"\"\" return set ([ node for node in self . _graph . nodes if isinstance ( node , UserNode )])","title":"user_nodes()"},{"location":"recsys/graph_based/graphs/nx_full/","text":"Full Graph Please remember that this class is a subclass of NXTripartiteGraph , so it inherits all its methods. You can check their documentation as well! NXFullGraph ( source_frame = None , item_exo_properties = None , item_contents_dir = None , user_exo_properties = None , user_contents_dir = None , link_label = None ) Bases: NXTripartiteGraph , FullDiGraph Class that implements a Full graph through networkx library. Info A Full Graph is a graph which doesn't impose any particular restriction It creates a graph from an initial Rating object. Consider the following matrix representation of the Rating object +------+-----------+-------+ | User | Item | Score | +------+-----------+-------+ | u1 | Tenet | 4 | | u2 | Inception | 5 | | ... | ... | ... | +------+-----------+-------+ The graph will be created with the following interactions: 4 u1 -----> Tenet 5 u2 -----> Inception where u1 and u2 become User nodes and Tenet and Inception become Item nodes , with the edge weighted depending on the score given If the link_label parameter is specified, then each link between users and items will be labeled with the label specified (e.g. link_label='score' ): (4, 'score') u1 -------------> Tenet (5, 'score') u2 -------------> Inception Then the framework tries to load 'Tenet' and 'Inception' from the item_contents_dir and 'u1' and 'u2' from user_contents_dir if they are specified and if it succeeds, adds in the graph their loaded properties as specified in the item_exo_properties parameter and user_exo_properties . Load exogenous properties In order to load properties in the graph, we must specify where users (and/or) items are serialized and which properties to add (the following is the same for item_exo_properties ): If user_exo_properties is specified as a set , then the graph will try to load all properties from said exogenous representation { 'my_exo_id' } If user_exo_properties is specified as a dict , then the graph will try to load said properties from said exogenous representation { 'my_exo_id' : [ 'my_prop1' , 'my_prop2' ]]} Parameters: Name Type Description Default source_frame Ratings The initial Ratings object needed to create the graph None item_exo_properties Union [ Dict , set ] Set or Dict which contains representations to load from items. Use a Set if you want to load all properties from specific representations, or use a Dict if you want to choose which properties to load from specific representations None item_contents_dir str The path containing items serialized with the Content Analyzer None user_exo_properties Union [ Dict , set ] Set or Dict which contains representations to load from items. Use a Set if you want to load all properties from specific representations, or use a Dict if you want to choose which properties to load from specific representations None user_contents_dir str The path containing users serialized with the Content Analyzer None link_label str If specified, each link will be labeled with the given label. Default is None None Source code in clayrs/recsys/graphs/nx_implementation/nx_full_graphs.py 94 95 96 97 98 99 100 101 102 103 104 105 106 def __init__ ( self , source_frame : Ratings = None , item_exo_properties : Union [ Dict , set ] = None , item_contents_dir : str = None , user_exo_properties : Union [ Dict , set ] = None , user_contents_dir : str = None , link_label : str = None ): NXTripartiteGraph . __init__ ( self , source_frame , item_exo_properties , item_contents_dir , link_label ) if source_frame is not None and user_contents_dir is not None : self . add_node_with_prop ([ UserNode ( user_id ) for user_id in set ( source_frame . user_id_column )], user_exo_properties , user_contents_dir ) add_link ( start_node , final_node , weight = None , label = None , timestamp = None ) Creates a weighted link connecting the 'start_node' to the 'final_node' Both nodes must be present in the graph before calling this method 'weight' and 'label' are optional parameters, if not specified default values will be used. Parameters: Name Type Description Default start_node object starting node of the link required final_node object ending node of the link required weight float weight of the link, default is 0.5 None label str label of the link, default is 'score_label' None Source code in clayrs/recsys/graphs/nx_implementation/nx_full_graphs.py 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 def add_link ( self , start_node : Union [ Node , List [ Node ]], final_node : Union [ Node , List [ Node ]], weight : float = None , label : str = None , timestamp : str = None ): \"\"\" Creates a weighted link connecting the 'start_node' to the 'final_node' Both nodes must be present in the graph before calling this method 'weight' and 'label' are optional parameters, if not specified default values will be used. Args: start_node (object): starting node of the link final_node (object): ending node of the link weight (float): weight of the link, default is 0.5 label (str): label of the link, default is 'score_label' \"\"\" if not isinstance ( start_node , list ): start_node = [ start_node ] if not isinstance ( final_node , list ): final_node = [ final_node ] self . add_node ( start_node ) self . add_node ( final_node ) not_none_dict = {} if label is not None : not_none_dict [ 'label' ] = label if weight is not None : not_none_dict [ 'weight' ] = weight if timestamp is not None : not_none_dict [ 'timestamp' ] = timestamp self . _graph . add_edges_from ( zip ( start_node , final_node ), ** not_none_dict ) add_node ( node ) Adds one or multiple Node objects to the graph. Since this is a Full Graph, any category of node is allowed No duplicates are allowed, but different category nodes with same id are (e.g. ItemNode('1') and UserNode('1') ) Parameters: Name Type Description Default node Union [ Node , List [ Node ]] Node(s) object(s) that needs to be added to the graph required Source code in clayrs/recsys/graphs/nx_implementation/nx_full_graphs.py 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 def add_node ( self , node : Union [ Node , List [ Node ]]): \"\"\" Adds one or multiple Node objects to the graph. Since this is a Full Graph, any category of node is allowed No duplicates are allowed, but different category nodes with same id are (e.g. `ItemNode('1')` and `UserNode('1')`) Args: node: Node(s) object(s) that needs to be added to the graph \"\"\" if not isinstance ( node , list ): node = [ node ] self . _graph . add_nodes_from ( node ) add_node_with_prop ( node , exo_properties , contents_dir , content_filename = None ) Adds one or multiple Node objects and its/their properties to the graph Since this is a Full Graph, no restriction are imposed and you can add any category of node together with its properties. In order to load properties in the graph, we must specify where contents are serialized and which properties to add (the following is the same for item_exo_properties ): If exo_properties is specified as a set , then the graph will try to load all properties from said exogenous representation { 'my_exo_id' } If exo_properties is specified as a dict , then the graph will try to load said properties from said exogenous representation { 'my_exo_id' : [ 'my_prop1' , 'my_prop2' ]]} In case you want your node to have a different id from serialized contents, via the content_filename parameter you can specify what is the filename of the node that you are adding, e.g. item_to_add = ItemNode('different_id') # content_filename is 'item_serialized_1.xz' graph.add_node_with_prop(item_to_add, ..., content_filename='item_serialized_1') In case you are adding a list of nodes, you can specify the filename for each node in the list. Parameters: Name Type Description Default node Union [ Node , List [ Node ]] Node(s) object(s) that needs to be added to the graph along with their properties required exo_properties Union [ Dict , set ] Set or Dict which contains representations to load from items. Use a Set if you want to load all properties from specific representations, or use a Dict if you want to choose which properties to load from specific representations required contents_dir str The path containing items serialized with the Content Analyzer required content_filename Union [ str , List [ str ]] Filename(s) of the node(s) to add None Raises: Type Description ValueError Exception raised when one of the node to add to the graph with their properties is not an ItemNode Source code in clayrs/recsys/graphs/nx_implementation/nx_full_graphs.py 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 def add_node_with_prop ( self , node : Union [ Node , List [ Node ]], exo_properties : Union [ Dict , set ], contents_dir : str , content_filename : Union [ str , List [ str ]] = None ): \"\"\" Adds one or multiple Node objects and its/their properties to the graph Since this is a Full Graph, no restriction are imposed and you can add any category of node together with its properties. In order to load properties in the graph, we must specify where contents are serialized and ***which properties to add*** (the following is the same for *item_exo_properties*): * If *exo_properties* is specified as a **set**, then the graph will try to load **all properties** from **said exogenous representation** ```python {'my_exo_id'} ``` * If *exo_properties* is specified as a **dict**, then the graph will try to load **said properties** from **said exogenous representation** ```python {'my_exo_id': ['my_prop1', 'my_prop2']]} ``` In case you want your node to have a different id from serialized contents, via the `content_filename` parameter you can specify what is the filename of the node that you are adding, e.g. ``` item_to_add = ItemNode('different_id') # content_filename is 'item_serialized_1.xz' graph.add_node_with_prop(item_to_add, ..., content_filename='item_serialized_1') ``` In case you are adding a list of nodes, you can specify the filename for each node in the list. Args: node: Node(s) object(s) that needs to be added to the graph along with their properties exo_properties: Set or Dict which contains representations to load from items. Use a `Set` if you want to load all properties from specific representations, or use a `Dict` if you want to choose which properties to load from specific representations contents_dir: The path containing items serialized with the Content Analyzer content_filename: Filename(s) of the node(s) to add Raises: ValueError: Exception raised when one of the node to add to the graph with their properties is not an ItemNode \"\"\" def node_prop_link_generator (): for n , id in zip ( progbar , content_filename ): item : Content = loaded_items . get ( id ) exo_props = self . _get_exo_props ( exo_properties , item ) single_item_prop_edges = [( n , PropertyNode ( prop_dict [ prop ]), { 'label' : prop }) for prop_dict in exo_props for prop in prop_dict ] yield from single_item_prop_edges if not isinstance ( node , list ): node = [ node ] if isinstance ( exo_properties , set ): exo_properties = dict . fromkeys ( exo_properties , None ) if content_filename is None : content_filename = [ n . value for n in node ] if not isinstance ( content_filename , list ): content_filename = [ content_filename ] loaded_items = LoadedContentsDict ( contents_dir , contents_to_load = set ( content_filename )) with get_progbar ( node ) as progbar : progbar . set_description ( \"Creating Node->Properties links\" ) self . _graph . add_edges_from (( tuple_to_add for tuple_to_add in node_prop_link_generator ()))","title":"Full Graph"},{"location":"recsys/graph_based/graphs/nx_full/#full-graph","text":"Please remember that this class is a subclass of NXTripartiteGraph , so it inherits all its methods. You can check their documentation as well!","title":"Full Graph"},{"location":"recsys/graph_based/graphs/nx_full/#clayrs.recsys.graphs.nx_implementation.nx_full_graphs.NXFullGraph","text":"Bases: NXTripartiteGraph , FullDiGraph Class that implements a Full graph through networkx library. Info A Full Graph is a graph which doesn't impose any particular restriction It creates a graph from an initial Rating object. Consider the following matrix representation of the Rating object +------+-----------+-------+ | User | Item | Score | +------+-----------+-------+ | u1 | Tenet | 4 | | u2 | Inception | 5 | | ... | ... | ... | +------+-----------+-------+ The graph will be created with the following interactions: 4 u1 -----> Tenet 5 u2 -----> Inception where u1 and u2 become User nodes and Tenet and Inception become Item nodes , with the edge weighted depending on the score given If the link_label parameter is specified, then each link between users and items will be labeled with the label specified (e.g. link_label='score' ): (4, 'score') u1 -------------> Tenet (5, 'score') u2 -------------> Inception Then the framework tries to load 'Tenet' and 'Inception' from the item_contents_dir and 'u1' and 'u2' from user_contents_dir if they are specified and if it succeeds, adds in the graph their loaded properties as specified in the item_exo_properties parameter and user_exo_properties . Load exogenous properties In order to load properties in the graph, we must specify where users (and/or) items are serialized and which properties to add (the following is the same for item_exo_properties ): If user_exo_properties is specified as a set , then the graph will try to load all properties from said exogenous representation { 'my_exo_id' } If user_exo_properties is specified as a dict , then the graph will try to load said properties from said exogenous representation { 'my_exo_id' : [ 'my_prop1' , 'my_prop2' ]]} Parameters: Name Type Description Default source_frame Ratings The initial Ratings object needed to create the graph None item_exo_properties Union [ Dict , set ] Set or Dict which contains representations to load from items. Use a Set if you want to load all properties from specific representations, or use a Dict if you want to choose which properties to load from specific representations None item_contents_dir str The path containing items serialized with the Content Analyzer None user_exo_properties Union [ Dict , set ] Set or Dict which contains representations to load from items. Use a Set if you want to load all properties from specific representations, or use a Dict if you want to choose which properties to load from specific representations None user_contents_dir str The path containing users serialized with the Content Analyzer None link_label str If specified, each link will be labeled with the given label. Default is None None Source code in clayrs/recsys/graphs/nx_implementation/nx_full_graphs.py 94 95 96 97 98 99 100 101 102 103 104 105 106 def __init__ ( self , source_frame : Ratings = None , item_exo_properties : Union [ Dict , set ] = None , item_contents_dir : str = None , user_exo_properties : Union [ Dict , set ] = None , user_contents_dir : str = None , link_label : str = None ): NXTripartiteGraph . __init__ ( self , source_frame , item_exo_properties , item_contents_dir , link_label ) if source_frame is not None and user_contents_dir is not None : self . add_node_with_prop ([ UserNode ( user_id ) for user_id in set ( source_frame . user_id_column )], user_exo_properties , user_contents_dir )","title":"NXFullGraph"},{"location":"recsys/graph_based/graphs/nx_full/#clayrs.recsys.graphs.nx_implementation.nx_full_graphs.NXFullGraph.add_link","text":"Creates a weighted link connecting the 'start_node' to the 'final_node' Both nodes must be present in the graph before calling this method 'weight' and 'label' are optional parameters, if not specified default values will be used. Parameters: Name Type Description Default start_node object starting node of the link required final_node object ending node of the link required weight float weight of the link, default is 0.5 None label str label of the link, default is 'score_label' None Source code in clayrs/recsys/graphs/nx_implementation/nx_full_graphs.py 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 def add_link ( self , start_node : Union [ Node , List [ Node ]], final_node : Union [ Node , List [ Node ]], weight : float = None , label : str = None , timestamp : str = None ): \"\"\" Creates a weighted link connecting the 'start_node' to the 'final_node' Both nodes must be present in the graph before calling this method 'weight' and 'label' are optional parameters, if not specified default values will be used. Args: start_node (object): starting node of the link final_node (object): ending node of the link weight (float): weight of the link, default is 0.5 label (str): label of the link, default is 'score_label' \"\"\" if not isinstance ( start_node , list ): start_node = [ start_node ] if not isinstance ( final_node , list ): final_node = [ final_node ] self . add_node ( start_node ) self . add_node ( final_node ) not_none_dict = {} if label is not None : not_none_dict [ 'label' ] = label if weight is not None : not_none_dict [ 'weight' ] = weight if timestamp is not None : not_none_dict [ 'timestamp' ] = timestamp self . _graph . add_edges_from ( zip ( start_node , final_node ), ** not_none_dict )","title":"add_link()"},{"location":"recsys/graph_based/graphs/nx_full/#clayrs.recsys.graphs.nx_implementation.nx_full_graphs.NXFullGraph.add_node","text":"Adds one or multiple Node objects to the graph. Since this is a Full Graph, any category of node is allowed No duplicates are allowed, but different category nodes with same id are (e.g. ItemNode('1') and UserNode('1') ) Parameters: Name Type Description Default node Union [ Node , List [ Node ]] Node(s) object(s) that needs to be added to the graph required Source code in clayrs/recsys/graphs/nx_implementation/nx_full_graphs.py 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 def add_node ( self , node : Union [ Node , List [ Node ]]): \"\"\" Adds one or multiple Node objects to the graph. Since this is a Full Graph, any category of node is allowed No duplicates are allowed, but different category nodes with same id are (e.g. `ItemNode('1')` and `UserNode('1')`) Args: node: Node(s) object(s) that needs to be added to the graph \"\"\" if not isinstance ( node , list ): node = [ node ] self . _graph . add_nodes_from ( node )","title":"add_node()"},{"location":"recsys/graph_based/graphs/nx_full/#clayrs.recsys.graphs.nx_implementation.nx_full_graphs.NXFullGraph.add_node_with_prop","text":"Adds one or multiple Node objects and its/their properties to the graph Since this is a Full Graph, no restriction are imposed and you can add any category of node together with its properties. In order to load properties in the graph, we must specify where contents are serialized and which properties to add (the following is the same for item_exo_properties ): If exo_properties is specified as a set , then the graph will try to load all properties from said exogenous representation { 'my_exo_id' } If exo_properties is specified as a dict , then the graph will try to load said properties from said exogenous representation { 'my_exo_id' : [ 'my_prop1' , 'my_prop2' ]]} In case you want your node to have a different id from serialized contents, via the content_filename parameter you can specify what is the filename of the node that you are adding, e.g. item_to_add = ItemNode('different_id') # content_filename is 'item_serialized_1.xz' graph.add_node_with_prop(item_to_add, ..., content_filename='item_serialized_1') In case you are adding a list of nodes, you can specify the filename for each node in the list. Parameters: Name Type Description Default node Union [ Node , List [ Node ]] Node(s) object(s) that needs to be added to the graph along with their properties required exo_properties Union [ Dict , set ] Set or Dict which contains representations to load from items. Use a Set if you want to load all properties from specific representations, or use a Dict if you want to choose which properties to load from specific representations required contents_dir str The path containing items serialized with the Content Analyzer required content_filename Union [ str , List [ str ]] Filename(s) of the node(s) to add None Raises: Type Description ValueError Exception raised when one of the node to add to the graph with their properties is not an ItemNode Source code in clayrs/recsys/graphs/nx_implementation/nx_full_graphs.py 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 def add_node_with_prop ( self , node : Union [ Node , List [ Node ]], exo_properties : Union [ Dict , set ], contents_dir : str , content_filename : Union [ str , List [ str ]] = None ): \"\"\" Adds one or multiple Node objects and its/their properties to the graph Since this is a Full Graph, no restriction are imposed and you can add any category of node together with its properties. In order to load properties in the graph, we must specify where contents are serialized and ***which properties to add*** (the following is the same for *item_exo_properties*): * If *exo_properties* is specified as a **set**, then the graph will try to load **all properties** from **said exogenous representation** ```python {'my_exo_id'} ``` * If *exo_properties* is specified as a **dict**, then the graph will try to load **said properties** from **said exogenous representation** ```python {'my_exo_id': ['my_prop1', 'my_prop2']]} ``` In case you want your node to have a different id from serialized contents, via the `content_filename` parameter you can specify what is the filename of the node that you are adding, e.g. ``` item_to_add = ItemNode('different_id') # content_filename is 'item_serialized_1.xz' graph.add_node_with_prop(item_to_add, ..., content_filename='item_serialized_1') ``` In case you are adding a list of nodes, you can specify the filename for each node in the list. Args: node: Node(s) object(s) that needs to be added to the graph along with their properties exo_properties: Set or Dict which contains representations to load from items. Use a `Set` if you want to load all properties from specific representations, or use a `Dict` if you want to choose which properties to load from specific representations contents_dir: The path containing items serialized with the Content Analyzer content_filename: Filename(s) of the node(s) to add Raises: ValueError: Exception raised when one of the node to add to the graph with their properties is not an ItemNode \"\"\" def node_prop_link_generator (): for n , id in zip ( progbar , content_filename ): item : Content = loaded_items . get ( id ) exo_props = self . _get_exo_props ( exo_properties , item ) single_item_prop_edges = [( n , PropertyNode ( prop_dict [ prop ]), { 'label' : prop }) for prop_dict in exo_props for prop in prop_dict ] yield from single_item_prop_edges if not isinstance ( node , list ): node = [ node ] if isinstance ( exo_properties , set ): exo_properties = dict . fromkeys ( exo_properties , None ) if content_filename is None : content_filename = [ n . value for n in node ] if not isinstance ( content_filename , list ): content_filename = [ content_filename ] loaded_items = LoadedContentsDict ( contents_dir , contents_to_load = set ( content_filename )) with get_progbar ( node ) as progbar : progbar . set_description ( \"Creating Node->Properties links\" ) self . _graph . add_edges_from (( tuple_to_add for tuple_to_add in node_prop_link_generator ()))","title":"add_node_with_prop()"},{"location":"recsys/graph_based/graphs/nx_tripartite/","text":"Tripartite Graph Please remember that this class is a subclass of NXBipartiteGraph , so it inherits all its methods. You can check their documentation as well! NXTripartiteGraph ( source_frame = None , item_exo_properties = None , item_contents_dir = None , link_label = None ) Bases: NXBipartiteGraph , TripartiteDiGraph Class that implements a Tripartite graph through networkx library. Info A Tripartite Graph is a graph which supports User nodes, Item nodes and Property nodes, but the latter can only be linked to Item nodes. If you need maximum flexibility, consider using a Full Graph It creates a graph from an initial Rating object. Consider the following matrix representation of the Rating object +------+-----------+-------+ | User | Item | Score | +------+-----------+-------+ | u1 | Tenet | 4 | | u2 | Inception | 5 | | ... | ... | ... | +------+-----------+-------+ The graph will be created with the following interactions: 4 u1 -----> Tenet 5 u2 -----> Inception where u1 and u2 become User nodes and Tenet and Inception become Item nodes , with the edge weighted depending on the score given If the link_label parameter is specified, then each link between users and items will be labeled with the label specified (e.g. link_label='score' ): (4, 'score') u1 -------------> Tenet (5, 'score') u2 -------------> Inception Then the framework tries to load 'Tenet' and 'Inception' from the item_contents_dir if it is specified and if succeeds, adds in the graph their loaded properties as specified in the item_exo_properties parameter. Load exogenous properties In order to load properties in the graph, we must specify where items are serialized and which properties to add : If item_exo_properties is specified as a set , then the graph will try to load all properties from said exogenous representation { 'my_exo_id' } If item_exo_properties is specified as a dict , then the graph will try to load said properties from said exogenous representation { 'my_exo_id' : [ 'my_prop1' , 'my_prop2' ]]} Parameters: Name Type Description Default source_frame Ratings The initial Ratings object needed to create the graph None item_exo_properties Union [ Dict , set ] Set or Dict which contains representations to load from items. Use a Set if you want to load all properties from specific representations, or use a Dict if you want to choose which properties to load from specific representations None item_contents_dir str The path containing items serialized with the Content Analyzer None link_label str If specified, each link will be labeled with the given label. Default is None None Source code in clayrs/recsys/graphs/nx_implementation/nx_tripartite_graphs.py 93 94 95 96 97 98 99 100 101 102 103 def __init__ ( self , source_frame : Ratings = None , item_exo_properties : Union [ Dict , set ] = None , item_contents_dir : str = None , link_label : str = None ): NXBipartiteGraph . __init__ ( self , source_frame , link_label ) if source_frame is not None and item_contents_dir is not None : self . add_node_with_prop ([ ItemNode ( item_id ) for item_id in set ( source_frame . item_id_column )], item_exo_properties , item_contents_dir ) add_link ( start_node , final_node , weight = None , label = None , timestamp = None ) Creates a link connecting the start_node to the final_node . If two lists are passed, then the node in position \\(i\\) in the start_node list will be linked to the node in position \\(i\\) in the final_node list. If nodes to link do not exist, they will be added automatically to the graph. Please remember that since this is a Tripartite Graph, only User nodes , Item nodes and Property nodes can be added! And Property nodes can only be linked to Item nodes ! A link can be weighted with the weight parameter and labeled with the label parameter. A timestamp can also be specified via timestamp parameter. All three are optional parameters, so they are not required Parameters: Name Type Description Default start_node Union [ Node , List [ Node ]] Single Node object or a list of Node objects. They will be the 'head' of the link, since it's a directed graph required final_node object Single Node object or a list Node objects. They will be the 'tail' of the link, since it's a directed graph required weight float weight of the link, default is None (no weight) None label str label of the link, default is None (no label) None timestamp str timestamp of the link, default is None (no timestamp) None Raises: Type Description ValueError Exception raised when Property nodes are tried to be linked with non-Item nodes Source code in clayrs/recsys/graphs/nx_implementation/nx_tripartite_graphs.py 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 def add_link ( self , start_node : Union [ Node , List [ Node ]], final_node : Union [ Node , List [ Node ]], weight : float = None , label : str = None , timestamp : str = None ): \"\"\" Creates a link connecting the `start_node` to the `final_node`. If two lists are passed, then the node in position $i$ in the `start_node` list will be linked to the node in position $i$ in the `final_node` list. If nodes to link do not exist, they will be added automatically to the graph. Please remember that since this is a Tripartite Graph, only *User nodes*, *Item nodes* and *Property nodes* can be added! And *Property nodes* can only be linked to *Item nodes*! A link can be weighted with the `weight` parameter and labeled with the `label` parameter. A timestamp can also be specified via `timestamp` parameter. All three are optional parameters, so they are not required Args: start_node: Single Node object or a list of Node objects. They will be the 'head' of the link, since it's a directed graph final_node (object): Single Node object or a list Node objects. They will be the 'tail' of the link, since it's a directed graph weight: weight of the link, default is None (no weight) label: label of the link, default is None (no label) timestamp: timestamp of the link, default is None (no timestamp) Raises: ValueError: Exception raised when Property nodes are tried to be linked with non-Item nodes \"\"\" def is_not_valid_link ( start_n : Node , final_n : Node ): return ( isinstance ( final_n , PropertyNode ) and not isinstance ( start_n , ItemNode )) or \\ ( isinstance ( start_n , PropertyNode ) and not isinstance ( final_n , ItemNode )) if not isinstance ( start_node , list ): start_node = [ start_node ] if not isinstance ( final_node , list ): final_node = [ final_node ] if any ( is_not_valid_link ( start_n , final_n ) for start_n , final_n in zip ( start_node , final_node )): raise ValueError ( \"Only item nodes can be linked to property nodes in a Tripartite Graph!\" ) self . add_node ( start_node ) self . add_node ( final_node ) not_none_dict = {} if label is not None : not_none_dict [ 'label' ] = label if weight is not None : not_none_dict [ 'weight' ] = weight if timestamp is not None : not_none_dict [ 'timestamp' ] = timestamp self . _graph . add_edges_from ( zip ( start_node , final_node ), ** not_none_dict ) add_node ( node ) Adds one or multiple Node objects to the graph. Since this is a Tripartite Graph, only User Node , Item Node and Property Node can be added! No duplicates are allowed, but different category nodes with same id are (e.g. ItemNode('1') and UserNode('1') ) Parameters: Name Type Description Default node Union [ Node , List [ Node ]] Node(s) object(s) that needs to be added to the graph required Raises: Type Description ValueError Exception raised when one of the node to add to the graph is not a User, Item or Property node Source code in clayrs/recsys/graphs/nx_implementation/nx_tripartite_graphs.py 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 def add_node ( self , node : Union [ Node , List [ Node ]]): \"\"\" Adds one or multiple Node objects to the graph. Since this is a Tripartite Graph, only `User Node`, `Item Node` and `Property Node` can be added! No duplicates are allowed, but different category nodes with same id are (e.g. `ItemNode('1')` and `UserNode('1')`) Args: node: Node(s) object(s) that needs to be added to the graph Raises: ValueError: Exception raised when one of the node to add to the graph is not a User, Item or Property node \"\"\" if not isinstance ( node , list ): node = [ node ] if any ( not isinstance ( n , ( UserNode , ItemNode , PropertyNode )) for n in node ): raise ValueError ( \"You can only add UserNodes or ItemNodes to a bipartite graph!\" ) self . _graph . add_nodes_from ( node ) add_node_with_prop ( node , item_exo_properties , item_contents_dir , item_filename = None ) Adds one or multiple Node objects and its/their properties to the graph. Since this is a Tripartite Graph, only Item Node are allowed to have properties! In order to load properties in the graph, we must specify where items are serialized and which properties to add : If item_exo_properties is specified as a set , then the graph will try to load all properties from said exogenous representation { 'my_exo_id' } If item_exo_properties is specified as a dict , then the graph will try to load said properties from said exogenous representation { 'my_exo_id' : [ 'my_prop1' , 'my_prop2' ]]} In case you want your node to have a different id from serialized contents, via the item_filename parameter you can specify what is the filename of the node that you are adding, e.g. item_to_add = ItemNode('different_id') # item_filename is 'item_serialized_1.xz' graph.add_node_with_prop(item_to_add, ..., item_filename='item_serialized_1') In case you are adding a list of nodes, you can specify the filename for each node in the list. Parameters: Name Type Description Default node Union [ ItemNode , List [ ItemNode ]] Node(s) object(s) that needs to be added to the graph along with their properties required item_exo_properties Union [ Dict , set ] Set or Dict which contains representations to load from items. Use a Set if you want to load all properties from specific representations, or use a Dict if you want to choose which properties to load from specific representations required item_contents_dir str The path containing items serialized with the Content Analyzer required item_filename Union [ str , List [ str ]] Filename(s) of the node(s) to add None Raises: Type Description ValueError Exception raised when one of the node to add to the graph with their properties is not an ItemNode Source code in clayrs/recsys/graphs/nx_implementation/nx_tripartite_graphs.py 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 def add_node_with_prop ( self , node : Union [ ItemNode , List [ ItemNode ]], item_exo_properties : Union [ Dict , set ], item_contents_dir : str , item_filename : Union [ str , List [ str ]] = None ): \"\"\" Adds one or multiple Node objects and its/their properties to the graph. Since this is a Tripartite Graph, only `Item Node` are allowed to have properties! In order to load properties in the graph, we must specify where items are serialized and ***which properties to add***: * If *item_exo_properties* is specified as a **set**, then the graph will try to load **all properties** from **said exogenous representation** ```python {'my_exo_id'} ``` * If *item_exo_properties* is specified as a **dict**, then the graph will try to load **said properties** from **said exogenous representation** ```python {'my_exo_id': ['my_prop1', 'my_prop2']]} ``` In case you want your node to have a different id from serialized contents, via the `item_filename` parameter you can specify what is the filename of the node that you are adding, e.g. ``` item_to_add = ItemNode('different_id') # item_filename is 'item_serialized_1.xz' graph.add_node_with_prop(item_to_add, ..., item_filename='item_serialized_1') ``` In case you are adding a list of nodes, you can specify the filename for each node in the list. Args: node: Node(s) object(s) that needs to be added to the graph along with their properties item_exo_properties: Set or Dict which contains representations to load from items. Use a `Set` if you want to load all properties from specific representations, or use a `Dict` if you want to choose which properties to load from specific representations item_contents_dir: The path containing items serialized with the Content Analyzer item_filename: Filename(s) of the node(s) to add Raises: ValueError: Exception raised when one of the node to add to the graph with their properties is not an ItemNode \"\"\" def node_prop_link_generator (): for n , id in zip ( progbar , item_filename ): item : Content = loaded_items . get ( id ) exo_props = self . _get_exo_props ( item_exo_properties , item ) single_item_prop_edges = [( n , PropertyNode ( prop_dict [ prop ]), { 'label' : prop }) for prop_dict in exo_props for prop in prop_dict ] yield from single_item_prop_edges if not isinstance ( node , list ): node = [ node ] if any ( not isinstance ( n , ItemNode ) for n in node ): raise ValueError ( \"Only item nodes can be linked to property nodes in a Tripartite Graph!\" ) if isinstance ( item_exo_properties , set ): item_exo_properties = dict . fromkeys ( item_exo_properties , None ) if item_filename is None : item_filename = [ n . value for n in node ] if not isinstance ( item_filename , list ): item_filename = [ item_filename ] loaded_items = LoadedContentsDict ( item_contents_dir , contents_to_load = set ( item_filename )) with get_progbar ( node ) as progbar : progbar . set_description ( \"Creating Item->Properties links\" ) self . _graph . add_edges_from (( tuple_to_add for tuple_to_add in node_prop_link_generator ())) property_nodes () property Returns a set of all Property nodes in the graph Source code in clayrs/recsys/graphs/nx_implementation/nx_tripartite_graphs.py 105 106 107 108 109 110 @property def property_nodes ( self ) -> Set [ PropertyNode ]: \"\"\" Returns a set of all *Property nodes* in the graph \"\"\" return set ( node for node in self . _graph . nodes if isinstance ( node , PropertyNode ))","title":"Tripartite Graph"},{"location":"recsys/graph_based/graphs/nx_tripartite/#tripartite-graph","text":"Please remember that this class is a subclass of NXBipartiteGraph , so it inherits all its methods. You can check their documentation as well!","title":"Tripartite Graph"},{"location":"recsys/graph_based/graphs/nx_tripartite/#clayrs.recsys.graphs.nx_implementation.nx_tripartite_graphs.NXTripartiteGraph","text":"Bases: NXBipartiteGraph , TripartiteDiGraph Class that implements a Tripartite graph through networkx library. Info A Tripartite Graph is a graph which supports User nodes, Item nodes and Property nodes, but the latter can only be linked to Item nodes. If you need maximum flexibility, consider using a Full Graph It creates a graph from an initial Rating object. Consider the following matrix representation of the Rating object +------+-----------+-------+ | User | Item | Score | +------+-----------+-------+ | u1 | Tenet | 4 | | u2 | Inception | 5 | | ... | ... | ... | +------+-----------+-------+ The graph will be created with the following interactions: 4 u1 -----> Tenet 5 u2 -----> Inception where u1 and u2 become User nodes and Tenet and Inception become Item nodes , with the edge weighted depending on the score given If the link_label parameter is specified, then each link between users and items will be labeled with the label specified (e.g. link_label='score' ): (4, 'score') u1 -------------> Tenet (5, 'score') u2 -------------> Inception Then the framework tries to load 'Tenet' and 'Inception' from the item_contents_dir if it is specified and if succeeds, adds in the graph their loaded properties as specified in the item_exo_properties parameter. Load exogenous properties In order to load properties in the graph, we must specify where items are serialized and which properties to add : If item_exo_properties is specified as a set , then the graph will try to load all properties from said exogenous representation { 'my_exo_id' } If item_exo_properties is specified as a dict , then the graph will try to load said properties from said exogenous representation { 'my_exo_id' : [ 'my_prop1' , 'my_prop2' ]]} Parameters: Name Type Description Default source_frame Ratings The initial Ratings object needed to create the graph None item_exo_properties Union [ Dict , set ] Set or Dict which contains representations to load from items. Use a Set if you want to load all properties from specific representations, or use a Dict if you want to choose which properties to load from specific representations None item_contents_dir str The path containing items serialized with the Content Analyzer None link_label str If specified, each link will be labeled with the given label. Default is None None Source code in clayrs/recsys/graphs/nx_implementation/nx_tripartite_graphs.py 93 94 95 96 97 98 99 100 101 102 103 def __init__ ( self , source_frame : Ratings = None , item_exo_properties : Union [ Dict , set ] = None , item_contents_dir : str = None , link_label : str = None ): NXBipartiteGraph . __init__ ( self , source_frame , link_label ) if source_frame is not None and item_contents_dir is not None : self . add_node_with_prop ([ ItemNode ( item_id ) for item_id in set ( source_frame . item_id_column )], item_exo_properties , item_contents_dir )","title":"NXTripartiteGraph"},{"location":"recsys/graph_based/graphs/nx_tripartite/#clayrs.recsys.graphs.nx_implementation.nx_tripartite_graphs.NXTripartiteGraph.add_link","text":"Creates a link connecting the start_node to the final_node . If two lists are passed, then the node in position \\(i\\) in the start_node list will be linked to the node in position \\(i\\) in the final_node list. If nodes to link do not exist, they will be added automatically to the graph. Please remember that since this is a Tripartite Graph, only User nodes , Item nodes and Property nodes can be added! And Property nodes can only be linked to Item nodes ! A link can be weighted with the weight parameter and labeled with the label parameter. A timestamp can also be specified via timestamp parameter. All three are optional parameters, so they are not required Parameters: Name Type Description Default start_node Union [ Node , List [ Node ]] Single Node object or a list of Node objects. They will be the 'head' of the link, since it's a directed graph required final_node object Single Node object or a list Node objects. They will be the 'tail' of the link, since it's a directed graph required weight float weight of the link, default is None (no weight) None label str label of the link, default is None (no label) None timestamp str timestamp of the link, default is None (no timestamp) None Raises: Type Description ValueError Exception raised when Property nodes are tried to be linked with non-Item nodes Source code in clayrs/recsys/graphs/nx_implementation/nx_tripartite_graphs.py 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 def add_link ( self , start_node : Union [ Node , List [ Node ]], final_node : Union [ Node , List [ Node ]], weight : float = None , label : str = None , timestamp : str = None ): \"\"\" Creates a link connecting the `start_node` to the `final_node`. If two lists are passed, then the node in position $i$ in the `start_node` list will be linked to the node in position $i$ in the `final_node` list. If nodes to link do not exist, they will be added automatically to the graph. Please remember that since this is a Tripartite Graph, only *User nodes*, *Item nodes* and *Property nodes* can be added! And *Property nodes* can only be linked to *Item nodes*! A link can be weighted with the `weight` parameter and labeled with the `label` parameter. A timestamp can also be specified via `timestamp` parameter. All three are optional parameters, so they are not required Args: start_node: Single Node object or a list of Node objects. They will be the 'head' of the link, since it's a directed graph final_node (object): Single Node object or a list Node objects. They will be the 'tail' of the link, since it's a directed graph weight: weight of the link, default is None (no weight) label: label of the link, default is None (no label) timestamp: timestamp of the link, default is None (no timestamp) Raises: ValueError: Exception raised when Property nodes are tried to be linked with non-Item nodes \"\"\" def is_not_valid_link ( start_n : Node , final_n : Node ): return ( isinstance ( final_n , PropertyNode ) and not isinstance ( start_n , ItemNode )) or \\ ( isinstance ( start_n , PropertyNode ) and not isinstance ( final_n , ItemNode )) if not isinstance ( start_node , list ): start_node = [ start_node ] if not isinstance ( final_node , list ): final_node = [ final_node ] if any ( is_not_valid_link ( start_n , final_n ) for start_n , final_n in zip ( start_node , final_node )): raise ValueError ( \"Only item nodes can be linked to property nodes in a Tripartite Graph!\" ) self . add_node ( start_node ) self . add_node ( final_node ) not_none_dict = {} if label is not None : not_none_dict [ 'label' ] = label if weight is not None : not_none_dict [ 'weight' ] = weight if timestamp is not None : not_none_dict [ 'timestamp' ] = timestamp self . _graph . add_edges_from ( zip ( start_node , final_node ), ** not_none_dict )","title":"add_link()"},{"location":"recsys/graph_based/graphs/nx_tripartite/#clayrs.recsys.graphs.nx_implementation.nx_tripartite_graphs.NXTripartiteGraph.add_node","text":"Adds one or multiple Node objects to the graph. Since this is a Tripartite Graph, only User Node , Item Node and Property Node can be added! No duplicates are allowed, but different category nodes with same id are (e.g. ItemNode('1') and UserNode('1') ) Parameters: Name Type Description Default node Union [ Node , List [ Node ]] Node(s) object(s) that needs to be added to the graph required Raises: Type Description ValueError Exception raised when one of the node to add to the graph is not a User, Item or Property node Source code in clayrs/recsys/graphs/nx_implementation/nx_tripartite_graphs.py 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 def add_node ( self , node : Union [ Node , List [ Node ]]): \"\"\" Adds one or multiple Node objects to the graph. Since this is a Tripartite Graph, only `User Node`, `Item Node` and `Property Node` can be added! No duplicates are allowed, but different category nodes with same id are (e.g. `ItemNode('1')` and `UserNode('1')`) Args: node: Node(s) object(s) that needs to be added to the graph Raises: ValueError: Exception raised when one of the node to add to the graph is not a User, Item or Property node \"\"\" if not isinstance ( node , list ): node = [ node ] if any ( not isinstance ( n , ( UserNode , ItemNode , PropertyNode )) for n in node ): raise ValueError ( \"You can only add UserNodes or ItemNodes to a bipartite graph!\" ) self . _graph . add_nodes_from ( node )","title":"add_node()"},{"location":"recsys/graph_based/graphs/nx_tripartite/#clayrs.recsys.graphs.nx_implementation.nx_tripartite_graphs.NXTripartiteGraph.add_node_with_prop","text":"Adds one or multiple Node objects and its/their properties to the graph. Since this is a Tripartite Graph, only Item Node are allowed to have properties! In order to load properties in the graph, we must specify where items are serialized and which properties to add : If item_exo_properties is specified as a set , then the graph will try to load all properties from said exogenous representation { 'my_exo_id' } If item_exo_properties is specified as a dict , then the graph will try to load said properties from said exogenous representation { 'my_exo_id' : [ 'my_prop1' , 'my_prop2' ]]} In case you want your node to have a different id from serialized contents, via the item_filename parameter you can specify what is the filename of the node that you are adding, e.g. item_to_add = ItemNode('different_id') # item_filename is 'item_serialized_1.xz' graph.add_node_with_prop(item_to_add, ..., item_filename='item_serialized_1') In case you are adding a list of nodes, you can specify the filename for each node in the list. Parameters: Name Type Description Default node Union [ ItemNode , List [ ItemNode ]] Node(s) object(s) that needs to be added to the graph along with their properties required item_exo_properties Union [ Dict , set ] Set or Dict which contains representations to load from items. Use a Set if you want to load all properties from specific representations, or use a Dict if you want to choose which properties to load from specific representations required item_contents_dir str The path containing items serialized with the Content Analyzer required item_filename Union [ str , List [ str ]] Filename(s) of the node(s) to add None Raises: Type Description ValueError Exception raised when one of the node to add to the graph with their properties is not an ItemNode Source code in clayrs/recsys/graphs/nx_implementation/nx_tripartite_graphs.py 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 def add_node_with_prop ( self , node : Union [ ItemNode , List [ ItemNode ]], item_exo_properties : Union [ Dict , set ], item_contents_dir : str , item_filename : Union [ str , List [ str ]] = None ): \"\"\" Adds one or multiple Node objects and its/their properties to the graph. Since this is a Tripartite Graph, only `Item Node` are allowed to have properties! In order to load properties in the graph, we must specify where items are serialized and ***which properties to add***: * If *item_exo_properties* is specified as a **set**, then the graph will try to load **all properties** from **said exogenous representation** ```python {'my_exo_id'} ``` * If *item_exo_properties* is specified as a **dict**, then the graph will try to load **said properties** from **said exogenous representation** ```python {'my_exo_id': ['my_prop1', 'my_prop2']]} ``` In case you want your node to have a different id from serialized contents, via the `item_filename` parameter you can specify what is the filename of the node that you are adding, e.g. ``` item_to_add = ItemNode('different_id') # item_filename is 'item_serialized_1.xz' graph.add_node_with_prop(item_to_add, ..., item_filename='item_serialized_1') ``` In case you are adding a list of nodes, you can specify the filename for each node in the list. Args: node: Node(s) object(s) that needs to be added to the graph along with their properties item_exo_properties: Set or Dict which contains representations to load from items. Use a `Set` if you want to load all properties from specific representations, or use a `Dict` if you want to choose which properties to load from specific representations item_contents_dir: The path containing items serialized with the Content Analyzer item_filename: Filename(s) of the node(s) to add Raises: ValueError: Exception raised when one of the node to add to the graph with their properties is not an ItemNode \"\"\" def node_prop_link_generator (): for n , id in zip ( progbar , item_filename ): item : Content = loaded_items . get ( id ) exo_props = self . _get_exo_props ( item_exo_properties , item ) single_item_prop_edges = [( n , PropertyNode ( prop_dict [ prop ]), { 'label' : prop }) for prop_dict in exo_props for prop in prop_dict ] yield from single_item_prop_edges if not isinstance ( node , list ): node = [ node ] if any ( not isinstance ( n , ItemNode ) for n in node ): raise ValueError ( \"Only item nodes can be linked to property nodes in a Tripartite Graph!\" ) if isinstance ( item_exo_properties , set ): item_exo_properties = dict . fromkeys ( item_exo_properties , None ) if item_filename is None : item_filename = [ n . value for n in node ] if not isinstance ( item_filename , list ): item_filename = [ item_filename ] loaded_items = LoadedContentsDict ( item_contents_dir , contents_to_load = set ( item_filename )) with get_progbar ( node ) as progbar : progbar . set_description ( \"Creating Item->Properties links\" ) self . _graph . add_edges_from (( tuple_to_add for tuple_to_add in node_prop_link_generator ()))","title":"add_node_with_prop()"},{"location":"recsys/graph_based/graphs/nx_tripartite/#clayrs.recsys.graphs.nx_implementation.nx_tripartite_graphs.NXTripartiteGraph.property_nodes","text":"Returns a set of all Property nodes in the graph Source code in clayrs/recsys/graphs/nx_implementation/nx_tripartite_graphs.py 105 106 107 108 109 110 @property def property_nodes ( self ) -> Set [ PropertyNode ]: \"\"\" Returns a set of all *Property nodes* in the graph \"\"\" return set ( node for node in self . _graph . nodes if isinstance ( node , PropertyNode ))","title":"property_nodes()"}]}