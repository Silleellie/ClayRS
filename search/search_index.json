{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Warning</p> <p>Docs are complete, but revision is still a Work in Progress. Sorry for any typos!</p> <p> </p>"},{"location":"#welcome-to-clayrss-documentation","title":"Welcome to ClayRS's documentation!","text":"<p>ClayRS is a python framework for (mainly) content-based recommender systems which allows you to perform several operations, starting from a raw representation of users and items to building and evaluating a recommender system. It also supports graph-based recommendation with feature selection algorithms and graph manipulation methods.</p> <p>The framework has three main modules, which you can also use individually:</p> <p> </p> <p>Given a raw source, the Content Analyzer:</p> <ul> <li>Creates and serializes contents,</li> <li>Using the chosen configuration</li> </ul> <p>The RecSys module allows to:</p> <ul> <li>Instantiate a recommender system<ul> <li>Using items and users serialized by the Content Analyzer</li> </ul> </li> <li>Make score prediction or recommend items for the active user(s)</li> </ul> <p>The EvalModel has the task of evaluating a recommender system, using several state-of-the-art metrics</p> <p>The various sections of this documentation will guide you in becoming a full expert of ClayRS!</p>"},{"location":"content_analyzer/config/","title":"Content Analyzer Config","text":""},{"location":"content_analyzer/config/#clayrs.content_analyzer.config.ContentAnalyzerConfig","title":"<code>ContentAnalyzerConfig(source, id, output_directory, field_dict=None, exogenous_representation_list=None, export_json=False)</code>","text":"<p>         Bases: <code>ABC</code></p> <p>Abstract class that represents the configuration for the content analyzer. The configuration specifies how the <code>Content Analyzer</code> needs to complexly represent contents, i.e. how to preprocess them and how to represent them</p> PARAMETER DESCRIPTION <code>source</code> <p>Raw data source wrapper which contains original information about contents to process</p> <p> TYPE: <code>RawInformationSource</code> </p> <code>id</code> <p>Field of the raw source which represents each content uniquely.</p> <p> TYPE: <code>Union[str, List[str]]</code> </p> <code>output_directory</code> <p>Where contents complexly represented will be serialized</p> <p> TYPE: <code>str</code> </p> <code>field_dict</code> <p>Dictionary object which contains, for each field of the raw source to process, a FieldConfig object (e.g. <code>{'plot': FieldConfig(SkLearnTfIdf(), 'genres': FieldConfig(WhooshTfIdf()))}</code>)</p> <p> TYPE: <code>Dict[str, List[FieldConfig]]</code> DEFAULT: <code>None</code> </p> <code>exogenous_representation_list</code> <p>List of <code>ExogenousTechnique</code> objects that will be used to expand each contents with data from external sources</p> <p> TYPE: <code>Union[ExogenousConfig, List[ExogenousConfig]]</code> DEFAULT: <code>None</code> </p> <code>export_json</code> <p>If set to True, contents complexly represented will be serialized in a human readable JSON, other than in a proprietary format of the framework</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>clayrs/content_analyzer/config.py</code> <pre><code>def __init__(self, source: RawInformationSource,\n             id: Union[str, List[str]],\n             output_directory: str,\n             field_dict: Dict[str, List[FieldConfig]] = None,\n             exogenous_representation_list: Union[ExogenousConfig, List[ExogenousConfig]] = None,\n             export_json: bool = False):\n    if field_dict is None:\n        field_dict = {}\n    if exogenous_representation_list is None:\n        exogenous_representation_list = []\n\n    self.__source = source\n    self.__id = id\n    self.__output_directory = output_directory\n    self.__field_dict = field_dict\n    self.__exogenous_representation_list = exogenous_representation_list\n    self.__export_json = export_json\n\n    if not isinstance(self.__exogenous_representation_list, list):\n        self.__exogenous_representation_list = [self.__exogenous_representation_list]\n\n    if not isinstance(self.__id, list):\n        self.__id = [self.__id]\n</code></pre>"},{"location":"content_analyzer/config/#clayrs.content_analyzer.config.ContentAnalyzerConfig.exogenous_representation_list","title":"<code>exogenous_representation_list: List[ExogenousConfig]</code>  <code>property</code>","text":"<p>Getter for the exogenous_representation_list</p>"},{"location":"content_analyzer/config/#clayrs.content_analyzer.config.ContentAnalyzerConfig.export_json","title":"<code>export_json: bool</code>  <code>property</code>","text":"<p>Getter for the export_json parameter</p>"},{"location":"content_analyzer/config/#clayrs.content_analyzer.config.ContentAnalyzerConfig.id","title":"<code>id: List[str]</code>  <code>property</code>","text":"<p>Getter for the id that represents the ids of the produced contents</p>"},{"location":"content_analyzer/config/#clayrs.content_analyzer.config.ContentAnalyzerConfig.output_directory","title":"<code>output_directory</code>  <code>property</code>","text":"<p>Getter for the output directory where the produced contents will be stored</p>"},{"location":"content_analyzer/config/#clayrs.content_analyzer.config.ContentAnalyzerConfig.source","title":"<code>source: RawInformationSource</code>  <code>property</code>","text":"<p>Getter for the raw information source where the original contents are stored</p>"},{"location":"content_analyzer/config/#clayrs.content_analyzer.config.ContentAnalyzerConfig.add_multiple_config","title":"<code>add_multiple_config(field_name, config_list)</code>","text":"<p>Method which adds multiple complex representations for the <code>field_name</code> of the raw source</p> <p>Examples:</p> <ul> <li>Represent preprocessed field \"Plot\" of the raw source with a tf-idf technique using sklearn and a word embedding technique using Word2Vec. For the latter, no preprocessing operation will be applied</li> </ul> <pre><code>&gt;&gt;&gt; import clayrs.content_analyzer as ca\n&gt;&gt;&gt; movies_ca_config.add_multiple_config(\"Plot\",\n&gt;&gt;&gt;                                       [FieldConfig(ca.SkLearnTfIdf(),\n&gt;&gt;&gt;                                                    preprocessing=ca.NLTK(stopwords_removal=True)),\n&gt;&gt;&gt;\n&gt;&gt;&gt;                                        FieldConfig(ca.WordEmbeddingTechnique(ca.GensimWord2Vec()))]\n</code></pre> PARAMETER DESCRIPTION <code>field_name</code> <p>field name of the raw source which must be complexly represented</p> <p> TYPE: <code>str</code> </p> <code>config_list</code> <p>List of <code>FieldConfig</code> objects specifying how to represent the field of the raw source</p> <p> TYPE: <code>List[FieldConfig]</code> </p> Source code in <code>clayrs/content_analyzer/config.py</code> <pre><code>def add_multiple_config(self, field_name: str, config_list: List[FieldConfig]):\n\"\"\"\n    Method which adds multiple complex representations for the `field_name` of the raw source\n\n    Examples:\n\n        * Represent preprocessed field \"Plot\" of the raw source with a tf-idf technique using sklearn and a word\n        embedding technique using Word2Vec. For the latter, no preprocessing operation will be applied\n        &gt;&gt;&gt; import clayrs.content_analyzer as ca\n        &gt;&gt;&gt; movies_ca_config.add_multiple_config(\"Plot\",\n        &gt;&gt;&gt;                                       [FieldConfig(ca.SkLearnTfIdf(),\n        &gt;&gt;&gt;                                                    preprocessing=ca.NLTK(stopwords_removal=True)),\n        &gt;&gt;&gt;\n        &gt;&gt;&gt;                                        FieldConfig(ca.WordEmbeddingTechnique(ca.GensimWord2Vec()))]\n\n    Args:\n        field_name: field name of the raw source which must be complexly represented\n        config_list: List of `FieldConfig` objects specifying how to represent the field of the raw source\n    \"\"\"\n    # If the field_name is not in the field_dict keys it means there is no list to append the FieldConfig to,\n    # so a new list is instantiated\n    if self.__field_dict.get(field_name) is not None:\n        self.__field_dict[field_name].extend(config_list)\n    else:\n        self.__field_dict[field_name] = list()\n        self.__field_dict[field_name].extend(config_list)\n</code></pre>"},{"location":"content_analyzer/config/#clayrs.content_analyzer.config.ContentAnalyzerConfig.add_multiple_exogenous","title":"<code>add_multiple_exogenous(config_list)</code>","text":"<p>Method which adds multiple exogenous representations which will be used to expand each content</p> <p>Examples:</p> <ul> <li>Expand each content by using DBPedia as external source and local dataset as external source</li> </ul> <pre><code>&gt;&gt;&gt; import clayrs.content_analyzer as ca\n&gt;&gt;&gt; movies_ca_config.add_single_exogenous(\n&gt;&gt;&gt;     [\n&gt;&gt;&gt;         ca.ExogenousConfig(\n&gt;&gt;&gt;             ca.DBPediaMappingTechnique('dbo:Film', 'Title', 'EN')\n&gt;&gt;&gt;         ),\n&gt;&gt;&gt;\n&gt;&gt;&gt;         ca.ExogenousConfig(\n&gt;&gt;&gt;             ca.PropertiesFromDataset(field_name_list=['director'])\n&gt;&gt;&gt;         ),\n&gt;&gt;&gt;     ]\n&gt;&gt;&gt; )\n</code></pre> PARAMETER DESCRIPTION <code>config_list</code> <p>List containing <code>ExogenousConfig</code> objects specifying how to expand each content</p> <p> TYPE: <code>List[ExogenousConfig]</code> </p> Source code in <code>clayrs/content_analyzer/config.py</code> <pre><code>def add_multiple_exogenous(self, config_list: List[ExogenousConfig]):\n\"\"\"\n    Method which adds multiple exogenous representations which will be used to expand each content\n\n    Examples:\n\n        * Expand each content by using DBPedia as external source and local dataset as external source\n        &gt;&gt;&gt; import clayrs.content_analyzer as ca\n        &gt;&gt;&gt; movies_ca_config.add_single_exogenous(\n        &gt;&gt;&gt;     [\n        &gt;&gt;&gt;         ca.ExogenousConfig(\n        &gt;&gt;&gt;             ca.DBPediaMappingTechnique('dbo:Film', 'Title', 'EN')\n        &gt;&gt;&gt;         ),\n        &gt;&gt;&gt;\n        &gt;&gt;&gt;         ca.ExogenousConfig(\n        &gt;&gt;&gt;             ca.PropertiesFromDataset(field_name_list=['director'])\n        &gt;&gt;&gt;         ),\n        &gt;&gt;&gt;     ]\n        &gt;&gt;&gt; )\n\n    Args:\n        config_list: List containing `ExogenousConfig` objects specifying how to expand each content\n    \"\"\"\n    self.__exogenous_representation_list.extend(config_list)\n</code></pre>"},{"location":"content_analyzer/config/#clayrs.content_analyzer.config.ContentAnalyzerConfig.add_single_config","title":"<code>add_single_config(field_name, field_config)</code>","text":"<p>Method which adds a single complex representation for the <code>field_name</code> of the raw source</p> <p>Examples:</p> <ul> <li>Represent field \"Plot\" of the raw source with a tf-idf technique using sklearn</li> </ul> <pre><code>&gt;&gt;&gt; import clayrs.content_analyzer as ca\n&gt;&gt;&gt; movies_ca_config.add_single_config(\"Plot\", FieldConfig(ca.SkLearnTfIdf()))\n</code></pre> PARAMETER DESCRIPTION <code>field_name</code> <p>field name of the raw source which must be complexly represented</p> <p> TYPE: <code>str</code> </p> <code>field_config</code> <p><code>FieldConfig</code> specifying how to represent the field of the raw source</p> <p> TYPE: <code>FieldConfig</code> </p> Source code in <code>clayrs/content_analyzer/config.py</code> <pre><code>def add_single_config(self, field_name: str, field_config: FieldConfig):\n\"\"\"\n    Method which adds a single complex representation for the `field_name` of the raw source\n\n    Examples:\n\n        * Represent field \"Plot\" of the raw source with a tf-idf technique using sklearn\n        &gt;&gt;&gt; import clayrs.content_analyzer as ca\n        &gt;&gt;&gt; movies_ca_config.add_single_config(\"Plot\", FieldConfig(ca.SkLearnTfIdf()))\n\n    Args:\n        field_name: field name of the raw source which must be complexly represented\n        field_config: `FieldConfig` specifying how to represent the field of the raw source\n    \"\"\"\n    # If the field_name is not in the field_dict keys it means there is no list to append the FieldConfig to,\n    # so a new list is instantiated\n    if self.__field_dict.get(field_name) is not None:\n        self.__field_dict[field_name].append(field_config)\n    else:\n        self.__field_dict[field_name] = list()\n        self.__field_dict[field_name].append(field_config)\n</code></pre>"},{"location":"content_analyzer/config/#clayrs.content_analyzer.config.ContentAnalyzerConfig.add_single_exogenous","title":"<code>add_single_exogenous(exogenous_config)</code>","text":"<p>Method which adds a single exogenous representation which will be used to expand each content</p> <p>Examples:</p> <ul> <li>Expand each content by using DBPedia as external source</li> </ul> <pre><code>&gt;&gt;&gt; import clayrs.content_analyzer as ca\n&gt;&gt;&gt; movies_ca_config.add_single_exogenous(\n&gt;&gt;&gt;     ca.ExogenousConfig(\n&gt;&gt;&gt;         ca.DBPediaMappingTechnique('dbo:Film', 'Title', 'EN')\n&gt;&gt;&gt;     )\n&gt;&gt;&gt; )\n</code></pre> PARAMETER DESCRIPTION <code>exogenous_config</code> <p><code>ExogenousConfig</code> object specifying how to expand each content</p> <p> TYPE: <code>ExogenousConfig</code> </p> Source code in <code>clayrs/content_analyzer/config.py</code> <pre><code>def add_single_exogenous(self, exogenous_config: ExogenousConfig):\n\"\"\"\n    Method which adds a single exogenous representation which will be used to expand each content\n\n    Examples:\n\n        * Expand each content by using DBPedia as external source\n        &gt;&gt;&gt; import clayrs.content_analyzer as ca\n        &gt;&gt;&gt; movies_ca_config.add_single_exogenous(\n        &gt;&gt;&gt;     ca.ExogenousConfig(\n        &gt;&gt;&gt;         ca.DBPediaMappingTechnique('dbo:Film', 'Title', 'EN')\n        &gt;&gt;&gt;     )\n        &gt;&gt;&gt; )\n\n    Args:\n        exogenous_config: `ExogenousConfig` object specifying how to expand each content\n    \"\"\"\n    self.__exogenous_representation_list.append(exogenous_config)\n</code></pre>"},{"location":"content_analyzer/config/#clayrs.content_analyzer.config.ContentAnalyzerConfig.get_configs_list","title":"<code>get_configs_list(field_name)</code>","text":"<p>Method which returns the list of all <code>FieldConfig</code> objects specified for the input <code>field_name</code> parameter</p> PARAMETER DESCRIPTION <code>field_name</code> <p>Name of the field for which the list of field configs will be retrieved</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>List[FieldConfig]</code> <p>List containing all <code>FieldConfig</code> objects specified for the input <code>field_name</code></p> Source code in <code>clayrs/content_analyzer/config.py</code> <pre><code>def get_configs_list(self, field_name: str) -&gt; List[FieldConfig]:\n\"\"\"\n    Method which returns the list of all `FieldConfig` objects specified for the input `field_name` parameter\n\n    Args:\n        field_name: Name of the field for which the list of field configs will be retrieved\n\n    Returns:\n        List containing all `FieldConfig` objects specified for the input `field_name`\n    \"\"\"\n    return [config for config in self.__field_dict[field_name]]\n</code></pre>"},{"location":"content_analyzer/config/#clayrs.content_analyzer.config.ContentAnalyzerConfig.get_field_name_list","title":"<code>get_field_name_list()</code>","text":"<p>Method which returns a list containing all the fields of the raw source for which at least one <code>FieldConfig</code> object has been assigned (i.e. at least one complex representations is specified)</p> RETURNS DESCRIPTION <code>List[str]</code> <p>List of all the fields of the raw source that must be complexly represented</p> Source code in <code>clayrs/content_analyzer/config.py</code> <pre><code>def get_field_name_list(self) -&gt; List[str]:\n\"\"\"\n    Method which returns a list containing all the fields of the raw source for which at least one `FieldConfig`\n    object has been assigned (i.e. at least one complex representations is specified)\n\n    Returns:\n        List of all the fields of the raw source that must be complexly represented\n    \"\"\"\n    return list(self.__field_dict.keys())\n</code></pre>"},{"location":"content_analyzer/config/#clayrs.content_analyzer.config.ExogenousConfig","title":"<code>ExogenousConfig(exogenous_technique, id=None)</code>","text":"<p>Class that represents the configuration for a single exogenous representation.</p> <p>The config allows the user to specify an <code>ExogenousPropertiesRetrieval</code> technique to use to expand each content. W.r.t <code>FieldConfig</code> objects, an <code>ExogenousConfig</code> does not refer to a particular field but to the whole content itself.</p> <p>You can use the <code>id</code> parameter to assign a custom id for the representation: by doing so the user can freely refer to it by using the custom id given, rather than positional integers (which are given automatically by the framework).</p> <ul> <li> <p>This will create an exogenous representation for the content by expanding it using DBPedia, said representation will be named 'test' <pre><code>ExogenousConfig(DBPediaMappingTechnique('dbo:Film', 'Title', 'EN'), id='test')\n</code></pre></p> </li> <li> <p>Same as the example above, but since no custom id was assigned, the exogenous representation can be referred to only with an integer (0 if it's the first exogenous representation specified for the contents, 1 if it's the second, etc.)</p> </li> </ul> <pre><code>ExogenousConfig(DBPediaMappingTechnique('dbo:Film', 'Title', 'EN'))\n</code></pre> PARAMETER DESCRIPTION <code>exogenous_technique</code> <p>Technique which will be used to expand each content with data from external sources. An example would be the DBPediaMappingTechnique which allows to retrieve properties from DBPedia.</p> <p> TYPE: <code>ExogenousPropertiesRetrieval</code> </p> <code>id</code> <p>Custom id that can be used later by the user to easily refer to the representation generated by this config. IDs for a single field should be unique! And should only contain '_', '-' and alphanumeric characters</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> Source code in <code>clayrs/content_analyzer/config.py</code> <pre><code>def __init__(self, exogenous_technique: ExogenousPropertiesRetrieval, id: str = None):\n    if id is not None:\n        self._check_custom_id(id)\n\n    self.__exogenous_technique = exogenous_technique\n    self.__id = id\n</code></pre>"},{"location":"content_analyzer/config/#clayrs.content_analyzer.config.ExogenousConfig.exogenous_technique","title":"<code>exogenous_technique</code>  <code>property</code>","text":"<p>Getter for the exogenous properties retrieval technique</p>"},{"location":"content_analyzer/config/#clayrs.content_analyzer.config.ExogenousConfig.id","title":"<code>id</code>  <code>property</code>","text":"<p>Getter for the ExogenousConfig id</p>"},{"location":"content_analyzer/config/#clayrs.content_analyzer.config.FieldConfig","title":"<code>FieldConfig(content_technique=OriginalData(), preprocessing=None, postprocessing=None, memory_interface=None, id=None)</code>","text":"<p>Class that represents the configuration for a single representation of a field. The configuration of a single representation is defined by a <code>FieldContentProductionTechnique</code> (e.g. an <code>EmbeddingTechnique</code>) that will be applied to the pre-processed data of said field.</p> <p>To specify how to preprocess data, simply specify an <code>InformationProcessor</code> in the <code>preprocessing</code> parameter. Multiple <code>InformationProcessor</code> can be wrapped in a list: in this case, the field will be preprocessed by performing operations all objects inside the list. If preprocessing is not defined, no preprocessing operations will be done on the field data.</p> <p>You can use the <code>id</code> parameter to assign a custom id for the representation: by doing so the user can freely refer to it by using the custom id given, rather than positional integers (which are given automatically by the framework).</p> <p>There is also a memory_interface attribute which allows to define a data structure where the representation will be serialized (e.g. an Index).</p> <p>Various configurations are possible depending on how the user wants to represent a particular field:</p> <ul> <li>This will produce a field representation using the SkLearnTfIdf technique on the field data   preprocessed by NLTK by performing stopwords removal, and the name of the produced representation will be   'field_example'</li> </ul> <pre><code>FieldConfig(SkLearnTfIdf(), NLTK(stopwords_removal=True), id='field_example')\n</code></pre> <ul> <li>This will produce the same result as above but the id for the field representation defined by this config will   be set by the ContentAnalyzer once it is being processed (0 integer if it's the first representation specified   for the field, 1 if it's the second, etc.)</li> </ul> <pre><code>FieldConfig(SkLearnTfIdf(), NLTK())\n</code></pre> <ul> <li>This will produce a field representation using the SkLearnTfIdf technique on the field data without applying   any preprocessing operation, but it will not be directly stored in the content, instead it will be   stored in a index</li> </ul> <pre><code>FieldConfig(SkLearnTfIdf(), memory_interface=SearchIndex(/somedir))\n</code></pre> <ul> <li>In the following nothing will be done on the field data, it will be represented as is</li> </ul> <pre><code>FieldConfig()\n</code></pre> PARAMETER DESCRIPTION <code>content_technique</code> <p>Technique that will be applied to the field in order to produce a complex representation of said field</p> <p> TYPE: <code>FieldContentProductionTechnique</code> DEFAULT: <code>OriginalData()</code> </p> <code>preprocessing</code> <p>Single <code>InformationProcessor</code> object or a list of <code>InformationProcessor</code> objects that will be used preprocess field data before applying the <code>content_technique</code></p> <p> TYPE: <code>Union[InformationProcessor, List[InformationProcessor]]</code> DEFAULT: <code>None</code> </p> <code>memory_interface</code> <p>complex structure where the content representation can be serialized (an Index for example)</p> <p> TYPE: <code>InformationInterface</code> DEFAULT: <code>None</code> </p> <code>id</code> <p>Custom id that can be used later by the user to easily refer to the representation generated by this config. IDs for a single field should be unique! And should only contain '_', '-' and alphanumeric characters</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> Source code in <code>clayrs/content_analyzer/config.py</code> <pre><code>def __init__(self,\n             content_technique: FieldContentProductionTechnique = OriginalData(),\n             preprocessing: Union[InformationProcessor, List[InformationProcessor]] = None,\n             postprocessing: Union[VisualPostProcessor, List[VisualPostProcessor]] = None,\n             memory_interface: InformationInterface = None,\n             id: str = None):\n\n    if preprocessing is None:\n        preprocessing = []\n\n    if postprocessing is None:\n        postprocessing = []\n\n    if id is not None:\n        self._check_custom_id(id)\n\n    self.__content_technique = content_technique\n    self.__preprocessing = preprocessing\n    self.__postprocessing = postprocessing\n    self.__memory_interface = memory_interface\n    self.__id = id\n\n    if not isinstance(self.__preprocessing, list):\n        self.__preprocessing = [self.__preprocessing]\n\n    if not isinstance(self.__postprocessing, list):\n        self.__postprocessing = [self.__postprocessing]\n</code></pre>"},{"location":"content_analyzer/config/#clayrs.content_analyzer.config.FieldConfig.content_technique","title":"<code>content_technique</code>  <code>property</code>","text":"<p>Getter for the field content production technique of the field</p>"},{"location":"content_analyzer/config/#clayrs.content_analyzer.config.FieldConfig.id","title":"<code>id</code>  <code>property</code>","text":"<p>Getter for the id of the field config</p>"},{"location":"content_analyzer/config/#clayrs.content_analyzer.config.FieldConfig.memory_interface","title":"<code>memory_interface</code>  <code>property</code>","text":"<p>Getter for the index associated to the field config</p>"},{"location":"content_analyzer/config/#clayrs.content_analyzer.config.FieldConfig.postprocessing","title":"<code>postprocessing</code>  <code>property</code>","text":"<p>Getter for the list of postprocessor of the field config</p>"},{"location":"content_analyzer/config/#clayrs.content_analyzer.config.FieldConfig.preprocessing","title":"<code>preprocessing</code>  <code>property</code>","text":"<p>Getter for the list of preprocessor of the field config</p>"},{"location":"content_analyzer/config/#clayrs.content_analyzer.config.ItemAnalyzerConfig","title":"<code>ItemAnalyzerConfig</code>","text":"<p>         Bases: <code>ContentAnalyzerConfig</code></p> <p>Class that represents the configuration for the content analyzer. The configuration specifies how the <code>Content Analyzer</code> needs to complexly represent contents, i.e. how to preprocess them and how to represent them In particular this class refers to items.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import clayrs.content_analyzer as ca\n&gt;&gt;&gt; raw_source = ca.JSONFile(json_path)\n&gt;&gt;&gt; movies_config = ca.ItemAnalyzerConfig(raw_source, id='movie_id', output_directory='movies_codified/')\n&gt;&gt;&gt; # add single field config\n&gt;&gt;&gt; movies_config.add_single_config('occupation', FieldConfig(content_technique=ca.OriginalData()))\n&gt;&gt;&gt; # add single exogenous technique\n&gt;&gt;&gt; movies_config.add_single_exogenous(ca.ExogenousConfig(ca.PropertiesFromDataset(field_name_list=['gender']))\n</code></pre>"},{"location":"content_analyzer/config/#clayrs.content_analyzer.config.UserAnalyzerConfig","title":"<code>UserAnalyzerConfig</code>","text":"<p>         Bases: <code>ContentAnalyzerConfig</code></p> <p>Class that represents the configuration for the content analyzer. The configuration specifies how the <code>Content Analyzer</code> needs to complexly represent contents, i.e. how to preprocess them and how to represent them In particular this class refers to users.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import clayrs.content_analyzer as ca\n&gt;&gt;&gt; raw_source = ca.JSONFile(json_path)\n&gt;&gt;&gt; users_config = ca.UserAnalyzerConfig(raw_source, id='user_id', output_directory='users_codified/')\n&gt;&gt;&gt; # add single field config\n&gt;&gt;&gt; users_config.add_single_config('occupation', FieldConfig(content_technique=ca.OriginalData()))\n&gt;&gt;&gt; # add single exogenous technique\n&gt;&gt;&gt; users_config.add_single_exogenous(ca.ExogenousConfig(ca.PropertiesFromDataset(field_name_list=['gender']))\n</code></pre>"},{"location":"content_analyzer/config/#content-analyzer-class","title":"Content Analyzer Class","text":""},{"location":"content_analyzer/config/#clayrs.content_analyzer.ContentAnalyzer","title":"<code>ContentAnalyzer(config, n_thread=1)</code>","text":"<p>Class to whom the control of the content analysis phase is delegated. It uses the data stored in the configuration file to create and serialize the contents the user wants to produce. It also checks that the configurations the user wants to run on the raw contents have unique ids (otherwise it would be impossible to refer to a particular field representation or exogenous representation)</p> PARAMETER DESCRIPTION <code>config</code> <p>configuration for processing the item fields. This parameter provides the possibility of customizing the way in which the input data is processed.</p> <p> TYPE: <code>ContentAnalyzerConfig</code> </p> Source code in <code>clayrs/content_analyzer/content_analyzer_main.py</code> <pre><code>def __init__(self, config: ContentAnalyzerConfig, n_thread: int = 1):\n    self._config: ContentAnalyzerConfig = config\n    self._n_thread = n_thread\n</code></pre>"},{"location":"content_analyzer/config/#clayrs.content_analyzer.content_analyzer_main.ContentAnalyzer.fit","title":"<code>fit()</code>","text":"<p>Processes the creation of the contents and serializes the contents. This method starts the content production process and initializes everything that will be used to create said contents, their fields and their representations</p> Source code in <code>clayrs/content_analyzer/content_analyzer_main.py</code> <pre><code>def fit(self):\n\"\"\"\n    Processes the creation of the contents and serializes the contents. This method starts the content production\n    process and initializes everything that will be used to create said contents, their fields and their\n    representations\n    \"\"\"\n    # before starting the process, the content analyzer manin checks that there are no duplicate id cases\n    # both in the field dictionary and in the exogenous representation list\n    # this is done now and not recursively for each content during the creation process, in order to avoid starting\n    # an operation that is going to fail\n    try:\n        self.__check_field_dict()\n        self.__check_exogenous_representation_list()\n    except ValueError as e:\n        raise e\n\n    # creates the directory where the data will be serialized and overwrites it if it already exists\n    output_path = self._config.output_directory\n    if os.path.exists(output_path):\n        shutil.rmtree(output_path)\n    os.makedirs(output_path)\n\n    contents_producer = ContentsProducer.get_instance()\n    contents_producer.set_config(self._config)\n    created_contents = contents_producer.create_contents()\n\n    if self._config.export_json:\n        json_path = os.path.join(self._config.output_directory, 'contents.json')\n        with open(json_path, \"w\") as data:\n            json.dump(created_contents, data, cls=ContentEncoder, indent=4)\n\n    # with get_progbar(created_contents) as pbar:\n    with get_iterator_thread(self._n_thread, self._serialize_content, created_contents,\n                             keep_order=False, progress_bar=True, total=len(created_contents)) as pbar:\n        pbar.set_description(\"Serializing contents\")\n\n        for _ in pbar:\n            pass\n</code></pre>"},{"location":"content_analyzer/index_interface/","title":"Index interface","text":""},{"location":"content_analyzer/index_interface/#clayrs.content_analyzer.memory_interfaces.text_interface.IndexInterface","title":"<code>IndexInterface(directory)</code>","text":"<p>         Bases: <code>TextInterface</code></p> <p>Abstract class that takes care of serializing and deserializing text in an indexed structure using the Whoosh library</p> PARAMETER DESCRIPTION <code>directory</code> <p>Path of the directory where the content will be serialized</p> <p> TYPE: <code>str</code> </p> Source code in <code>clayrs/content_analyzer/memory_interfaces/text_interface.py</code> <pre><code>def __init__(self, directory: str):\n    super().__init__(directory)\n    self.__doc = None  # document that is currently being created and will be added to the index\n    self.__writer = None  # index writer\n    self.__doc_index = 0  # current position the document will have in the index once it is serialized\n    self.__schema_changed = False  # true if the schema has been changed, false otherwise\n</code></pre>"},{"location":"content_analyzer/index_interface/#clayrs.content_analyzer.memory_interfaces.text_interface.IndexInterface.schema_type","title":"<code>schema_type</code>  <code>property</code> <code>abstractmethod</code>","text":"<p>Whoosh uses a Schema that defines, for each field of the content, how to store the data. In the case of this project, every field will have the same structure and will share the same field type. This method returns said field type.</p>"},{"location":"content_analyzer/index_interface/#clayrs.content_analyzer.memory_interfaces.text_interface.IndexInterface.get_field","title":"<code>get_field(field_name, content_id)</code>","text":"<p>Uses a search index to retrieve the content corresponding to the content_id (if it is a string) or in the corresponding position (if it is an integer), and returns the data in the field corresponding to the field_name</p> PARAMETER DESCRIPTION <code>field_name</code> <p>name of the field from which the data will be retrieved</p> <p> TYPE: <code>str</code> </p> <code>content_id</code> <p>either the position or Id of the content that contains the specified field</p> <p> TYPE: <code>Union[str, int]</code> </p> RETURNS DESCRIPTION <code>str</code> <p>Data contained in the field of the content</p> Source code in <code>clayrs/content_analyzer/memory_interfaces/text_interface.py</code> <pre><code>def get_field(self, field_name: str, content_id: Union[str, int]) -&gt; str:\n\"\"\"\n    Uses a search index to retrieve the content corresponding to the content_id (if it is a string) or in the\n    corresponding position (if it is an integer), and returns the data in the field corresponding to the field_name\n\n    Args:\n        field_name (str): name of the field from which the data will be retrieved\n        content_id (Union[str, int]): either the position or Id of the content that contains the specified field\n\n    Returns:\n        Data contained in the field of the content\n    \"\"\"\n    ix = open_dir(self.directory)\n    with ix.searcher() as searcher:\n        if isinstance(content_id, str):\n            query = Term(\"content_id\", content_id)\n            result = searcher.search(query)\n            result = result[0][field_name]\n        elif isinstance(content_id, int):\n            result = searcher.reader().stored_fields(content_id)[field_name]\n        return result\n</code></pre>"},{"location":"content_analyzer/index_interface/#clayrs.content_analyzer.memory_interfaces.text_interface.IndexInterface.get_tf_idf","title":"<code>get_tf_idf(field_name, content_id)</code>","text":"<p>Calculates the tf-idf for the words contained in the field of the content whose id is content_id (if it is a string) or in the given position (if it is an integer).</p> <p>The tf-idf computation formula is:</p> \\[ tf \\mbox{-} idf = (1 + log10(tf)) * log10(idf) \\] PARAMETER DESCRIPTION <code>field_name</code> <p>Name of the field containing the words for which calculate the tf-idf</p> <p> TYPE: <code>str</code> </p> <code>content_id</code> <p>either the position or Id of the content that contains the specified field</p> <p> TYPE: <code>Union[str, int]</code> </p> RETURNS DESCRIPTION <code>words_bag</code> <p>Dictionary whose keys are the words contained in the field, and the corresponding values are the tf-idf values</p> <p> TYPE: <code>Dict[str, float]</code> </p> Source code in <code>clayrs/content_analyzer/memory_interfaces/text_interface.py</code> <pre><code>def get_tf_idf(self, field_name: str, content_id: Union[str, int]) -&gt; Dict[str, float]:\nr\"\"\"\n    Calculates the tf-idf for the words contained in the field of the content whose id\n    is content_id (if it is a string) or in the given position (if it is an integer).\n\n    The tf-idf computation formula is:\n\n    $$\n    tf \\mbox{-} idf = (1 + log10(tf)) * log10(idf)\n    $$\n\n    Args:\n        field_name: Name of the field containing the words for which calculate the tf-idf\n        content_id: either the position or Id of the content that contains the specified field\n\n    Returns:\n        words_bag: Dictionary whose keys are the words contained in the field, and the\n            corresponding values are the tf-idf values\n    \"\"\"\n    ix = open_dir(self.directory)\n    words_bag = {}\n    with ix.searcher() as searcher:\n        if isinstance(content_id, str):\n            query = Term(\"content_id\", content_id)\n            doc_num = searcher.search(query).docnum(0)\n        elif isinstance(content_id, int):\n            doc_num = content_id\n\n        # if the document has the field == \"\" (length == 0) then the bag of word is empty\n        if len(searcher.ixreader.stored_fields(doc_num)[field_name]) &gt; 0:\n            # retrieves the frequency vector (used for tf)\n            list_with_freq = [term_with_freq for term_with_freq\n                              in searcher.vector(doc_num, field_name).items_as(\"frequency\")]\n            for term, freq in list_with_freq:\n                tf = 1 + math.log10(freq)\n                idf = math.log10(searcher.doc_count()/searcher.doc_frequency(field_name, term))\n                words_bag[term] = tf*idf\n    return words_bag\n</code></pre>"},{"location":"content_analyzer/index_interface/#clayrs.content_analyzer.memory_interfaces.text_interface.IndexInterface.init_writing","title":"<code>init_writing(delete_old=False)</code>","text":"<p>Creates the index locally (in the directory passed in the constructor) and initializes the index writer. If an index already exists in the directory, what happens depend on the attribute delete_old passed as argument</p> PARAMETER DESCRIPTION <code>delete_old</code> <p>if True, the index that was in the same directory is destroyed and replaced; if False, the index is simply opened</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>clayrs/content_analyzer/memory_interfaces/text_interface.py</code> <pre><code>def init_writing(self, delete_old: bool = False):\n\"\"\"\n    Creates the index locally (in the directory passed in the constructor) and initializes the index writer.\n    If an index already exists in the directory, what happens depend on the attribute delete_old passed as argument\n\n    Args:\n        delete_old (bool): if True, the index that was in the same directory is destroyed and replaced;\n            if False, the index is simply opened\n    \"\"\"\n    if os.path.exists(self.directory):\n        if delete_old:\n            self.delete()\n            os.mkdir(self.directory)\n            ix = create_in(self.directory, Schema())\n            self.__writer = ix.writer()\n        else:\n            ix = open_dir(self.directory)\n            self.__writer = ix.writer()\n            self.__doc_index = self.__writer.reader().doc_count()\n    else:\n        os.mkdir(self.directory)\n        ix = create_in(self.directory, Schema())\n        self.__writer = ix.writer()\n</code></pre>"},{"location":"content_analyzer/index_interface/#clayrs.content_analyzer.memory_interfaces.text_interface.IndexInterface.new_content","title":"<code>new_content()</code>","text":"<p>The new content is a document that will be indexed. In this case the document is a dictionary with the name of the field as key and the data inside the field as value</p> Source code in <code>clayrs/content_analyzer/memory_interfaces/text_interface.py</code> <pre><code>def new_content(self):\n\"\"\"\n    The new content is a document that will be indexed. In this case the document is a dictionary with\n    the name of the field as key and the data inside the field as value\n    \"\"\"\n    self.__doc = {}\n</code></pre>"},{"location":"content_analyzer/index_interface/#clayrs.content_analyzer.memory_interfaces.text_interface.IndexInterface.new_field","title":"<code>new_field(field_name, field_data)</code>","text":"<p>Adds a new field to the document that is being created. Since the index Schema is generated dynamically, if the field name is not in the Schema already it is added to it</p> PARAMETER DESCRIPTION <code>field_name</code> <p>Name of the new field</p> <p> TYPE: <code>str</code> </p> <code>field_data</code> <p>Data to put into the field</p> <p> TYPE: <code>object</code> </p> Source code in <code>clayrs/content_analyzer/memory_interfaces/text_interface.py</code> <pre><code>def new_field(self, field_name: str, field_data: object):\n\"\"\"\n    Adds a new field to the document that is being created. Since the index Schema is generated dynamically, if\n    the field name is not in the Schema already it is added to it\n\n    Args:\n        field_name (str): Name of the new field\n        field_data (object): Data to put into the field\n    \"\"\"\n    if field_name not in open_dir(self.directory).schema.names():\n        self.__writer.add_field(field_name, self.schema_type)\n        self.__schema_changed = True\n    self.__doc[field_name] = field_data\n</code></pre>"},{"location":"content_analyzer/index_interface/#clayrs.content_analyzer.memory_interfaces.text_interface.IndexInterface.query","title":"<code>query(string_query, results_number, mask_list=None, candidate_list=None, classic_similarity=True)</code>","text":"<p>Uses a search index to query the index in order to retrieve specific contents using a query expressed in string form</p> PARAMETER DESCRIPTION <code>string_query</code> <p>query expressed as a string</p> <p> TYPE: <code>str</code> </p> <code>results_number</code> <p>number of results the searcher will return for the query</p> <p> TYPE: <code>int</code> </p> <code>mask_list</code> <p>list of content_ids of items to ignore in the search process</p> <p> TYPE: <code>list</code> DEFAULT: <code>None</code> </p> <code>candidate_list</code> <p>list of content_ids of items to consider in the search process, if it is not None only items in the list will be considered</p> <p> TYPE: <code>list</code> DEFAULT: <code>None</code> </p> <code>classic_similarity</code> <p>if True, classic tf idf is used for scoring, otherwise BM25F is used</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>results</code> <p>the final results dictionary containing the results found from the search index for the query. The dictionary will be in the following form:</p> <pre><code>{content_id: {\"item\": item_dictionary, \"score\": item_score}, ...}\n</code></pre> <p>content_id is the content_id for the corresponding item item_dictionary is the dictionary of the item containing the fields as keys and the contents as values. So it will be in the following form:</p> <pre><code>{\"Plot\": \"this is the plot\", \"Genre\": \"this is the Genre\"}\n</code></pre> <p>The item_dictionary will not contain the content_id since it is already defined and used as key of the external dictionary items_score is the score given to the item for the query by the index searcher</p> <p> TYPE: <code>dict</code> </p> Source code in <code>clayrs/content_analyzer/memory_interfaces/text_interface.py</code> <pre><code>def query(self, string_query: str, results_number: int, mask_list: list = None,\n          candidate_list: list = None, classic_similarity: bool = True) -&gt; dict:\n\"\"\"\n    Uses a search index to query the index in order to retrieve specific contents using a query expressed in string\n    form\n\n    Args:\n        string_query: query expressed as a string\n        results_number: number of results the searcher will return for the query\n        mask_list: list of content_ids of items to ignore in the search process\n        candidate_list: list of content_ids of items to consider in the search process,\n            if it is not None only items in the list will be considered\n        classic_similarity: if True, classic tf idf is used for scoring, otherwise BM25F is used\n\n    Returns:\n        results: the final results dictionary containing the results found from the search index for the\n            query. The dictionary will be in the following form:\n\n                {content_id: {\"item\": item_dictionary, \"score\": item_score}, ...}\n\n            content_id is the content_id for the corresponding item\n            item_dictionary is the dictionary of the item containing the fields as keys and the contents as values.\n            So it will be in the following form:\n\n                {\"Plot\": \"this is the plot\", \"Genre\": \"this is the Genre\"}\n\n            The item_dictionary will not contain the content_id since it is already defined and used as key of the\n            external dictionary\n            items_score is the score given to the item for the query by the index searcher\n    \"\"\"\n    ix = open_dir(self.directory)\n    with ix.searcher(weighting=TF_IDF if classic_similarity else BM25F) as searcher:\n        candidate_query_list = None\n        mask_query_list = None\n\n        # the mask list contains the content_id for the items to ignore in the searching process\n        # from the mask list a mask query is created and it will be used by the searcher\n        if mask_list is not None:\n            mask_query_list = []\n            for document in mask_list:\n                mask_query_list.append(Term(\"content_id\", document))\n            mask_query_list = Or(mask_query_list)\n\n        # the candidate list contains the content_id for the items to consider in the searching process\n        # from the candidate list a candidate query is created and it will be used by the searcher\n        if candidate_list is not None:\n            candidate_query_list = []\n            for candidate in candidate_list:\n                candidate_query_list.append(Term(\"content_id\", candidate))\n            candidate_query_list = Or(candidate_query_list)\n\n        schema = ix.schema\n        parser = QueryParser(\"content_id\", schema=schema, group=OrGroup)\n        # regular expression to match the possible field styles\n        # examples: \"content_id\" or \"Genre#2\" or \"Genre#2#custom_id\"\n        parser.add_plugin(FieldsPlugin(r'(?P&lt;text&gt;[\\w-]+(\\#[\\w-]+(\\#[\\w-]+)?)?|[*]):'))\n        query = parser.parse(string_query)\n        score_docs = \\\n            searcher.search(query, limit=results_number, filter=candidate_query_list, mask=mask_query_list)\n\n        # creation of the results dictionary, This phase is necessary because the Hit objects returned by the\n        # searcher as results need the reader inside the search index in order to return information\n        # so it would be impossible to access a field or the score of the item from outside this method\n        # because of that this dictionary containing the most important infos is created\n        results = {}\n        for hit in score_docs:\n            hit_dict = dict(hit)\n            content_id = hit_dict.pop(\"content_id\")\n            results[content_id] = {}\n            results[content_id][\"item\"] = hit_dict\n            results[content_id][\"score\"] = hit.score\n        return results\n</code></pre>"},{"location":"content_analyzer/index_interface/#clayrs.content_analyzer.memory_interfaces.text_interface.IndexInterface.serialize_content","title":"<code>serialize_content()</code>","text":"<p>Serializes the content in the index. If the schema changed, the writer will commit the changes to the schema before adding the document to the index. Once the document is indexed, it can be deleted from the IndexInterface and the document position in the index is returned</p> Source code in <code>clayrs/content_analyzer/memory_interfaces/text_interface.py</code> <pre><code>def serialize_content(self) -&gt; int:\n\"\"\"\n    Serializes the content in the index. If the schema changed, the writer will commit the changes to the schema\n    before adding the document to the index. Once the document is indexed, it can be deleted from the IndexInterface\n    and the document position in the index is returned\n    \"\"\"\n    if self.__schema_changed:\n        self.__writer.commit()\n        self.__writer = open_dir(self.directory).writer()\n        self.__schema_changed = False\n    self.__writer.add_document(**self.__doc)\n    del self.__doc\n    self.__doc_index += 1\n    return self.__doc_index - 1\n</code></pre>"},{"location":"content_analyzer/index_interface/#clayrs.content_analyzer.memory_interfaces.text_interface.IndexInterface.stop_writing","title":"<code>stop_writing()</code>","text":"<p>Stops the index writer and commits the operations</p> Source code in <code>clayrs/content_analyzer/memory_interfaces/text_interface.py</code> <pre><code>def stop_writing(self):\n\"\"\"\n    Stops the index writer and commits the operations\n    \"\"\"\n    self.__writer.commit()\n    del self.__writer\n</code></pre>"},{"location":"content_analyzer/index_interface/#clayrs.content_analyzer.memory_interfaces.text_interface.KeywordIndex","title":"<code>KeywordIndex(directory)</code>","text":"<p>         Bases: <code>IndexInterface</code></p> <p>This class implements the schema_type method: KeyWord. This is useful for splitting the indexed text in a list of tokens. The Frequency vector is also added so that the tf calculation is possible. Commas is true in case of a \"content_id\" field data containing white spaces</p> Source code in <code>clayrs/content_analyzer/memory_interfaces/text_interface.py</code> <pre><code>def __init__(self, directory: str):\n    super().__init__(directory)\n</code></pre>"},{"location":"content_analyzer/index_interface/#clayrs.content_analyzer.memory_interfaces.text_interface.SearchIndex","title":"<code>SearchIndex(directory)</code>","text":"<p>         Bases: <code>IndexInterface</code></p> <p>This class implements the schema_type method: Text. By using a SimpleAnalyzer for the field, the data is kept as much as the original as possible</p> Source code in <code>clayrs/content_analyzer/memory_interfaces/text_interface.py</code> <pre><code>def __init__(self, directory: str):\n    super().__init__(directory)\n</code></pre>"},{"location":"content_analyzer/introduction/","title":"Introduction","text":"<p>Warning</p> <p>Docs are complete, but revision is still a Work in Progress. Sorry for any typos!</p>"},{"location":"content_analyzer/introduction/#introduction","title":"Introduction","text":"<p>The Content Analyzer module has the task to build a complex representation for chosen contents, starting from their raw representation</p> <p>The following will introduce you to the standard usage pipeline for this module, showing you all the various operations that can be performed</p>"},{"location":"content_analyzer/introduction/#item-config","title":"Item Config","text":"<p>Suppose the following JSON file which contains information about movies: It will act as raw source for items.</p> JSON items raw source<pre><code>[\n    {\n        \"movielens_id\": \"1\",\n        \"title\": \"Toy Story\",\n        \"plot\": \"A cowboy doll is profoundly threatened and jealous when a new spaceman figure supplants him as top toy\"\n        \"genres\": \"Animation, Adventure, Comedy, Family, Fantasy\",\n        \"year\": \"1995\",\n        \"rating\": \"8.3\",\n        \"directors\": \"John Lasseter\",\n        \"dbpedia_uri\": \"http://dbpedia.org/resource/Toy_Story\",\n        \"dbpedia_label\": \"Toy Story\"\n    }\n]\n</code></pre> <p>In order to define the item representation, the <code>ItemAnalyzerConfig</code> class must be instantiated and the following  parameters should be defined:</p> <ul> <li>source: the path of the file containing items info</li> <li>id: the field that uniquely identifies an item</li> <li>output_directory: the path where serialized representations are saved</li> </ul> <p>Info</p> <p>In the following suppose that the raw source is a <code>JSON</code> file, but ClayRS is able to read from different sources, as CSVFile, DATFile, and more.</p> <ul> <li>Refer to the Raw source wrappers section for more</li> </ul> <pre><code>from clayrs import content_analyzer as ca\n\njson_source = ca.JSONFile('items_info.json')\n\n# Configuration of item representation \nmovies_ca_config = ca.ItemAnalyzerConfig(\n    source=json_source,\n    id='movielens_id',\n    output_directory='movies_codified/',\n)\n</code></pre> <p>Once we have initialized our items configuration, we are ready to complexly represent one or more fields of the specified raw source</p>"},{"location":"content_analyzer/introduction/#complex-representation","title":"Complex representation","text":"<p>Every field of the raw source can be represented using several techniques, such as 'tfidf', 'embeddings', etc.</p> <p>It is possible to process the content of each field using a Natural Language Processing (NLP) pipeline. The preprocessing will be done before assigning a complex representation to said field. It is also possible to assign a custom id for each generated representation, in order to allow a simpler reference  in the recommendation phase.</p> <ul> <li>Both NLP pipeline and custom id are optional parameters</li> <li>If a list of NLP preprocessors is passed to the <code>preprocessing</code> parameter, then all operations specified will be performed in order</li> </ul> <p>So, for example, we could represent the 'plot' field by performing lemmatization and stopwords removal,  and represent it using tfidf:</p> <pre><code>movies_ca_config.add_single_config(\n    'plot',\n    ca.FieldConfig(ca.SkLearnTfIdf(),\n                   preprocessing=ca.NLTK(stopwords_removal=True, lemmatization=True),\n                   id='tfidf')  # Custom id\n)\n</code></pre> <p>But we could also specify for the same field multiple complex representations at once with the  <code>add_multiple_config()</code> method:</p> <ul> <li>In this case each representation can be preceded by different preprocessing operations!</li> </ul> <p>So, for example, we could represent the 'genres' field by:</p> <ol> <li>Removing punctuation and representing it using the pre-trained glove-twitter-50 model from Gensim;</li> <li>Performing lemmatization and representing it by using the Word2Vec model which will be trained from scratch  on our corpus <pre><code>movies_ca_config.add_multiple_config(\n    'genres',\n    [   \n        # first representation\n        ca.FieldConfig(ca.WordEmbeddingTechnique(ca.Gensim('glove-twitter-50')),\n                       preprocessing=ca.NLTK(remove_punctuation=True),\n                       id='glove'),\n\n        # second representation\n        ca.FieldConfig(ca.WordEmbeddingTechnique(ca.GensimWord2Vec()),\n                       preprocessing=ca.Spacy(lemmatization=True),\n                       id='word2vec')\n    ]\n)\n</code></pre></li> </ol>"},{"location":"content_analyzer/introduction/#exogenous-representation","title":"Exogenous representation","text":"<p>We could expand each item by using Exogenous techniques: they are very useful if you plan to use a graph based recommender system later in the experiment.</p> <p>In order to do that, we call the <code>add_single_exogenous()</code> method (or <code>add_multiple_exogenous()</code> in case of multiple  exogenous techniques) and pass the instantiated <code>ExogenousTechnique</code> object.</p> <p>Info</p> <p>Exogenous properties are those extracted from an external source, more info  here</p> <p>In this case we expand each content with properties extracted from the DBPedia ontology:</p> <ul> <li>The first parameter of the <code>DBPediaMappingTechnique</code> object is the entity type of every content  (dbo:Film in this case). Multiple prefixes such as <code>rdf</code>, <code>rdfs</code>, <code>foaf</code>, <code>dbo</code> are imported by default, but if you need another type of entity  you can pass its uri directly</li> </ul> <p><code>'dbo:Film' &lt;-EQUIVALENT-&gt; '&lt;http://dbpedia.org/ontology/Film&gt;'</code></p> <ul> <li>The second parameter instead is the field in the raw source which must exactly match the string representation of the  rdfs:label of the content on DBPedia</li> </ul> <pre><code>movies_ca_config.add_single_exogenous(\n    ca.ExogenousConfig(ca.DBPediaMappingTechnique('dbo:Film', 'dbpedia_label'),\n                       id='dbpedia')\n)\n</code></pre>"},{"location":"content_analyzer/introduction/#store-in-an-index","title":"Store in an index","text":"<p>You could also store in a complex data structure certain representation codified for the contents.</p> <p>In the following we are exporting the textual data \"as is\" and preprocessed with stopwords_removal and stemming in a <code>Whoosh</code> index</p> <p>Info</p> <p>Textual representations stored in an index can be exploited later in the RecSys phase by the  IndexQuery algorithm</p> <pre><code>movies_ca_config.add_multiple_config(\n    'genres',\n    [   \n        # first representation - no preprocessing\n        ca.FieldConfig(ca.OriginalData(),\n                       memory_interfaces=ca.SearchIndex('index_folder'),\n                       id='index_original'),\n\n        # first representation - with preprocessing\n        ca.FieldConfig(ca.OriginalData(),\n                       preprocessing=ca.NLTK(stopwords_removal=True, stemming=True),\n                       memory_interfaces=ca.SearchIndex('index_folder'),\n                       id='index_original'),\n    ]\n)\n</code></pre>"},{"location":"content_analyzer/introduction/#user-config","title":"User Config","text":"<p>Suppose the following JSON file which contains information about movies: It will act as raw source for users.</p> CSV users raw source<pre><code>user_id,age,gender,occupation,zip_code\n1,24,M,technician,85711\n2,53,F,other,94043\n</code></pre> <p>In order to define the user representation, the <code>UserAnalyzerConfig</code> class must be instantiated and the following  parameters should be defined:</p> <ul> <li>source: the path of the file containing users info</li> <li>id: the field that uniquely identifies an user</li> <li>output_directory: the path where serialized representations are saved</li> </ul> <pre><code># Configuration of user representation\nusers_ca_config = ca.UserAnalyzerConfig(\n    ca.CSVFile('users_info.csv'),\n    id='user_id',\n    output_directory='users_codified/',\n)\n</code></pre> <p>The operations you could perform for users are exactly the same you could perform on items! So please refer to the  above section</p> <p>For example, we could just expand each user with exogenous properties extracted from local dataset:</p> <p><code>PropertiesFromDataset()</code> exogenous technique allows specifying which fields to use in order to expand every user info</p> <ul> <li>If no field is specified, all fields from the raw source will be used</li> </ul> <p>In this case, we expand every user with <code>gender</code> and <code>occupation</code></p> <pre><code>users_ca_config.add_single_exogenous(\n    ca.ExogenousConfig(\n        ca.PropertiesFromDataset(field_name_list=['gender', 'occupation'])\n    )\n)\n</code></pre>"},{"location":"content_analyzer/introduction/#serializing-content","title":"Serializing Content","text":"<p>At the end of the configuration step, we provide the configuration (regardless if it's for items or users) to the  <code>ContentAnalyzer</code> class and call the <code>fit()</code> method:</p> <ul> <li>The Content Analyzer will represent and serialize every item.</li> </ul> <pre><code># complexly represent items\nca.ContentAnalyzer(config=movies_ca_config).fit()\n\n# complexly represent users\nca.ContentAnalyzer(config=users_ca_config).fit()\n</code></pre>"},{"location":"content_analyzer/introduction/#exporting-to-json-file","title":"Exporting to JSON file","text":"<p>There is also the optional parameter <code>export_json</code> in the <code>ItemAnalyzerConfig</code> or  <code>UserAnalyzerConfig</code>:</p> <ul> <li>If set to True, contents complexly represented will also be serialized in a human readable JSON</li> </ul> <pre><code># Configuration of item representation \nmovies_ca_config = ca.ItemAnalyzerConfig(\n    source=ca.JSONFile('items_info.json'),\n    id='movielens_id',\n    output_directory='movies_codified/',\n    export_json=True\n)\n</code></pre> <p>After specifying a fitting representation for items and calling the <code>fit()</code> method of the <code>ContentAnalyzer</code>, the output folder will have the following structure:</p> <pre><code>\ud83d\udcc1 movies_codified/\n\u2514\u2500\u2500 \ud83d\udcc4 contents.json\n\u2514\u2500\u2500 \ud83d\udcc4 1.xz\n\u2514\u2500\u2500 \ud83d\udcc4 2.xz\n\u2514\u2500\u2500 \ud83d\udcc4 ...\n</code></pre>"},{"location":"content_analyzer/raw_sources/","title":"Raw Source Wrappers","text":""},{"location":"content_analyzer/raw_sources/#clayrs.content_analyzer.raw_information_source.CSVFile","title":"<code>CSVFile(file_path, separator=',', has_header=True, encoding='utf-8-sig')</code>","text":"<p>         Bases: <code>RawInformationSource</code></p> <p>Wrapper for a CSV file. This class is able to read from a CSV file where each entry is separated by the a certain separator (<code>,</code> by default). So by using this class you can also read TSV file for examples, by specifying <code>separator='\\t'</code>.</p> <p>A CSV File most typically has a header: in this case, each entry can be referenced with its column header. In case the CSV File hasn't a header, simply specify <code>has_header=False</code>: in this case, each entry can be referenced with a string representing its positional index (e.g. '0' for entry in the first position, '1' for the entry in the second position, etc.)</p> <p>You can iterate over the whole content of the raw source with a simple for loop: each row will be returned as a dictionary where keys are strings representing the positional indices, values are the entries</p> <p>Examples:</p> <p>Consider the following CSV file with header <pre><code>movie_id,movie_title,release_year\n1,Jumanji,1995\n2,Toy Story,1995\n</code></pre></p> <pre><code>&gt;&gt;&gt; file = CSVFile(csv_path)\n&gt;&gt;&gt; print(list(file))\n[{'movie_id': '1', 'movie_title': 'Jumanji', 'release_year': '1995'},\n{'movie_id': '2', 'movie_title': 'Toy Story', 'release_year': '1995'}]\n</code></pre> <p>Consider the following TSV file with no header <pre><code>1   Jumanji 1995\n2   Toy Story   1995\n</code></pre></p> <pre><code>&gt;&gt;&gt; file = CSVFile(tsv_path, separator='\\t', has_header=False)\n&gt;&gt;&gt; print(list(file))\n[{'0': '1', '1': 'Jumanji', '2': '1995'},\n{'0': '2', '1': 'Toy Story', '2': '1995'}]\n</code></pre> PARAMETER DESCRIPTION <code>file_path</code> <p>Path of the dat file</p> <p> TYPE: <code>str</code> </p> <code>separator</code> <p>Character which separates each entry. By default is a comma (<code>,</code>), but in case you need to read from a TSV file simply change this parameter to <code>\\t</code></p> <p> TYPE: <code>str</code> DEFAULT: <code>','</code> </p> <code>has_header</code> <p>Boolean value which specifies if the file has an header or not. Default is True</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>encoding</code> <p>Define the type of encoding of data stored in the source (example: \"utf-8\")</p> <p> TYPE: <code>str</code> DEFAULT: <code>'utf-8-sig'</code> </p> Source code in <code>clayrs/content_analyzer/raw_information_source.py</code> <pre><code>def __init__(self, file_path: str, separator: str = ',', has_header: bool = True, encoding: str = \"utf-8-sig\"):\n    super().__init__(file_path, encoding)\n    self.__has_header = has_header\n    self.__separator = separator\n</code></pre>"},{"location":"content_analyzer/raw_sources/#clayrs.content_analyzer.raw_information_source.CSVFile.representative_name","title":"<code>representative_name: str</code>  <code>property</code>","text":"<p>Method which returns a meaningful name for the raw source.</p> <p>In this case it's simply the file name + its extension</p> RETURNS DESCRIPTION <code>str</code> <p>The representative name for the raw source</p>"},{"location":"content_analyzer/raw_sources/#clayrs.content_analyzer.raw_information_source.DATFile","title":"<code>DATFile(file_path, encoding='utf-8')</code>","text":"<p>         Bases: <code>RawInformationSource</code></p> <p>Wrapper for a DAT file. This class is able to read from a DAT file where each entry is separated by the <code>::</code> string. Since a DAT file has no header, each entry can be referenced with a string representing its positional index (e.g. '0' for entry in the first position, '1' for the entry in the second position, etc.)</p> <p>You can iterate over the whole content of the raw source with a simple for loop: each row will be returned as a dictionary where keys are strings representing the positional indices, values are the entries</p> <p>Examples:</p> <p>Consider the following DAT file <pre><code>10::worker::75011\n11::without occupation::76112\n</code></pre></p> <pre><code>&gt;&gt;&gt; file = DATFile(dat_path)\n&gt;&gt;&gt; print(list(file))\n[{'0': '10', '1': 'worker', '2': '75011'},\n{'0': '11', '1': 'without occupation', '2': '76112'}]\n</code></pre> PARAMETER DESCRIPTION <code>file_path</code> <p>path of the dat file</p> <p> TYPE: <code>str</code> </p> <code>encoding</code> <p>define the type of encoding of data stored in the source (example: \"utf-8\")</p> <p> TYPE: <code>str</code> DEFAULT: <code>'utf-8'</code> </p> Source code in <code>clayrs/content_analyzer/raw_information_source.py</code> <pre><code>def __init__(self, file_path: str, encoding: str = \"utf-8\"):\n    super().__init__(file_path, encoding)\n</code></pre>"},{"location":"content_analyzer/raw_sources/#clayrs.content_analyzer.raw_information_source.DATFile.representative_name","title":"<code>representative_name: str</code>  <code>property</code>","text":"<p>Method which returns a meaningful name for the raw source.</p> <p>In this case it's simply the file name + its extension</p> RETURNS DESCRIPTION <code>str</code> <p>The representative name for the raw source</p>"},{"location":"content_analyzer/raw_sources/#clayrs.content_analyzer.raw_information_source.JSONFile","title":"<code>JSONFile(file_path, encoding='utf-8')</code>","text":"<p>         Bases: <code>RawInformationSource</code></p> <p>Wrapper for a JSON file. This class is able to read from a JSON file where each \"row\" is a dictionary-like object inside a list</p> <p>You can iterate over the whole content of the raw source with a simple for loop: each row will be returned as a dictionary</p> <p>Examples:</p> <p>Consider the following JSON file <pre><code>[{\"Title\":\"Jumanji\",\"Year\":\"1995\"},\n {\"Title\":\"Toy Story\",\"Year\":\"1995\"}]\n</code></pre></p> <pre><code>&gt;&gt;&gt; file = JSONFile(json_path)\n&gt;&gt;&gt; print(list(file))\n[{'Title': 'Jumanji', 'Year': '1995'},\n {'Title': 'Toy Story', 'Year': '1995'}]\n</code></pre> PARAMETER DESCRIPTION <code>file_path</code> <p>path of the dat file</p> <p> TYPE: <code>str</code> </p> <code>encoding</code> <p>define the type of encoding of data stored in the source (example: \"utf-8\")</p> <p> TYPE: <code>str</code> DEFAULT: <code>'utf-8'</code> </p> Source code in <code>clayrs/content_analyzer/raw_information_source.py</code> <pre><code>def __init__(self, file_path: str, encoding: str = \"utf-8\"):\n    super().__init__(file_path, encoding)\n</code></pre>"},{"location":"content_analyzer/raw_sources/#clayrs.content_analyzer.raw_information_source.JSONFile.representative_name","title":"<code>representative_name: str</code>  <code>property</code>","text":"<p>Method which returns a meaningful name for the raw source.</p> <p>In this case it's simply the file name + its extension</p> RETURNS DESCRIPTION <code>str</code> <p>The representative name for the raw source</p>"},{"location":"content_analyzer/raw_sources/#clayrs.content_analyzer.raw_information_source.SQLDatabase","title":"<code>SQLDatabase(host, username, password, database_name, table_name, encoding='utf-8')</code>","text":"<p>         Bases: <code>RawInformationSource</code></p> <p>Wrapper for a SQL database.</p> <p>You can iterate over the whole content of the raw source with a simple for loop: each row will be returned as a dictionary where keys are strings representing the positional indices, values are the entries</p> <p>Examples:</p> <p>Consider the following SQL table for the databaase 'movies' in localhost <pre><code>+----------+-------------+--------------+\n| Movie ID | Movie Title | Release Year |\n+----------+-------------+--------------+\n|        1 | Jumanji     |         1995 |\n|        2 | Toy Story   |         1995 |\n+----------+-------------+--------------+\n</code></pre></p> <pre><code>&gt;&gt;&gt; file = SQLDatabase(host='127.0.0.1', username='root', password='root',\n&gt;&gt;&gt;                    database_name='movies', table_name='movies_table')\n&gt;&gt;&gt; print(list(file))\n[{'Movie ID': '1', 'Movie Title': 'Jumanji', 'Release Year': '1995'},\n{'Movie ID': '2', 'Movie Title': 'Toy Story', 'Release Year': '1995'}]\n</code></pre> PARAMETER DESCRIPTION <code>host</code> <p>host ip of the sql server</p> <p> TYPE: <code>str</code> </p> <code>username</code> <p>username for the access</p> <p> TYPE: <code>str</code> </p> <code>password</code> <p>password for the access</p> <p> TYPE: <code>str</code> </p> <code>database_name</code> <p>name of database</p> <p> TYPE: <code>str</code> </p> <code>table_name</code> <p>name of the database table where data is stored</p> <p> TYPE: <code>str</code> </p> <code>encoding</code> <p>Define the type of encoding of data stored in the source (example: \"utf-8\")</p> <p> TYPE: <code>str</code> DEFAULT: <code>'utf-8'</code> </p> Source code in <code>clayrs/content_analyzer/raw_information_source.py</code> <pre><code>def __init__(self, host: str,\n             username: str,\n             password: str,\n             database_name: str,\n             table_name: str,\n             encoding: str = \"utf-8\"):\n    super().__init__('', encoding)\n    self.__host: str = host\n    self.__username: str = username\n    self.__password: str = password\n    self.__database_name: str = database_name\n    self.__table_name: str = table_name\n\n    conn = mysql.connector.connect(host=self.__host,\n                                   user=self.__username,\n                                   password=self.__password,\n                                   charset=self.encoding)\n    cursor = conn.cursor()\n    query = \"\"\"USE \"\"\" + self.__database_name + \"\"\";\"\"\"\n    cursor.execute(query)\n    conn.commit()\n    self.__conn = conn\n</code></pre>"},{"location":"content_analyzer/raw_sources/#clayrs.content_analyzer.raw_information_source.SQLDatabase.representative_name","title":"<code>representative_name: str</code>  <code>property</code>","text":"<p>Method which returns a meaningful name for the raw source.</p> <p>In this case it's the host name followed by the table name</p> RETURNS DESCRIPTION <code>str</code> <p>The representative name for the raw source</p>"},{"location":"content_analyzer/content_techniques/from_npy/","title":"Import from NPY","text":""},{"location":"content_analyzer/content_techniques/from_npy/#clayrs.content_analyzer.FromNPY","title":"<code>FromNPY(npy_file_path)</code>","text":"<p>         Bases: <code>FieldContentProductionTechnique</code></p> <p>Technique used to import a collection of numpy arrays where each row will be treated as a separate instance of data for the specified field</p> <p>In this case, the expected field data from the source is a string representing an integer (that is the row of the numpy collection corresponding to the representation associated to that content instance)</p> <p>Note that if specified, preprocessing operations will NOT be applied! Preprocessing is skipped with this technique</p> <p>This technique is particularly useful if the user wants to import data generated by a different library</p> PARAMETER DESCRIPTION <code>npy_file_path</code> <p>Path where the numpy collection is stored</p> <p> TYPE: <code>str</code> </p> Source code in <code>clayrs/content_analyzer/field_content_production_techniques/field_content_production_technique.py</code> <pre><code>def __init__(self, npy_file_path: str):\n\n    self.npy_file_path = npy_file_path\n    self.np_matrix = np.load(npy_file_path)\n\n    if len(self.np_matrix) &gt; 0:\n        self.dim_if_missing = self.np_matrix[0].shape\n    else:\n        raise ValueError('Matrix should have at least 1 row')\n\n    self._missing: Optional[int] = None\n</code></pre>"},{"location":"content_analyzer/content_techniques/textual_techniques/original_data/","title":"Original Data","text":""},{"location":"content_analyzer/content_techniques/textual_techniques/original_data/#clayrs.content_analyzer.OriginalData","title":"<code>OriginalData(dtype=str)</code>","text":"<p>         Bases: <code>SingleContentTechnique</code></p> <p>Technique used to retrieve the original data within the content's raw source without applying any processing operation.</p> <p>Note that if specified, preprocessing operations will still be applied!</p> <p>This technique is particularly useful if the user wants to keep the original data of the contents</p> PARAMETER DESCRIPTION <code>dtype</code> <p>If specified, data will be cast to the chosen dtype</p> <p> TYPE: <code>Callable</code> DEFAULT: <code>str</code> </p> Source code in <code>clayrs/content_analyzer/field_content_production_techniques/field_content_production_technique.py</code> <pre><code>def __init__(self, dtype: Callable = str):\n    super().__init__()\n    self.__dtype = dtype\n</code></pre>"},{"location":"content_analyzer/content_techniques/textual_techniques/synset_df_frequency/","title":"Synset Document Frequency","text":""},{"location":"content_analyzer/content_techniques/textual_techniques/synset_df_frequency/#clayrs.content_analyzer.PyWSDSynsetDocumentFrequency","title":"<code>PyWSDSynsetDocumentFrequency()</code>","text":"<p>         Bases: <code>SynsetDocumentFrequency</code></p> <p>Class that produces a sparse vector for each content representing the document frequency of each synset found inside the document. The synsets are computed thanks to PyWSD library.</p> <p>Consider this textual representation: <pre><code>content1: \"After being trapped in a jungle board game for 26 years\"\ncontent2: \"After considering jungle County, it was trapped in a jungle\"\n</code></pre></p> <p>This technique will produce the following sparse vectors:</p> <pre><code># vocabulary of the features\nvocabulary = {'trap.v.04': 4, 'jungle.n.03': 2, 'board.n.09': 0,\n              'plot.n.01': 3, 'twenty-six.s.01': 5,\n              'year.n.03': 7, 'view.v.02': 6, 'county.n.02': 1}\n\ncontent1:\n    (0, 4)  1\n    (0, 2)  1\n    (0, 0)  1\n    (0, 3)  1\n    (0, 5)  1\n    (0, 7)  1\n\ncontent2:\n    (0, 4)  1\n    (0, 2)  2\n    (0, 6)  1\n    (0, 1)  1\n</code></pre> Source code in <code>clayrs/content_analyzer/field_content_production_techniques/synset_document_frequency.py</code> <pre><code>def __init__(self):\n    # The import is here since pywsd has a long warm up phase that should affect the computation\n    # only when effectively instantiated\n    from pywsd import disambiguate\n\n    self.disambiguate = disambiguate\n    super().__init__()\n</code></pre>"},{"location":"content_analyzer/content_techniques/textual_techniques/tfidf/","title":"TfIdf","text":""},{"location":"content_analyzer/content_techniques/textual_techniques/tfidf/#clayrs.content_analyzer.SkLearnTfIdf","title":"<code>SkLearnTfIdf(max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=np.float64, norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False)</code>","text":"<p>         Bases: <code>TfIdfTechnique</code></p> <p>Class that produces a sparse vector for each content representing the tf-idf scores of its terms using SkLearn.</p> <p>Please refer to its documentation for more information about how it's computed</p> PARAMETER DESCRIPTION <code>max_df</code> <p>When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words). If float in range [0.0, 1.0], the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None.</p> <p> TYPE: <code>Union[float, int]</code> DEFAULT: <code>1.0</code> </p> <code>min_df</code> <p>When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature. If float in range of [0.0, 1.0], the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None.</p> <p> TYPE: <code>Union[float, int]</code> DEFAULT: <code>1</code> </p> <code>max_features</code> <p>If not None, build a vocabulary that only consider the top max_features ordered by term frequency across the corpus.</p> <p>This parameter is ignored if vocabulary is not None.</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>vocabulary</code> <p>Either a Mapping (e.g., a dict) where keys are terms and values are indices in the feature matrix, or an iterable over terms. If not given, a vocabulary is determined from the input documents.</p> <p> TYPE: <code>Union[Mapping, Iterable]</code> DEFAULT: <code>None</code> </p> <code>binary</code> <p>If True, all non-zero term counts are set to 1. This does not mean outputs will have only 0/1 values, only that the tf term in tf-idf is binary. (Set idf and normalization to False to get 0/1 outputs).</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>dtype</code> <p>Precision of the tf-idf scores</p> <p> TYPE: <code>Callable</code> DEFAULT: <code>np.float64</code> </p> <code>norm</code> <p>Each output row will have unit norm, either:</p> <ul> <li>'l2': Sum of squares of vector elements is 1. The cosine   similarity between two vectors is their dot product when l2 norm has   been applied.</li> <li>'l1': Sum of absolute values of vector elements is 1.   See :func:<code>preprocessing.normalize</code>.</li> </ul> <p> TYPE: <code>str</code> DEFAULT: <code>'l2'</code> </p> <code>use_idf</code> <p>Enable inverse-document-frequency reweighting. If False, idf(t) = 1.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>smooth_idf</code> <p>Smooth idf weights by adding one to document frequencies, as if an extra document was seen containing every term in the collection exactly once. Prevents zero divisions.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>sublinear_tf</code> <p>Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf).</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>clayrs/content_analyzer/field_content_production_techniques/tf_idf.py</code> <pre><code>def __init__(self, max_df: Union[float, int] = 1.0, min_df: Union[float, int] = 1, max_features: int = None,\n             vocabulary: Union[Mapping, Iterable] = None, binary: bool = False, dtype: Callable = np.float64,\n             norm: str = 'l2', use_idf: bool = True, smooth_idf: bool = True, sublinear_tf: bool = False):\n\n    super().__init__()\n    self._sk_vectorizer = TfidfVectorizer(max_df=max_df, min_df=min_df, max_features=max_features,\n                                          vocabulary=vocabulary, binary=binary, dtype=dtype,\n                                          norm=norm, use_idf=use_idf, smooth_idf=smooth_idf,\n                                          sublinear_tf=sublinear_tf)\n</code></pre>"},{"location":"content_analyzer/content_techniques/textual_techniques/tfidf/#clayrs.content_analyzer.WhooshTfIdf","title":"<code>WhooshTfIdf()</code>","text":"<p>         Bases: <code>TfIdfTechnique</code></p> <p>Class that produces a sparse vector for each content representing the tf-idf scores of its terms using Whoosh</p> <p>The tf-idf computation formula is:</p> \\[ tf \\mbox{-} idf = (1 + log10(tf)) * log10(idf) \\] Source code in <code>clayrs/content_analyzer/field_content_production_techniques/tf_idf.py</code> <pre><code>def __init__(self):\n    super().__init__()\n</code></pre>"},{"location":"content_analyzer/content_techniques/textual_techniques/embedding_techniques/combining_embeddings/","title":"Combine Embeddings","text":"<p>Via the following, you can obtain embeddings of coarser granularity from models which return embeddings of finer granularity (e.g. obtain sentence embeddings from a model which returns word embeddings)</p> <pre><code>from clayrs import content_analyzer as ca\n\n# obtain sentence embeddings combining token embeddings with a \n# centroid technique\nca.Word2SentenceEmbedding(embedding_source=ca.Gensim('glove-twitter-50'),\n                          combining_technique=ca.Centroid())\n</code></pre>"},{"location":"content_analyzer/content_techniques/textual_techniques/embedding_techniques/combining_embeddings/#clayrs.content_analyzer.Word2SentenceEmbedding","title":"<code>Word2SentenceEmbedding(embedding_source, combining_technique)</code>","text":"<p>         Bases: <code>CombiningSentenceEmbeddingTechnique</code></p> <p>Class that makes use of a word granularity embedding source to produce sentence embeddings</p> PARAMETER DESCRIPTION <code>embedding_source</code> <p>Any <code>WordEmbedding</code> model</p> <p> TYPE: <code>Union[WordEmbeddingLoader, WordEmbeddingLearner, str]</code> </p> <code>combining_technique</code> <p>Technique used to combine embeddings of finer granularity (word-level) to obtain embeddings of coarser granularity (sentence-level)</p> <p> TYPE: <code>CombiningTechnique</code> </p> Source code in <code>clayrs/content_analyzer/field_content_production_techniques/embedding_technique/embedding_technique.py</code> <pre><code>def __init__(self, embedding_source: Union[WordEmbeddingLoader, WordEmbeddingLearner, str],\n             combining_technique: CombiningTechnique):\n    # if isinstance(embedding_source, str):\n    #     embedding_source = self.from_str_to_embedding_source(embedding_source, WordEmbeddingLoader)\n    super().__init__(embedding_source, combining_technique)\n</code></pre>"},{"location":"content_analyzer/content_techniques/textual_techniques/embedding_techniques/combining_embeddings/#clayrs.content_analyzer.Word2DocEmbedding","title":"<code>Word2DocEmbedding(embedding_source, combining_technique)</code>","text":"<p>         Bases: <code>CombiningDocumentEmbeddingTechnique</code></p> <p>Class that makes use of a word granularity embedding source to produce embeddings of document granularity</p> PARAMETER DESCRIPTION <code>embedding_source</code> <p>Any <code>WordEmbedding</code> model</p> <p> TYPE: <code>Union[WordEmbeddingLoader, WordEmbeddingLearner, str]</code> </p> <code>combining_technique</code> <p>Technique used to combine embeddings of finer granularity (word-level) to obtain embeddings of coarser granularity (doc-level)</p> <p> TYPE: <code>CombiningTechnique</code> </p> Source code in <code>clayrs/content_analyzer/field_content_production_techniques/embedding_technique/embedding_technique.py</code> <pre><code>def __init__(self, embedding_source: Union[WordEmbeddingLoader, WordEmbeddingLearner, str],\n             combining_technique: CombiningTechnique):\n    # if isinstance(embedding_source, str):\n    #     embedding_source = self.from_str_to_embedding_source(embedding_source, WordEmbeddingLoader)\n    super().__init__(embedding_source, combining_technique)\n</code></pre>"},{"location":"content_analyzer/content_techniques/textual_techniques/embedding_techniques/combining_embeddings/#clayrs.content_analyzer.Sentence2DocEmbedding","title":"<code>Sentence2DocEmbedding(embedding_source, combining_technique)</code>","text":"<p>         Bases: <code>CombiningDocumentEmbeddingTechnique</code></p> <p>Class that makes use of a sentence granularity embedding source to produce embeddings of document granularity</p> PARAMETER DESCRIPTION <code>embedding_source</code> <p>Any <code>SentenceEmbedding</code> model</p> <p> TYPE: <code>Union[SentenceEmbeddingLoader, SentenceEmbeddingLearner, str]</code> </p> <code>combining_technique</code> <p>Technique used to combine embeddings of finer granularity (sentence-level) to obtain embeddings of coarser granularity (doc-level)</p> <p> TYPE: <code>CombiningTechnique</code> </p> Source code in <code>clayrs/content_analyzer/field_content_production_techniques/embedding_technique/embedding_technique.py</code> <pre><code>def __init__(self, embedding_source: Union[SentenceEmbeddingLoader, SentenceEmbeddingLearner, str],\n             combining_technique: CombiningTechnique):\n    # if isinstance(embedding_source, str):\n    #     embedding_source = self.from_str_to_embedding_source(embedding_source, SentenceEmbeddingLoader)\n    super().__init__(embedding_source, combining_technique)\n</code></pre>"},{"location":"content_analyzer/content_techniques/textual_techniques/embedding_techniques/combining_embeddings/#combining-techniques","title":"Combining Techniques","text":""},{"location":"content_analyzer/content_techniques/textual_techniques/embedding_techniques/combining_embeddings/#clayrs.content_analyzer.Centroid","title":"<code>Centroid</code>","text":"<p>         Bases: <code>CombiningTechnique</code></p> <p>This class computes the centroid vector of a matrix.</p>"},{"location":"content_analyzer/content_techniques/textual_techniques/embedding_techniques/combining_embeddings/#clayrs.content_analyzer.field_content_production_techniques.embedding_technique.combining_technique.Centroid.combine","title":"<code>combine(embedding_matrix)</code>","text":"<p>Calculates the centroid of the input matrix</p> PARAMETER DESCRIPTION <code>embedding_matrix</code> <p>np bi-dimensional array where rows are words columns are hidden dimension whose centroid will be calculated</p> <p> TYPE: <code>np.ndarray</code> </p> RETURNS DESCRIPTION <code>np.ndarray</code> <p>Centroid vector of the input matrix</p> Source code in <code>clayrs/content_analyzer/field_content_production_techniques/embedding_technique/combining_technique.py</code> <pre><code>def combine(self, embedding_matrix: np.ndarray) -&gt; np.ndarray:\n\"\"\"\n    Calculates the centroid of the input matrix\n\n    Args:\n        embedding_matrix: np bi-dimensional array where rows are words columns are hidden dimension\n            whose centroid will be calculated\n\n    Returns:\n        Centroid vector of the input matrix\n    \"\"\"\n    return np.nanmean(embedding_matrix, axis=0)\n</code></pre>"},{"location":"content_analyzer/content_techniques/textual_techniques/embedding_techniques/combining_embeddings/#clayrs.content_analyzer.Sum","title":"<code>Sum</code>","text":"<p>         Bases: <code>CombiningTechnique</code></p> <p>This class computes the sum vector of a matrix.</p>"},{"location":"content_analyzer/content_techniques/textual_techniques/embedding_techniques/combining_embeddings/#clayrs.content_analyzer.field_content_production_techniques.embedding_technique.combining_technique.Sum.combine","title":"<code>combine(embedding_matrix)</code>","text":"<p>Calculates the sum vector of the input matrix</p> PARAMETER DESCRIPTION <code>embedding_matrix</code> <p>np bi-dimensional array where rows are words columns are hidden dimension whose sum vector will be calculated</p> <p> TYPE: <code>np.ndarray</code> </p> RETURNS DESCRIPTION <code>np.ndarray</code> <p>Sum vector of the input matrix</p> Source code in <code>clayrs/content_analyzer/field_content_production_techniques/embedding_technique/combining_technique.py</code> <pre><code>def combine(self, embedding_matrix: np.ndarray) -&gt; np.ndarray:\n\"\"\"\n    Calculates the sum vector of the input matrix\n\n    Args:\n        embedding_matrix: np bi-dimensional array where rows are words columns are hidden dimension\n            whose sum vector will be calculated\n\n    Returns:\n        Sum vector of the input matrix\n    \"\"\"\n    return np.sum(embedding_matrix, axis=0)\n</code></pre>"},{"location":"content_analyzer/content_techniques/textual_techniques/embedding_techniques/combining_embeddings/#clayrs.content_analyzer.SingleToken","title":"<code>SingleToken(token_index)</code>","text":"<p>         Bases: <code>CombiningTechnique</code></p> <p>Class which takes a specific row as representative of the whole matrix</p> PARAMETER DESCRIPTION <code>token_index</code> <p>index of the row of the matrix to take</p> <p> TYPE: <code>int</code> </p> Source code in <code>clayrs/content_analyzer/field_content_production_techniques/embedding_technique/combining_technique.py</code> <pre><code>def __init__(self, token_index: int):\n    self.token_index = token_index\n    super().__init__()\n</code></pre>"},{"location":"content_analyzer/content_techniques/textual_techniques/embedding_techniques/combining_embeddings/#clayrs.content_analyzer.field_content_production_techniques.embedding_technique.combining_technique.SingleToken.combine","title":"<code>combine(embedding_matrix)</code>","text":"<p>Takes the row with index <code>token_index</code> (set in the constructor) from the input <code>embedding_matrix</code></p> PARAMETER DESCRIPTION <code>embedding_matrix</code> <p>np bi-dimensional array where rows are words columns are hidden dimension from where the single token will be extracted</p> <p> TYPE: <code>np.ndarray</code> </p> RETURNS DESCRIPTION <code>np.ndarray</code> <p>Single row as representative of the whole matrix</p> RAISES DESCRIPTION <code>IndexError</code> <p>Exception raised when <code>token_index</code> (set in the constructor) is out of bounds for the input matrix</p> Source code in <code>clayrs/content_analyzer/field_content_production_techniques/embedding_technique/combining_technique.py</code> <pre><code>def combine(self, embedding_matrix: np.ndarray) -&gt; np.ndarray:\n\"\"\"\n    Takes the row with index `token_index` (set in the constructor) from the input `embedding_matrix`\n\n    Args:\n        embedding_matrix: np bi-dimensional array where rows are words columns are hidden dimension\n            from where the single token will be extracted\n\n    Returns:\n        Single row as representative of the whole matrix\n\n    Raises:\n        IndexError: Exception raised when `token_index` (set in the constructor) is out of bounds for the input\n            matrix\n    \"\"\"\n    try:\n        sentence_embedding = embedding_matrix[self.token_index]\n    except IndexError:\n        raise IndexError(f'The embedding matrix has {embedding_matrix.shape[1]} '\n                         f'embeddings but you tried to take the {self.token_index+1}th')\n    return sentence_embedding\n</code></pre>"},{"location":"content_analyzer/content_techniques/textual_techniques/embedding_techniques/contextualized_embeddings/","title":"Contextualized Embeddings","text":"<p>Via the following, you can obtain embeddings of finer granularity from models which are able to return also embeddings of coarser granularity (e.g. obtain word embeddings from a model which is also able to return sentence  embeddings).</p> <p>For now only models working at sentence and token level are implemented</p> <pre><code>from clayrs import content_analyzer as ca\n\n# obtain sentence embeddings combining token embeddings with a \n# centroid technique\nca.Sentence2WordEmbedding(embedding_source=ca.BertTransformers('bert-base-uncased'))\n</code></pre>"},{"location":"content_analyzer/content_techniques/textual_techniques/embedding_techniques/contextualized_embeddings/#clayrs.content_analyzer.Sentence2WordEmbedding","title":"<code>Sentence2WordEmbedding(embedding_source)</code>","text":"<p>         Bases: <code>DecombiningInWordsEmbeddingTechnique</code></p> <p>Class that makes use of a sentence granularity embedding source to produce an embedding matrix with word granularity</p> Source code in <code>clayrs/content_analyzer/field_content_production_techniques/embedding_technique/embedding_technique.py</code> <pre><code>def __init__(self, embedding_source: Union[SentenceEmbeddingLoader, SentenceEmbeddingLearner]):\n    # if isinstance(embedding_source, str):\n    #     embedding_source = self.from_str_to_embedding_source(embedding_source, SentenceEmbeddingLoader)\n    super().__init__(embedding_source)\n</code></pre>"},{"location":"content_analyzer/content_techniques/textual_techniques/embedding_techniques/contextualized_embeddings/#clayrs.content_analyzer.field_content_production_techniques.embedding_technique.embedding_technique.Sentence2WordEmbedding.produce_single_repr","title":"<code>produce_single_repr(field_data)</code>","text":"<p>Produces a single matrix where each row is the embedding representation of each token of the sentence, while the columns are the hidden dimension of the chosen model</p> PARAMETER DESCRIPTION <code>field_data</code> <p>textual data to complexly represent</p> <p> TYPE: <code>Union[List[str], str]</code> </p> RETURNS DESCRIPTION <code>EmbeddingField</code> <p>Embedding for each token of the sentence</p> Source code in <code>clayrs/content_analyzer/field_content_production_techniques/embedding_technique/embedding_technique.py</code> <pre><code>def produce_single_repr(self, field_data: Union[List[str], str]) -&gt; EmbeddingField:\n\"\"\"\n    Produces a single matrix where each row is the embedding representation of each token of the sentence,\n    while the columns are the hidden dimension of the chosen model\n\n    Args:\n        field_data: textual data to complexly represent\n\n    Returns:\n        Embedding for each token of the sentence\n\n    \"\"\"\n    field_data = check_not_tokenized(field_data)\n    embedding_source: Union[SentenceEmbeddingLoader, SentenceEmbeddingLearner] = self.embedding_source\n    words_embeddings = embedding_source.get_embedding_token(field_data)\n    return EmbeddingField(words_embeddings)\n</code></pre>"},{"location":"content_analyzer/content_techniques/textual_techniques/embedding_techniques/contextualized_embeddings/#model-able-to-return-sentence-and-token-embeddings","title":"Model able to return sentence and token embeddings","text":""},{"location":"content_analyzer/content_techniques/textual_techniques/embedding_techniques/contextualized_embeddings/#clayrs.content_analyzer.BertTransformers","title":"<code>BertTransformers(model_name='bert-base-uncased', vec_strategy=CatStrategy(1), pooling_strategy=Centroid())</code>","text":"<p>         Bases: <code>Transformers</code></p> <p>Class that produces sentences/token embeddings using any Bert model from hugging face.</p> PARAMETER DESCRIPTION <code>model_name</code> <p>Name of the embeddings model to download or path where the model is stored locally</p> <p> TYPE: <code>str</code> DEFAULT: <code>'bert-base-uncased'</code> </p> <code>vec_strategy</code> <p>Strategy which will be used to combine each output layer to obtain a single one</p> <p> TYPE: <code>VectorStrategy</code> DEFAULT: <code>CatStrategy(1)</code> </p> <code>pooling_strategy</code> <p>Strategy which will be used to combine the embedding representation of each token into a single one, representing the embedding of the whole sentence</p> <p> TYPE: <code>CombiningTechnique</code> DEFAULT: <code>Centroid()</code> </p> Source code in <code>clayrs/content_analyzer/embeddings/embedding_loader/transformer.py</code> <pre><code>def __init__(self, model_name: str = 'bert-base-uncased',\n             vec_strategy: VectorStrategy = CatStrategy(1),\n             pooling_strategy: CombiningTechnique = Centroid()):\n    super().__init__(model_name, vec_strategy, pooling_strategy)\n</code></pre>"},{"location":"content_analyzer/content_techniques/textual_techniques/embedding_techniques/contextualized_embeddings/#clayrs.content_analyzer.T5Transformers","title":"<code>T5Transformers(model_name='t5-small', vec_strategy=CatStrategy(1), pooling_strategy=Centroid())</code>","text":"<p>         Bases: <code>Transformers</code></p> <p>Class that produces sentences/token embeddings using sbert.</p> PARAMETER DESCRIPTION <code>model_name</code> <p>Name of the embeddings model to download or path where the model is stored locally</p> <p> TYPE: <code>str</code> DEFAULT: <code>'t5-small'</code> </p> <code>vec_strategy</code> <p>Strategy which will be used to combine each output layer to obtain a single one</p> <p> TYPE: <code>VectorStrategy</code> DEFAULT: <code>CatStrategy(1)</code> </p> <code>pooling_strategy</code> <p>Strategy which will be used to combine the embedding representation of each token into a single one, representing the embedding of the whole sentence</p> <p> TYPE: <code>CombiningTechnique</code> DEFAULT: <code>Centroid()</code> </p> Source code in <code>clayrs/content_analyzer/embeddings/embedding_loader/transformer.py</code> <pre><code>def __init__(self, model_name: str = 't5-small',\n             vec_strategy: VectorStrategy = CatStrategy(1),\n             pooling_strategy: CombiningTechnique = Centroid()):\n    super().__init__(model_name, vec_strategy, pooling_strategy)\n</code></pre>"},{"location":"content_analyzer/content_techniques/textual_techniques/embedding_techniques/document_embeddings/","title":"Document Embeddings","text":"<p>Via the following, you can obtain embeddings of document granularity</p> <pre><code>from clayrs import content_analyzer as ca\n\n# obtain document embeddings by training LDA model\n# on corpus of contents to complexly represent\nca.DocumentEmbeddingTechnique(embedding_source=ca.GensimLDA())\n</code></pre>"},{"location":"content_analyzer/content_techniques/textual_techniques/embedding_techniques/document_embeddings/#clayrs.content_analyzer.DocumentEmbeddingTechnique","title":"<code>DocumentEmbeddingTechnique(embedding_source)</code>","text":"<p>         Bases: <code>StandardEmbeddingTechnique</code></p> <p>Class that makes use of a document granularity embedding source to produce document embeddings</p> PARAMETER DESCRIPTION <code>embedding_source</code> <p>Any <code>DocumentEmbedding</code> model</p> <p> TYPE: <code>Union[DocumentEmbeddingLoader, DocumentEmbeddingLearner, str]</code> </p> Source code in <code>clayrs/content_analyzer/field_content_production_techniques/embedding_technique/embedding_technique.py</code> <pre><code>def __init__(self, embedding_source: Union[DocumentEmbeddingLoader, DocumentEmbeddingLearner, str]):\n    # if isinstance(embedding_source, str):\n    #     embedding_source = self.from_str_to_embedding_source(embedding_source, DocumentEmbeddingLoader)\n    super().__init__(embedding_source)\n</code></pre>"},{"location":"content_analyzer/content_techniques/textual_techniques/embedding_techniques/document_embeddings/#document-embedding-models","title":"Document Embedding models","text":""},{"location":"content_analyzer/content_techniques/textual_techniques/embedding_techniques/document_embeddings/#clayrs.content_analyzer.GensimLatentSemanticAnalysis","title":"<code>GensimLatentSemanticAnalysis(reference=None, auto_save=True, **kwargs)</code>","text":"<p>         Bases: <code>GensimDocumentEmbeddingLearner</code></p> <p>Class that implements Latent Semantic Analysis (A.K.A. Latent Semantic Indexing) (LSI) thanks to the Gensim library.</p> <p>If a pre-trained local Word2Vec model must be loaded, put its path in the <code>reference</code> parameter. Otherwise, a Word2Vec model will be trained from scratch based on the preprocessed corpus of the contents to complexly represent</p> <p>If you'd like to save the model once trained, set the path in the <code>reference</code> parameter and set <code>auto_save=True</code>. If <code>reference</code> is None, trained model won't be saved after training and will only be used to produce contents in the current run</p> <p>Additional parameters regarding the model itself could be passed, check gensim documentation to see what else can be customized</p> PARAMETER DESCRIPTION <code>reference</code> <p>Path of the model to load/where the model trained will be saved if <code>auto_save=True</code>. If None the trained model won't be saved after training and will only be used to produce contents in the current run</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>auto_save</code> <p>If True, the model will be saved in the path specified in <code>reference</code> parameter</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> Source code in <code>clayrs/content_analyzer/embeddings/embedding_learner/latent_semantic_analysis.py</code> <pre><code>def __init__(self, reference: str = None, auto_save: bool = True,  **kwargs):\n    super().__init__(reference, auto_save, \".model\", **kwargs)\n</code></pre>"},{"location":"content_analyzer/content_techniques/textual_techniques/embedding_techniques/document_embeddings/#clayrs.content_analyzer.GensimLDA","title":"<code>GensimLDA(reference=None, auto_save=True, **kwargs)</code>","text":"<p>         Bases: <code>GensimDocumentEmbeddingLearner</code></p> <p>Class that implements Latent Dirichlet Allocation (LDA) thanks to the Gensim library.</p> <p>If a pre-trained local Word2Vec model must be loaded, put its path in the <code>reference</code> parameter. Otherwise, a Word2Vec model will be trained from scratch based on the preprocessed corpus of the contents to complexly represent</p> <p>If you'd like to save the model once trained, set the path in the <code>reference</code> parameter and set <code>auto_save=True</code>. If <code>reference</code> is None, trained model won't be saved after training and will only be used to produce contents in the current run</p> <p>Additional parameters regarding the model itself could be passed, check gensim documentation to see what else can be customized</p> PARAMETER DESCRIPTION <code>reference</code> <p>Path of the model to load/where the model trained will be saved if <code>auto_save=True</code>. If None the trained model won't be saved after training and will only be used to produce contents in the current run</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>auto_save</code> <p>If True, the model will be saved in the path specified in <code>reference</code> parameter</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> Source code in <code>clayrs/content_analyzer/embeddings/embedding_learner/lda.py</code> <pre><code>def __init__(self, reference: str = None, auto_save: bool = True, **kwargs):\n    super().__init__(reference, auto_save, \".model\", **kwargs)\n</code></pre>"},{"location":"content_analyzer/content_techniques/textual_techniques/embedding_techniques/sentence_embeddings/","title":"Sentence Embeddings","text":"<p>Via the following, you can obtain embeddings of sentence granularity</p> <pre><code>from clayrs import content_analyzer as ca\n\n# obtain sentence embeddings using pre-trained model 'glove-twitter-50'\n# from SBERT library\nca.SentenceEmbeddingTechnique(embedding_source=ca.Sbert('paraphrase-distilroberta-base-v1'))\n</code></pre>"},{"location":"content_analyzer/content_techniques/textual_techniques/embedding_techniques/sentence_embeddings/#clayrs.content_analyzer.SentenceEmbeddingTechnique","title":"<code>SentenceEmbeddingTechnique(embedding_source)</code>","text":"<p>         Bases: <code>StandardEmbeddingTechnique</code></p> <p>Class that makes use of a sentence granularity embedding source to produce sentence embeddings</p> PARAMETER DESCRIPTION <code>embedding_source</code> <p>Any <code>SentenceEmbedding</code> model</p> <p> TYPE: <code>Union[SentenceEmbeddingLoader, SentenceEmbeddingLearner, str]</code> </p> Source code in <code>clayrs/content_analyzer/field_content_production_techniques/embedding_technique/embedding_technique.py</code> <pre><code>def __init__(self, embedding_source: Union[SentenceEmbeddingLoader, SentenceEmbeddingLearner, str]):\n    # if isinstance(embedding_source, str):\n    #     embedding_source = self.from_str_to_embedding_source(embedding_source, SentenceEmbeddingLoader)\n    super().__init__(embedding_source)\n</code></pre>"},{"location":"content_analyzer/content_techniques/textual_techniques/embedding_techniques/sentence_embeddings/#sentence-embedding-models","title":"Sentence Embedding models","text":""},{"location":"content_analyzer/content_techniques/textual_techniques/embedding_techniques/sentence_embeddings/#clayrs.content_analyzer.BertTransformers","title":"<code>BertTransformers(model_name='bert-base-uncased', vec_strategy=CatStrategy(1), pooling_strategy=Centroid())</code>","text":"<p>         Bases: <code>Transformers</code></p> <p>Class that produces sentences/token embeddings using any Bert model from hugging face.</p> PARAMETER DESCRIPTION <code>model_name</code> <p>Name of the embeddings model to download or path where the model is stored locally</p> <p> TYPE: <code>str</code> DEFAULT: <code>'bert-base-uncased'</code> </p> <code>vec_strategy</code> <p>Strategy which will be used to combine each output layer to obtain a single one</p> <p> TYPE: <code>VectorStrategy</code> DEFAULT: <code>CatStrategy(1)</code> </p> <code>pooling_strategy</code> <p>Strategy which will be used to combine the embedding representation of each token into a single one, representing the embedding of the whole sentence</p> <p> TYPE: <code>CombiningTechnique</code> DEFAULT: <code>Centroid()</code> </p> Source code in <code>clayrs/content_analyzer/embeddings/embedding_loader/transformer.py</code> <pre><code>def __init__(self, model_name: str = 'bert-base-uncased',\n             vec_strategy: VectorStrategy = CatStrategy(1),\n             pooling_strategy: CombiningTechnique = Centroid()):\n    super().__init__(model_name, vec_strategy, pooling_strategy)\n</code></pre>"},{"location":"content_analyzer/content_techniques/textual_techniques/embedding_techniques/sentence_embeddings/#clayrs.content_analyzer.Sbert","title":"<code>Sbert(model_name_or_file_path='paraphrase-distilroberta-base-v1')</code>","text":"<p>         Bases: <code>SentenceEmbeddingLoader</code></p> <p>Class that produces sentences embeddings using sbert.</p> <p>The model will be automatically downloaded if not present locally.</p> PARAMETER DESCRIPTION <code>model_name_or_file_path</code> <p>name of the model to download or path where the model is stored locally</p> <p> TYPE: <code>str</code> DEFAULT: <code>'paraphrase-distilroberta-base-v1'</code> </p> Source code in <code>clayrs/content_analyzer/embeddings/embedding_loader/sbert.py</code> <pre><code>def __init__(self, model_name_or_file_path: str = 'paraphrase-distilroberta-base-v1'):\n    super().__init__(model_name_or_file_path)\n</code></pre>"},{"location":"content_analyzer/content_techniques/textual_techniques/embedding_techniques/sentence_embeddings/#clayrs.content_analyzer.T5Transformers","title":"<code>T5Transformers(model_name='t5-small', vec_strategy=CatStrategy(1), pooling_strategy=Centroid())</code>","text":"<p>         Bases: <code>Transformers</code></p> <p>Class that produces sentences/token embeddings using sbert.</p> PARAMETER DESCRIPTION <code>model_name</code> <p>Name of the embeddings model to download or path where the model is stored locally</p> <p> TYPE: <code>str</code> DEFAULT: <code>'t5-small'</code> </p> <code>vec_strategy</code> <p>Strategy which will be used to combine each output layer to obtain a single one</p> <p> TYPE: <code>VectorStrategy</code> DEFAULT: <code>CatStrategy(1)</code> </p> <code>pooling_strategy</code> <p>Strategy which will be used to combine the embedding representation of each token into a single one, representing the embedding of the whole sentence</p> <p> TYPE: <code>CombiningTechnique</code> DEFAULT: <code>Centroid()</code> </p> Source code in <code>clayrs/content_analyzer/embeddings/embedding_loader/transformer.py</code> <pre><code>def __init__(self, model_name: str = 't5-small',\n             vec_strategy: VectorStrategy = CatStrategy(1),\n             pooling_strategy: CombiningTechnique = Centroid()):\n    super().__init__(model_name, vec_strategy, pooling_strategy)\n</code></pre>"},{"location":"content_analyzer/content_techniques/textual_techniques/embedding_techniques/word_embeddings/","title":"Word Embeddings","text":"<p>Via the following, you can obtain embeddings of word granularity</p> <pre><code>from clayrs import content_analyzer as ca\n\n# obtain word embeddings using pre-trained model 'glove-twitter-50'\n# from Gensim library\nca.WordEmbeddingTechnique(embedding_source=ca.Gensim('glove-twitter-50'))\n</code></pre>"},{"location":"content_analyzer/content_techniques/textual_techniques/embedding_techniques/word_embeddings/#clayrs.content_analyzer.WordEmbeddingTechnique","title":"<code>WordEmbeddingTechnique(embedding_source)</code>","text":"<p>         Bases: <code>StandardEmbeddingTechnique</code></p> <p>Class that makes use of a word granularity embedding source to produce word embeddings</p> PARAMETER DESCRIPTION <code>embedding_source</code> <p>Any <code>WordEmbedding</code> model</p> <p> TYPE: <code>Union[WordEmbeddingLoader, WordEmbeddingLearner, str]</code> </p> Source code in <code>clayrs/content_analyzer/field_content_production_techniques/embedding_technique/embedding_technique.py</code> <pre><code>def __init__(self, embedding_source: Union[WordEmbeddingLoader, WordEmbeddingLearner, str]):\n    # if isinstance(embedding_source, str):\n    #     embedding_source = self.from_str_to_embedding_source(embedding_source, WordEmbeddingLoader)\n    super().__init__(embedding_source)\n</code></pre>"},{"location":"content_analyzer/content_techniques/textual_techniques/embedding_techniques/word_embeddings/#word-embedding-models","title":"Word Embedding models","text":""},{"location":"content_analyzer/content_techniques/textual_techniques/embedding_techniques/word_embeddings/#clayrs.content_analyzer.Gensim","title":"<code>Gensim(model_name='glove-twitter-25')</code>","text":"<p>         Bases: <code>WordEmbeddingLoader</code></p> <p>Class that produces word embeddings using gensim pre-trained models.</p> <p>The model will be automatically downloaded using the gensim downloader api if not present locally.</p> PARAMETER DESCRIPTION <code>model_name</code> <p>Name of the model to load/download</p> <p> TYPE: <code>str</code> DEFAULT: <code>'glove-twitter-25'</code> </p> Source code in <code>clayrs/content_analyzer/embeddings/embedding_loader/gensim.py</code> <pre><code>def __init__(self, model_name: str = 'glove-twitter-25'):\n    super().__init__(model_name)\n</code></pre>"},{"location":"content_analyzer/content_techniques/textual_techniques/embedding_techniques/word_embeddings/#clayrs.content_analyzer.GensimDoc2Vec","title":"<code>GensimDoc2Vec(reference=None, auto_save=True, **kwargs)</code>","text":"<p>         Bases: <code>GensimWordEmbeddingLearner</code></p> <p>Class that implements Doc2Vec model thanks to the Gensim library.</p> <p>If a pre-trained local Word2Vec model must be loaded, put its path in the <code>reference</code> parameter. Otherwise, a Word2Vec model will be trained from scratch based on the preprocessed corpus of the contents to complexly represent</p> <p>If you'd like to save the model once trained, set the path in the <code>reference</code> parameter and set <code>auto_save=True</code>. If <code>reference</code> is None, trained model won't be saved after training and will only be used to produce contents in the current run</p> <p>Additional parameters regarding the model itself could be passed, check gensim documentation to see what else can be customized</p> PARAMETER DESCRIPTION <code>reference</code> <p>Path of the model to load/where the model trained will be saved if <code>auto_save=True</code>. If None the trained model won't be saved after training and will only be used to produce contents in the current run</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>auto_save</code> <p>If True, the model will be saved in the path specified in <code>reference</code> parameter</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> Source code in <code>clayrs/content_analyzer/embeddings/embedding_learner/doc2vec.py</code> <pre><code>def __init__(self, reference: str = None, auto_save: bool = True, **kwargs):\n    super().__init__(reference, auto_save, \".kv\", **kwargs)\n</code></pre>"},{"location":"content_analyzer/content_techniques/textual_techniques/embedding_techniques/word_embeddings/#clayrs.content_analyzer.GensimFastText","title":"<code>GensimFastText(reference=None, auto_save=True, **kwargs)</code>","text":"<p>         Bases: <code>GensimWordEmbeddingLearner</code></p> <p>Class that implements FastText model thanks to the Gensim library.</p> <p>If a pre-trained local Word2Vec model must be loaded, put its path in the <code>reference</code> parameter. Otherwise, a Word2Vec model will be trained from scratch based on the preprocessed corpus of the contents to complexly represent</p> <p>If you'd like to save the model once trained, set the path in the <code>reference</code> parameter and set <code>auto_save=True</code>. If <code>reference</code> is None, trained model won't be saved after training and will only be used to produce contents in the current run</p> <p>Additional parameters regarding the model itself could be passed, check gensim documentation to see what else can be customized</p> PARAMETER DESCRIPTION <code>reference</code> <p>Path of the model to load/where the model trained will be saved if <code>auto_save=True</code>. If None the trained model won't be saved after training and will only be used to produce contents in the current run</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>auto_save</code> <p>If True, the model will be saved in the path specified in <code>reference</code> parameter</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> Source code in <code>clayrs/content_analyzer/embeddings/embedding_learner/fasttext.py</code> <pre><code>def __init__(self, reference: str = None, auto_save: bool = True, **kwargs):\n    super().__init__(reference, auto_save, \".kv\", **kwargs)\n</code></pre>"},{"location":"content_analyzer/content_techniques/textual_techniques/embedding_techniques/word_embeddings/#clayrs.content_analyzer.GensimRandomIndexing","title":"<code>GensimRandomIndexing(reference=None, auto_save=True, **kwargs)</code>","text":"<p>         Bases: <code>GensimDocumentEmbeddingLearner</code></p> <p>Class that implements RandomIndexing model thanks to the Gensim library.</p> <p>If a pre-trained local Word2Vec model must be loaded, put its path in the <code>reference</code> parameter. Otherwise, a Word2Vec model will be trained from scratch based on the preprocessed corpus of the contents to complexly represent</p> <p>If you'd like to save the model once trained, set the path in the <code>reference</code> parameter and set <code>auto_save=True</code>. If <code>reference</code> is None, trained model won't be saved after training and will only be used to produce contents in the current run</p> <p>Additional parameters regarding the model itself could be passed, check gensim documentation to see what else can be customized</p> PARAMETER DESCRIPTION <code>reference</code> <p>Path of the model to load/where the model trained will be saved if <code>auto_save=True</code>. If None the trained model won't be saved after training and will only be used to produce contents in the current run</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>auto_save</code> <p>If True, the model will be saved in the path specified in <code>reference</code> parameter</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> Source code in <code>clayrs/content_analyzer/embeddings/embedding_learner/random_indexing.py</code> <pre><code>def __init__(self, reference: str = None, auto_save: bool = True, **kwargs):\n    super().__init__(reference, auto_save, \".model\", **kwargs)\n</code></pre>"},{"location":"content_analyzer/content_techniques/textual_techniques/embedding_techniques/word_embeddings/#clayrs.content_analyzer.GensimWord2Vec","title":"<code>GensimWord2Vec(reference=None, auto_save=True, **kwargs)</code>","text":"<p>         Bases: <code>GensimWordEmbeddingLearner</code></p> <p>Class that implements Word2Vec model thanks to the Gensim library.</p> <p>If a pre-trained local Word2Vec model must be loaded, put its path in the <code>reference</code> parameter. Otherwise, a Word2Vec model will be trained from scratch based on the preprocessed corpus of the contents to complexly represent</p> <p>If you'd like to save the model once trained, set the path in the <code>reference</code> parameter and set <code>auto_save=True</code>. If <code>reference</code> is None, trained model won't be saved after training and will only be used to produce contents in the current run</p> <p>Additional parameters regarding the model itself could be passed, check gensim documentation to see what else can be customized</p> PARAMETER DESCRIPTION <code>reference</code> <p>Path of the model to load/where the model trained will be saved if <code>auto_save=True</code>. If None the trained model won't be saved after training and will only be used to produce contents in the current run</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>auto_save</code> <p>If True, the model will be saved in the path specified in <code>reference</code> parameter</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> Source code in <code>clayrs/content_analyzer/embeddings/embedding_learner/word2vec.py</code> <pre><code>def __init__(self, reference: str = None, auto_save: bool = True, **kwargs):\n    super().__init__(reference, auto_save, \".kv\", **kwargs)\n</code></pre>"},{"location":"content_analyzer/content_techniques/visual_techniques/high_level_visual/","title":"High level techniques","text":""},{"location":"content_analyzer/content_techniques/visual_techniques/high_level_visual/#clayrs.content_analyzer.field_content_production_techniques.visual_techniques.high_level_techniques.PytorchImageModels","title":"<code>PytorchImageModels(model_name, feature_layer=-1, flatten=True, device='cpu', apply_on_output=None, imgs_dirs='imgs_dirs', max_timeout=2, max_retries=5, max_workers=0, batch_size=64, resize_size=(227, 227))</code>","text":"<p>         Bases: <code>HighLevelVisual</code></p> <p>High level technique which uses the [timm library] (https://timm.fast.ai/) for feature extraction from images using pre-trained models</p> PARAMETER DESCRIPTION <code>model_name</code> <p>a model name supported by the timm library</p> <p> TYPE: <code>str</code> </p> <code>feature_layer</code> <p>the layer index from which the features will be retrieved NOTE: the model is loaded from the timm library with the parameter \"features_only\" set at True, meaning that only feature layers of the model will be available and accessible through the index</p> <p> TYPE: <code>int</code> DEFAULT: <code>-1</code> </p> <code>flatten</code> <p>whether the features obtained from the model should be flattened or not</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>imgs_dirs</code> <p>directory where the images are stored (or will be stored in the case of fields containing links)</p> <p> TYPE: <code>str</code> DEFAULT: <code>'imgs_dirs'</code> </p> <code>max_timeout</code> <p>maximum time to wait before considering a request failed (image from link)</p> <p> TYPE: <code>int</code> DEFAULT: <code>2</code> </p> <code>max_retries</code> <p>maximum number of retries to retrieve an image from a link</p> <p> TYPE: <code>int</code> DEFAULT: <code>5</code> </p> <code>max_workers</code> <p>maximum number of workers for parallelism</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>batch_size</code> <p>batch size for the images dataloader</p> <p> TYPE: <code>int</code> DEFAULT: <code>64</code> </p> <code>resize_size</code> <p>since the Tensorflow dataset requires all images to be of the same size, they will all be resized to the specified size. Note that if you were to specify a resize transformer in the preprocessing pipeline, the size specified in the latter will be the final resize size</p> <p> TYPE: <code>Tuple[int, int]</code> DEFAULT: <code>(227, 227)</code> </p> Source code in <code>clayrs/content_analyzer/field_content_production_techniques/visual_techniques/high_level_techniques.py</code> <pre><code>def __init__(self, model_name: str, feature_layer: int = -1, flatten: bool = True, device: str = 'cpu',\n             apply_on_output: Callable[[torch.Tensor], torch.Tensor] = None,\n             imgs_dirs: str = \"imgs_dirs\", max_timeout: int = 2, max_retries: int = 5,\n             max_workers: int = 0, batch_size: int = 64, resize_size: Tuple[int, int] = (227, 227)):\n\n    super().__init__(imgs_dirs, max_timeout, max_retries, max_workers, batch_size, resize_size)\n    original_model = timm.create_model(model_name, pretrained=True)\n\n    feature_layer = list(original_model._modules.keys())[feature_layer]\n\n    layers = {}\n    for layer_name, layer in original_model._modules.items():\n        layers[layer_name] = layer\n        if layer_name == feature_layer:\n            break\n\n    self.model = torch.nn.Sequential(OrderedDict(layers)).eval()\n\n    def return_self(x: torch.Tensor) -&gt; torch.Tensor:\n        return x\n\n    self.apply_on_output = return_self if apply_on_output is None else apply_on_output\n\n    self.model.to(device)\n    self.device = device\n    self.flatten = flatten\n    self.model_name = model_name\n\n    self._repr_string = autorepr(self, inspect.currentframe())\n</code></pre>"},{"location":"content_analyzer/content_techniques/visual_techniques/low_level_visual/","title":"Low level techniques","text":""},{"location":"content_analyzer/content_techniques/visual_techniques/low_level_visual/#clayrs.content_analyzer.field_content_production_techniques.visual_techniques.low_level_techniques.ColorQuantization","title":"<code>ColorQuantization(n_colors=3, init='k-means++', n_init=10, max_iter=300, tol=0.0001, random_state=None, copy_x=True, algorithm='auto', flatten=False, imgs_dirs='imgs_dirs', max_timeout=2, max_retries=5, max_workers=0, batch_size=64, resize_size=(227, 227))</code>","text":"<p>         Bases: <code>LowLevelVisual</code></p> <p>Low level technique which returns the colors obtained from applying a clustering technique (in this case KMeans)</p> <p>Arguments for SkLearn KMeans</p> PARAMETER DESCRIPTION <code>imgs_dirs</code> <p>directory where the images are stored (or will be stored in the case of fields containing links)</p> <p> TYPE: <code>str</code> DEFAULT: <code>'imgs_dirs'</code> </p> <code>max_timeout</code> <p>maximum time to wait before considering a request failed (image from link)</p> <p> TYPE: <code>int</code> DEFAULT: <code>2</code> </p> <code>max_retries</code> <p>maximum number of retries to retrieve an image from a link</p> <p> TYPE: <code>int</code> DEFAULT: <code>5</code> </p> <code>max_workers</code> <p>maximum number of workers for parallelism</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>batch_size</code> <p>batch size for the images dataloader</p> <p> TYPE: <code>int</code> DEFAULT: <code>64</code> </p> <code>resize_size</code> <p>since the Tensorflow dataset requires all images to be of the same size, they will all be resized to the specified size. Note that if you were to specify a resize transformer in the preprocessing pipeline, the size specified in the latter will be the final resize size</p> <p> TYPE: <code>Tuple[int, int]</code> DEFAULT: <code>(227, 227)</code> </p> Source code in <code>clayrs/content_analyzer/field_content_production_techniques/visual_techniques/low_level_techniques.py</code> <pre><code>def __init__(self, n_colors: Any = 3, init: Any = \"k-means++\", n_init: Any = 10, max_iter: Any = 300,\n             tol: Any = 1e-4, random_state: Any = None, copy_x: Any = True, algorithm: Any = \"auto\",\n             flatten: bool = False, imgs_dirs: str = \"imgs_dirs\", max_timeout: int = 2, max_retries: int = 5,\n             max_workers: int = 0, batch_size: int = 64, resize_size: Tuple[int, int] = (227, 227)):\n\n    super().__init__(imgs_dirs, max_timeout, max_retries, max_workers, batch_size, resize_size)\n    self.k_means = KMeans(n_clusters=n_colors, init=init, n_init=n_init, max_iter=max_iter, tol=tol,\n                          random_state=random_state, copy_x=copy_x, algorithm=algorithm)\n    self.flatten = flatten\n    self._repr_string = autorepr(self, inspect.currentframe())\n</code></pre>"},{"location":"content_analyzer/content_techniques/visual_techniques/low_level_visual/#clayrs.content_analyzer.field_content_production_techniques.visual_techniques.low_level_techniques.ColorsHist","title":"<code>ColorsHist(imgs_dirs='imgs_dirs', max_timeout=2, max_retries=5, max_workers=0, batch_size=64, resize_size=(227, 227))</code>","text":"<p>         Bases: <code>LowLevelVisual</code></p> <p>Low level technique which generates a color histogram for each channel of each RGB image</p> <p>The technique retrieves all the values for each one of the three RGB channels in the image, flattens them and returns an EmbeddingField representation containing a numpy two-dimensional array with three rows (one for each channel)</p> PARAMETER DESCRIPTION <code>imgs_dirs</code> <p>directory where the images are stored (or will be stored in the case of fields containing links)</p> <p> TYPE: <code>str</code> DEFAULT: <code>'imgs_dirs'</code> </p> <code>max_timeout</code> <p>maximum time to wait before considering a request failed (image from link)</p> <p> TYPE: <code>int</code> DEFAULT: <code>2</code> </p> <code>max_retries</code> <p>maximum number of retries to retrieve an image from a link</p> <p> TYPE: <code>int</code> DEFAULT: <code>5</code> </p> <code>max_workers</code> <p>maximum number of workers for parallelism</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>batch_size</code> <p>batch size for the images dataloader</p> <p> TYPE: <code>int</code> DEFAULT: <code>64</code> </p> <code>resize_size</code> <p>since the Tensorflow dataset requires all images to be of the same size, they will all be resized to the specified size. Note that if you were to specify a resize transformer in the preprocessing pipeline, the size specified in the latter will be the final resize size</p> <p> TYPE: <code>Tuple[int, int]</code> DEFAULT: <code>(227, 227)</code> </p> Source code in <code>clayrs/content_analyzer/field_content_production_techniques/visual_techniques/low_level_techniques.py</code> <pre><code>def __init__(self, imgs_dirs: str = \"imgs_dirs\", max_timeout: int = 2, max_retries: int = 5,\n             max_workers: int = 0, batch_size: int = 64, resize_size: Tuple[int, int] = (227, 227)):\n\n    super().__init__(imgs_dirs, max_timeout, max_retries, max_workers, batch_size, resize_size)\n    self._repr_string = autorepr(self, inspect.currentframe())\n</code></pre>"},{"location":"content_analyzer/content_techniques/visual_techniques/low_level_visual/#clayrs.content_analyzer.field_content_production_techniques.visual_techniques.low_level_techniques.CustomFilterConvolution","title":"<code>CustomFilterConvolution(weights, mode='reflect', cval=0.0, origin=0, flatten=False, imgs_dirs='imgs_dirs', max_timeout=2, max_retries=5, max_workers=0, batch_size=64, resize_size=(227, 227))</code>","text":"<p>         Bases: <code>LowLevelVisual</code></p> <p>Low level technique which implements a custom filter for convolution over an image, using the convolve method from the scipy library</p> <p>Parameters are the same ones you would pass to the convolve function in scipy together with some framework specific parameters</p> <p>Arguments for Scipy convolve</p> PARAMETER DESCRIPTION <code>flatten</code> <p>whether the output of the technique should be flattened or not</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>imgs_dirs</code> <p>directory where the images are stored (or will be stored in the case of fields containing links)</p> <p> TYPE: <code>str</code> DEFAULT: <code>'imgs_dirs'</code> </p> <code>max_timeout</code> <p>maximum time to wait before considering a request failed (image from link)</p> <p> TYPE: <code>int</code> DEFAULT: <code>2</code> </p> <code>max_retries</code> <p>maximum number of retries to retrieve an image from a link</p> <p> TYPE: <code>int</code> DEFAULT: <code>5</code> </p> <code>max_workers</code> <p>maximum number of workers for parallelism</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>batch_size</code> <p>batch size for the images dataloader</p> <p> TYPE: <code>int</code> DEFAULT: <code>64</code> </p> <code>resize_size</code> <p>since the Tensorflow dataset requires all images to be of the same size, they will all be resized to the specified size. Note that if you were to specify a resize transformer in the preprocessing pipeline, the size specified in the latter will be the final resize size</p> <p> TYPE: <code>Tuple[int, int]</code> DEFAULT: <code>(227, 227)</code> </p> Source code in <code>clayrs/content_analyzer/field_content_production_techniques/visual_techniques/low_level_techniques.py</code> <pre><code>def __init__(self, weights, mode='reflect', cval=0.0, origin=0, flatten: bool = False,\n             imgs_dirs: str = \"imgs_dirs\", max_timeout: int = 2, max_retries: int = 5,\n             max_workers: int = 0, batch_size: int = 64, resize_size: Tuple[int, int] = (227, 227)):\n\n    super().__init__(imgs_dirs, max_timeout, max_retries, max_workers, batch_size, resize_size)\n    self.convolve = lambda x: convolve(x, weights=weights, mode=mode, cval=cval, origin=origin)\n    self.flatten = flatten\n    self._repr_string = autorepr(self, inspect.currentframe())\n</code></pre>"},{"location":"content_analyzer/content_techniques/visual_techniques/low_level_visual/#clayrs.content_analyzer.field_content_production_techniques.visual_techniques.low_level_techniques.SkImageCannyEdgeDetector","title":"<code>SkImageCannyEdgeDetector(sigma=1.0, low_threshold=None, high_threshold=None, mask=None, use_quantiles=False, mode='constant', cval=0.0, flatten=False, imgs_dirs='imgs_dirs', max_timeout=2, max_retries=5, max_workers=0, batch_size=64, resize_size=(227, 227))</code>","text":"<p>         Bases: <code>LowLevelVisual</code></p> <p>Low level technique which implements the Canny Edge Detector using the SkImage library</p> <p>Parameters are the same ones you would pass to the canny function in SkImage together with some framework specific parameters</p> <p>Arguments for SkImage Canny</p> PARAMETER DESCRIPTION <code>flatten</code> <p>whether the output of the technique should be flattened or not</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>imgs_dirs</code> <p>directory where the images are stored (or will be stored in the case of fields containing links)</p> <p> TYPE: <code>str</code> DEFAULT: <code>'imgs_dirs'</code> </p> <code>max_timeout</code> <p>maximum time to wait before considering a request failed (image from link)</p> <p> TYPE: <code>int</code> DEFAULT: <code>2</code> </p> <code>max_retries</code> <p>maximum number of retries to retrieve an image from a link</p> <p> TYPE: <code>int</code> DEFAULT: <code>5</code> </p> <code>max_workers</code> <p>maximum number of workers for parallelism</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>batch_size</code> <p>batch size for the images dataloader</p> <p> TYPE: <code>int</code> DEFAULT: <code>64</code> </p> <code>resize_size</code> <p>since the Tensorflow dataset requires all images to be of the same size, they will all be resized to the specified size. Note that if you were to specify a resize transformer in the preprocessing pipeline, the size specified in the latter will be the final resize size</p> <p> TYPE: <code>Tuple[int, int]</code> DEFAULT: <code>(227, 227)</code> </p> Source code in <code>clayrs/content_analyzer/field_content_production_techniques/visual_techniques/low_level_techniques.py</code> <pre><code>def __init__(self, sigma=1.0, low_threshold=None, high_threshold=None, mask=None, use_quantiles=False,\n             mode='constant', cval=0.0, flatten: bool = False,\n             imgs_dirs: str = \"imgs_dirs\", max_timeout: int = 2, max_retries: int = 5,\n             max_workers: int = 0, batch_size: int = 64, resize_size: Tuple[int, int] = (227, 227)):\n\n    super().__init__(imgs_dirs, max_timeout, max_retries, max_workers, batch_size, resize_size)\n    self.canny = lambda x: canny(image=x, sigma=sigma, low_threshold=low_threshold,\n                                 high_threshold=high_threshold, mask=mask,\n                                 use_quantiles=use_quantiles, mode=mode, cval=cval)\n    self.flatten = flatten\n    self._repr_string = autorepr(self, inspect.currentframe())\n</code></pre>"},{"location":"content_analyzer/content_techniques/visual_techniques/low_level_visual/#clayrs.content_analyzer.field_content_production_techniques.visual_techniques.low_level_techniques.SkImageHogDescriptor","title":"<code>SkImageHogDescriptor(orientations=9, pixels_per_cell=(8, 8), cells_per_block=(3, 3), block_norm='L2-Hys', transform_sqrt=False, flatten=False, imgs_dirs='imgs_dirs', max_timeout=2, max_retries=5, max_workers=0, batch_size=64, resize_size=(227, 227))</code>","text":"<p>         Bases: <code>LowLevelVisual</code></p> <p>Low level technique which implements the Hog Descriptor using the SkImage library</p> <p>Parameters are the same ones you would pass to the hog function in SkImage together with some framework specific parameters</p> <p>Arguments for SkImage Hog</p> PARAMETER DESCRIPTION <code>flatten</code> <p>whether the output of the technique should be flattened or not</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>imgs_dirs</code> <p>directory where the images are stored (or will be stored in the case of fields containing links)</p> <p> TYPE: <code>str</code> DEFAULT: <code>'imgs_dirs'</code> </p> <code>max_timeout</code> <p>maximum time to wait before considering a request failed (image from link)</p> <p> TYPE: <code>int</code> DEFAULT: <code>2</code> </p> <code>max_retries</code> <p>maximum number of retries to retrieve an image from a link</p> <p> TYPE: <code>int</code> DEFAULT: <code>5</code> </p> <code>max_workers</code> <p>maximum number of workers for parallelism</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>batch_size</code> <p>batch size for the images dataloader</p> <p> TYPE: <code>int</code> DEFAULT: <code>64</code> </p> <code>resize_size</code> <p>since the Tensorflow dataset requires all images to be of the same size, they will all be resized to the specified size. Note that if you were to specify a resize transformer in the preprocessing pipeline, the size specified in the latter will be the final resize size</p> <p> TYPE: <code>Tuple[int, int]</code> DEFAULT: <code>(227, 227)</code> </p> Source code in <code>clayrs/content_analyzer/field_content_production_techniques/visual_techniques/low_level_techniques.py</code> <pre><code>def __init__(self, orientations=9, pixels_per_cell=(8, 8), cells_per_block=(3, 3),\n             block_norm='L2-Hys', transform_sqrt=False, flatten: bool = False,\n             imgs_dirs: str = \"imgs_dirs\", max_timeout: int = 2, max_retries: int = 5,\n             max_workers: int = 0, batch_size: int = 64, resize_size: Tuple[int, int] = (227, 227)):\n\n    super().__init__(imgs_dirs, max_timeout, max_retries, max_workers, batch_size, resize_size)\n    self.hog = lambda x, channel_axis: hog(x, orientations=orientations, pixels_per_cell=pixels_per_cell,\n                                           cells_per_block=cells_per_block, block_norm=block_norm,\n                                           transform_sqrt=transform_sqrt, feature_vector=flatten,\n                                           channel_axis=channel_axis)\n\n    self._repr_string = autorepr(self, inspect.currentframe())\n</code></pre>"},{"location":"content_analyzer/content_techniques/visual_techniques/low_level_visual/#clayrs.content_analyzer.field_content_production_techniques.visual_techniques.low_level_techniques.SkImageLBP","title":"<code>SkImageLBP(p, r, method='default', flatten=False, as_image=False, imgs_dirs='imgs_dirs', max_timeout=2, max_retries=5, max_workers=0, batch_size=64, resize_size=(227, 227))</code>","text":"<p>         Bases: <code>LowLevelVisual</code></p> <p>Low level technique which allows for LBP feature detection from SkImage</p> <p>Parameters are the same ones you would pass to the local_binary_pattern function in SkImage together with some framework specific parameters</p> <p>Furthermore, in this case, there is also an additional parameter, that is 'as_image'</p> <p>Arguments for SkImage lbp</p> PARAMETER DESCRIPTION <code>as_image</code> <p>if True, the lbp image obtained from SkImage will be returned, otherwise the number of occurences of each binary pattern will be returned (as if it was a feature vector)</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>imgs_dirs</code> <p>directory where the images are stored (or will be stored in the case of fields containing links)</p> <p> TYPE: <code>str</code> DEFAULT: <code>'imgs_dirs'</code> </p> <code>max_timeout</code> <p>maximum time to wait before considering a request failed (image from link)</p> <p> TYPE: <code>int</code> DEFAULT: <code>2</code> </p> <code>max_retries</code> <p>maximum number of retries to retrieve an image from a link</p> <p> TYPE: <code>int</code> DEFAULT: <code>5</code> </p> <code>max_workers</code> <p>maximum number of workers for parallelism</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>batch_size</code> <p>batch size for the images dataloader</p> <p> TYPE: <code>int</code> DEFAULT: <code>64</code> </p> <code>resize_size</code> <p>since the Tensorflow dataset requires all images to be of the same size, they will all be resized to the specified size. Note that if you were to specify a resize transformer in the preprocessing pipeline, the size specified in the latter will be the final resize size</p> <p> TYPE: <code>Tuple[int, int]</code> DEFAULT: <code>(227, 227)</code> </p> Source code in <code>clayrs/content_analyzer/field_content_production_techniques/visual_techniques/low_level_techniques.py</code> <pre><code>def __init__(self, p: int, r: float, method='default', flatten: bool = False, as_image: bool = False,\n             imgs_dirs: str = \"imgs_dirs\", max_timeout: int = 2, max_retries: int = 5,\n             max_workers: int = 0, batch_size: int = 64, resize_size: Tuple[int, int] = (227, 227)):\n\n    super().__init__(imgs_dirs, max_timeout, max_retries, max_workers, batch_size, resize_size)\n    self.lbp = lambda x: local_binary_pattern(x, P=p, R=r, method=method)\n    self.flatten = flatten\n    self.as_image = as_image\n    self._repr_string = autorepr(self, inspect.currentframe())\n</code></pre>"},{"location":"content_analyzer/content_techniques/visual_techniques/low_level_visual/#clayrs.content_analyzer.field_content_production_techniques.visual_techniques.low_level_techniques.SkImageSIFT","title":"<code>SkImageSIFT(upsampling=2, n_octaves=8, n_scales=3, sigma_min=1.6, sigma_in=0.5, c_dog=0.013333333333333334, c_edge=10, n_bins=36, lambda_ori=1.5, c_max=0.8, lambda_descr=6, n_hist=4, n_ori=8, flatten=False, imgs_dirs='imgs_dirs', max_timeout=2, max_retries=5, max_workers=0, batch_size=64, resize_size=(227, 227))</code>","text":"<p>         Bases: <code>LowLevelVisual</code></p> <p>Low level technique which allows for SIFT feature detection from SkImage</p> <p>Parameters are the same ones you would pass to the SIFT object in SkImage together with some framework specific parameters</p> <p>Arguments for SkImage SIFT</p> PARAMETER DESCRIPTION <code>imgs_dirs</code> <p>directory where the images are stored (or will be stored in the case of fields containing links)</p> <p> TYPE: <code>str</code> DEFAULT: <code>'imgs_dirs'</code> </p> <code>max_timeout</code> <p>maximum time to wait before considering a request failed (image from link)</p> <p> TYPE: <code>int</code> DEFAULT: <code>2</code> </p> <code>max_retries</code> <p>maximum number of retries to retrieve an image from a link</p> <p> TYPE: <code>int</code> DEFAULT: <code>5</code> </p> <code>max_workers</code> <p>maximum number of workers for parallelism</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>batch_size</code> <p>batch size for the images dataloader</p> <p> TYPE: <code>int</code> DEFAULT: <code>64</code> </p> <code>resize_size</code> <p>since the Tensorflow dataset requires all images to be of the same size, they will all be resized to the specified size. Note that if you were to specify a resize transformer in the preprocessing pipeline, the size specified in the latter will be the final resize size</p> <p> TYPE: <code>Tuple[int, int]</code> DEFAULT: <code>(227, 227)</code> </p> Source code in <code>clayrs/content_analyzer/field_content_production_techniques/visual_techniques/low_level_techniques.py</code> <pre><code>def __init__(self, upsampling=2, n_octaves=8, n_scales=3, sigma_min=1.6, sigma_in=0.5, c_dog=0.013333333333333334,\n             c_edge=10, n_bins=36, lambda_ori=1.5, c_max=0.8, lambda_descr=6, n_hist=4, n_ori=8,\n             flatten: bool = False, imgs_dirs: str = \"imgs_dirs\", max_timeout: int = 2, max_retries: int = 5,\n             max_workers: int = 0, batch_size: int = 64, resize_size: Tuple[int, int] = (227, 227)):\n\n    super().__init__(imgs_dirs, max_timeout, max_retries, max_workers, batch_size, resize_size)\n    self.sift = SIFT(upsampling=upsampling, n_octaves=n_octaves, n_scales=n_scales, sigma_min=sigma_min,\n                     sigma_in=sigma_in, c_dog=c_dog, c_edge=c_edge, n_bins=n_bins, lambda_ori=lambda_ori,\n                     c_max=c_max, lambda_descr=lambda_descr, n_hist=n_hist, n_ori=n_ori)\n    self.flatten = flatten\n    self._repr_string = autorepr(self, inspect.currentframe())\n</code></pre>"},{"location":"content_analyzer/exogenous_techniques/babelfy/","title":"Properties from DBPedia ontology","text":""},{"location":"content_analyzer/exogenous_techniques/babelfy/#clayrs.content_analyzer.BabelPyEntityLinking","title":"<code>BabelPyEntityLinking(field_to_link, api_key=None, lang='EN')</code>","text":"<p>         Bases: <code>EntityLinking</code></p> <p>Exogenous technique which expands each content by using as external source the the BabelFy library.</p> <p>Each content will be expanded with the following babelfy properties (if available):</p> <ul> <li>'babelSynsetID',</li> <li>'DBPediaURL',</li> <li>'BabelNetURL',</li> <li>'score',</li> <li>'coherenceScore',</li> <li>'globalScore',</li> <li>'source'</li> </ul> PARAMETER DESCRIPTION <code>field_to_link</code> <p>Field of the raw source which will be used to search for the content properties in BabelFy</p> <p> TYPE: <code>str</code> </p> <code>api_key</code> <p>String obtained by registering to babelfy website. If None only few queries can be executed</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>lang</code> <p>Language of the properties to retrieve</p> <p> TYPE: <code>str</code> DEFAULT: <code>'EN'</code> </p> Source code in <code>clayrs/content_analyzer/exogenous_properties_retrieval.py</code> <pre><code>def __init__(self, field_to_link: str, api_key: str = None, lang: str = \"EN\"):\n    super().__init__(\"all_retrieved\")  # fixed mode since it doesn't make sense for babelfy\n    self.__field_to_link = field_to_link\n    self.__api_key = api_key\n    self.__lang = lang\n    self.__babel_client = BabelfyClient(self.__api_key, {\"lang\": lang})\n</code></pre>"},{"location":"content_analyzer/exogenous_techniques/dbpedia/","title":"Properties from DBPedia ontology","text":""},{"location":"content_analyzer/exogenous_techniques/dbpedia/#clayrs.content_analyzer.DBPediaMappingTechnique","title":"<code>DBPediaMappingTechnique(entity_type, label_field, lang='EN', mode='only_retrieved_evaluated', return_prop_as_uri=False, max_timeout=5)</code>","text":"<p>         Bases: <code>ExogenousPropertiesRetrieval</code></p> <p>Exogenous technique which expands each content by using as external source the DBPedia ontology</p> <p>It needs the entity of the contents for which a mapping is required (e.g. entity_type=<code>dbo:Film</code>) and the field of the raw source that will be used for the actual mapping:</p> <p>Different modalities are available:</p> <ul> <li> <p>If <code>mode='only_retrieved_evaluated'</code>, all properties from DBPedia will be retrieved but discarding the ones with a blank value (i.e. '')</p> </li> <li> <p>If <code>mode='all'</code>, all properties in DBPedia + all properties in local raw source will be retrieved. Local properties will be overwritten by dbpedia values if there's a conflict (same property in dbpedia and in local dataset)</p> </li> <li> <p>If <code>mode='all_retrieved'</code>, all properties in DBPedia only will be retrieved</p> </li> <li> <p>If <code>mode='original_retrieved'</code>, all local properties with their DBPedia value will be retrieved</p> </li> </ul> PARAMETER DESCRIPTION <code>entity_type</code> <p>Domain of the contents you want to process (e.g. 'dbo:Film')</p> <p> TYPE: <code>str</code> </p> <code>label_field</code> <p>Field of the raw source that will be used to map each content, DBPedia node with property rdfs:label equal to specified field value will be retrieved</p> <p> TYPE: <code>str</code> </p> <code>lang</code> <p>Language of the <code>rdfs:label</code> that should match with <code>label_field</code> in the raw source</p> <p> TYPE: <code>str</code> DEFAULT: <code>'EN'</code> </p> <code>mode</code> <p>Parameter which specifies which properties should be retrieved.</p> <p>Possible values are ['only_retrieved_evaluated', 'all', 'all_retrieved', 'original_retrieved']:</p> <pre><code>1. 'only retrieved evaluated' will retrieve properties which have a\nvalue, discarding ones with a blank value (i.e. '')\n2. 'all' will retrieve all properties from DBPedia + local source,\nregardless if they have a value or not\n3. 'all_retrieved' will retrieve all properties from DBPedia only\n4. 'original_retrieved' will retrieve all local properties with\ntheir DBPedia value\n</code></pre> <p> TYPE: <code>str</code> DEFAULT: <code>'only_retrieved_evaluated'</code> </p> <code>return_prop_as_uri</code> <p>If set to True, properties will be returned in their full uri form rather than in their rdfs:label form (e.g. \"http://dbpedia.org/ontology/director\" rather than \"film director\")</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>max_timeout</code> <p>Sometimes when mapping content to dbpedia, a batch of query may take longer than the max time allowed by the server due to internet issues: the framework will re-try the exact query <code>max_timeout</code> times before raising a <code>TimeoutError</code></p> <p> TYPE: <code>int</code> DEFAULT: <code>5</code> </p> Source code in <code>clayrs/content_analyzer/exogenous_properties_retrieval.py</code> <pre><code>def __init__(self, entity_type: str, label_field: str, lang: str = 'EN',\n             mode: str = 'only_retrieved_evaluated', return_prop_as_uri: bool = False,\n             max_timeout: int = 5):\n    super().__init__(mode)\n\n    self._entity_type = entity_type\n    self._label_field = label_field\n    self._prop_as_uri = return_prop_as_uri\n    self._lang = lang\n    self._max_timeout = max_timeout\n\n    self._sparql = SPARQLWrapper(\"https://dbpedia.org/sparql\")\n    self._sparql.setReturnFormat(JSON)\n\n    self._class_properties = self._get_properties_class()\n</code></pre>"},{"location":"content_analyzer/exogenous_techniques/properties_from_dataset/","title":"Properties from local dataset","text":""},{"location":"content_analyzer/exogenous_techniques/properties_from_dataset/#clayrs.content_analyzer.PropertiesFromDataset","title":"<code>PropertiesFromDataset(mode='only_retrieved_evaluated', field_name_list=None)</code>","text":"<p>         Bases: <code>ExogenousPropertiesRetrieval</code></p> <p>Exogenous technique which expands each content by using as external source the raw source itself</p> <p>Different modalities are available:</p> <ul> <li>If <code>mode='only_retrieved_evaluated'</code> all fields for the content will be retrieved from raw source but discarding the ones with a blank value (i.e. '')</li> </ul> JSON raw source<pre><code>[{'Title': 'Jumanji', 'Year': 1995},\n{'Title': 'Toy Story', 'Year': ''}]\n</code></pre> <pre><code>json_file = JSONFile(json_path)\nPropertiesFromDataset(mode='only_retrieved_evaluated').get_properties(json_file)\n# output is a list of PropertiesDict object with the following values:\n# [{'Title': 'Jumanji', 'Year': 1995},\n#  {'Title': 'Toy Story'}]\n</code></pre> <ul> <li>If <code>mode='all'</code> all fields for the content will be retrieved from raw source including the ones with a blank value</li> </ul> JSON raw source<pre><code>[{'Title': 'Jumanji', 'Year': 1995},\n{'Title': 'Toy Story', 'Year': ''}]\n</code></pre> <pre><code>json_file = JSONFile(json_path)\nPropertiesFromDataset(mode='only_retrieved_evaluated').get_properties(json_file)\n# output is a list of PropertiesDict object with the following values:\n# [{'Title': 'Jumanji', 'Year': 1995},\n#  {'Title': 'Toy Story', 'Year': ''}]\n</code></pre> <p>You could also choose exactly which fields to use to expand each content with the <code>field_name_list</code> parameter</p> JSON raw source<pre><code>[{'Title': 'Jumanji', 'Year': 1995},\n{'Title': 'Toy Story', 'Year': ''}]\n</code></pre> <pre><code>json_file = JSONFile(json_path)\nPropertiesFromDataset(mode='only_retrieved_evaluated',\n                      field_name_list=['Title']).get_properties(json_file)\n# output is a list of PropertiesDict object with the following values:\n# [{'Title': 'Jumanji'},\n#  {'Title': 'Toy Story'}]\n</code></pre> PARAMETER DESCRIPTION <code>mode</code> <p>Parameter which specifies which properties should be retrieved.</p> <p>Possible values are ['only_retrieved_evaluated', 'all']:</p> <pre><code>1. 'only retrieved evaluated' will retrieve properties which have a\nvalue, discarding ones with a blank value (i.e. '')\n2. 'all' will retrieve all properties, regardless if they have a value\nor not\n</code></pre> <p> TYPE: <code>str</code> DEFAULT: <code>'only_retrieved_evaluated'</code> </p> <code>field_name_list</code> <p>List of fields from the raw source that will be retrieved. Useful if you want to expand each content with only a subset of available properties from the local dataset</p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>None</code> </p> Source code in <code>clayrs/content_analyzer/exogenous_properties_retrieval.py</code> <pre><code>def __init__(self, mode: str = 'only_retrieved_evaluated', field_name_list: List[str] = None):\n    super().__init__(mode)\n    self.__field_name_list: List[str] = field_name_list\n</code></pre>"},{"location":"content_analyzer/information_preprocessors/postprocessors/postprocessor/","title":"Postprocessor","text":""},{"location":"content_analyzer/information_preprocessors/postprocessors/postprocessor/#clayrs.content_analyzer.information_processor.postprocessors.postprocessor.CountVisualBagOfWords","title":"<code>CountVisualBagOfWords(n_clusters=8, init='k-means++', n_init=10, max_iter=300, tol=0.0001, random_state=None, copy_x=True, algorithm='auto', with_mean=True, with_std=True)</code>","text":"<p>         Bases: <code>VisualBagOfWords</code></p> <p>Class which implements the count weighting schema, which means that the final representation will contain counts of each visual word appearing from the codebook</p> Example <p>codebook = [[0.6, 1.7, 0.3],             [0.2, 0.7, 1.8]]</p> <p>repr = [[0.6, 1.7, 0.3],         [0.6, 1.7, 0.3]]</p> <p>output of weighting schema = [2, 0]</p> <p>NOTE: a preliminary step is necessary, that is feature extraction. You should do that using one of the provided visual techniques and setting this as postprocessor for the output of that technique as follows:</p> <pre><code>import clayrs.content_analyzer as ca\nca.FieldConfig(ca.SkImageCannyEdgeDetector(), postprocessing=[ca.CountVisualBagOfWords()])\n</code></pre> <p>ADDITIONAL NOTE: the technique requires 2D arrays of features for each image, such as edges in the case of the Canny Edge detector. In case any other dimensionality is provided, a ValueError will be raised.</p> <p>Arguments for SkLearn KMeans</p> <p>Arguments for SkLearn StandardScaler</p> <p>NOTE: for this technique it is mandatory for the parameter \"with_std\" to be set to True</p> Source code in <code>clayrs/content_analyzer/information_processor/postprocessors/postprocessor.py</code> <pre><code>def __init__(self, n_clusters: Any = 8, init: Any = \"k-means++\", n_init: Any = 10, max_iter: Any = 300,\n             tol: Any = 1e-4, random_state: Any = None, copy_x: Any = True, algorithm: Any = \"auto\",\n             with_mean: bool = True, with_std: bool = True):\n    super().__init__(n_clusters=n_clusters, init=init, n_init=n_init, max_iter=max_iter, tol=tol,\n                     random_state=random_state, copy_x=copy_x, algorithm=algorithm, with_mean=with_mean,\n                     with_std=with_std)\n    self._repr_string = autorepr(self, inspect.currentframe())\n</code></pre>"},{"location":"content_analyzer/information_preprocessors/postprocessors/postprocessor/#clayrs.content_analyzer.information_processor.postprocessors.postprocessor.CountVisualBagOfWords.apply_weights","title":"<code>apply_weights(sparse_matrix)</code>","text":"<p>Apply a count wight schema to the representations obtained from the vector quantization step</p> PARAMETER DESCRIPTION <code>sparse_matrix</code> <p>scipy sparse csr matrix containing the count of occurrences of each visual word</p> <p> TYPE: <code>scipy.sparse.csr_matrix</code> </p> Source code in <code>clayrs/content_analyzer/information_processor/postprocessors/postprocessor.py</code> <pre><code>def apply_weights(self, sparse_matrix: scipy.sparse.csr_matrix) -&gt; scipy.sparse.csr_matrix:\n\"\"\"\n    Apply a count wight schema to the representations obtained from the vector quantization step\n\n    Args:\n        sparse_matrix: scipy sparse csr matrix containing the count of occurrences of each visual word\n    \"\"\"\n    return sparse_matrix\n</code></pre>"},{"location":"content_analyzer/information_preprocessors/postprocessors/postprocessor/#clayrs.content_analyzer.information_processor.postprocessors.postprocessor.TfIdfVisualBagOfWords","title":"<code>TfIdfVisualBagOfWords(n_clusters=8, init='k-means++', n_init=10, max_iter=300, tol=0.0001, random_state=None, copy_x=True, algorithm='auto', with_mean=True, with_std=True, norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False)</code>","text":"<p>         Bases: <code>VisualBagOfWords</code></p> <p>Class which implements the tf-idf weighting schema, which means that the final representation will contain tf-idf scores of each visual word appearing from the codebook</p> Example <p>codebook = [[0.6, 1.7, 0.3],             [0.2, 0.7, 1.8]]</p> <p>repr1 = [[0.6, 1.7, 0.3],          [0.6, 1.7, 0.3]]</p> <p>repr2 = [[0.6, 1.7, 0.3],          [0.2, 0.7, 1.8]]</p> <p>output of weighting schema = [[2, 0], [1, 1.69]]</p> <p>NOTE: a preliminary step is necessary, that is feature extraction. You should do that using one of the provided visual techniques and setting this as postprocessor for the output of that technique as follows:</p> <pre><code>import clayrs.content_analyzer as ca\nca.FieldConfig(ca.SkImageCannyEdgeDetector(), postprocessing=[ca.TfIdfVisualBagOfWords()])\n</code></pre> <p>ADDITIONAL NOTE: the technique requires 2D arrays of features for each image, such as edges in the case of the Canny Edge detector. In case any other dimensionality is provided, a ValueError will be raised.</p> <p>Arguments for SkLearn KMeans</p> <p>Arguments for SkLearn StandardScaler</p> <p>Arguments for SkLearn TfIdf Transformer</p> <p>NOTE: for this technique it is mandatory for the parameter \"with_std\" to be set to True</p> Source code in <code>clayrs/content_analyzer/information_processor/postprocessors/postprocessor.py</code> <pre><code>def __init__(self, n_clusters: Any = 8, init: Any = \"k-means++\", n_init: Any = 10, max_iter: Any = 300,\n             tol: Any = 1e-4, random_state: Any = None, copy_x: Any = True, algorithm: Any = \"auto\",\n             with_mean: bool = True, with_std: bool = True,\n             norm: Optional[str] = \"l2\", use_idf: bool = True,\n             smooth_idf: bool = True, sublinear_tf: bool = False):\n    super().__init__(n_clusters=n_clusters, init=init, n_init=n_init, max_iter=max_iter, tol=tol,\n                     random_state=random_state, copy_x=copy_x, algorithm=algorithm, with_mean=with_mean,\n                     with_std=with_std)\n\n    self.tf_idf_params = {\"norm\": norm,\n                          \"use_idf\": use_idf,\n                          \"smooth_idf\": smooth_idf,\n                          \"sublinear_tf\": sublinear_tf}\n\n    self._repr_string = autorepr(self, inspect.currentframe())\n</code></pre>"},{"location":"content_analyzer/information_preprocessors/postprocessors/postprocessor/#clayrs.content_analyzer.information_processor.postprocessors.postprocessor.TfIdfVisualBagOfWords.apply_weights","title":"<code>apply_weights(sparse_matrix)</code>","text":"<p>Apply a tf-idf weighting schema to the representations obtained from the vector quantization step</p> PARAMETER DESCRIPTION <code>sparse_matrix</code> <p>scipy sparse csr matrix containing the count of occurrences of each visual word</p> <p> TYPE: <code>scipy.sparse.csr_matrix</code> </p> Source code in <code>clayrs/content_analyzer/information_processor/postprocessors/postprocessor.py</code> <pre><code>def apply_weights(self, sparse_matrix: scipy.sparse.csr_matrix) -&gt; scipy.sparse.csr_matrix:\n\"\"\"\n    Apply a tf-idf weighting schema to the representations obtained from the vector quantization step\n\n    Args:\n        sparse_matrix: scipy sparse csr matrix containing the count of occurrences of each visual word\n    \"\"\"\n    return TfidfTransformer(**self.tf_idf_params).fit_transform(sparse_matrix.todok())\n</code></pre>"},{"location":"content_analyzer/information_preprocessors/postprocessors/postprocessor/#clayrs.content_analyzer.information_processor.postprocessors.postprocessor.ScipyVQ","title":"<code>ScipyVQ(n_clusters=8, init='k-means++', n_init=10, max_iter=300, tol=0.0001, random_state=None, copy_x=True, algorithm='auto', with_mean=True, with_std=True)</code>","text":"<p>         Bases: <code>EmbeddingInputPostProcessor</code></p> <p>Vector quantization using Scipy implementation and SkLearn KMeans. The idea behind this technique is to \"approximate\" feature vectors, using only a finite set of prototype vectors from a codebook. The codebook is computed using the SkLearn KMeans implementation. After that, for each feature in the representation, the closest one from the codebook is found using the Vector Quantization implementation from scipy and the retrieved vector is replaced to the original one in the final representation.</p> <pre><code>import clayrs.content_analyzer as ca\nca.FieldConfig(ca.SkImageCannyEdgeDetector(), postprocessing=[ca.ScipyVQ()])\n</code></pre> <p>Arguments for SkLearn KMeans</p> <p>Arguments for SkLearn StandardScaler</p> <p>NOTE: for this technique it is mandatory for the parameter \"with_std\" to be set to True</p> Source code in <code>clayrs/content_analyzer/information_processor/postprocessors/postprocessor.py</code> <pre><code>def __init__(self, n_clusters: Any = 8, init: Any = \"k-means++\", n_init: Any = 10, max_iter: Any = 300,\n             tol: Any = 1e-4, random_state: Any = None, copy_x: Any = True, algorithm: Any = \"auto\",\n             with_mean: bool = True, with_std: bool = True):\n    self.k_means = KMeans(n_clusters=n_clusters, init=init, n_init=n_init, max_iter=max_iter, tol=tol,\n                          random_state=random_state, copy_x=copy_x, algorithm=algorithm)\n    self.with_mean = with_mean\n    self.with_std = with_std\n    self._repr_string = autorepr(self, inspect.currentframe())\n</code></pre>"},{"location":"content_analyzer/information_preprocessors/postprocessors/postprocessor/#clayrs.content_analyzer.information_processor.postprocessors.postprocessor.SkLearnPCA","title":"<code>SkLearnPCA(n_components=None, copy=True, whiten=False, svd_solver='auto', tol=0.0, iterated_power='auto', random_state=None)</code>","text":"<p>         Bases: <code>DimensionalityReduction</code></p> <p>Dimensionality reduction using the PCA implementation from SkLearn</p> <p>Usage example:</p> <pre><code>import clayrs.content_analyzer as ca\nca.FieldConfig(ca.SkImageCannyEdgeDetector(), postprocessing=[ca.SkLearnPCA()])\n</code></pre> <p>Arguments for SkLearn PCA</p> Source code in <code>clayrs/content_analyzer/information_processor/postprocessors/postprocessor.py</code> <pre><code>def __init__(self, n_components=None, copy=True, whiten=False, svd_solver='auto', tol=0.0,\n             iterated_power='auto', random_state=None):\n    super().__init__()\n    self.pca = PCA(n_components=n_components, copy=copy, whiten=whiten, svd_solver=svd_solver, tol=tol,\n                   iterated_power=iterated_power, random_state=random_state)\n    self._repr_string = autorepr(self, inspect.currentframe())\n</code></pre>"},{"location":"content_analyzer/information_preprocessors/postprocessors/postprocessor/#clayrs.content_analyzer.information_processor.postprocessors.postprocessor.SkLearnGaussianRandomProjections","title":"<code>SkLearnGaussianRandomProjections(n_components='auto', eps=0.1, random_state=None)</code>","text":"<p>         Bases: <code>DimensionalityReduction</code></p> <p>Dimensionality reduction using the Gaussian Random Projections implementation from SkLearn</p> <p>Usage example:</p> <pre><code>import clayrs.content_analyzer as ca\nca.FieldConfig(ca.SkImageCannyEdgeDetector(), postprocessing=[ca.SkLearnGaussianRandomProjections()])\n</code></pre> <p>Arguments for SkLearn Gaussian Random Projection</p> Source code in <code>clayrs/content_analyzer/information_processor/postprocessors/postprocessor.py</code> <pre><code>def __init__(self, n_components='auto', eps=0.1, random_state=None):\n    super().__init__()\n    self.random_proj = GaussianRandomProjection(n_components=n_components, eps=eps, random_state=random_state)\n    self._repr_string = autorepr(self, inspect.currentframe())\n</code></pre>"},{"location":"content_analyzer/information_preprocessors/postprocessors/postprocessor/#clayrs.content_analyzer.information_processor.postprocessors.postprocessor.SkLearnFeatureAgglomeration","title":"<code>SkLearnFeatureAgglomeration(n_clusters=2, affinity='euclidean', memory=None, connectivity=None, compute_full_tree='auto', linkage='ward', pooling_func=np.mean, distance_threshold=None, compute_distances=False)</code>","text":"<p>         Bases: <code>DimensionalityReduction</code></p> <p>Dimensionality reduction using the Feature Agglomeration implementation from SkLearn</p> <p>Usage example:</p> <pre><code>import clayrs.content_analyzer as ca\nca.FieldConfig(ca.SkImageCannyEdgeDetector(), postprocessing=[ca.SkLearnFeatureAgglomeration()])\n</code></pre> <p>Arguments for SkLearn Feature Agglomeration</p> Source code in <code>clayrs/content_analyzer/information_processor/postprocessors/postprocessor.py</code> <pre><code>def __init__(self, n_clusters=2, affinity='euclidean', memory=None, connectivity=None, compute_full_tree='auto',\n             linkage='ward', pooling_func=np.mean, distance_threshold=None, compute_distances=False):\n    super().__init__()\n    self.feature_agg = FeatureAgglomeration(n_clusters=n_clusters, affinity=affinity, memory=memory,\n                                            connectivity=connectivity, compute_full_tree=compute_full_tree,\n                                            linkage=linkage, pooling_func=pooling_func,\n                                            distance_threshold=distance_threshold,\n                                            compute_distances=compute_distances)\n    self._repr_string = autorepr(self, inspect.currentframe())\n</code></pre>"},{"location":"content_analyzer/information_preprocessors/textual_preprocessors/ekphrasis/","title":"Ekphrasis Preprocessor","text":""},{"location":"content_analyzer/information_preprocessors/textual_preprocessors/ekphrasis/#clayrs.content_analyzer.Ekphrasis","title":"<code>Ekphrasis(*, omit=None, normalize=None, unpack_contractions=False, unpack_hashtags=False, annotate=None, corrector=None, tokenizer=social_tokenizer_ekphrasis, segmenter=None, all_caps_tag=None, spell_correction=False, segmentation=False, dicts=None, spell_correct_elong=False)</code>","text":"<p>         Bases: <code>NLP</code></p> <p>Interface to the Ekphrasis library for natural language processing features</p> <p>Examples:</p> <ul> <li>Normalize email and percentage tokens but omit email ones:</li> </ul> <pre><code>&gt;&gt;&gt; ek = Ekphrasis(omit=['email'], normalize=['email', 'percent'])\n&gt;&gt;&gt; ek.process(\"this is an email: alias@mail.com and this is a percent 23%\")\n['this', 'is', 'an', 'email', ':', 'and', 'this', 'is', 'a', 'percent', '&lt;percent&gt;']\n</code></pre> <ul> <li>Unpack contractions on running text:</li> </ul> <pre><code>&gt;&gt;&gt; ek = Ekphrasis(unpack_contractions=True)\n&gt;&gt;&gt; ek.process(\"I can't do this because I won't and I shouldn't\")\n['i', 'can', 'not', 'do', 'this', 'because', 'i', 'will', 'not', 'and', 'i', 'should', 'not']\n</code></pre> <ul> <li>Unpack hashtag using statistics from 'twitter' corpus:</li> </ul> <pre><code>&gt;&gt;&gt; ek = Ekphrasis(unpack_hashtags=True, segmenter='twitter')\n&gt;&gt;&gt; ek.process(\"#next #gamedev #retrogaming #coolphoto no unpack\")\n['next', 'game', 'dev', 'retro', 'gaming', 'cool', 'photo', 'no', 'unpack']\n</code></pre> <ul> <li>Annotate words in CAPS and repeated tokens with single tag for CAPS words:</li> </ul> <pre><code>&gt;&gt;&gt; ek = Ekphrasis(annotate=['allcaps', 'repeated'], all_caps_tag='single')\n&gt;&gt;&gt; ek.process(\"this is good !!! text and a SHOUTED one\")\n['this', 'is', 'good', '!', '&lt;repeated&gt;', 'text', 'and', 'a', 'shouted', '&lt;allcaps&gt;', 'one']\n</code></pre> <ul> <li>Perform segmentation using statistics from 'twitter' corpus:</li> </ul> <pre><code>&gt;&gt;&gt; ek = Ekphrasis(segmentation=True, segmenter='twitter')\n&gt;&gt;&gt; ek.process(\"thewatercooler exponentialbackoff no segmentation\")\n['the', 'watercooler', 'exponential', 'back', 'off', 'no', 'segmentation']\n</code></pre> <ul> <li>Substitute words with custom tokens:</li> </ul> <pre><code>&gt;&gt;&gt; ek = Ekphrasis(dicts=[{':)': '&lt;happy&gt;', ':(': '&lt;sad&gt;'}])\n&gt;&gt;&gt; ek.process(\"Hello :) how are you? :(\")\n['Hello', '&lt;happy&gt;', 'how', 'are', 'you', '?', '&lt;sad&gt;']\n</code></pre> <ul> <li>Perform spell correction on text and on elongated words by using statistics from default 'english' corpus:</li> </ul> <pre><code>&gt;&gt;&gt; Ekphrasis(spell_correction=True, spell_correct_elong=True)\n&gt;&gt;&gt; ek.process(\"This is huuuuge. The korrect way of doing tihngs is not the followingt\")\n[\"this\", 'is', 'huge', '.', 'the', 'correct', \"way\", \"of\", \"doing\", \"things\", \"is\", 'not', 'the',\n'following']\n</code></pre> PARAMETER DESCRIPTION <code>omit</code> <p>Choose what tokens that you want to omit from the text.</p> <p>Possible values: ['email', 'percent', 'money', 'phone', 'user','time', 'url', 'date', 'hashtag']</p> <p>Important Notes:</p> <pre><code>1 - the token in this list must be present in the `normalize`\n    list to have any effect!\n2 - put url at front, if you plan to use it.\n    Messes with the regexes!\n3 - if you use hashtag then unpack_hashtags will\n    automatically be set to False\n</code></pre> <p> TYPE: <code>List</code> DEFAULT: <code>None</code> </p> <code>normalize</code> <p>Choose what tokens that you want to normalize from the text. Possible values: ['email', 'percent', 'money', 'phone', 'user', 'time', 'url', 'date', 'hashtag']</p> <p>For example: myaddress@mysite.com -&gt; <code>&lt;email&gt;</code></p> <p>Important Notes:</p> <pre><code>1 - put url at front, if you plan to use it.\n    Messes with the regexes!\n2 - if you use hashtag then unpack_hashtags will\n    automatically be set to False\n</code></pre> <p> TYPE: <code>List</code> DEFAULT: <code>None</code> </p> <code>unpack_contractions</code> <p>Replace English contractions in running text with their unshortened forms</p> <p>for example: can't -&gt; can not, wouldn't -&gt; would not, and so on...</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>unpack_hashtags</code> <p>split a hashtag to its constituent words.</p> <p>for example: #ilikedogs -&gt; i like dogs</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>annotate</code> <p>add special tags to special tokens.</p> <p>Possible values: ['hashtag', 'allcaps', 'elongated', 'repeated']</p> <p>for example: myaddress@mysite.com -&gt; myaddress@mysite.com  <p> TYPE: <code>List</code> DEFAULT: <code>None</code> </p> <code>corrector</code> <p>define the statistics of what corpus you would like to use [english, twitter]. Be sure to set <code>spell_correction</code> to True if you want to perform spell correction on the running text</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>tokenizer</code> <p>callable function that accepts a string and returns a list of strings. If no tokenizer is provided then the text will be tokenized on whitespace</p> <p> TYPE: <code>Callable</code> DEFAULT: <code>social_tokenizer_ekphrasis</code> </p> <code>segmenter</code> <p>define the statistics of what corpus you would like to use [english, twitter]. Be sure to set <code>segmentation</code> to True if you want to perform segmentation on the running text</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>all_caps_tag</code> <p>how to wrap the capitalized words Note: applicable only when <code>allcaps</code> is included in the <code>annotate</code> list Possible values [single, wrap, every]:</p> <pre><code>- single: add a tag after the last capitalized word\n    for example: \"SHOUTED TEXT\" -&gt; \"shouted text &lt;allcaps&gt;\"\n- wrap: wrap all words with opening and closing tags\n    for example: \"SHOUTED TEXT\" -&gt; \"&lt;allcaps&gt; shouted text &lt;/allcaps&gt;\"\n- every: add a tag after each word\n    for example: \"SHOUTED TEXT\" -&gt; \"shouted &lt;allcaps&gt; text &lt;allcaps&gt;\"\n</code></pre> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>spell_correction</code> <p>If set to True, running text will be spell corrected using statistics of corpus set in <code>corrector</code> parameter</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>segmentation</code> <p>If set to True, running text will be segmented using statistics of corpus set in <code>corrector</code> parameter</p> <p>for example: exponentialbackoff -&gt; exponential back off</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>spell_correct_elong</code> <p>choose if you want to perform spell correction after the normalization of elongated words.</p> <p>significantly affects performance (speed)</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>spell_correction</code> <p>choose if you want to perform spell correction to the text.</p> <p>significantly affects performance (speed)</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>clayrs/content_analyzer/information_processor/ekphrasis_processor.py</code> <pre><code>def __init__(self, *,\n             omit: List = None,\n             normalize: List = None,\n             unpack_contractions: bool = False,\n             unpack_hashtags: bool = False,\n             annotate: List = None,\n             corrector: str = None,\n             tokenizer: Callable = social_tokenizer_ekphrasis,\n             segmenter: str = None,\n             all_caps_tag: str = None,\n             spell_correction: bool = False,\n             segmentation: bool = False,\n             dicts: List[Dict] = None,\n             spell_correct_elong: bool = False):\n\n    # ekphrasis has default values for arguments not passed. So if they are not evaluated in our class,\n    # we simply don't pass them to ekphrasis\n    kwargs_to_pass = {argument: arg_value for argument, arg_value in zip(locals().keys(), locals().values())\n                      if argument != 'self' and arg_value is not None}\n\n    self.text_processor = TextPreProcessor(**kwargs_to_pass)\n\n    self.spell_correct_elong = spell_correct_elong\n\n    self.sc = None\n    if spell_correction is True:\n        if corrector is not None:\n            self.sc = SpellCorrector(corpus=corrector)\n        else:\n            self.sc = SpellCorrector()\n\n    self.segmentation = segmentation\n    self.ws = None\n    if segmentation is True:\n        if segmenter is not None:\n            self.ws = Segmenter(corpus=segmenter)\n        else:\n            self.ws = Segmenter()\n\n    self._repr_string = autorepr(self, inspect.currentframe())\n</code></pre>"},{"location":"content_analyzer/information_preprocessors/textual_preprocessors/ekphrasis/#clayrs.content_analyzer.information_processor.ekphrasis_processor.Ekphrasis.process","title":"<code>process(field_data)</code>","text":"PARAMETER DESCRIPTION <code>field_data</code> <p>Running text to be processed</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>field_data</code> <p>List of str representing running text preprocessed</p> <p> TYPE: <code>List[str]</code> </p> Source code in <code>clayrs/content_analyzer/information_processor/ekphrasis_processor.py</code> <pre><code>def process(self, field_data: str) -&gt; List[str]:\n\"\"\"\n    Args:\n        field_data: Running text to be processed\n    Returns:\n        field_data: List of str representing running text preprocessed\n    \"\"\"\n    field_data = self.text_processor.pre_process_doc(field_data)\n    if self.sc is not None:\n        field_data = self.__spell_check(field_data)\n    if self.ws is not None:\n        field_data = self.__word_segmenter(field_data)\n    return field_data\n</code></pre>"},{"location":"content_analyzer/information_preprocessors/textual_preprocessors/nltk/","title":"NLTK Preprocessor","text":""},{"location":"content_analyzer/information_preprocessors/textual_preprocessors/nltk/#clayrs.content_analyzer.NLTK","title":"<code>NLTK(*, strip_multiple_whitespaces=True, remove_punctuation=False, stopwords_removal=False, url_tagging=False, lemmatization=False, stemming=False, pos_tag=False, lang='english')</code>","text":"<p>         Bases: <code>NLP</code></p> <p>Interface to the NLTK library for natural language processing features.</p> <p>Examples:</p> <ul> <li>Strip multiple whitespaces from running text</li> </ul> <pre><code>&gt;&gt;&gt; nltk_obj = NLTK(strip_multiple_whitespaces=True)\n&gt;&gt;&gt; nltk_obj.process('This   has  a lot  of   spaces')\n['This', 'has', 'a', 'lot', 'of', 'spaces']\n</code></pre> <ul> <li>Remove punctuation from running text</li> </ul> <pre><code>&gt;&gt;&gt; nltk_obj = NLTK(remove_punctuation=True)\n&gt;&gt;&gt; nltk_obj.process(\"Hello there. How are you? I'm fine, thanks.\")\n[\"Hello\", \"there\", \"How\", \"are\", \"you\", \"I\", \"m\", \"fine\", \"thanks\"]\n</code></pre> <ul> <li>Remove stopwords using from running text</li> </ul> <pre><code>&gt;&gt;&gt; nltk_obj = NLTK(stopwords_removal=True)\n&gt;&gt;&gt; nltk_obj.process(\"The striped bats are hanging on their feet for the best\")\n[\"striped\", \"bats\", \"hanging\", \"feet\", \"best\"]\n</code></pre> <ul> <li>Replace URL with a normalized token <code>&lt;URL&gt;</code></li> </ul> <pre><code>&gt;&gt;&gt; nltk_obj = NLTK(url_tagging=True)\n&gt;&gt;&gt; nltk_obj.process(\"This is facebook http://facebook.com and github https://github.com\")\n['This', 'is', 'facebook', '&lt;URL&gt;', 'and', 'github', '&lt;URL&gt;']\n</code></pre> <ul> <li>Perform lemmatization on running text</li> </ul> <pre><code>&gt;&gt;&gt; nltk_obj = NLTK(lemmatization=True)\n&gt;&gt;&gt; nltk_obj.process(\"The striped bats are hanging on their feet for best\")\n[\"The\", \"strip\", \"bat\", \"be\", \"hang\", \"on\", \"their\", \"foot\", \"for\", \"best\"]\n</code></pre> <ul> <li>Perform stemming on running text</li> </ul> <pre><code>&gt;&gt;&gt; nltk_obj = NLTK(stemming=True)\n&gt;&gt;&gt; nltk_obj.process(\"These unbelievable abnormous objects\")\n['these', 'unbeliev', 'abnorm', 'object']\n</code></pre> <ul> <li>Label each token in the running text with its POS tag</li> </ul> <pre><code>&gt;&gt;&gt; nltk_obj = NLTK(pos_tag=True)\n&gt;&gt;&gt; nltk_obj.process(\"Facebook was fined by Hewlett Packard for spending 100\u20ac\")\n['Facebook_NNP', 'was_VBD', 'fined_VBN', 'by_IN', 'Hewlett_NNP', 'Packard_NNP', 'for_IN', 'spending_VBG',\n'100\u20ac_CD']\n</code></pre> PARAMETER DESCRIPTION <code>strip_multiple_whitespaces</code> <p>If set to True, all multiple whitespaces will be reduced to only one white space</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>remove_punctuation</code> <p>If set to True, all punctuation from the running text will be removed</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>stopwords_removal</code> <p>If set to True, all stowpwords from the running text will be removed</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>url_tagging</code> <p>If set to True, all urls in the running text will be replaced with the <code>&lt;URL&gt;</code> token</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>lemmatization</code> <p>If set to True, each token in the running text will be brought to its lemma</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>stemming</code> <p>If set to True, each token in the running text will be brought to its stem</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>pos_tag</code> <p>If set to True, each token in the running text will be labeled with its POS tag in the form <code>token_TAG</code></p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>lang</code> <p>Language of the running text</p> <p> TYPE: <code>str</code> DEFAULT: <code>'english'</code> </p> Source code in <code>clayrs/content_analyzer/information_processor/nltk_processor.py</code> <pre><code>def __init__(self, *,\n             strip_multiple_whitespaces: bool = True,\n             remove_punctuation: bool = False,\n             stopwords_removal: bool = False,\n             url_tagging: bool = False,\n             lemmatization: bool = False,\n             stemming: bool = False,\n             pos_tag: bool = False,\n             lang: str = 'english'):\n\n    if not NLTK._corpus_downloaded:\n        self.__download_corpus()\n        NLTK._corpus_downloaded = True\n\n    self.stopwords_removal = stopwords_removal\n\n    self.stemming = stemming\n    self.stemmer = SnowballStemmer(language=lang)\n\n    self.lemmatization = lemmatization\n    self.lemmatizer = WordNetLemmatizer()\n\n    self.strip_multiple_whitespaces = strip_multiple_whitespaces\n    self.url_tagging = url_tagging\n    self.remove_punctuation = remove_punctuation\n    self.pos_tag = pos_tag\n    self.__full_lang_code = lang\n</code></pre>"},{"location":"content_analyzer/information_preprocessors/textual_preprocessors/spacy/","title":"Spacy preprocessor","text":""},{"location":"content_analyzer/information_preprocessors/textual_preprocessors/spacy/#clayrs.content_analyzer.Spacy","title":"<code>Spacy(model='en_core_web_sm', *, strip_multiple_whitespaces=True, remove_punctuation=False, stopwords_removal=False, new_stopwords=None, not_stopwords=None, lemmatization=False, url_tagging=False, named_entity_recognition=False)</code>","text":"<p>         Bases: <code>NLP</code></p> <p>Interface to the Spacy library for natural language processing features</p> <p>Examples:</p> <ul> <li>Strip multiple whitespaces from running text</li> </ul> <pre><code>&gt;&gt;&gt; spacy_obj = Spacy(strip_multiple_whitespaces=True)\n&gt;&gt;&gt; spacy_obj.process('This   has  a lot  of   spaces')\n['This', 'has', 'a', 'lot', 'of', 'spaces']\n</code></pre> <ul> <li>Remove punctuation from running text</li> </ul> <pre><code>&gt;&gt;&gt; spacy_obj = Spacy(remove_punctuation=True)\n&gt;&gt;&gt; spacy_obj.process(\"Hello there. How are you? I'm fine, thanks.\")\n[\"Hello\", \"there\", \"How\", \"are\", \"you\", \"I\", \"'m\", \"fine\", \"thanks\"]\n</code></pre> <ul> <li>Remove stopwords using default stopwords corpus of spacy from running text</li> </ul> <pre><code>&gt;&gt;&gt; spacy_obj = Spacy(stopwords_removal=True)\n&gt;&gt;&gt; spacy_obj.process(\"The striped bats are hanging on their feet for the best\")\n[\"striped\", \"bats\", \"hanging\", \"feet\", \"best\"]\n</code></pre> <ul> <li>Remove stopwords using default stopwords corpus of spacy + <code>new_stopwords</code> list from running text</li> </ul> <pre><code>&gt;&gt;&gt; spacy_obj = Spacy(stopwords_removal=True, new_stopwords=['bats', 'best'])\n&gt;&gt;&gt; spacy_obj.process(\"The striped bats are hanging on their feet for the best\")\n[\"striped\", \"hanging\", \"feet\"]\n</code></pre> <ul> <li>Remove stopwords using default stopwords corpus of spacy - <code>not_stopwords</code> list from running text</li> </ul> <pre><code>&gt;&gt;&gt; spacy_obj = Spacy(stopwords_removal=True, not_stopwords=['The', 'the', 'on'])\n&gt;&gt;&gt; spacy_obj.process(\"The striped bats are hanging on their feet for the best\")\n[\"The\", \"striped\", \"bats\", \"hanging\", \"on\", \"feet\", \"the\", \"best\"]\n</code></pre> <ul> <li>Replace URL with a normalized token <code>&lt;URL&gt;</code></li> </ul> <pre><code>&gt;&gt;&gt; spacy_obj = Spacy(url_tagging=True)\n&gt;&gt;&gt; spacy_obj.process(\"This is facebook http://facebook.com and github https://github.com\")\n['This', 'is', 'facebook', '&lt;URL&gt;', 'and', 'github', '&lt;URL&gt;']\n</code></pre> <ul> <li>Perform lemmatization on running text</li> </ul> <pre><code>&gt;&gt;&gt; spacy_obj = Spacy(lemmatization=True)\n&gt;&gt;&gt; spacy_obj.process(\"The striped bats are hanging on their feet for best\")\n[\"The\", \"strip\", \"bat\", \"be\", \"hang\", \"on\", \"their\", \"foot\", \"for\", \"best\"]\n</code></pre> <ul> <li>Perform NER on running text (NEs will be tagged with BIO tagging)</li> </ul> <pre><code>&gt;&gt;&gt; spacy_obj = Spacy(named_entity_recognition=True)\n&gt;&gt;&gt; spacy_obj.process(\"Facebook was fined by Hewlett Packard for spending 100\u20ac\")\n[\"Facebook\", \"was\", \"fined\", \"by\", \"&lt;Hewlett_ORG_B&gt;\", \"&lt;Packard_ORG_I&gt;\", \"for\", \"spending\",\n\"&lt;100_MONEY_B&gt;\", \"&lt;\u20ac_MONEY_I&gt;\"]\n</code></pre> PARAMETER DESCRIPTION <code>model</code> <p>Spacy model that will be used to perform nlp operations. It will be downloaded if not present locally</p> <p> TYPE: <code>str</code> DEFAULT: <code>'en_core_web_sm'</code> </p> <code>strip_multiple_whitespaces</code> <p>If set to True, all multiple whitespaces will be reduced to only one white space</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>remove_punctuation</code> <p>If set to True, all punctuation from the running text will be removed</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>stopwords_removal</code> <p>If set to True, all stowpwords from the running text will be removed</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>new_stopwords</code> <p>List which contains custom defined stopwords that will be removed if <code>stopwords_removal=True</code></p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>None</code> </p> <code>not_stopwords</code> <p>List which contains custom defined stopwords that will not be considered as such, therefore won't be removed if <code>stopwords_removal=True</code></p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>None</code> </p> <code>url_tagging</code> <p>If set to True, all urls in the running text will be replaced with the <code>&lt;URL&gt;</code> token</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>lemmatization</code> <p>If set to True, each token in the running text will be brought to its lemma</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>named_entity_recognition</code> <p>If set to True, named entities recognized will be labeled in the form <code>&lt;token_B_TAG&gt;</code> or <code>&lt;token_I_TAG&gt;</code>, according to BIO tagging strategy</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>clayrs/content_analyzer/information_processor/spacy_processor.py</code> <pre><code>def __init__(self, model: str = 'en_core_web_sm', *,\n             strip_multiple_whitespaces: bool = True,\n             remove_punctuation: bool = False,\n             stopwords_removal: bool = False,\n             new_stopwords: List[str] = None,\n             not_stopwords: List[str] = None,\n             lemmatization: bool = False,\n             url_tagging: bool = False,\n             named_entity_recognition: bool = False):\n\n    self.model = model\n    self.stopwords_removal = stopwords_removal\n    self.lemmatization = lemmatization\n    self.strip_multiple_whitespaces = strip_multiple_whitespaces\n    self.url_tagging = url_tagging\n    self.remove_punctuation = remove_punctuation\n    self.named_entity_recognition = named_entity_recognition\n\n    # download the model if not present. In any case load it\n    if model not in spacy.cli.info()['pipelines']:\n        spacy.cli.download(model)\n    self._nlp = spacy.load(model)\n\n    # Adding custom rule of preserving '&lt;URL&gt;' token and in general token\n    # wrapped by '&lt;...&gt;'\n    prefixes = list(self._nlp.Defaults.prefixes)\n    prefixes.remove('&lt;')\n    prefix_regex = spacy.util.compile_prefix_regex(prefixes)\n    self._nlp.tokenizer.prefix_search = prefix_regex.search\n\n    suffixes = list(self._nlp.Defaults.suffixes)\n    suffixes.remove('&gt;')\n    suffix_regex = spacy.util.compile_suffix_regex(suffixes)\n    self._nlp.tokenizer.suffix_search = suffix_regex.search\n\n    self.not_stopwords_list = not_stopwords\n    if not_stopwords is not None:\n        for stopword in not_stopwords:\n            self._nlp.vocab[stopword].is_stop = False\n\n    self.new_stopwords_list = new_stopwords\n    if new_stopwords is not None:\n        for stopword in new_stopwords:\n            self._nlp.vocab[stopword].is_stop = True\n</code></pre>"},{"location":"content_analyzer/information_preprocessors/textual_preprocessors/spacy/#clayrs.content_analyzer.information_processor.spacy_processor.Spacy.process","title":"<code>process(field_data)</code>","text":"PARAMETER DESCRIPTION <code>field_data</code> <p>content to be processed</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>field_data</code> <p>list of str or dict in case of named entity recognition</p> <p> TYPE: <code>List[str]</code> </p> Source code in <code>clayrs/content_analyzer/information_processor/spacy_processor.py</code> <pre><code>def process(self, field_data: str) -&gt; List[str]:\n\"\"\"\n    Args:\n        field_data: content to be processed\n\n    Returns:\n        field_data: list of str or dict in case of named entity recognition\n\n    \"\"\"\n    field_data = check_not_tokenized(field_data)\n    if self.strip_multiple_whitespaces:\n        field_data = self.__strip_multiple_whitespaces_operation(field_data)\n    field_data = self.__tokenization_operation(field_data)\n    if self.named_entity_recognition:\n        field_data = self.__named_entity_recognition_operation(field_data)\n    if self.remove_punctuation:\n        field_data = self.__remove_punctuation(field_data)\n    if self.stopwords_removal:\n        field_data = self.__stopwords_removal_operation(field_data)\n    if self.lemmatization:\n        field_data = self.__lemmatization_operation(field_data)\n    if self.url_tagging:\n        field_data = self.__url_tagging_operation(field_data)\n\n    return self.__token_to_string(field_data)\n</code></pre>"},{"location":"content_analyzer/information_preprocessors/visual_preprocessors/torch_preprocessors/","title":"Torch Preprocessors","text":""},{"location":"content_analyzer/information_preprocessors/visual_preprocessors/torch_preprocessors/#torch-transformers","title":"Torch transformers","text":""},{"location":"content_analyzer/information_preprocessors/visual_preprocessors/torch_preprocessors/#clayrs.content_analyzer.information_processor.visual_preprocessors.torch_builtin_transformer.TorchCenterCrop","title":"<code>TorchCenterCrop(size)</code>","text":"<p>         Bases: <code>TorchBuiltInTransformer</code></p> <p>Class that implements the CenterCrop Transformer from torchvision. The parameters one could pass are the same ones you would pass instantiating the transformer CenterCrop directly from torchvision.</p> <p>TorchVision documentation: here</p> Source code in <code>clayrs/content_analyzer/information_processor/visual_preprocessors/torch_builtin_transformer.py</code> <pre><code>def __init__(self, size: int):\n    super().__init__(transforms.CenterCrop(size))\n</code></pre>"},{"location":"content_analyzer/information_preprocessors/visual_preprocessors/torch_preprocessors/#clayrs.content_analyzer.information_processor.visual_preprocessors.torch_builtin_transformer.TorchColorJitter","title":"<code>TorchColorJitter(brightness=0, contrast=0, saturation=0, hue=0)</code>","text":"<p>         Bases: <code>TorchBuiltInTransformer</code></p> <p>Class that implements the ColorJitter Transformer from torchvision. The parameters one could pass are the same ones you would pass instantiating the transformer ColorJitter directly from torchvision.</p> <p>TorchVision documentation: here</p> Source code in <code>clayrs/content_analyzer/information_processor/visual_preprocessors/torch_builtin_transformer.py</code> <pre><code>def __init__(self, brightness: Any = 0, contrast: Any = 0, saturation: Any = 0, hue: Any = 0):\n    super().__init__(transforms.ColorJitter(brightness, contrast, saturation, hue))\n</code></pre>"},{"location":"content_analyzer/information_preprocessors/visual_preprocessors/torch_preprocessors/#clayrs.content_analyzer.information_processor.visual_preprocessors.torch_builtin_transformer.TorchCompose","title":"<code>TorchCompose(transforms_list)</code>","text":"<p>         Bases: <code>TorchBuiltInTransformer</code></p> <p>Class that implements the Compose Transformer from torchvision. The parameters one could pass are the same ones you would pass instantiating the transformer Compose directly from torchvision.</p> <p>TorchVision documentation: here</p> <p>The only difference w.r.t. the TorchVision implementation is that while the original implementation expects a list of Transformer objects as parameter, this implementation expects a list of ImageProcessor (so other image pre-processors) as parameter</p> Source code in <code>clayrs/content_analyzer/information_processor/visual_preprocessors/torch_builtin_transformer.py</code> <pre><code>def __init__(self, transforms_list: List[ImageProcessor]):\n    super().__init__(transforms.Compose(transforms_list))\n</code></pre>"},{"location":"content_analyzer/information_preprocessors/visual_preprocessors/torch_preprocessors/#clayrs.content_analyzer.information_processor.visual_preprocessors.torch_builtin_transformer.TorchConvertImageDtype","title":"<code>TorchConvertImageDtype(dtype)</code>","text":"<p>         Bases: <code>TorchBuiltInTransformer</code></p> <p>Class that implements the ConvertImageDtype Transformer from torchvision. The parameters one could pass are the same ones you would pass instantiating the transformer ConvertImageDtype directly from torchvision.</p> <p>TorchVision documentation: here</p> Source code in <code>clayrs/content_analyzer/information_processor/visual_preprocessors/torch_builtin_transformer.py</code> <pre><code>def __init__(self, dtype: torch.dtype):\n    super().__init__(transforms.ConvertImageDtype(dtype))\n</code></pre>"},{"location":"content_analyzer/information_preprocessors/visual_preprocessors/torch_preprocessors/#clayrs.content_analyzer.information_processor.visual_preprocessors.torch_builtin_transformer.TorchGaussianBlur","title":"<code>TorchGaussianBlur(kernel_size, sigma=(0.1, 2.0))</code>","text":"<p>         Bases: <code>TorchBuiltInTransformer</code></p> <p>Class that implements the GaussianBlur Transformer from torchvision. The parameters one could pass are the same ones you would pass instantiating the transformer GaussianBlur directly from torchvision.</p> <p>TorchVision documentation: here</p> Source code in <code>clayrs/content_analyzer/information_processor/visual_preprocessors/torch_builtin_transformer.py</code> <pre><code>def __init__(self, kernel_size: int, sigma: Any = (0.1, 2.0)):\n    super().__init__(transforms.GaussianBlur(kernel_size, sigma))\n</code></pre>"},{"location":"content_analyzer/information_preprocessors/visual_preprocessors/torch_preprocessors/#clayrs.content_analyzer.information_processor.visual_preprocessors.torch_builtin_transformer.TorchGrayscale","title":"<code>TorchGrayscale(num_output_channels=1)</code>","text":"<p>         Bases: <code>TorchBuiltInTransformer</code></p> <p>Class that implements the Grayscale Transformer from torchvision. The parameters one could pass are the same ones you would pass instantiating the transformer Grayscale directly from torchvision.</p> <p>TorchVision documentation: here</p> Source code in <code>clayrs/content_analyzer/information_processor/visual_preprocessors/torch_builtin_transformer.py</code> <pre><code>def __init__(self, num_output_channels: int = 1):\n    super().__init__(transforms.Grayscale(num_output_channels))\n</code></pre>"},{"location":"content_analyzer/information_preprocessors/visual_preprocessors/torch_preprocessors/#clayrs.content_analyzer.information_processor.visual_preprocessors.torch_builtin_transformer.TorchLambda","title":"<code>TorchLambda(lambd)</code>","text":"<p>         Bases: <code>TorchBuiltInTransformer</code></p> <p>Class that implements the Lambda Transformer from torchvision. The parameters one could pass are the same ones you would pass instantiating the transformer Lambda directly from torchvision.</p> <p>TorchVision documentation: here</p> Source code in <code>clayrs/content_analyzer/information_processor/visual_preprocessors/torch_builtin_transformer.py</code> <pre><code>def __init__(self, lambd: callable):\n    super().__init__(transforms.Lambda(lambd))\n</code></pre>"},{"location":"content_analyzer/information_preprocessors/visual_preprocessors/torch_preprocessors/#clayrs.content_analyzer.information_processor.visual_preprocessors.torch_builtin_transformer.TorchLinearTransformation","title":"<code>TorchLinearTransformation(transformation_matrix, mean_vector)</code>","text":"<p>         Bases: <code>TorchBuiltInTransformer</code></p> <p>Class that implements the LinearTransformation Transformer from torchvision. The parameters one could pass are the same ones you would pass instantiating the transformer LinearTransformation directly from torchvision.</p> <p>TorchVision documentation: here</p> Source code in <code>clayrs/content_analyzer/information_processor/visual_preprocessors/torch_builtin_transformer.py</code> <pre><code>def __init__(self, transformation_matrix: torch.Tensor, mean_vector: torch.Tensor):\n    super().__init__(transforms.LinearTransformation(transformation_matrix, mean_vector))\n</code></pre>"},{"location":"content_analyzer/information_preprocessors/visual_preprocessors/torch_preprocessors/#clayrs.content_analyzer.information_processor.visual_preprocessors.torch_builtin_transformer.TorchNormalize","title":"<code>TorchNormalize(mean, std)</code>","text":"<p>         Bases: <code>TorchBuiltInTransformer</code></p> <p>Class that implements the Normalize Transformer from torchvision. The parameters one could pass are the same ones you would pass instantiating the transformer Normalize directly from torchvision.</p> <p>TorchVision documentation: here</p> Source code in <code>clayrs/content_analyzer/information_processor/visual_preprocessors/torch_builtin_transformer.py</code> <pre><code>def __init__(self, mean: Any, std: Any):\n    super().__init__(transforms.Normalize(mean, std, inplace=False))\n</code></pre>"},{"location":"content_analyzer/information_preprocessors/visual_preprocessors/torch_preprocessors/#clayrs.content_analyzer.information_processor.visual_preprocessors.torch_builtin_transformer.TorchPad","title":"<code>TorchPad(padding, fill=0, padding_mode='constant')</code>","text":"<p>         Bases: <code>TorchBuiltInTransformer</code></p> <p>Class that implements the Pad Transformer from torchvision. The parameters one could pass are the same ones you would pass instantiating the transformer Pad directly from torchvision.</p> <p>TorchVision documentation: here</p> Source code in <code>clayrs/content_analyzer/information_processor/visual_preprocessors/torch_builtin_transformer.py</code> <pre><code>def __init__(self, padding: int, fill: int = 0, padding_mode: str = \"constant\"):\n    super().__init__(transforms.Pad(padding, fill, padding_mode))\n</code></pre>"},{"location":"content_analyzer/information_preprocessors/visual_preprocessors/torch_preprocessors/#clayrs.content_analyzer.information_processor.visual_preprocessors.torch_builtin_transformer.TorchRandomAdjustSharpness","title":"<code>TorchRandomAdjustSharpness(sharpness_factor, p=0.5)</code>","text":"<p>         Bases: <code>TorchBuiltInTransformer</code></p> <p>Class that implements the RandomAdjustSharpness Transformer from torchvision. The parameters one could pass are the same ones you would pass instantiating the transformer RandomAdjustSharpness directly from torchvision.</p> <p>TorchVision documentation: here</p> Source code in <code>clayrs/content_analyzer/information_processor/visual_preprocessors/torch_builtin_transformer.py</code> <pre><code>def __init__(self, sharpness_factor: float, p: float = 0.5):\n    super().__init__(transforms.RandomAdjustSharpness(sharpness_factor, p))\n</code></pre>"},{"location":"content_analyzer/information_preprocessors/visual_preprocessors/torch_preprocessors/#clayrs.content_analyzer.information_processor.visual_preprocessors.torch_builtin_transformer.TorchRandomAffine","title":"<code>TorchRandomAffine(degrees, translate=None, scale=None, shear=None, interpolation=InterpolationMode.NEAREST, fill=0, center=None)</code>","text":"<p>         Bases: <code>TorchBuiltInTransformer</code></p> <p>Class that implements the RandomAffine Transformer from torchvision. The parameters one could pass are the same ones you would pass instantiating the transformer RandomAffine directly from torchvision.</p> <p>TorchVision documentation: here</p> Source code in <code>clayrs/content_analyzer/information_processor/visual_preprocessors/torch_builtin_transformer.py</code> <pre><code>def __init__(self, degrees: Any, translate: Any = None, scale: Any = None,\n             shear: Any = None, interpolation: InterpolationMode = InterpolationMode.NEAREST,\n             fill: Any = 0, center: Any = None):\n\n    super().__init__(transforms.RandomAffine(degrees, translate, scale, shear, interpolation, fill, center))\n</code></pre>"},{"location":"content_analyzer/information_preprocessors/visual_preprocessors/torch_preprocessors/#clayrs.content_analyzer.information_processor.visual_preprocessors.torch_builtin_transformer.TorchRandomApply","title":"<code>TorchRandomApply(transforms_list, p=0.5)</code>","text":"<p>         Bases: <code>TorchBuiltInTransformer</code></p> <p>Class that implements the RandomApply Transformer from torchvision. The parameters one could pass are the same ones you would pass instantiating the transformer RandomApply directly from torchvision.</p> <p>TorchVision documentation: here</p> <p>The only difference w.r.t. the TorchVision implementation is that while the original implementation expects a list of Transformer objects as parameter, this implementation expects a list of ImageProcessor (so other image pre-processors) as parameter</p> Source code in <code>clayrs/content_analyzer/information_processor/visual_preprocessors/torch_builtin_transformer.py</code> <pre><code>def __init__(self, transforms_list: List[ImageProcessor], p: float = 0.5):\n    super().__init__(transforms.RandomApply(transforms_list, p))\n</code></pre>"},{"location":"content_analyzer/information_preprocessors/visual_preprocessors/torch_preprocessors/#clayrs.content_analyzer.information_processor.visual_preprocessors.torch_builtin_transformer.TorchRandomAutocontrast","title":"<code>TorchRandomAutocontrast(p=0.5)</code>","text":"<p>         Bases: <code>TorchBuiltInTransformer</code></p> <p>Class that implements the RandomAutocontrast Transformer from torchvision. The parameters one could pass are the same ones you would pass instantiating the transformer RandomAutocontrast directly from torchvision.</p> <p>TorchVision documentation: here</p> Source code in <code>clayrs/content_analyzer/information_processor/visual_preprocessors/torch_builtin_transformer.py</code> <pre><code>def __init__(self, p: float = 0.5):\n    super().__init__(transforms.RandomAutocontrast(p))\n</code></pre>"},{"location":"content_analyzer/information_preprocessors/visual_preprocessors/torch_preprocessors/#clayrs.content_analyzer.information_processor.visual_preprocessors.torch_builtin_transformer.TorchRandomChoice","title":"<code>TorchRandomChoice(transforms_list, p=None)</code>","text":"<p>         Bases: <code>TorchBuiltInTransformer</code></p> <p>Class that implements the RandomChoice Transformer from torchvision. The parameters one could pass are the same ones you would pass instantiating the transformer RandomChoice directly from torchvision.</p> <p>TorchVision documentation: here</p> <p>The only difference w.r.t. the TorchVision implementation is that while the original implementation expects a list of Transformer objects as parameter, this implementation expects a list of ImageProcessor (so other image pre-processors) as parameter</p> Source code in <code>clayrs/content_analyzer/information_processor/visual_preprocessors/torch_builtin_transformer.py</code> <pre><code>def __init__(self, transforms_list: List[ImageProcessor], p: Any = None):\n    super().__init__(transforms.RandomChoice(transforms_list, p))\n</code></pre>"},{"location":"content_analyzer/information_preprocessors/visual_preprocessors/torch_preprocessors/#clayrs.content_analyzer.information_processor.visual_preprocessors.torch_builtin_transformer.TorchRandomCrop","title":"<code>TorchRandomCrop(size, padding=None, pad_if_needed=False, fill=0, padding_mode='constant')</code>","text":"<p>         Bases: <code>TorchBuiltInTransformer</code></p> <p>Class that implements the RandomCrop Transformer from torchvision. The parameters one could pass are the same ones you would pass instantiating the transformer RandomCrop directly from torchvision.</p> <p>TorchVision documentation: here</p> Source code in <code>clayrs/content_analyzer/information_processor/visual_preprocessors/torch_builtin_transformer.py</code> <pre><code>def __init__(self, size: int, padding: Any = None, pad_if_needed: bool = False,\n             fill: tuple = 0, padding_mode: str = \"constant\"):\n    super().__init__(transforms.RandomCrop(size, padding, pad_if_needed, fill, padding_mode))\n</code></pre>"},{"location":"content_analyzer/information_preprocessors/visual_preprocessors/torch_preprocessors/#clayrs.content_analyzer.information_processor.visual_preprocessors.torch_builtin_transformer.TorchRandomEqualize","title":"<code>TorchRandomEqualize(p=0.5)</code>","text":"<p>         Bases: <code>TorchBuiltInTransformer</code></p> <p>Class that implements the RandomEqualize Transformer from torchvision. The parameters one could pass are the same ones you would pass instantiating the transformer RandomEqualize directly from torchvision.</p> <p>TorchVision documentation: here</p> Source code in <code>clayrs/content_analyzer/information_processor/visual_preprocessors/torch_builtin_transformer.py</code> <pre><code>def __init__(self, p: float = 0.5):\n    super().__init__(transforms.RandomEqualize(p))\n</code></pre>"},{"location":"content_analyzer/information_preprocessors/visual_preprocessors/torch_preprocessors/#clayrs.content_analyzer.information_processor.visual_preprocessors.torch_builtin_transformer.TorchRandomErasing","title":"<code>TorchRandomErasing(p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0, inplace=False)</code>","text":"<p>         Bases: <code>TorchBuiltInTransformer</code></p> <p>Class that implements the RandomErasing Transformer from torchvision. The parameters one could pass are the same ones you would pass instantiating the transformer RandomErasing directly from torchvision.</p> <p>TorchVision documentation: here</p> Source code in <code>clayrs/content_analyzer/information_processor/visual_preprocessors/torch_builtin_transformer.py</code> <pre><code>def __init__(self, p: float = 0.5, scale: Tuple[float, float] = (0.02, 0.33),\n             ratio: Tuple[float, float] = (0.3, 3.3), value: int = 0, inplace: bool = False):\n    super().__init__(transforms.RandomErasing(p, scale, ratio, value, inplace))\n</code></pre>"},{"location":"content_analyzer/information_preprocessors/visual_preprocessors/torch_preprocessors/#clayrs.content_analyzer.information_processor.visual_preprocessors.torch_builtin_transformer.TorchRandomGrayscale","title":"<code>TorchRandomGrayscale(p=0.1)</code>","text":"<p>         Bases: <code>TorchBuiltInTransformer</code></p> <p>Class that implements the RandomGrayscale Transformer from torchvision. The parameters one could pass are the same ones you would pass instantiating the transformer RandomGrayscale directly from torchvision.</p> <p>TorchVision documentation: here</p> Source code in <code>clayrs/content_analyzer/information_processor/visual_preprocessors/torch_builtin_transformer.py</code> <pre><code>def __init__(self, p: float = 0.1):\n    super().__init__(transforms.RandomGrayscale(p))\n</code></pre>"},{"location":"content_analyzer/information_preprocessors/visual_preprocessors/torch_preprocessors/#clayrs.content_analyzer.information_processor.visual_preprocessors.torch_builtin_transformer.TorchRandomHorizontalFlip","title":"<code>TorchRandomHorizontalFlip(p=0.5)</code>","text":"<p>         Bases: <code>TorchBuiltInTransformer</code></p> <p>Class that implements the RandomHorizontalFlip Transformer from torchvision. The parameters one could pass are the same ones you would pass instantiating the transformer RandomHorizontalFlip directly from torchvision.</p> <p>TorchVision documentation: here</p> Source code in <code>clayrs/content_analyzer/information_processor/visual_preprocessors/torch_builtin_transformer.py</code> <pre><code>def __init__(self, p: float = 0.5):\n    super().__init__(transforms.RandomHorizontalFlip(p))\n</code></pre>"},{"location":"content_analyzer/information_preprocessors/visual_preprocessors/torch_preprocessors/#clayrs.content_analyzer.information_processor.visual_preprocessors.torch_builtin_transformer.TorchRandomInvert","title":"<code>TorchRandomInvert(p=0.5)</code>","text":"<p>         Bases: <code>TorchBuiltInTransformer</code></p> <p>Class that implements the RandomInvert Transformer from torchvision. The parameters one could pass are the same ones you would pass instantiating the transformer RandomInvert directly from torchvision.</p> <p>TorchVision documentation: here</p> Source code in <code>clayrs/content_analyzer/information_processor/visual_preprocessors/torch_builtin_transformer.py</code> <pre><code>def __init__(self, p: float = 0.5):\n    super().__init__(transforms.RandomInvert(p))\n</code></pre>"},{"location":"content_analyzer/information_preprocessors/visual_preprocessors/torch_preprocessors/#clayrs.content_analyzer.information_processor.visual_preprocessors.torch_builtin_transformer.TorchRandomOrder","title":"<code>TorchRandomOrder(transforms_list)</code>","text":"<p>         Bases: <code>TorchBuiltInTransformer</code></p> <p>Class that implements the RandomOrder Transformer from torchvision. The parameters one could pass are the same ones you would pass instantiating the transformer RandomOrder directly from torchvision.</p> <p>TorchVision documentation: here</p> <p>The only difference w.r.t. the TorchVision implementation is that while the original implementation expects a list of Transformer objects as parameter, this implementation expects a list of ImageProcessor (so other image pre-processors) as parameter</p> Source code in <code>clayrs/content_analyzer/information_processor/visual_preprocessors/torch_builtin_transformer.py</code> <pre><code>def __init__(self, transforms_list: List[ImageProcessor]):\n    super().__init__(transforms.RandomOrder(transforms_list))\n</code></pre>"},{"location":"content_analyzer/information_preprocessors/visual_preprocessors/torch_preprocessors/#clayrs.content_analyzer.information_processor.visual_preprocessors.torch_builtin_transformer.TorchRandomPerspective","title":"<code>TorchRandomPerspective(distortion_scale=0.5, p=0.5, interpolation=InterpolationMode.BILINEAR, fill=0)</code>","text":"<p>         Bases: <code>TorchBuiltInTransformer</code></p> <p>Class that implements the RandomPerspective Transformer from torchvision. The parameters one could pass are the same ones you would pass instantiating the transformer RandomPerspective directly from torchvision.</p> <p>TorchVision documentation: here</p> Source code in <code>clayrs/content_analyzer/information_processor/visual_preprocessors/torch_builtin_transformer.py</code> <pre><code>def __init__(self, distortion_scale: float = 0.5, p: float = 0.5,\n             interpolation: InterpolationMode = InterpolationMode.BILINEAR, fill: Any = 0):\n    super().__init__(transforms.RandomPerspective(distortion_scale, p, interpolation, fill))\n</code></pre>"},{"location":"content_analyzer/information_preprocessors/visual_preprocessors/torch_preprocessors/#clayrs.content_analyzer.information_processor.visual_preprocessors.torch_builtin_transformer.TorchRandomPosterize","title":"<code>TorchRandomPosterize(bits, p=0.5)</code>","text":"<p>         Bases: <code>TorchBuiltInTransformer</code></p> <p>Class that implements the RandomPosterize Transformer from torchvision. The parameters one could pass are the same ones you would pass instantiating the transformer RandomPosterize directly from torchvision.</p> <p>TorchVision documentation: here</p> Source code in <code>clayrs/content_analyzer/information_processor/visual_preprocessors/torch_builtin_transformer.py</code> <pre><code>def __init__(self, bits: int, p: float = 0.5):\n    super().__init__(transforms.RandomPosterize(bits, p))\n</code></pre>"},{"location":"content_analyzer/information_preprocessors/visual_preprocessors/torch_preprocessors/#clayrs.content_analyzer.information_processor.visual_preprocessors.torch_builtin_transformer.TorchRandomResizedCrop","title":"<code>TorchRandomResizedCrop(size, scale=(0.08, 1.0), ratio=(3.0 / 4.0, 4.0 / 3.0), interpolation=InterpolationMode.BILINEAR, antialias=None)</code>","text":"<p>         Bases: <code>TorchBuiltInTransformer</code></p> <p>Class that implements the RandomResizedCrop Transformer from torchvision. The parameters one could pass are the same ones you would pass instantiating the transformer RandomResizedCrop directly from torchvision.</p> <p>TorchVision documentation: here</p> Source code in <code>clayrs/content_analyzer/information_processor/visual_preprocessors/torch_builtin_transformer.py</code> <pre><code>def __init__(self, size: int, scale: Tuple[float] = (0.08, 1.0), ratio: Tuple[float] = (3.0 / 4.0, 4.0 / 3.0),\n             interpolation: InterpolationMode = InterpolationMode.BILINEAR, antialias: Optional[bool] = None):\n    super().__init__(transforms.RandomResizedCrop(size, scale, ratio, interpolation, antialias))\n</code></pre>"},{"location":"content_analyzer/information_preprocessors/visual_preprocessors/torch_preprocessors/#clayrs.content_analyzer.information_processor.visual_preprocessors.torch_builtin_transformer.TorchRandomRotation","title":"<code>TorchRandomRotation(degrees, interpolation=InterpolationMode.NEAREST, expand=False, center=None, fill=0)</code>","text":"<p>         Bases: <code>TorchBuiltInTransformer</code></p> <p>Class that implements the RandomRotation Transformer from torchvision. The parameters one could pass are the same ones you would pass instantiating the transformer RandomRotation directly from torchvision.</p> <p>TorchVision documentation: here</p> Source code in <code>clayrs/content_analyzer/information_processor/visual_preprocessors/torch_builtin_transformer.py</code> <pre><code>def __init__(self, degrees: Any, interpolation: InterpolationMode = InterpolationMode.NEAREST, expand: Any = False,\n             center: Any = None, fill: Any = 0):\n    super().__init__(transforms.RandomRotation(degrees, interpolation, expand, center, fill))\n</code></pre>"},{"location":"content_analyzer/information_preprocessors/visual_preprocessors/torch_preprocessors/#clayrs.content_analyzer.information_processor.visual_preprocessors.torch_builtin_transformer.TorchRandomSolarize","title":"<code>TorchRandomSolarize(threshold, p=0.5)</code>","text":"<p>         Bases: <code>TorchBuiltInTransformer</code></p> <p>Class that implements the RandomSolarize Transformer from torchvision. The parameters one could pass are the same ones you would pass instantiating the transformer RandomSolarize directly from torchvision.</p> <p>TorchVision documentation: here</p> Source code in <code>clayrs/content_analyzer/information_processor/visual_preprocessors/torch_builtin_transformer.py</code> <pre><code>def __init__(self, threshold: float, p: float = 0.5):\n    super().__init__(transforms.RandomSolarize(threshold, p))\n</code></pre>"},{"location":"content_analyzer/information_preprocessors/visual_preprocessors/torch_preprocessors/#clayrs.content_analyzer.information_processor.visual_preprocessors.torch_builtin_transformer.TorchRandomVerticalFlip","title":"<code>TorchRandomVerticalFlip(p=0.5)</code>","text":"<p>         Bases: <code>TorchBuiltInTransformer</code></p> <p>Class that implements the RandomVerticalFlip Transformer from torchvision. The parameters one could pass are the same ones you would pass instantiating the transformer RandomVerticalFlip directly from torchvision.</p> <p>TorchVision documentation: here</p> Source code in <code>clayrs/content_analyzer/information_processor/visual_preprocessors/torch_builtin_transformer.py</code> <pre><code>def __init__(self, p: float = 0.5):\n    super().__init__(transforms.RandomVerticalFlip(p))\n</code></pre>"},{"location":"content_analyzer/information_preprocessors/visual_preprocessors/torch_preprocessors/#clayrs.content_analyzer.information_processor.visual_preprocessors.torch_builtin_transformer.TorchResize","title":"<code>TorchResize(size, interpolation=InterpolationMode.BILINEAR, max_size=None, antialias=None)</code>","text":"<p>         Bases: <code>TorchBuiltInTransformer</code></p> <p>Class that implements the Resize Transformer from torchvision. The parameters one could pass are the same ones you would pass instantiating the transformer Resize directly from torchvision.</p> <p>TorchVision documentation: here</p> Source code in <code>clayrs/content_analyzer/information_processor/visual_preprocessors/torch_builtin_transformer.py</code> <pre><code>def __init__(self, size: int, interpolation=InterpolationMode.BILINEAR, max_size: Any = None,\n             antialias: Any = None):\n    super().__init__(transforms.Resize(size, interpolation, max_size, antialias))\n</code></pre>"},{"location":"content_analyzer/information_preprocessors/visual_preprocessors/torch_preprocessors/#torch-augmenters","title":"Torch augmenters","text":""},{"location":"content_analyzer/information_preprocessors/visual_preprocessors/torch_preprocessors/#clayrs.content_analyzer.information_processor.visual_preprocessors.torch_builtin_augmenter.TorchAutoAugment","title":"<code>TorchAutoAugment(policy=AutoAugmentPolicy.IMAGENET, interpolation=InterpolationMode.NEAREST, fill=None)</code>","text":"<p>         Bases: <code>TorchBuiltInTransformer</code></p> <p>Class that implements the AutoAugment Transformer from torchvision. The parameters one could pass are the same ones you would pass instantiating the transformer AutoAugment directly from torchvision.</p> <p>TorchVision documentation: here</p> <p>NOTE: the augmented result will SUBSTITUTE the original input</p> Source code in <code>clayrs/content_analyzer/information_processor/visual_preprocessors/torch_builtin_augmenter.py</code> <pre><code>def __init__(self, policy: AutoAugmentPolicy = AutoAugmentPolicy.IMAGENET,\n             interpolation: InterpolationMode = InterpolationMode.NEAREST,\n             fill: Optional[List[float]] = None):\n\n    super().__init__(transforms.AutoAugment(policy, interpolation, fill))\n</code></pre>"},{"location":"content_analyzer/information_preprocessors/visual_preprocessors/torch_preprocessors/#clayrs.content_analyzer.information_processor.visual_preprocessors.torch_builtin_augmenter.TorchRandAugment","title":"<code>TorchRandAugment(num_ops=2, magnitude=9, num_magnitude_bins=31, interpolation=InterpolationMode.NEAREST, fill=None)</code>","text":"<p>         Bases: <code>TorchBuiltInTransformer</code></p> <p>Class that implements the RandAugment Transformer from torchvision. The parameters one could pass are the same ones you would pass instantiating the transformer RandAugment directly from torchvision.</p> <p>TorchVision documentation: here</p> <p>NOTE: the augmented result will SUBSTITUTE the original input</p> Source code in <code>clayrs/content_analyzer/information_processor/visual_preprocessors/torch_builtin_augmenter.py</code> <pre><code>def __init__(\n    self,\n    num_ops: int = 2,\n    magnitude: int = 9,\n    num_magnitude_bins: int = 31,\n    interpolation: InterpolationMode = InterpolationMode.NEAREST,\n    fill: Optional[List[float]] = None,\n) -&gt; None:\n    super().__init__(transforms.RandAugment(num_ops, magnitude, num_magnitude_bins, interpolation, fill))\n</code></pre>"},{"location":"content_analyzer/information_preprocessors/visual_preprocessors/torch_preprocessors/#clayrs.content_analyzer.information_processor.visual_preprocessors.torch_builtin_augmenter.TorchTrivialAugmentWide","title":"<code>TorchTrivialAugmentWide(num_magnitude_bins=31, interpolation=InterpolationMode.NEAREST, fill=None)</code>","text":"<p>         Bases: <code>TorchBuiltInTransformer</code></p> <p>Class that implements the TrivialAugmentWide Transformer from torchvision. The parameters one could pass are the same ones you would pass instantiating the transformer TrivialAugmentWide directly from torchvision.</p> <p>TorchVision documentation: here</p> <p>NOTE: the augmented result will SUBSTITUTE the original input</p> Source code in <code>clayrs/content_analyzer/information_processor/visual_preprocessors/torch_builtin_augmenter.py</code> <pre><code>def __init__(\n    self,\n    num_magnitude_bins: int = 31,\n    interpolation: InterpolationMode = InterpolationMode.NEAREST,\n    fill: Optional[List[float]] = None,\n) -&gt; None:\n    super().__init__(transforms.TrivialAugmentWide(num_magnitude_bins, interpolation, fill))\n</code></pre>"},{"location":"content_analyzer/ratings/ratings/","title":"Ratings class","text":"<p>The <code>Ratings</code> class is the main responsible for importing a dataset containing interactions between users and items</p>"},{"location":"content_analyzer/ratings/ratings/#clayrs.content_analyzer.ratings_manager.Ratings","title":"<code>Ratings(source, user_id_column=0, item_id_column=1, score_column=2, timestamp_column=None, score_processor=None, item_map=None, user_map=None)</code>","text":"<p>Class responsible for importing an interaction frame into the framework</p> <p>If the source file contains users, items and ratings in this order, no additional parameters are needed, otherwise the mapping must be explicitly specified using:</p> <ul> <li>'user_id' column,</li> <li>'item_id' column,</li> <li>'score' column</li> </ul> <p>The score column can also be processed: in case you would like to consider as score the sentiment of a textual review, or maybe normalizing all scores in \\([0, 1]\\) range. Check the example below for more</p> <p>Note that, during the import phase, the user and item ids will be converted to integers and a mapping between the newly created ids and the original string ids will be created. For replicability purposes, it is possible to pass your custom item and user map instead of leaving this task to the framework. Check the example below to see how</p> <p>Examples:</p> CSV raw source<pre><code>user_id,item_id,rating,timestamp,review\nu1,i1,4,00112,good movie\nu2,i1,3,00113,an average movie\nu2,i32,2,00114,a bad movie\n</code></pre> <p>As you can see the user id column, item id column and score column are the first three column and are already in sequential order, so no additional parameter is required to the Ratings class:</p> <pre><code>&gt;&gt;&gt; import clayrs.content_analyzer as ca\n&gt;&gt;&gt; ratings_raw_source = ca.CSVFile('ratings.csv')\n&gt;&gt;&gt; # add timestamp='timestamp' to the following if\n&gt;&gt;&gt; # you want to load also the timestamp\n&gt;&gt;&gt; ratings = ca.Ratings(ratings_raw_source)\n</code></pre> <p>In case columns in the raw source are not in the above order you must specify an appropriate mapping via positional index (useful in case your raw source doesn't have a header) or via column ids:</p> <pre><code>&gt;&gt;&gt; # (mapping by index) EQUIVALENT:\n&gt;&gt;&gt; ratings = ca.Ratings(\n&gt;&gt;&gt; ca.CSVFile('ratings.csv'),\n&gt;&gt;&gt; user_id_column=0,  # (1)\n&gt;&gt;&gt; item_id_column=1,  # (2)\n&gt;&gt;&gt; score_column=2  # (3)\n&gt;&gt;&gt; )\n</code></pre> <ol> <li>First column of raw source is the column containing all user ids</li> <li>Second column of raw source is the column containing all item ids</li> <li>Third column of raw source is the column containing all the scores</li> </ol> <pre><code>&gt;&gt;&gt; # (mapping by column name) EQUIVALENT:\n&gt;&gt;&gt; ratings = ca.Ratings(\n&gt;&gt;&gt; ca.CSVFile('ratings.csv'),\n&gt;&gt;&gt; user_id_column='user_id',  # (1)\n&gt;&gt;&gt; item_id_column='item_id',  # (2)\n&gt;&gt;&gt; score_column='rating'  # (3)\n&gt;&gt;&gt; )\n</code></pre> <ol> <li>The column with id 'user_id' of raw source is the column containing all user ids</li> <li>The column with id 'item_id' of raw source is the column containing all item ids</li> <li>The column with id 'rating' of raw source is the column containing all the scores</li> </ol> <p>In case you would like to use the sentiment of the <code>review</code> column of the above raw source as score column, simply specify the appropriate <code>ScoreProcessor</code> object</p> <pre><code>&gt;&gt;&gt; ratings_raw_source = ca.CSVFile('ratings.csv')\n&gt;&gt;&gt; ratings = ca.Ratings(ratings_raw_source,\n&gt;&gt;&gt;                      score_column='review',\n&gt;&gt;&gt;                      score_processor=ca.TextBlobSentimentAnalysis())\n</code></pre> <p>In case you would like to specify the mappings for items or users, simply specify them in the corresponding parameters</p> <pre><code>&gt;&gt;&gt; ratings_raw_source = ca.CSVFile('ratings.csv')\n&gt;&gt;&gt; custom_item_map = {'i1': 0, 'i2': 2, 'i3': 1}\n&gt;&gt;&gt; custom_user_map = {'u1': 0, 'u2': 2, 'u3': 1}\n&gt;&gt;&gt; ratings = ca.Ratings(ratings_raw_source,\n&gt;&gt;&gt;                      item_map=custom_item_map,\n&gt;&gt;&gt;                      user_map=custom_user_map)\n</code></pre> PARAMETER DESCRIPTION <code>source</code> <p>Source containing the raw interaction frame</p> <p> TYPE: <code>RawInformationSource</code> </p> <code>user_id_column</code> <p>Name or positional index of the field of the raw source representing users column</p> <p> TYPE: <code>Union[str, int]</code> DEFAULT: <code>0</code> </p> <code>item_id_column</code> <p>Name or positional index of the field of the raw source representing items column</p> <p> TYPE: <code>Union[str, int]</code> DEFAULT: <code>1</code> </p> <code>score_column</code> <p>Name or positional index of the field of the raw source representing score column</p> <p> TYPE: <code>Union[str, int]</code> DEFAULT: <code>2</code> </p> <code>timestamp_column</code> <p>Name or positional index of the field of the raw source representing timesamp column</p> <p> TYPE: <code>Union[str, int]</code> DEFAULT: <code>None</code> </p> <code>score_processor</code> <p><code>ScoreProcessor</code> object which will process the <code>score_column</code> accordingly. Useful if you want to perform sentiment analysis on a textual column or you want to normalize all scores in \\([0, 1]\\) range</p> <p> TYPE: <code>ScoreProcessor</code> DEFAULT: <code>None</code> </p> <code>item_map</code> <p>dictionary with string keys (the item ids) and integer values (the corresponding unique integer ids) used to create the item mapping. If not specified, it will be automatically created internally</p> <p> TYPE: <code>Dict[str, int]</code> DEFAULT: <code>None</code> </p> <code>user_map</code> <p>dictionary with string keys (the user ids) and integer values (the corresponding unique integer ids) used to create the user mapping. If not specified, it will be automatically created internally</p> <p> TYPE: <code>Dict[str, int]</code> DEFAULT: <code>None</code> </p> Source code in <code>clayrs/content_analyzer/ratings_manager/ratings.py</code> <pre><code>def __init__(self, source: RawInformationSource,\n             user_id_column: Union[str, int] = 0,\n             item_id_column: Union[str, int] = 1,\n             score_column: Union[str, int] = 2,\n             timestamp_column: Union[str, int] = None,\n             score_processor: ScoreProcessor = None,\n             item_map: Dict[str, int] = None,\n             user_map: Dict[str, int] = None):\n\n    # utility dictionary that will contain each user index as key and a numpy array, containing the indexes of the\n    # rows in the uir matrix which refer to an interaction for that user, as value. This is done to optimize\n    # performance when requesting all interactions of a certain user\n    self._user2rows: Dict\n\n    self._uir: np.ndarray\n    self.item_map: StrIntMap\n    self.user_map: StrIntMap\n\n    self._import_ratings(source, user_id_column, item_id_column,\n                         score_column, timestamp_column, score_processor, item_map, user_map)\n</code></pre>"},{"location":"content_analyzer/ratings/ratings/#clayrs.content_analyzer.ratings_manager.ratings.Ratings.item_id_column","title":"<code>item_id_column: np.ndarray</code>  <code>property</code> <code>cached</code>","text":"<p>Getter for the 'item_id' column of the interaction frame. This will return the item column \"as is\", so it will contain duplicate items. Use the 'unique_item_id_column' method to get unique items.</p> RETURNS DESCRIPTION <code>np.ndarray</code> <p>Items column with duplicates (string ids)</p>"},{"location":"content_analyzer/ratings/ratings/#clayrs.content_analyzer.ratings_manager.ratings.Ratings.item_idx_column","title":"<code>item_idx_column: np.ndarray</code>  <code>property</code> <code>cached</code>","text":"<p>Getter for the 'item_idx' column of the uir matrix. This will return the item column \"as is\", so it will contain duplicate items. Use the 'unique_item_idx_column' method to get unique items.</p> RETURNS DESCRIPTION <code>np.ndarray</code> <p>Items column with duplicates (integer ids)</p>"},{"location":"content_analyzer/ratings/ratings/#clayrs.content_analyzer.ratings_manager.ratings.Ratings.score_column","title":"<code>score_column: np.ndarray</code>  <code>property</code> <code>cached</code>","text":"<p>Getter for the score column. This will return the score column \"as is\".</p> RETURNS DESCRIPTION <code>np.ndarray</code> <p>Score column</p>"},{"location":"content_analyzer/ratings/ratings/#clayrs.content_analyzer.ratings_manager.ratings.Ratings.timestamp_column","title":"<code>timestamp_column: np.ndarray</code>  <code>property</code> <code>cached</code>","text":"<p>Getter for the timestamp column. This will return the score column \"as is\". If no timestamp is present then an empty list is returned</p> RETURNS DESCRIPTION <code>np.ndarray</code> <p>Timestamp column or empty list if no timestamp is present</p>"},{"location":"content_analyzer/ratings/ratings/#clayrs.content_analyzer.ratings_manager.ratings.Ratings.uir","title":"<code>uir: np.ndarray</code>  <code>property</code>","text":"<p>Getter for the uir matrix created from the interaction frame. The imported ratings are converted in the form of a numpy ndarray where each row will represent an interaction. This uir matrix can be seen in a tabular representation as follows:</p> UIR matrix visualized: tabular format<pre><code>+----------+----------+--------+-----------+\n| user_idx | item_idx | score  | timestamp |\n+----------+----------+--------+-----------+\n| 0.       | 0.       | 4      | np.nan    |\n| 0.       | 1.       | 3      | np.nan    |\n| 1.       | 4.       | 1      | np.nan    |\n+----------+----------+--------+-----------+\n</code></pre> <p>Where the 'user_idx' and 'item_idx' columns contain the integer ids from the mapping of the <code>Ratings</code> object itself (these integer ids match the string ids that are in the original interaction frame)</p>"},{"location":"content_analyzer/ratings/ratings/#clayrs.content_analyzer.ratings_manager.ratings.Ratings.unique_item_id_column","title":"<code>unique_item_id_column: np.ndarray</code>  <code>property</code> <code>cached</code>","text":"<p>Getter for the 'item_id' column of the interaction frame. This will return the item column without duplicates.</p> RETURNS DESCRIPTION <code>np.ndarray</code> <p>Items column without duplicates (string ids)</p>"},{"location":"content_analyzer/ratings/ratings/#clayrs.content_analyzer.ratings_manager.ratings.Ratings.unique_item_idx_column","title":"<code>unique_item_idx_column: np.ndarray</code>  <code>property</code> <code>cached</code>","text":"<p>Getter for the 'item_idx' column of the uir matrix. This will return the item column without duplicates.</p> RETURNS DESCRIPTION <code>np.ndarray</code> <p>Items column without duplicates (integer ids)</p>"},{"location":"content_analyzer/ratings/ratings/#clayrs.content_analyzer.ratings_manager.ratings.Ratings.unique_user_id_column","title":"<code>unique_user_id_column: np.ndarray</code>  <code>property</code> <code>cached</code>","text":"<p>Getter for the 'user_id' column of the interaction frame. This will return the user column without duplicates.</p> RETURNS DESCRIPTION <code>np.ndarray</code> <p>Users column without duplicates (string ids)</p>"},{"location":"content_analyzer/ratings/ratings/#clayrs.content_analyzer.ratings_manager.ratings.Ratings.unique_user_idx_column","title":"<code>unique_user_idx_column: np.ndarray</code>  <code>property</code> <code>cached</code>","text":"<p>Getter for the 'user_idx' column of the uir matrix. This will return the user column without duplicates.</p> RETURNS DESCRIPTION <code>np.ndarray</code> <p>Users column without duplicates (integer ids)</p>"},{"location":"content_analyzer/ratings/ratings/#clayrs.content_analyzer.ratings_manager.ratings.Ratings.user_id_column","title":"<code>user_id_column: np.ndarray</code>  <code>property</code> <code>cached</code>","text":"<p>Getter for the 'user_id' column of the interaction frame. This will return the user column \"as is\", so it will contain duplicate users. Use the 'unique_user_id_column' method to get unique users.</p> RETURNS DESCRIPTION <code>np.ndarray</code> <p>Users column with duplicates (string ids)</p>"},{"location":"content_analyzer/ratings/ratings/#clayrs.content_analyzer.ratings_manager.ratings.Ratings.user_idx_column","title":"<code>user_idx_column: np.ndarray</code>  <code>property</code> <code>cached</code>","text":"<p>Getter for the 'user_idx' column of the uir matrix. This will return the user column \"as is\", so it will contain duplicate users. Use the 'unique_user_idx_column' method to get unique users.</p> RETURNS DESCRIPTION <code>np.ndarray</code> <p>Users column with duplicates (integer ids)</p>"},{"location":"content_analyzer/ratings/ratings/#clayrs.content_analyzer.ratings_manager.ratings.Ratings.__iter__","title":"<code>__iter__()</code>","text":"<p>Note: iteration is done on integer ids, if you want to iterate over string ids you need to iterate over the 'user_id_column' or 'item_id_column'</p> Source code in <code>clayrs/content_analyzer/ratings_manager/ratings.py</code> <pre><code>def __iter__(self):\n\"\"\"\n    Note: iteration is done on integer ids, if you want to iterate over string ids you need to iterate over the\n    'user_id_column' or 'item_id_column'\n    \"\"\"\n    yield from iter(self._uir)\n</code></pre>"},{"location":"content_analyzer/ratings/ratings/#clayrs.content_analyzer.ratings_manager.ratings.Ratings.filter_ratings","title":"<code>filter_ratings(user_list)</code>","text":"<p>Method which will filter the rating frame by keeping only interactions of users appearing in the <code>user_list</code>. This method will return a new <code>Ratings</code> object without changing the original</p> <p>Examples:</p> Starting Rating object<pre><code>+---------+---------+-------+\n| user_id | item_id | score |\n+---------+---------+-------+\n| u1      | i1      |     4 |\n| u1      | i2      |     3 |\n| u2      | i5      |     1 |\n+---------+---------+-------+\n</code></pre> Starting Rating object: corresponding uir matrix<pre><code>+----------+----------+--------+-----------+\n| user_idx | item_idx | score  | timestamp |\n+----------+----------+--------+-----------+\n| 0.       | 0.       | 4      | np.nan    |\n| 0.       | 1.       | 3      | np.nan    |\n| 1.       | 4.       | 1      | np.nan    |\n+----------+----------+--------+-----------+\n</code></pre> <pre><code>&gt;&gt;&gt; rating_frame.filter_ratings([0])\n</code></pre> Returned Rating object<pre><code>+---------+---------+-------+\n| user_id | item_id | score |\n+---------+---------+-------+\n| u1      | i1      |     4 |\n| u1      | i2      |     3 |\n+---------+---------+-------+\n</code></pre> Returned Rating object: corresponding uir matrix<pre><code>+----------+----------+--------+-----------+\n| user_idx | item_idx | score  | timestamp |\n+----------+----------+--------+-----------+\n| 0.       | 0.       | 4      | np.nan    |\n| 0.       | 1.       | 3      | np.nan    |\n+----------+----------+--------+-----------+\n</code></pre> <p>If you don't know the integer ids for the users, you can obtain them using the user map as follows:</p> <pre><code>&gt;&gt;&gt; user_idxs = rating_frame.user_map[['u1']]\n&gt;&gt;&gt; rating_frame.filter_ratings(user_list=user_idxs)\n</code></pre> PARAMETER DESCRIPTION <code>user_list</code> <p>List of user integer ids that will be present in the filtered <code>Ratings</code> object</p> <p> TYPE: <code>Sequence[int]</code> </p> <p>Returns     The filtered Ratings object which contains only interactions of selected users</p> Source code in <code>clayrs/content_analyzer/ratings_manager/ratings.py</code> <pre><code>def filter_ratings(self, user_list: Sequence[int]) -&gt; Ratings:\n\"\"\"\n    Method which will filter the rating frame by keeping only interactions of users appearing in the `user_list`.\n    This method will return a new `Ratings` object without changing the original\n\n    Examples:\n\n        ```title=\"Starting Rating object\"\n        +---------+---------+-------+\n        | user_id | item_id | score |\n        +---------+---------+-------+\n        | u1      | i1      |     4 |\n        | u1      | i2      |     3 |\n        | u2      | i5      |     1 |\n        +---------+---------+-------+\n        ```\n\n        ```title=\"Starting Rating object: corresponding uir matrix\"\n        +----------+----------+--------+-----------+\n        | user_idx | item_idx | score  | timestamp |\n        +----------+----------+--------+-----------+\n        | 0.       | 0.       | 4      | np.nan    |\n        | 0.       | 1.       | 3      | np.nan    |\n        | 1.       | 4.       | 1      | np.nan    |\n        +----------+----------+--------+-----------+\n        ```\n\n        &gt;&gt;&gt; rating_frame.filter_ratings([0])\n\n        ```title=\"Returned Rating object\"\n        +---------+---------+-------+\n        | user_id | item_id | score |\n        +---------+---------+-------+\n        | u1      | i1      |     4 |\n        | u1      | i2      |     3 |\n        +---------+---------+-------+\n        ```\n\n        ```title=\"Returned Rating object: corresponding uir matrix\"\n        +----------+----------+--------+-----------+\n        | user_idx | item_idx | score  | timestamp |\n        +----------+----------+--------+-----------+\n        | 0.       | 0.       | 4      | np.nan    |\n        | 0.       | 1.       | 3      | np.nan    |\n        +----------+----------+--------+-----------+\n        ```\n\n        If you don't know the integer ids for the users, you can obtain them using the user map as follows:\n\n        &gt;&gt;&gt; user_idxs = rating_frame.user_map[['u1']]\n        &gt;&gt;&gt; rating_frame.filter_ratings(user_list=user_idxs)\n\n    Args:\n        user_list: List of user integer ids that will be present in the filtered `Ratings` object\n\n    Returns\n        The filtered Ratings object which contains only interactions of selected users\n    \"\"\"\n    valid_indexes = np.where(np.isin(self.user_idx_column, user_list))\n    new_uir = self._uir[valid_indexes]\n\n    return Ratings.from_uir(new_uir, self.user_map.map, self.item_map.map)\n</code></pre>"},{"location":"content_analyzer/ratings/ratings/#clayrs.content_analyzer.ratings_manager.ratings.Ratings.from_dataframe","title":"<code>from_dataframe(interaction_frame, user_column=0, item_column=1, score_column=2, timestamp_column=None, user_map=None, item_map=None)</code>  <code>classmethod</code>","text":"<p>Class method which allows to instantiate a <code>Ratings</code> object by using an existing pandas DataFrame</p> <p>If the pandas DataFrame contains users, items and ratings in this order, no additional parameters are needed, otherwise the mapping must be explicitly specified using:</p> <ul> <li>'user_id' column,</li> <li>'item_id' column,</li> <li>'score' column</li> </ul> <p>Check documentation of the <code>Ratings</code> class for examples on mapping columns explicitly, the functioning is the same</p> <p>Furthermore, it is also possible to specify the user and item mapping between original string ids and integer ones. However, differently from the <code>Ratings</code> class documentation, it is possible not only to specify them as dictionaries but also as numpy arrays or <code>StrIntMap</code> objects directly. The end result will be the same independently of the type, but it is suggested to check the <code>StrIntMap</code> class documentation to understand the differences between the three possible types</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; ratings_df = pd.DataFrame({'user_id': ['u1', 'u1', 'u1'],\n&gt;&gt;&gt;                            'item_id': ['i1', 'i2', 'i3'],\n&gt;&gt;&gt;                            'score': [4, 3, 3])\n&gt;&gt;&gt; Ratings.from_dataframe(ratings_df)\n</code></pre> <p>or</p> <pre><code>&gt;&gt;&gt; user_map = {'u1': 0}\n&gt;&gt;&gt; item_map = {'i1': 0, 'i2': 2, 'i3': 1}\n&gt;&gt;&gt; ratings_df = pd.DataFrame({'user_id': ['u1', 'u1', 'u1'],\n&gt;&gt;&gt;                            'item_id': ['i1', 'i2', 'i3'],\n&gt;&gt;&gt;                            'score': [4, 3, 3])\n&gt;&gt;&gt; Ratings.from_dataframe(ratings_df, user_map=user_map, item_map=item_map)\n</code></pre> PARAMETER DESCRIPTION <code>interaction_frame</code> <p>pandas DataFrame which represents the original interactions frame</p> <p> TYPE: <code>pd.DataFrame</code> </p> <code>user_column</code> <p>Name or positional index of the field of the DataFrame representing users column</p> <p> TYPE: <code>Union[str, int]</code> DEFAULT: <code>0</code> </p> <code>item_column</code> <p>Name or positional index of the field of the DataFrame representing items column</p> <p> TYPE: <code>Union[str, int]</code> DEFAULT: <code>1</code> </p> <code>score_column</code> <p>Name or positional index of the field of the DataFrame representing score column</p> <p> TYPE: <code>Union[str, int]</code> DEFAULT: <code>2</code> </p> <code>timestamp_column</code> <p>Name or positional index of the field of the raw source representing timesamp column</p> <p> TYPE: <code>Union[str, int]</code> DEFAULT: <code>None</code> </p> <code>item_map</code> <p>dictionary with string keys (the item ids) and integer values (the corresponding unique integer ids) used to create the item mapping. If not specified, it will be automatically created internally</p> <p> TYPE: <code>Union[Dict[str, int], np.ndarray, StrIntMap]</code> DEFAULT: <code>None</code> </p> <code>user_map</code> <p>dictionary with string keys (the user ids) and integer values (the corresponding unique integer ids) used to create the user mapping. If not specified, it will be automatically created internally</p> <p> TYPE: <code>Union[Dict[str, int], np.ndarray, StrIntMap]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Ratings</code> <p><code>Ratings</code> object instantiated thanks to an existing Pandas DataFrame</p> Source code in <code>clayrs/content_analyzer/ratings_manager/ratings.py</code> <pre><code>@classmethod\n@handler_score_not_float\ndef from_dataframe(cls, interaction_frame: pd.DataFrame,\n                   user_column: Union[str, int] = 0,\n                   item_column: Union[str, int] = 1,\n                   score_column: Union[str, int] = 2,\n                   timestamp_column: Union[str, int] = None,\n                   user_map: Union[Dict[str, int], np.ndarray, StrIntMap] = None,\n                   item_map: Union[Dict[str, int], np.ndarray, StrIntMap] = None) -&gt; Ratings:\n\"\"\"\n    Class method which allows to instantiate a `Ratings` object by using an existing pandas DataFrame\n\n    **If** the pandas DataFrame contains users, items and ratings in this order,\n    no additional parameters are needed, **otherwise** the mapping must be explicitly specified using:\n\n    * **'user_id'** column,\n    * **'item_id'** column,\n    * **'score'** column\n\n    Check documentation of the `Ratings` class for examples on mapping columns explicitly, the functioning is the\n    same\n\n    Furthermore, it is also possible to specify the user and item mapping between original string ids and\n    integer ones. However, differently from the `Ratings` class documentation, it is possible not only to specify\n    them as dictionaries but also as numpy arrays or `StrIntMap` objects directly. The end result will be the same\n    independently of the type, but it is suggested to check the `StrIntMap` class documentation to understand\n    the differences between the three possible types\n\n    Examples:\n\n        &gt;&gt;&gt; ratings_df = pd.DataFrame({'user_id': ['u1', 'u1', 'u1'],\n        &gt;&gt;&gt;                            'item_id': ['i1', 'i2', 'i3'],\n        &gt;&gt;&gt;                            'score': [4, 3, 3])\n        &gt;&gt;&gt; Ratings.from_dataframe(ratings_df)\n\n        or\n\n        &gt;&gt;&gt; user_map = {'u1': 0}\n        &gt;&gt;&gt; item_map = {'i1': 0, 'i2': 2, 'i3': 1}\n        &gt;&gt;&gt; ratings_df = pd.DataFrame({'user_id': ['u1', 'u1', 'u1'],\n        &gt;&gt;&gt;                            'item_id': ['i1', 'i2', 'i3'],\n        &gt;&gt;&gt;                            'score': [4, 3, 3])\n        &gt;&gt;&gt; Ratings.from_dataframe(ratings_df, user_map=user_map, item_map=item_map)\n\n    Args:\n        interaction_frame: pandas DataFrame which represents the original interactions frame\n        user_column: Name or positional index of the field of the DataFrame representing *users* column\n        item_column: Name or positional index of the field of the DataFrame representing *items* column\n        score_column: Name or positional index of the field of the DataFrame representing *score* column\n        timestamp_column: Name or positional index of the field of the raw source representing *timesamp* column\n        item_map: dictionary with string keys (the item ids) and integer values (the corresponding unique integer\n            ids) used to create the item mapping. If not specified, it will be automatically created internally\n        user_map: dictionary with string keys (the user ids) and integer values (the corresponding unique integer\n            ids) used to create the user mapping. If not specified, it will be automatically created internally\n\n    Returns:\n        `Ratings` object instantiated thanks to an existing Pandas DataFrame\n    \"\"\"\n\n    def get_value_row_df(row, column, dtype):\n        try:\n            if isinstance(column, str):\n                value = row[column]\n            else:\n                # it's an int, so we get the column id and then we get the corresponding value in the row\n                key_dict = interaction_frame.columns[column]\n                value = row[key_dict]\n        except (KeyError, IndexError) as e:\n            if isinstance(e, KeyError):\n                raise KeyError(f\"Column {column} not found in interaction frame!\")\n            else:\n                raise IndexError(f\"Column {column} not found in interaction frame!\")\n\n        return dtype(value) if value is not None else None\n\n    obj = cls.__new__(cls)  # Does not call __init__\n    super(Ratings, obj).__init__()  # Don't forget to call any polymorphic base class initializers\n\n    # lists that will contain the original data temporarily\n    # this is so that the conversion from string ids to integers will be called only once\n    # said lists will also be used to create the mappings if not specified in the parameters\n    tmp_user_id_column = []\n    tmp_item_id_column = []\n    tmp_score_column = []\n    tmp_timestamp_column = []\n\n    for i, row in enumerate(interaction_frame.to_dict(orient='records')):\n        user_id = get_value_row_df(row, user_column, str)\n        item_id = get_value_row_df(row, item_column, str)\n        score = get_value_row_df(row, score_column, float)\n        timestamp = get_value_row_df(row, timestamp_column, int) if timestamp_column is not None else np.nan\n\n        tmp_user_id_column.append(user_id)\n        tmp_item_id_column.append(item_id)\n        tmp_score_column.append(score)\n        tmp_timestamp_column.append(timestamp)\n\n    # create the item_map from the item_id column if not specified\n    if item_map is None:\n        obj.item_map = StrIntMap(np.array(list(dict.fromkeys(tmp_item_id_column))))\n    else:\n        obj.item_map = StrIntMap(item_map)\n\n    # create the user_map from the user_id column if not specified\n    if user_map is None:\n        obj.user_map = StrIntMap(np.array(list(dict.fromkeys(tmp_user_id_column))))\n    else:\n        obj.user_map = StrIntMap(user_map)\n\n    tmp_user_id_column = np.array(tmp_user_id_column)\n\n    if np.any(tmp_user_id_column == None):\n        raise UserNone('User column cannot contain None values') from None\n\n    tmp_item_id_column = np.array(tmp_item_id_column)\n\n    if np.any(tmp_item_id_column == None):\n        raise ItemNone('Item column cannot contain None values') from None\n\n    # convert user and item ids and create the uir matrix\n    obj._uir = np.array((\n        obj.user_map.convert_seq_str2int(tmp_user_id_column),\n        obj.item_map.convert_seq_str2int(tmp_item_id_column),\n        tmp_score_column, tmp_timestamp_column\n    )).T\n\n    obj._uir[:, 2] = obj._uir[:, 2].astype(float)\n    obj._uir[:, 3] = obj._uir[:, 3].astype(float)\n\n    # create the utility dictionary user2rows\n    obj._user2rows = {\n        user_idx: np.where(obj._uir[:, 0] == user_idx)[0]\n        for user_idx in obj.unique_user_idx_column\n    }\n\n    return obj\n</code></pre>"},{"location":"content_analyzer/ratings/ratings/#clayrs.content_analyzer.ratings_manager.ratings.Ratings.from_list","title":"<code>from_list(interaction_list, user_map=None, item_map=None)</code>  <code>classmethod</code>","text":"<p>Class method which allows to instantiate a <code>Ratings</code> object by using an existing list of tuples or its generator</p> <p>Furthermore, it is also possible to specify the user and item mapping between original string ids and integer ones. However, differently from the <code>Ratings</code> class documentation, it is possible not only to specify them as dictionaries but also as numpy arrays or <code>StrIntMap</code> objects directly. The end result will be the same independently of the type, but it is suggested to check the <code>StrIntMap</code> class documentation to understand the differences between the three possible types</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; interactions_list = [('u1', 'i1', 5), ('u2', 'i1', 4)]\n&gt;&gt;&gt; Ratings.from_list(interactions_list)\n</code></pre> <p>or</p> <pre><code>&gt;&gt;&gt; user_map = {'u1': 0, 'u2': 1}\n&gt;&gt;&gt; item_map = {'i1': 0}\n&gt;&gt;&gt; interactions_list = [('u1', 'i1', 5), ('u2', 'i1', 4)]\n&gt;&gt;&gt; Ratings.from_list(interactions_list, user_map=user_map, item_map=item_map)\n</code></pre> PARAMETER DESCRIPTION <code>interaction_list</code> <p>List containing tuples or its generator</p> <p> TYPE: <code>Union[List[Tuple], Iterator]</code> </p> <code>item_map</code> <p>dictionary with string keys (the item ids) and integer values (the corresponding unique integer ids) used to create the item mapping. If not specified, it will be automatically created internally</p> <p> TYPE: <code>Union[Dict[str, int], np.ndarray, StrIntMap]</code> DEFAULT: <code>None</code> </p> <code>user_map</code> <p>dictionary with string keys (the user ids) and integer values (the corresponding unique integer ids) used to create the user mapping. If not specified, it will be automatically created internally</p> <p> TYPE: <code>Union[Dict[str, int], np.ndarray, StrIntMap]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Ratings</code> <p><code>Ratings</code> object instantiated thanks to an existing interaction list</p> Source code in <code>clayrs/content_analyzer/ratings_manager/ratings.py</code> <pre><code>@classmethod\n@handler_score_not_float\ndef from_list(cls, interaction_list: Union[List[Tuple], Iterator],\n              user_map: Union[Dict[str, int], np.ndarray, StrIntMap] = None,\n              item_map: Union[Dict[str, int], np.ndarray, StrIntMap] = None) -&gt; Ratings:\n\"\"\"\n    Class method which allows to instantiate a `Ratings` object by using an existing list of tuples or its generator\n\n    Furthermore, it is also possible to specify the user and item mapping between original string ids and\n    integer ones. However, differently from the `Ratings` class documentation, it is possible not only to specify\n    them as dictionaries but also as numpy arrays or `StrIntMap` objects directly. The end result will be the same\n    independently of the type, but it is suggested to check the `StrIntMap` class documentation to understand\n    the differences between the three possible types\n\n    Examples:\n\n        &gt;&gt;&gt; interactions_list = [('u1', 'i1', 5), ('u2', 'i1', 4)]\n        &gt;&gt;&gt; Ratings.from_list(interactions_list)\n\n        or\n\n        &gt;&gt;&gt; user_map = {'u1': 0, 'u2': 1}\n        &gt;&gt;&gt; item_map = {'i1': 0}\n        &gt;&gt;&gt; interactions_list = [('u1', 'i1', 5), ('u2', 'i1', 4)]\n        &gt;&gt;&gt; Ratings.from_list(interactions_list, user_map=user_map, item_map=item_map)\n\n    Args:\n        interaction_list: List containing tuples or its generator\n        item_map: dictionary with string keys (the item ids) and integer values (the corresponding unique integer\n            ids) used to create the item mapping. If not specified, it will be automatically created internally\n        user_map: dictionary with string keys (the user ids) and integer values (the corresponding unique integer\n            ids) used to create the user mapping. If not specified, it will be automatically created internally\n\n    Returns:\n        `Ratings` object instantiated thanks to an existing interaction list\n    \"\"\"\n    obj = cls.__new__(cls)  # Does not call __init__\n    super(Ratings, obj).__init__()  # Don't forget to call any polymorphic base class initializers\n\n    # lists that will contain the original data temporarily\n    # this is so that the conversion from string ids to integers will be called only once\n    # said lists will also be used to create the mappings if not specified in the parameters\n    tmp_user_id_column = []\n    tmp_item_id_column = []\n    tmp_score_column = []\n    tmp_timestamp_column = []\n\n    for i, interaction in enumerate(interaction_list):\n\n        tmp_user_id_column.append(interaction[0])\n        tmp_item_id_column.append(interaction[1])\n        tmp_score_column.append(interaction[2])\n\n        if len(interaction) == 4:\n            tmp_timestamp_column.append(interaction[3])\n        else:\n            tmp_timestamp_column.append(np.nan)\n\n    # create the item_map from the item_id column if not specified\n    if item_map is None:\n        obj.item_map = StrIntMap(np.array(list(dict.fromkeys(tmp_item_id_column))))\n    else:\n        obj.item_map = StrIntMap(item_map)\n\n    # create the user_map from the user_id column if not specified\n    if user_map is None:\n        obj.user_map = StrIntMap(np.array(list(dict.fromkeys(tmp_user_id_column))))\n    else:\n        obj.user_map = StrIntMap(user_map)\n\n    tmp_user_id_column = np.array(tmp_user_id_column)\n\n    if np.any(tmp_user_id_column == None):\n        raise UserNone('User column cannot contain None values')\n\n    tmp_item_id_column = np.array(tmp_item_id_column)\n\n    if np.any(tmp_item_id_column == None):\n        raise ItemNone('Item column cannot contain None values')\n\n    # convert user and item ids and create the uir matrix\n    obj._uir = np.array((\n        obj.user_map.convert_seq_str2int(tmp_user_id_column),\n        obj.item_map.convert_seq_str2int(tmp_item_id_column),\n        tmp_score_column, tmp_timestamp_column\n    )).T\n\n    obj._uir[:, 2] = obj._uir[:, 2].astype(float)\n    obj._uir[:, 3] = obj._uir[:, 3].astype(float)\n\n    # create the utility dictionary user2rows\n    obj._user2rows = {\n        user_idx: np.where(obj._uir[:, 0] == user_idx)[0]\n        for user_idx in obj.unique_user_idx_column\n    }\n\n    return obj\n</code></pre>"},{"location":"content_analyzer/ratings/ratings/#clayrs.content_analyzer.ratings_manager.ratings.Ratings.from_uir","title":"<code>from_uir(uir, user_map, item_map)</code>  <code>classmethod</code>","text":"<p>Class method which allows to instantiate a <code>Ratings</code> object by using an existing uir matrix</p> <p>The uir matrix should be a two-dimensional numpy ndarray where each row represents a user interaction. Each row should be in the following format:</p> <pre><code>[0. 0. 4] or [0. 0. 4 np.nan] (without or with the timestamp)\n</code></pre> <p>In the case of a different format for the rows, a ValueError exception will be raised. Furthermore, if the uir matrix is not of dtype np.float64, a TypeError exception will be raised.</p> <p>In this case the 'user_map' and 'item_map' parameters MUST be specified, since there is no information regarding the original string ids in the uir matrix</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; uir_matrix = np.array([[0, 0, 4], [1, 0, 3]])\n&gt;&gt;&gt; user_map = {'u1': 0, 'u2': 1}\n&gt;&gt;&gt; item_map = {'i1': 0}\n&gt;&gt;&gt; Ratings.from_uir(uir_matrix, user_map=user_map, item_map=item_map)\n</code></pre> PARAMETER DESCRIPTION <code>uir</code> <p>uir matrix which will be used to create the new <code>Ratings</code> object</p> <p> TYPE: <code>np.ndarray</code> </p> <code>item_map</code> <p>dictionary with string keys (the item ids) and integer values (the corresponding unique integer ids) used to create the item mapping</p> <p> TYPE: <code>Union[Dict[str, int], np.ndarray, StrIntMap]</code> </p> <code>user_map</code> <p>dictionary with string keys (the user ids) and integer values (the corresponding unique integer ids) used to create the user mapping</p> <p> TYPE: <code>Union[Dict[str, int], np.ndarray, StrIntMap]</code> </p> RETURNS DESCRIPTION <code>Ratings</code> <p><code>Ratings</code> object instantiated thanks to an existing uir matrix</p> Source code in <code>clayrs/content_analyzer/ratings_manager/ratings.py</code> <pre><code>@classmethod\ndef from_uir(cls, uir: np.ndarray,\n             user_map: Union[Dict[str, int], np.ndarray, StrIntMap],\n             item_map: Union[Dict[str, int], np.ndarray, StrIntMap]) -&gt; Ratings:\n\"\"\"\n    Class method which allows to instantiate a `Ratings` object by using an existing uir matrix\n\n    The uir matrix should be a two-dimensional numpy ndarray where each row represents a user interaction.\n    Each row should be in the following format:\n\n    ```\n    [0. 0. 4] or [0. 0. 4 np.nan] (without or with the timestamp)\n    ```\n\n    In the case of a different format for the rows, a ValueError exception will be raised.\n    Furthermore, if the uir matrix is not of dtype np.float64, a TypeError exception will be raised.\n\n    In this case the 'user_map' and 'item_map' parameters ***MUST*** be specified, since there is no information\n    regarding the original string ids in the uir matrix\n\n    Examples:\n\n        &gt;&gt;&gt; uir_matrix = np.array([[0, 0, 4], [1, 0, 3]])\n        &gt;&gt;&gt; user_map = {'u1': 0, 'u2': 1}\n        &gt;&gt;&gt; item_map = {'i1': 0}\n        &gt;&gt;&gt; Ratings.from_uir(uir_matrix, user_map=user_map, item_map=item_map)\n\n    Args:\n        uir: uir matrix which will be used to create the new `Ratings` object\n        item_map: dictionary with string keys (the item ids) and integer values (the corresponding unique integer ids)\n            used to create the item mapping\n        user_map: dictionary with string keys (the user ids) and integer values (the corresponding unique integer\n            ids) used to create the user mapping\n\n    Returns:\n        `Ratings` object instantiated thanks to an existing uir matrix\n    \"\"\"\n    obj = cls.__new__(cls)  # Does not call __init__\n    super(Ratings, obj).__init__()  # Don't forget to call any polymorphic base class initializers\n\n    if uir.shape[0] &gt; 0 and uir.shape[1] &gt; 0:\n        if uir.shape[1] &lt; 3:\n            raise ValueError('User item ratings matrix should have at least 3 rows '\n                             '(one for users, one for items and one for ratings scores)')\n        elif uir.shape[1] == 3:\n            uir = np.append(uir, np.full((uir.shape[0], 1), fill_value=np.nan), axis=1)\n\n        if uir.dtype != np.float64:\n            raise TypeError('User id columns and item id columns should be mapped to their respective integer')\n    else:\n        uir = np.array([])\n\n    obj._uir = uir\n\n    obj.user_map = StrIntMap(user_map)\n    obj.item_map = StrIntMap(item_map)\n\n    obj._user2rows = {\n        user_idx: np.where(obj._uir[:, 0] == user_idx)[0]\n        for user_idx in obj.unique_user_idx_column\n    }\n\n    return obj\n</code></pre>"},{"location":"content_analyzer/ratings/ratings/#clayrs.content_analyzer.ratings_manager.ratings.Ratings.get_user_interactions","title":"<code>get_user_interactions(user_idx, head=None, as_indices=False)</code>","text":"<p>Method which returns a two-dimensional numpy array containing all the rows from the uir matrix for a single user, one for each interaction of the user. Then you can easily access the columns of the resulting array to obtain useful information</p> <p>Examples:</p> <p>So if the rating frame is the following:</p> <pre><code>+---------+---------+-------+\n| user_id | item_id | score |\n+---------+---------+-------+\n| u1      | i1      |     4 |\n| u1      | i2      |     3 |\n| u2      | i5      |     1 |\n+---------+---------+-------+\n</code></pre> <p>The corresponding uir matrix will be the following:</p> <pre><code>+----------+----------+--------+-----------+\n| user_idx | item_idx | score  | timestamp |\n+----------+----------+--------+-----------+\n| 0.       | 0.       | 4      | np.nan    |\n| 0.       | 1.       | 3      | np.nan    |\n| 1.       | 4.       | 1      | np.nan    |\n+----------+----------+--------+-----------+\n</code></pre> <pre><code>&gt;&gt;&gt; rating_frame.get_user_interactions(0)\nnp.ndarray([\n    [0. 0. 4 np.nan],\n    [0. 1. 3 np.nan],\n])\n</code></pre> <p>So you could easily extract all the ratings that a user has given, for example:</p> <pre><code>&gt;&gt;&gt; rating_frame.get_user_interactions(0)[:, 2]\nnp.ndarray([4,\n            3])\n</code></pre> <p>If you only want the first \\(k\\) interactions of the user, set <code>head=k</code>. The interactions returned are the first \\(k\\) according to their order of appearance in the rating frame:</p> <pre><code>&gt;&gt;&gt; rating_frame.get_user_interactions(0, head=1)\nnp.ndarray([\n    [0. 0. 4 np.nan]\n])\n</code></pre> <p>If you want to have the indices of the uir matrix corresponding to the user interactions instead of the actual interactions, set <code>as_indices=True</code>. This will return a numpy array containing the indexes of the rows of the uir matrix for the interactions of the specified user</p> <pre><code>&gt;&gt;&gt; rating_frame.get_user_interactions(0, as_indices=True)\nnp.ndarray([0, 1])\n</code></pre> <p>If you don't know the <code>user_idx</code> for a specific user, you can obtain it using the user map as follows:</p> <pre><code>&gt;&gt;&gt; user_idx = rating_frame.user_map['u1']\n&gt;&gt;&gt; rating_frame.get_user_interactions(user_idx=user_idx)\nnp.ndarray([\n    [0. 0. 4 np.nan],\n    [0. 1. 3 np.nan],\n])\n</code></pre> PARAMETER DESCRIPTION <code>user_idx</code> <p>Integer id of the user for which you want to retrieve the interactions</p> <p> TYPE: <code>int</code> </p> <code>head</code> <p>Integer which will cut the list of interactions of the user returned. The interactions returned are the first \\(k\\) according to their order of appearance</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>as_indices</code> <p>Instead of returning the user interactions, the indices of the rows in the uir matrix corresponding to interactions for the specified user will be returned</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>np.ndarray</code> <p>If <code>as_indices=False</code>, numpy ndarray containing the rows from the uir matrix for the specified user, otherwise numpy array containing the indexes of the rows from the uir matrix for the interactions of the specified user</p> Source code in <code>clayrs/content_analyzer/ratings_manager/ratings.py</code> <pre><code>def get_user_interactions(self, user_idx: int, head: int = None, as_indices: bool = False) -&gt; np.ndarray:\n\"\"\"\n    Method which returns a two-dimensional numpy array containing all the rows from the uir matrix for a single\n    user, one for each interaction of the user.\n    Then you can easily access the columns of the resulting array to obtain useful information\n\n    Examples:\n\n        So if the rating frame is the following:\n\n        ```\n        +---------+---------+-------+\n        | user_id | item_id | score |\n        +---------+---------+-------+\n        | u1      | i1      |     4 |\n        | u1      | i2      |     3 |\n        | u2      | i5      |     1 |\n        +---------+---------+-------+\n        ```\n\n        The corresponding uir matrix will be the following:\n\n        ```\n        +----------+----------+--------+-----------+\n        | user_idx | item_idx | score  | timestamp |\n        +----------+----------+--------+-----------+\n        | 0.       | 0.       | 4      | np.nan    |\n        | 0.       | 1.       | 3      | np.nan    |\n        | 1.       | 4.       | 1      | np.nan    |\n        +----------+----------+--------+-----------+\n        ```\n\n        &gt;&gt;&gt; rating_frame.get_user_interactions(0)\n        np.ndarray([\n            [0. 0. 4 np.nan],\n            [0. 1. 3 np.nan],\n        ])\n\n        So you could easily extract all the ratings that a user has given, for example:\n\n        &gt;&gt;&gt; rating_frame.get_user_interactions(0)[:, 2]\n        np.ndarray([4,\n                    3])\n\n        If you only want the first $k$ interactions of the user, set `head=k`. The interactions returned are the\n        first $k$ according to their order of appearance in the rating frame:\n\n        &gt;&gt;&gt; rating_frame.get_user_interactions(0, head=1)\n        np.ndarray([\n            [0. 0. 4 np.nan]\n        ])\n\n        If you want to have the indices of the uir matrix corresponding to the user interactions instead of the\n        actual interactions, set `as_indices=True`. This will return a numpy array containing the indexes of\n        the rows of the uir matrix for the interactions of the specified user\n\n        &gt;&gt;&gt; rating_frame.get_user_interactions(0, as_indices=True)\n        np.ndarray([0, 1])\n\n        If you don't know the `user_idx` for a specific user, you can obtain it using the user map as follows:\n\n        &gt;&gt;&gt; user_idx = rating_frame.user_map['u1']\n        &gt;&gt;&gt; rating_frame.get_user_interactions(user_idx=user_idx)\n        np.ndarray([\n            [0. 0. 4 np.nan],\n            [0. 1. 3 np.nan],\n        ])\n\n    Args:\n        user_idx: Integer id of the user for which you want to retrieve the interactions\n        head: Integer which will cut the list of interactions of the user returned. The interactions returned are\n            the first $k$ according to their order of appearance\n        as_indices: Instead of returning the user interactions, the indices of the rows in the uir matrix\n            corresponding to interactions for the specified user will be returned\n\n    Returns:\n        If `as_indices=False`, numpy ndarray containing the rows from the uir matrix for the specified user,\n            otherwise numpy array containing the indexes of the rows from the uir matrix for the interactions of the\n            specified user\n\n    \"\"\"\n    user_rows = self._user2rows.get(user_idx, [])[:head]\n    return user_rows if as_indices else self._uir[user_rows]\n</code></pre>"},{"location":"content_analyzer/ratings/ratings/#clayrs.content_analyzer.ratings_manager.ratings.Ratings.take_head_all","title":"<code>take_head_all(head)</code>","text":"<p>Method which will retain only \\(k\\) interactions for each user. The \\(k\\) interactions retained are the first which appear in the rating frame.</p> <p>This method will return a new <code>Ratings</code> object without changing the original</p> <p>Examples:</p> Starting Rating object<pre><code>+---------+---------+-------+\n| user_id | item_id | score |\n+---------+---------+-------+\n| u1      | i1      |     4 |\n| u1      | i2      |     3 |\n| u2      | i5      |     1 |\n| u2      | i6      |     2 |\n+---------+---------+-------+\n</code></pre> Starting Rating object: corresponding uir matrix<pre><code>+----------+----------+--------+-----------+\n| user_idx | item_idx | score  | timestamp |\n+----------+----------+--------+-----------+\n| 0.       | 0.       | 4      | np.nan    |\n| 0.       | 1.       | 3      | np.nan    |\n| 1.       | 4.       | 1      | np.nan    |\n| 1.       | 5.       | 2      | np.nan    |\n+----------+----------+--------+-----------+\n</code></pre> <pre><code>&gt;&gt;&gt; rating_frame.take_head_all(head=1)\n</code></pre> Returned Rating object<pre><code>+---------+---------+-------+\n| user_id | item_id | score |\n+---------+---------+-------+\n| u1      | i1      |     4 |\n| u2      | i5      |     1 |\n+---------+---------+-------+\n</code></pre> Returned Rating object: corresponding uir matrix<pre><code>+----------+----------+--------+-----------+\n| user_idx | item_idx | score  | timestamp |\n+----------+----------+--------+-----------+\n| 0.       | 0.       | 4      | np.nan    |\n| 1.       | 4.       | 1      | np.nan    |\n+----------+----------+--------+-----------+\n</code></pre> PARAMETER DESCRIPTION <code>head</code> <p>The number of interactions to retain for each user</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>Ratings</code> <p>The filtered Ratings object which contains only first \\(k\\) interactions for each user</p> Source code in <code>clayrs/content_analyzer/ratings_manager/ratings.py</code> <pre><code>def take_head_all(self, head: int) -&gt; Ratings:\n\"\"\"\n    Method which will retain only $k$ interactions for each user. The $k$ interactions retained are the first which\n    appear in the rating frame.\n\n    This method will return a new `Ratings` object without changing the original\n\n    Examples:\n\n        ```title=\"Starting Rating object\"\n        +---------+---------+-------+\n        | user_id | item_id | score |\n        +---------+---------+-------+\n        | u1      | i1      |     4 |\n        | u1      | i2      |     3 |\n        | u2      | i5      |     1 |\n        | u2      | i6      |     2 |\n        +---------+---------+-------+\n        ```\n\n        ```title=\"Starting Rating object: corresponding uir matrix\"\n        +----------+----------+--------+-----------+\n        | user_idx | item_idx | score  | timestamp |\n        +----------+----------+--------+-----------+\n        | 0.       | 0.       | 4      | np.nan    |\n        | 0.       | 1.       | 3      | np.nan    |\n        | 1.       | 4.       | 1      | np.nan    |\n        | 1.       | 5.       | 2      | np.nan    |\n        +----------+----------+--------+-----------+\n        ```\n\n        &gt;&gt;&gt; rating_frame.take_head_all(head=1)\n\n        ```title=\"Returned Rating object\"\n        +---------+---------+-------+\n        | user_id | item_id | score |\n        +---------+---------+-------+\n        | u1      | i1      |     4 |\n        | u2      | i5      |     1 |\n        +---------+---------+-------+\n        ```\n\n        ```title=\"Returned Rating object: corresponding uir matrix\"\n        +----------+----------+--------+-----------+\n        | user_idx | item_idx | score  | timestamp |\n        +----------+----------+--------+-----------+\n        | 0.       | 0.       | 4      | np.nan    |\n        | 1.       | 4.       | 1      | np.nan    |\n        +----------+----------+--------+-----------+\n        ```\n\n    Args:\n        head: The number of interactions to retain for each user\n\n    Returns:\n        The filtered Ratings object which contains only first $k$ interactions for each user\n    \"\"\"\n    cut_rows = np.hstack((rows[:head] for rows in self._user2rows.values()))\n    new_uir = self._uir[cut_rows]\n\n    return Ratings.from_uir(new_uir, self.user_map.map, self.item_map.map)\n</code></pre>"},{"location":"content_analyzer/ratings/ratings/#clayrs.content_analyzer.ratings_manager.ratings.Ratings.to_csv","title":"<code>to_csv(output_directory='.', file_name='ratings_frame', overwrite=False, ids_as_str=True)</code>","text":"<p>Method which will save the <code>Ratings</code> object to a <code>csv</code> file</p> PARAMETER DESCRIPTION <code>output_directory</code> <p>directory which will contain the csv file</p> <p> TYPE: <code>str</code> DEFAULT: <code>'.'</code> </p> <code>file_name</code> <p>Name of the csv_file</p> <p> TYPE: <code>str</code> DEFAULT: <code>'ratings_frame'</code> </p> <code>overwrite</code> <p>If set to True and a csv file exists in the same output directory with the same file name, it will be overwritten</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ids_as_str</code> <p>If True the original string ids for users and items will be used, otherwise their integer ids</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> Source code in <code>clayrs/content_analyzer/ratings_manager/ratings.py</code> <pre><code>def to_csv(self, output_directory: str = '.', file_name: str = 'ratings_frame', overwrite: bool = False,\n           ids_as_str: bool = True):\n\"\"\"\n    Method which will save the `Ratings` object to a `csv` file\n\n    Args:\n        output_directory: directory which will contain the csv file\n        file_name: Name of the csv_file\n        overwrite: If set to True and a csv file exists in the same output directory with the same file name, it\n            will be overwritten\n        ids_as_str: If True the original string ids for users and items will be used, otherwise their integer ids\n    \"\"\"\n    Path(output_directory).mkdir(parents=True, exist_ok=True)\n\n    file_name = get_valid_filename(output_directory, file_name, 'csv', overwrite)\n\n    frame = self.to_dataframe(ids_as_str=ids_as_str)\n    frame.to_csv(os.path.join(output_directory, file_name), index=False, header=True)\n</code></pre>"},{"location":"content_analyzer/ratings/ratings/#clayrs.content_analyzer.ratings_manager.ratings.Ratings.to_dataframe","title":"<code>to_dataframe(ids_as_str=True)</code>","text":"<p>Method which will convert the <code>Rating</code> object to a <code>pandas DataFrame object</code>.</p> <p>The returned DataFrame object will contain the 'user_id', 'item_id' and 'score' column and optionally the 'timestamp' column, if at least one interaction has a timestamp.</p> PARAMETER DESCRIPTION <code>ids_as_str</code> <p>If True, the original string ids for users and items will be used, otherwise their integer ids</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>pd.DataFrame</code> <p>The rating frame converted to a pandas DataFrame with 'user_id', 'item_id', 'score' column and optionally the 'timestamp' column</p> Source code in <code>clayrs/content_analyzer/ratings_manager/ratings.py</code> <pre><code>def to_dataframe(self, ids_as_str: bool = True) -&gt; pd.DataFrame:\n\"\"\"\n    Method which will convert the `Rating` object to a `pandas DataFrame object`.\n\n    The returned DataFrame object will contain the 'user_id', 'item_id' and 'score' column and optionally the\n    'timestamp' column, if at least one interaction has a timestamp.\n\n    Args:\n        ids_as_str: If True, the original string ids for users and items will be used, otherwise their integer ids\n\n    Returns:\n        The rating frame converted to a pandas DataFrame with 'user_id', 'item_id', 'score' column and optionally\n            the 'timestamp' column\n\n    \"\"\"\n    if ids_as_str:\n        will_be_frame = {'user_id': self.user_id_column,\n                         'item_id': self.item_id_column,\n                         'score': self.score_column}\n    else:\n        will_be_frame = {'user_id': self.user_idx_column,\n                         'item_id': self.item_idx_column,\n                         'score': self.score_column}\n\n    if len(self.timestamp_column) != 0:\n        will_be_frame['timestamp'] = self.timestamp_column\n\n    return pd.DataFrame(will_be_frame)\n</code></pre>"},{"location":"content_analyzer/ratings/score_processors/","title":"Score Processors","text":""},{"location":"content_analyzer/ratings/score_processors/#clayrs.content_analyzer.ratings_manager.NumberNormalizer","title":"<code>NumberNormalizer(scale, decimal_rounding=None)</code>","text":"<p>         Bases: <code>ScoreProcessor</code></p> <p>Class that normalizes numeric scores to a scale in the range \\([-1.0, 1.0]\\)</p> PARAMETER DESCRIPTION <code>scale</code> <p>Tuple where the first value is the minimum of the actual scale, second value is the maximum of the actual scale (e.g. <code>(1, 5)</code> represents an actual scale of scores from 1 (included) to 5 (included))</p> <p> TYPE: <code>Tuple[float, float]</code> </p> <code>decimal_rounding</code> <p>If set, the normalized score will be rounded to the chosen decimal digit</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> Source code in <code>clayrs/content_analyzer/ratings_manager/score_processor.py</code> <pre><code>def __init__(self, scale: Tuple[float, float], decimal_rounding: int = None):\n    super().__init__(decimal_rounding)\n\n    if len(scale) != 2:\n        raise ValueError(\"The voting scale should be a tuple containing exactly two values,\"\n                         \"the minimum of the scale and the maximum!\")\n\n    self._old_min = scale[0]\n    self._old_max = scale[1]\n</code></pre>"},{"location":"content_analyzer/ratings/score_processors/#clayrs.content_analyzer.ratings_manager.score_processor.NumberNormalizer.fit","title":"<code>fit(score_data)</code>","text":"<p>Method which will normalize the given score</p> PARAMETER DESCRIPTION <code>score_data</code> <p>score that will be normalized</p> <p> TYPE: <code>float</code> </p> RETURNS DESCRIPTION <code>float</code> <p>score normalized in the interval \\([-1, 1]\\)</p> Source code in <code>clayrs/content_analyzer/ratings_manager/score_processor.py</code> <pre><code>def fit(self, score_data: float) -&gt; float:\n\"\"\"\n    Method which will normalize the given score\n\n    Args:\n        score_data: score that will be normalized\n\n    Returns:\n        score normalized in the interval $[-1, 1]$\n    \"\"\"\n    def convert_into_range(value: float, old_min: float, old_max: float, new_min: int = -1, new_max: int = 1):\n        new_value = ((value - old_min) / (old_max - old_min)) * (new_max - new_min) + new_min\n        if self.decimal_rounding:\n            new_value = np.round(new_value, self.decimal_rounding)\n\n        return new_value\n\n    return convert_into_range(float(score_data), self._old_min, self._old_max)\n</code></pre>"},{"location":"content_analyzer/ratings/score_processors/#clayrs.content_analyzer.ratings_manager.TextBlobSentimentAnalysis","title":"<code>TextBlobSentimentAnalysis(decimal_rounding=None)</code>","text":"<p>         Bases: <code>SentimentAnalysis</code></p> <p>Class that compute sentiment polarity on a textual field using TextBlob library.</p> <p>The given score will be in the \\([-1.0, 1.0]\\) range</p> Source code in <code>clayrs/content_analyzer/ratings_manager/sentiment_analysis.py</code> <pre><code>def __init__(self, decimal_rounding: int = None):\n    super().__init__(decimal_rounding)\n</code></pre>"},{"location":"content_analyzer/ratings/score_processors/#clayrs.content_analyzer.ratings_manager.sentiment_analysis.TextBlobSentimentAnalysis.fit","title":"<code>fit(score_data)</code>","text":"<p>This method calculates the sentiment polarity score on textual reviews</p> PARAMETER DESCRIPTION <code>score_data</code> <p>text for which sentiment polarity must be computed and considered as score</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>float</code> <p>The sentiment polarity of the textual data in range \\([-1.0, 1.0]\\)</p> Source code in <code>clayrs/content_analyzer/ratings_manager/sentiment_analysis.py</code> <pre><code>def fit(self, score_data: str) -&gt; float:\n\"\"\"\n    This method calculates the sentiment polarity score on textual reviews\n\n    Args:\n        score_data: text for which sentiment polarity must be computed and considered as score\n\n    Returns:\n        The sentiment polarity of the textual data in range $[-1.0, 1.0]$\n    \"\"\"\n    polarity_score = TextBlob(score_data).sentiment.polarity\n\n    if self.decimal_rounding:\n        polarity_score = round(polarity_score, self.decimal_rounding)\n\n    return polarity_score\n</code></pre>"},{"location":"evaluation/eval_model/","title":"Eval Model class","text":""},{"location":"evaluation/eval_model/#clayrs.evaluation.eval_model.EvalModel","title":"<code>EvalModel(pred_list, truth_list, metric_list)</code>","text":"<p>Class for evaluating a recommender system.</p> <p>The Evaluation module needs the following parameters:</p> <ul> <li>A list of computed rank/predictions (in case multiple splits must be evaluated)</li> <li>A list of truths (in case multiple splits must be evaluated)</li> <li>List of metrics to compute</li> </ul> <p>Obviously the list of computed rank/predictions and list of truths must have the same length, and the rank/prediction in position \\(i\\) will be compared with the truth at position \\(i\\)</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import clayrs.evaluation as eva\n&gt;&gt;&gt;\n&gt;&gt;&gt; em = eva.EvalModel(\n&gt;&gt;&gt;         pred_list=rank_list,\n&gt;&gt;&gt;         truth_list=truth_list,\n&gt;&gt;&gt;         metric_list=[\n&gt;&gt;&gt;             eva.NDCG(),\n&gt;&gt;&gt;             eva.Precision()\n&gt;&gt;&gt;             eva.RecallAtK(k=5, sys_average='micro')\n&gt;&gt;&gt;         ]\n&gt;&gt;&gt; )\n</code></pre> <p>Then call the fit() method of the instantiated EvalModel to perform the actual evaluation</p> PARAMETER DESCRIPTION <code>pred_list</code> <p>Recommendations list to evaluate. It's a list in case multiple splits must be evaluated. Both Rank objects (where items are ordered and the score is not relevant) or Prediction objects (where the score  predicted is the predicted rating for the user regarding a certain item) can be evaluated</p> <p> TYPE: <code>Union[List[Prediction], List[Rank]]</code> </p> <code>truth_list</code> <p>Ground truths list used to compare recommendations. It's a list in case multiple splits must be evaluated.</p> <p> TYPE: <code>List[Ratings]</code> </p> <code>metric_list</code> <p>List of metrics that will be used to evaluate recommendation list specified</p> <p> TYPE: <code>List[Metric]</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>ValueError is raised in case the pred_list and truth_list are empty or have different length</p> Source code in <code>clayrs/evaluation/eval_model.py</code> <pre><code>def __init__(self,\n             pred_list: Union[List[Prediction], List[Rank]],\n             truth_list: List[Ratings],\n             metric_list: List[Metric]):\n\n    if len(pred_list) == 0 and len(truth_list) == 0:\n        raise ValueError(\"List containing predictions and list containing ground truths are empty!\")\n    elif len(pred_list) != len(truth_list):\n        raise ValueError(\"List containing predictions and list containing ground truths must have the same length!\")\n\n    self._pred_list = pred_list\n    self._truth_list = truth_list\n    self._metric_list = metric_list\n\n    self._yaml_report_result = None\n</code></pre>"},{"location":"evaluation/eval_model/#clayrs.evaluation.eval_model.EvalModel.metric_list","title":"<code>metric_list: List[Metric]</code>  <code>property</code>","text":"<p>List of metrics used to evaluate recommendation lists</p> RETURNS DESCRIPTION <code>List[Metric]</code> <p>The list containing all metrics</p>"},{"location":"evaluation/eval_model/#clayrs.evaluation.eval_model.EvalModel.pred_list","title":"<code>pred_list: Union[List[Prediction], List[Rank]]</code>  <code>property</code>","text":"<p>List containing recommendations frame</p> RETURNS DESCRIPTION <code>Union[List[Prediction], List[Rank]]</code> <p>The list containing recommendations frame</p>"},{"location":"evaluation/eval_model/#clayrs.evaluation.eval_model.EvalModel.truth_list","title":"<code>truth_list: List[Ratings]</code>  <code>property</code>","text":"<p>List containing ground truths</p> RETURNS DESCRIPTION <code>List[Ratings]</code> <p>The list containing ground truths</p>"},{"location":"evaluation/eval_model/#clayrs.evaluation.eval_model.EvalModel.append_metric","title":"<code>append_metric(metric)</code>","text":"<p>Append a metric to the metric list that will be used to evaluate recommendation lists</p> PARAMETER DESCRIPTION <code>metric</code> <p>Metric to append to the metric list</p> <p> TYPE: <code>Metric</code> </p> Source code in <code>clayrs/evaluation/eval_model.py</code> <pre><code>def append_metric(self, metric: Metric):\n\"\"\"\n    Append a metric to the metric list that will be used to evaluate recommendation lists\n\n    Args:\n        metric: Metric to append to the metric list\n    \"\"\"\n    self._metric_list.append(metric)\n</code></pre>"},{"location":"evaluation/eval_model/#clayrs.evaluation.eval_model.EvalModel.fit","title":"<code>fit(user_id_list=None)</code>","text":"<p>This method performs the actual evaluation of the recommendation frames passed as input in the constructor of the class</p> <p>In case you want to perform evaluation for selected users, specify their ids parameter of this method. Otherwise, all users in the recommendation frames will be considered in the evaluation process</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import clayrs.evaluation as eva\n&gt;&gt;&gt; selected_users = ['u1', 'u22', 'u3'] # (1)\n&gt;&gt;&gt; em = eva.EvalModel(\n&gt;&gt;&gt;         pred_list,\n&gt;&gt;&gt;         truth_list,\n&gt;&gt;&gt;         metric_list=[eva.Precision(), eva.Recall()]\n&gt;&gt;&gt; )\n&gt;&gt;&gt; em.fit(selected_users)\n</code></pre> <p>The method returns two pandas DataFrame: one containing system results for every metric in the metric list, one containing users results for every metric eligible</p> PARAMETER DESCRIPTION <code>user_id_list</code> <p>list of string ids for the users to consider in the evaluation (note that only string ids are accepted and not their mapped integers)</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>pd.DataFrame</code> <p>The first DataFrame contains the system result for every metric inside the metric_list</p> <code>pd.DataFrame</code> <p>The second DataFrame contains every user results for every metric eligible inside the metric_list</p> Source code in <code>clayrs/evaluation/eval_model.py</code> <pre><code>def fit(self, user_id_list: Optional[List[str]] = None) -&gt; Tuple[pd.DataFrame, pd.DataFrame]:\n\"\"\"\n    This method performs the actual evaluation of the recommendation frames passed as input in the constructor of\n    the class\n\n    In case you want to perform evaluation for selected users, specify their ids parameter of this method.\n    Otherwise, all users in the recommendation frames will be considered in the evaluation process\n\n    Examples:\n\n        &gt;&gt;&gt; import clayrs.evaluation as eva\n        &gt;&gt;&gt; selected_users = ['u1', 'u22', 'u3'] # (1)\n        &gt;&gt;&gt; em = eva.EvalModel(\n        &gt;&gt;&gt;         pred_list,\n        &gt;&gt;&gt;         truth_list,\n        &gt;&gt;&gt;         metric_list=[eva.Precision(), eva.Recall()]\n        &gt;&gt;&gt; )\n        &gt;&gt;&gt; em.fit(selected_users)\n\n    The method returns two pandas DataFrame: one containing ***system results*** for every metric in the metric\n    list, one containing ***users results*** for every metric eligible\n\n    Args:\n        user_id_list: list of string ids for the users to consider in the evaluation (note that only string ids are\n            accepted and not their mapped integers)\n\n    Returns:\n        The first DataFrame contains the **system result** for every metric inside the metric_list\n\n        The second DataFrame contains every **user results** for every metric eligible inside the metric_list\n    \"\"\"\n    logger.info('Performing evaluation on metrics chosen')\n\n    final_pred_list = []\n    final_truth_list = []\n\n    # if user id list is passed, convert it to int if necessary and append the new ratings filtered with\n    # only the users of interest\n    if user_id_list is not None:\n\n        for pred, truth in zip(self._pred_list, self._truth_list):\n\n            split_users = user_id_list\n            split_truth_users = set(truth.user_map.convert_seq_str2int(split_users))\n            split_pred_users = set(pred.user_map.convert_seq_str2int(split_users))\n\n            final_pred_list.append(pred.filter_ratings(list(split_pred_users)))\n            final_truth_list.append(truth.filter_ratings(list(split_truth_users)))\n\n    # otherwise the original lists are kept\n    else:\n\n        final_pred_list = self._pred_list\n        final_truth_list = self._truth_list\n\n    sys_result, users_result = MetricEvaluator(final_pred_list, final_truth_list).eval_metrics(self.metric_list)\n\n    # we save the sys result for report yaml\n    self._yaml_report_result = sys_result.to_dict(orient='index')\n\n    return sys_result, users_result\n</code></pre>"},{"location":"evaluation/introduction/","title":"Introduction","text":"<p>Warning</p> <p>Docs are complete, but revision is still a Work in Progress. Sorry for any typos!</p>"},{"location":"evaluation/introduction/#introduction","title":"Introduction","text":"<p>The Evaluation module has the task of evaluating a recommender system, using several state-of-the-art metrics</p> <p>The usage pipeline it's pretty simple, all the work is done by the  <code>EvalModel</code> class class. Suppose you want to evaluate recommendation lists using NDCG, macro Precision, micro Recall@5, you need to instantiate the EvalModel class with the following parameters:</p> <ul> <li>A list of computed rank/predictions (in case multiple splits must be evaluated)</li> <li>A list of truths (in case multiple splits must be evaluated)</li> <li>List of metrics to compute</li> </ul> <p>Obviously the list of computed rank/predictions and list of truths must have the same length, and the rank/prediction in position \\(i\\) will be compared with the truth at position \\(i\\)</p>"},{"location":"evaluation/introduction/#usage-example","title":"Usage example","text":"<p>In this case <code>rank_list</code> and <code>truth_list</code> are results obtained from the RecSys module of the framework </p> <pre><code>import clayrs.evaluation as eva\n\nem = eva.EvalModel(\n    pred_list=rank_list,\n    truth_list=truth_list,\n    metric_list=[\n        eva.NDCG(),\n        eva.Precision(), # (1)\n        eva.RecallAtK(k=5, sys_average='micro')\n    ]\n)\n</code></pre> <ol> <li>If not specified, by default system average is computed as macro</li> </ol> <p>Info</p> <p><code>Precision</code>, <code>Recall</code>, and in general all classification metrics require a threshold which separates relevant items from non-relevant.</p> <ul> <li>If a threshold is specified, then it is fixed for all users</li> <li>If no threshold is specified, the mean rating score of each user will be used</li> </ul> <p>Check documentation of each metric for more</p> <p>Then simply call the <code>fit()</code> method of the instantiated object</p> <ul> <li>It will return two pandas DataFrame: the first one contains the metrics aggregated for the system, while the second contains the metrics computed for each user (where possible)</li> </ul> <pre><code>sys_result, users_result =  em.fit()\n</code></pre>"},{"location":"evaluation/introduction/#evaluating-external-recommendation-lists","title":"Evaluating external recommendation lists","text":"<p>The evaluation module is completely independent from the Recsys and Content Analyzer module: that means that we can easily evaluate recommendation lists computed by other frameworks/tools!</p> <p>Let's suppose we have recommendations (and related truths) generated via other tools in a csv format. We first import them into the framework and then pass them to the <code>EvalModel</code> class</p> <pre><code>import clayrs.content_analyzer as ca\n\ncsv_rank_1 = ca.CSVFile('rank_split_1.csv')\ncsv_truth_1 = ca.CSVFile('truth_split_1.csv')\n\ncsv_rank_2 = ca.CSVFile('rank_split_2.csv')\ncsv_truth_2 = ca.CSVFile('truth_split_2.csv')\n\n# Importing split 1 (1)\nrank_1 = ca.Rank(csv_rank_1)\ntruth_1 = ca.Ratings(csv_truth_1)\n\n# Importing split 2 (2)\nrank_2 = ca.Rank(csv_rank_2)\ntruth_2 = ca.Ratings(csv_truth_2)\n\n# since multiple splits, we wrap ranks and truths in lists\nimported_ranks = [rank_1, rank_2]\nimported_truths = [truth_1, truth_2]\n</code></pre> <ol> <li> <p>Remember that this instantiation to the <code>Rank/Ratings</code> class assumes a certain order of the columns of your raw source. Otherwise, you need to manually map columns.  Check related documentation for more</p> </li> <li> <p>Remember that this instantiation to the <code>Rank/Ratings</code> class assumes a certain order of the columns of your raw source. Otherwise, you need to manually map columns. Check related documentation for more</p> </li> </ol> <p>Then simply evaluate them exactly in the same way as shown before! <pre><code>import clayrs.evaluation as eva\n\nem = eva.EvalModel(\n    pred_list=imported_ranks,\n    truth_list=imported_truths,\n    metric_list=[\n        # ... Choose your own metrics\n    ]\n)\n\nsys_results_df, users_results_df = em.fit()\n</code></pre></p>"},{"location":"evaluation/introduction/#perform-a-statistical-test","title":"Perform a statistical test","text":"<p>ClayRS lets you also compare different learning schemas by performing statistical tests:</p> <ul> <li>Simply instantiate the desired test and call its <code>perform()</code> method. The parameter it expects is the list of <code>user_results</code> dataframe obtained in the evaluation step, one for each learning schema to compare.</li> </ul> <pre><code>ttest = eva.Ttest()\n\nall_comb_df = ttest.perform([user_result1, user_result2, user_result3])\n</code></pre> <p>Info</p> <p>In this case since the Ttest it's a paired test, the final result is a pandas DataFrame which contains learning schemas compared in pair:</p> <ul> <li>(system1, system2)</li> <li>(system1, system3)</li> <li>(system2, system3)</li> </ul>"},{"location":"evaluation/metrics/classification_metrics/","title":"Classification metrics","text":"<p>A classification metric uses confusion matrix terminology (true positive, false positive, true negative, false negative) to classify each item predicted, and in general it needs a way to discern relevant items from non-relevant items for users</p>"},{"location":"evaluation/metrics/classification_metrics/#clayrs.evaluation.metrics.classification_metrics.FMeasure","title":"<code>FMeasure(beta=1, relevant_threshold=None, sys_average='macro', precision=np.float64)</code>","text":"<p>         Bases: <code>ClassificationMetric</code></p> <p>The FMeasure metric combines Precision and Recall into a single metric. It is calculated as such for the single user:</p> \\[ FMeasure_u = (1 + \\beta^2) \\cdot \\frac{P_u \\cdot R_u}{(\\beta^2 \\cdot P_u) + R_u} \\] <p>Where:</p> <ul> <li>\\(P_u\\) is the Precision calculated for the user u</li> <li>\\(R_u\\) is the Recall calculated for the user u</li> <li> <p>\\(\\beta\\) is a real factor which could weight differently Recall or Precision based on its value:</p> <ul> <li>\\(\\beta = 1\\): Equally weight Precision and Recall</li> <li>\\(\\beta &gt; 1\\): Weight Recall more</li> <li>\\(\\beta &lt; 1\\): Weight Precision more</li> </ul> </li> </ul> <p>A famous FMeasure is the F1 Metric, where \\(\\beta = 1\\), which basically is the harmonic mean of recall and precision:</p> \\[ F1_u = \\frac{2 \\cdot P_u \\cdot R_u}{P_u + R_u} \\] <p>The FMeasure metric is calculated as such for the entire system, depending if 'macro' average or 'micro' average has been chosen:</p> \\[ FMeasure_{sys} - micro = (1 + \\beta^2) \\cdot \\frac{P_u \\cdot R_u}{(\\beta^2 \\cdot P_u) + R_u} \\] \\[ FMeasure_{sys} - macro = \\frac{\\sum_{u \\in U} FMeasure_u}{|U|} \\] PARAMETER DESCRIPTION <code>beta</code> <p>real factor which could weight differently Recall or Precision based on its value. Default is 1</p> <p> TYPE: <code>float</code> DEFAULT: <code>1</code> </p> <code>relevant_threshold</code> <p>parameter needed to discern relevant items and non-relevant items for every user. If not specified, the mean rating score of every user will be used</p> <p> TYPE: <code>float</code> DEFAULT: <code>None</code> </p> <code>sys_average</code> <p>specify how the system average must be computed. Default is 'macro'</p> <p> TYPE: <code>str</code> DEFAULT: <code>'macro'</code> </p> Source code in <code>clayrs/evaluation/metrics/classification_metrics.py</code> <pre><code>def __init__(self, beta: float = 1, relevant_threshold: float = None, sys_average: str = 'macro',\n             precision: [Callable] = np.float64):\n    super().__init__(relevant_threshold, sys_average, precision)\n    self.__beta = beta\n</code></pre>"},{"location":"evaluation/metrics/classification_metrics/#clayrs.evaluation.metrics.classification_metrics.FMeasureAtK","title":"<code>FMeasureAtK(k, beta=1, relevant_threshold=None, sys_average='macro')</code>","text":"<p>         Bases: <code>FMeasure</code></p> <p>The FMeasure@K metric combines Precision@K and Recall@K into a single metric. It is calculated as such for the single user:</p> \\[ FMeasure@K_u = (1 + \\beta^2) \\cdot \\frac{P@K_u \\cdot R@K_u}{(\\beta^2 \\cdot P@K_u) + R@K_u} \\] <p>Where:</p> <ul> <li>\\(P@K_u\\) is the Precision at K calculated for the user u</li> <li>\\(R@K_u\\) is the Recall at K calculated for the user u</li> <li> <p>\\(\\beta\\) is a real factor which could weight differently Recall or Precision based on its value:</p> <ul> <li>\\(\\beta = 1\\): Equally weight Precision and Recall</li> <li>\\(\\beta &gt; 1\\): Weight Recall more</li> <li>\\(\\beta &lt; 1\\): Weight Precision more</li> </ul> </li> </ul> <p>A famous FMeasure@K is the F1@K Metric, where :math:<code>\\beta = 1</code>, which basically is the harmonic mean of recall and precision:</p> \\[ F1@K_u = \\frac{2 \\cdot P@K_u \\cdot R@K_u}{P@K_u + R@K_u} \\] <p>The FMeasure@K metric is calculated as such for the entire system, depending if 'macro' average or 'micro' average has been chosen:</p> \\[ FMeasure@K_{sys} - micro = (1 + \\beta^2) \\cdot \\frac{P@K_u \\cdot R@K_u}{(\\beta^2 \\cdot P@K_u) + R@K_u} \\] \\[ FMeasure@K_{sys} - macro = \\frac{\\sum_{u \\in U} FMeasure@K_u}{|U|} \\] PARAMETER DESCRIPTION <code>k</code> <p>cutoff parameter. Will be used for the computation of Precision@K and Recall@K</p> <p> TYPE: <code>int</code> </p> <code>beta</code> <p>real factor which could weight differently Recall or Precision based on its value. Default is 1</p> <p> TYPE: <code>float</code> DEFAULT: <code>1</code> </p> <code>relevant_threshold</code> <p>parameter needed to discern relevant items and non-relevant items for every user. If not specified, the mean rating score of every user will be used</p> <p> TYPE: <code>float</code> DEFAULT: <code>None</code> </p> <code>sys_average</code> <p>specify how the system average must be computed. Default is 'macro'</p> <p> TYPE: <code>str</code> DEFAULT: <code>'macro'</code> </p> Source code in <code>clayrs/evaluation/metrics/classification_metrics.py</code> <pre><code>def __init__(self, k: int, beta: int = 1, relevant_threshold: float = None, sys_average: str = 'macro'):\n    super().__init__(beta, relevant_threshold, sys_average)\n    if k &lt; 1:\n        raise ValueError('k={} not valid! k must be &gt;= 1!'.format(k))\n    self.__k = k\n</code></pre>"},{"location":"evaluation/metrics/classification_metrics/#clayrs.evaluation.metrics.classification_metrics.Precision","title":"<code>Precision(relevant_threshold=None, sys_average='macro', precision=np.float64)</code>","text":"<p>         Bases: <code>ClassificationMetric</code></p> <p>The Precision metric is calculated as such for the single user:</p> \\[ Precision_u = \\frac{tp_u}{tp_u + fp_u} \\] <p>Where:</p> <ul> <li>\\(tp_u\\) is the number of items which are in the recommendation list of the user and have a   rating &gt;= relevant_threshold in its 'ground truth'</li> <li>\\(fp_u\\) is the number of items which are in the recommendation list of the user and have a   rating &lt; relevant_threshold in its 'ground truth'</li> </ul> <p>And it is calculated as such for the entire system, depending if 'macro' average or 'micro' average has been chosen:</p> \\[  Precision_{sys} - micro = \\frac{\\sum_{u \\in U} tp_u}{\\sum_{u \\in U} tp_u + \\sum_{u \\in U} fp_u} \\] \\[ Precision_{sys} - macro = \\frac{\\sum_{u \\in U} Precision_u}{|U|} \\] PARAMETER DESCRIPTION <code>relevant_threshold</code> <p>parameter needed to discern relevant items and non-relevant items for every user. If not specified, the mean rating score of every user will be used</p> <p> TYPE: <code>float</code> DEFAULT: <code>None</code> </p> <code>sys_average</code> <p>specify how the system average must be computed. Default is 'macro'</p> <p> TYPE: <code>str</code> DEFAULT: <code>'macro'</code> </p> Source code in <code>clayrs/evaluation/metrics/classification_metrics.py</code> <pre><code>def __init__(self, relevant_threshold: float = None, sys_average: str = 'macro',\n             precision: [Callable] = np.float64):\n    super(Precision, self).__init__(relevant_threshold, sys_average, precision)\n</code></pre>"},{"location":"evaluation/metrics/classification_metrics/#clayrs.evaluation.metrics.classification_metrics.PrecisionAtK","title":"<code>PrecisionAtK(k, relevant_threshold=None, sys_average='macro', precision=np.float64)</code>","text":"<p>         Bases: <code>Precision</code></p> <p>The Precision@K metric is calculated as such for the single user:</p> \\[ Precision@K_u = \\frac{tp@K_u}{tp@K_u + fp@K_u} \\] <p>Where:</p> <ul> <li>\\(tp@K_u\\) is the number of items which are in the recommendation list  of the user   cutoff to the first K items and have a rating &gt;= relevant_threshold in its 'ground truth'</li> <li>\\(tp@K_u\\) is the number of items which are in the recommendation list  of the user   cutoff to the first K items and have a rating &lt; relevant_threshold in its 'ground truth'</li> </ul> <p>And it is calculated as such for the entire system, depending if 'macro' average or 'micro' average has been chosen:</p> \\[ Precision@K_{sys} - micro = \\frac{\\sum_{u \\in U} tp@K_u}{\\sum_{u \\in U} tp@K_u + \\sum_{u \\in U} fp@K_u} \\] \\[ Precision@K_{sys} - macro = \\frac{\\sum_{u \\in U} Precision@K_u}{|U|} \\] PARAMETER DESCRIPTION <code>k</code> <p>cutoff parameter. Only the first k items of the recommendation list will be considered</p> <p> TYPE: <code>int</code> </p> <code>relevant_threshold</code> <p>parameter needed to discern relevant items and non-relevant items for every user. If not specified, the mean rating score of every user will be used</p> <p> TYPE: <code>float</code> DEFAULT: <code>None</code> </p> <code>sys_average</code> <p>specify how the system average must be computed. Default is 'macro'</p> <p> TYPE: <code>str</code> DEFAULT: <code>'macro'</code> </p> Source code in <code>clayrs/evaluation/metrics/classification_metrics.py</code> <pre><code>def __init__(self, k: int, relevant_threshold: float = None, sys_average: str = 'macro',\n             precision: [Callable] = np.float64):\n    super().__init__(relevant_threshold, sys_average, precision)\n    if k &lt; 1:\n        raise ValueError('k={} not valid! k must be &gt;= 1!'.format(k))\n    self.__k = k\n</code></pre>"},{"location":"evaluation/metrics/classification_metrics/#clayrs.evaluation.metrics.classification_metrics.RPrecision","title":"<code>RPrecision(relevant_threshold=None, sys_average='macro', precision=np.float64)</code>","text":"<p>         Bases: <code>Precision</code></p> <p>The R-Precision metric is calculated as such for the single user:</p> \\[ R-Precision_u = \\frac{tp@R_u}{tp@R_u + fp@R_u} \\] <p>Where:</p> <ul> <li>\\(R\\) it's the number of relevant items for the user u</li> <li>\\(tp@R_u\\) is the number of items which are in the recommendation list  of the user   cutoff to the first R items and have a rating &gt;= relevant_threshold in its 'ground truth'</li> <li>\\(tp@R_u\\) is the number of items which are in the recommendation list  of the user   cutoff to the first R items and have a rating &lt; relevant_threshold in its 'ground truth'</li> </ul> <p>And it is calculated as such for the entire system, depending if 'macro' average or 'micro' average has been chosen:</p> \\[ Precision@R_{sys} - micro = \\frac{\\sum_{u \\in U} tp@R_u}{\\sum_{u \\in U} tp@R_u + \\sum_{u \\in U} fp@R_u} \\] \\[ Precision@R_{sys} - macro = \\frac{\\sum_{u \\in U} R-Precision_u}{|U|} \\] PARAMETER DESCRIPTION <code>relevant_threshold</code> <p>parameter needed to discern relevant items and non-relevant items for every user. If not specified, the mean rating score of every user will be used</p> <p> TYPE: <code>float</code> DEFAULT: <code>None</code> </p> <code>sys_average</code> <p>specify how the system average must be computed. Default is 'macro'</p> <p> TYPE: <code>str</code> DEFAULT: <code>'macro'</code> </p> Source code in <code>clayrs/evaluation/metrics/classification_metrics.py</code> <pre><code>def __init__(self, relevant_threshold: float = None, sys_average: str = 'macro',\n             precision: [Callable] = np.float64):\n    super().__init__(relevant_threshold, sys_average, precision)\n</code></pre>"},{"location":"evaluation/metrics/classification_metrics/#clayrs.evaluation.metrics.classification_metrics.Recall","title":"<code>Recall(relevant_threshold=None, sys_average='macro', precision=np.float64)</code>","text":"<p>         Bases: <code>ClassificationMetric</code></p> <p>The Recall metric is calculated as such for the single user:</p> \\[ Recall_u = \\frac{tp_u}{tp_u + fn_u} \\] <p>Where:</p> <ul> <li>\\(tp_u\\) is the number of items which are in the recommendation list of the user and have a   rating &gt;= relevant_threshold in its 'ground truth'</li> <li>\\(fn_u\\) is the number of items which are NOT in the recommendation list of the user and have a   rating &gt;= relevant_threshold in its 'ground truth'</li> </ul> <p>And it is calculated as such for the entire system, depending if 'macro' average or 'micro' average has been chosen:</p> \\[ Recall_{sys} - micro = \\frac{\\sum_{u \\in U} tp_u}{\\sum_{u \\in U} tp_u + \\sum_{u \\in U} fn_u} \\] \\[ Recall_{sys} - macro = \\frac{\\sum_{u \\in U} Recall_u}{|U|} \\] PARAMETER DESCRIPTION <code>relevant_threshold</code> <p>parameter needed to discern relevant items and non-relevant items for every user. If not specified, the mean rating score of every user will be used</p> <p> TYPE: <code>float</code> DEFAULT: <code>None</code> </p> <code>sys_average</code> <p>specify how the system average must be computed. Default is 'macro'</p> <p> TYPE: <code>str</code> DEFAULT: <code>'macro'</code> </p> Source code in <code>clayrs/evaluation/metrics/classification_metrics.py</code> <pre><code>def __init__(self, relevant_threshold: float = None, sys_average: str = 'macro',\n             precision: [Callable] = np.float64):\n    super().__init__(relevant_threshold, sys_average, precision)\n</code></pre>"},{"location":"evaluation/metrics/classification_metrics/#clayrs.evaluation.metrics.classification_metrics.RecallAtK","title":"<code>RecallAtK(k, relevant_threshold=None, sys_average='macro', precision=np.float64)</code>","text":"<p>         Bases: <code>Recall</code></p> <p>The Recall@K metric is calculated as such for the single user:</p> \\[ Recall@K_u = \\frac{tp@K_u}{tp@K_u + fn@K_u} \\] <p>Where:</p> <ul> <li>\\(tp@K_u\\) is the number of items which are in the recommendation list  of the user   cutoff to the first K items and have a rating &gt;= relevant_threshold in its 'ground truth'</li> <li>\\(tp@K_u\\) is the number of items which are NOT in the recommendation list  of the user   cutoff to the first K items and have a rating &gt;= relevant_threshold in its 'ground truth'</li> </ul> <p>And it is calculated as such for the entire system, depending if 'macro' average or 'micro' average has been chosen:</p> \\[ Recall@K_{sys} - micro = \\frac{\\sum_{u \\in U} tp@K_u}{\\sum_{u \\in U} tp@K_u + \\sum_{u \\in U} fn@K_u} \\] \\[ Recall@K_{sys} - macro = \\frac{\\sum_{u \\in U} Recall@K_u}{|U|} \\] PARAMETER DESCRIPTION <code>k</code> <p>cutoff parameter. Only the first k items of the recommendation list will be considered</p> <p> TYPE: <code>int</code> </p> <code>relevant_threshold</code> <p>parameter needed to discern relevant items and non-relevant items for every user. If not specified, the mean rating score of every user will be used</p> <p> TYPE: <code>float</code> DEFAULT: <code>None</code> </p> <code>sys_average</code> <p>specify how the system average must be computed. Default is 'macro'</p> <p> TYPE: <code>str</code> DEFAULT: <code>'macro'</code> </p> Source code in <code>clayrs/evaluation/metrics/classification_metrics.py</code> <pre><code>def __init__(self, k: int, relevant_threshold: float = None, sys_average: str = 'macro',\n             precision: [Callable] = np.float64):\n    super().__init__(relevant_threshold, sys_average, precision)\n    if k &lt; 1:\n        raise ValueError('k={} not valid! k must be &gt;= 1!'.format(k))\n    self.__k = k\n</code></pre>"},{"location":"evaluation/metrics/error_metrics/","title":"Error metrics","text":"<p>Error metrics evaluate 'how wrong' the recommender system was in predicting a rating</p>"},{"location":"evaluation/metrics/error_metrics/#clayrs.evaluation.metrics.error_metrics.MAE","title":"<code>MAE</code>","text":"<p>         Bases: <code>ErrorMetric</code></p> <p>The MAE (Mean Absolute Error) metric is calculated as such for the single user:</p> \\[ MAE_u = \\sum_{i \\in T_u} \\frac{|r_{u,i} - \\hat{r}_{u,i}|}{|T_u|} \\] <p>Where:</p> <ul> <li>\\(T_u\\) is the test set of the user \\(u\\)</li> <li>\\(r_{u, i}\\) is the actual score give by user \\(u\\) to item \\(i\\)</li> <li>\\(\\hat{r}_{u, i}\\) is the predicted score give by user \\(u\\) to item \\(i\\)</li> </ul> <p>And it is calculated as such for the entire system:</p> \\[ MAE_{sys} = \\sum_{u \\in T} \\frac{MAE_u}{|T|} \\] <p>Where:</p> <ul> <li>\\(T\\) is the test set</li> <li>\\(MAE_u\\) is the MAE calculated for user \\(u\\)</li> </ul> <p>There may be cases in which some items of the test set of the user could not be predicted (eg. A CBRS was chosen and items were not present locally, a methodology different than TestRatings was chosen).</p> <p>In those cases the \\(MAE_u\\) formula becomes</p> \\[ MAE_u = \\sum_{i \\in T_u} \\frac{|r_{u,i} - \\hat{r}_{u,i}|}{|T_u| - unk} \\] <p>Where:</p> <ul> <li>\\(unk\\) (unknown) is the number of items of the user test set that could not be predicted</li> </ul> <p>If no items of the user test set has been predicted (\\(|T_u| - unk = 0\\)), then:</p> \\[ MAE_u = NaN \\]"},{"location":"evaluation/metrics/error_metrics/#clayrs.evaluation.metrics.error_metrics.MSE","title":"<code>MSE</code>","text":"<p>         Bases: <code>ErrorMetric</code></p> <p>The MSE (Mean Squared Error) metric is calculated as such for the single user:</p> \\[ MSE_u = \\sum_{i \\in T_u} \\frac{(r_{u,i} - \\hat{r}_{u,i})^2}{|T_u|} \\] <p>Where:</p> <ul> <li>\\(T_u\\) is the test set of the user \\(u\\)</li> <li>\\(r_{u, i}\\) is the actual score give by user \\(u\\) to item \\(i\\)</li> <li>\\(\\hat{r}_{u, i}\\) is the predicted score give by user \\(u\\) to item \\(i\\)</li> </ul> <p>And it is calculated as such for the entire system:</p> <p>$$ MSE_{sys} = \\sum_{u \\in T} \\frac{MSE_u}{|T|} $$ Where:</p> <ul> <li>\\(T\\) is the test set</li> <li>\\(MSE_u\\) is the MSE calculated for user \\(u\\)</li> </ul> <p>There may be cases in which some items of the test set of the user could not be predicted (eg. A CBRS was chosen and items were not present locally)</p> <p>In those cases the \\(MSE_u\\) formula becomes</p> \\[ MSE_u = \\sum_{i \\in T_u} \\frac{(r_{u,i} - \\hat{r}_{u,i})^2}{|T_u| - unk} \\] <p>Where:</p> <ul> <li>\\(unk\\) (unknown) is the number of items of the user test set that could not be predicted</li> </ul> <p>If no items of the user test set has been predicted (\\(|T_u| - unk = 0\\)), then:</p> \\[ MSE_u = NaN \\]"},{"location":"evaluation/metrics/error_metrics/#clayrs.evaluation.metrics.error_metrics.RMSE","title":"<code>RMSE</code>","text":"<p>         Bases: <code>ErrorMetric</code></p> <p>The RMSE (Root Mean Squared Error) metric is calculated as such for the single user:</p> \\[ RMSE_u = \\sqrt{\\sum_{i \\in T_u} \\frac{(r_{u,i} - \\hat{r}_{u,i})^2}{|T_u|}} \\] <p>Where:</p> <ul> <li>\\(T_u\\) is the test set of the user \\(u\\)</li> <li>\\(r_{u, i}\\) is the actual score give by user \\(u\\) to item \\(i\\)</li> <li>\\(\\hat{r}_{u, i}\\) is the predicted score give by user \\(u\\) to item \\(i\\)</li> </ul> <p>And it is calculated as such for the entire system:</p> \\[ RMSE_{sys} = \\sum_{u \\in T} \\frac{RMSE_u}{|T|} \\] <p>Where:</p> <ul> <li>\\(T\\) is the test set</li> <li>\\(RMSE_u\\) is the RMSE calculated for user \\(u\\)</li> </ul> <p>There may be cases in which some items of the test set of the user could not be predicted (eg. A CBRS was chosen and items were not present locally, a methodology different than TestRatings was chosen).</p> <p>In those cases the \\(RMSE_u\\) formula becomes</p> \\[ RMSE_u = \\sqrt{\\sum_{i \\in T_u} \\frac{(r_{u,i} - \\hat{r}_{u,i})^2}{|T_u| - unk}} \\] <p>Where:</p> <ul> <li>\\(unk\\) (unknown) is the number of items of the user test set that could not be predicted</li> </ul> <p>If no items of the user test set has been predicted (\\(|T_u| - unk = 0\\)), then:</p> \\[ RMSE_u = NaN \\]"},{"location":"evaluation/metrics/fairness_metrics/","title":"Fairness metrics","text":"<p>Fairness metrics evaluate how unbiased the recommendation lists are (e.g. unbiased towards popularity of the items)</p>"},{"location":"evaluation/metrics/fairness_metrics/#clayrs.evaluation.metrics.fairness_metrics.CatalogCoverage","title":"<code>CatalogCoverage(catalog, top_n=None, k=None)</code>","text":"<p>         Bases: <code>PredictionCoverage</code></p> <p>The Catalog Coverage metric measures in percentage how many distinct items are being recommended in relation to all available items. It's a system wide metric, so only its result it will be returned and not those of every user. It differs from the Prediction Coverage since it allows for different parameters to come into play. If no parameter is passed then it's a simple Prediction Coverage. The metric is calculated as such:</p> \\[ Catalog Coverage_{sys} = (\\frac{|\\bigcup_{j=1...N}reclist(u_j)|}{|I|})\\cdot100 \\] <p>Where:</p> <ul> <li>\\(N\\) is the total number of users</li> <li>\\(reclist(u_j)\\) is the set of items contained in the recommendation list of user \\(j\\)</li> <li>\\(I\\) is the set of all available items</li> </ul> <p>The \\(I\\) must be specified through the 'catalog' parameter</p> <p>The recommendation list of every user (\\(reclist(u_j)\\)) can be reduced to the first n parameter with the top-n parameter, so that catalog coverage is measured considering only the most highest ranked items.</p> <p>With the 'k' parameter one could specify the number of users that will be used to calculate catalog coverage: k users will be randomly sampled and their recommendation lists will be used. The formula above becomes:</p> \\[ Catalog Coverage_{sys} = (\\frac{|\\bigcup_{j=1...k}reclist(u_j)|}{|I|})\\cdot100 \\] <p>Where:</p> <ul> <li>\\(k\\) is the parameter specified</li> </ul> <p>Obviously 'k' &lt; N, else simply recommendation lists of all users will be used</p> <p>Check the 'Beyond Accuracy: Evaluating Recommender Systems  by Coverage and Serendipity' paper and page 13 of the 'Comparison of group recommendation algorithms' paper for more</p> PARAMETER DESCRIPTION <code>catalog</code> <p>set of item id of the catalog on which the prediction coverage must be computed</p> <p> TYPE: <code>Set[str]</code> </p> <code>top_n</code> <p>it's a cutoff parameter, if specified the Catalog Coverage will be calculated considering only the first 'n' items of every recommendation list of all users. Default is None</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>k</code> <p>number of users randomly sampled. If specified, k users will be randomly sampled across all users and only their recommendation lists will be used to compute the CatalogCoverage</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> Source code in <code>clayrs/evaluation/metrics/fairness_metrics.py</code> <pre><code>def __init__(self, catalog: Set[str], top_n: int = None, k: int = None):\n    super().__init__(catalog)\n    self.__top_n = top_n\n    self.__k = k\n</code></pre>"},{"location":"evaluation/metrics/fairness_metrics/#clayrs.evaluation.metrics.fairness_metrics.DeltaGap","title":"<code>DeltaGap(user_groups, user_profiles, original_ratings, top_n=None, pop_percentage=0.2)</code>","text":"<p>         Bases: <code>GroupFairnessMetric</code></p> <p>The Delta GAP (Group Average popularity) metric lets you compare the average popularity \"requested\" by one or multiple groups of users and the average popularity \"obtained\" with the recommendation given by the recsys. It's a system wise metric and results of every group will be returned.</p> <p>It is calculated as such:</p> \\[ \\Delta GAP = \\frac{recs_GAP - profile_GAP}{profile_GAP} \\] <p>Users are split into groups based on the user_groups parameter, which contains names of the groups as keys, and percentage of how many user must contain a group as values. For example:</p> <pre><code>user_groups = {'popular_users': 0.3, 'medium_popular_users': 0.2, 'low_popular_users': 0.5}\n</code></pre> <p>Every user will be inserted in a group based on how many popular items the user has rated (in relation to the percentage of users we specified as value in the dictionary):</p> <ul> <li>users with many popular items will be inserted into the first group</li> <li>users with niche items rated will be inserted into one of the last groups.</li> </ul> <p>In general users are grouped by \\(Popularity\\_ratio\\) in a descending order. \\(Popularity\\_ratio\\) for a single user \\(u\\) is defined as:</p> \\[ Popularity\\_ratio_u = n\\_most\\_popular\\_items\\_rated_u / n\\_items\\_rated_u \\] <p>The most popular items are the first <code>pop_percentage</code>% items of all items ordered in a descending order by popularity.</p> <p>The popularity of an item is defined as the number of times it is rated in the <code>original_ratings</code> parameter divided by the total number of users in the <code>original_ratings</code>.</p> <p>It can happen that for a particular user of a group no recommendation are available: in that case it will be skipped and it won't be considered in the \\(\\Delta GAP\\) computation of its group. In case no user of a group has recs available, a warning will be printed and the whole group won't be considered.</p> <p>If the 'top_n' parameter is specified, then the \\(\\Delta GAP\\) will be calculated considering only the first n items of every recommendation list of all users</p> PARAMETER DESCRIPTION <code>user_groups</code> <p>Dict containing group names as keys and percentage of users as value, used to split users in groups. Users with more popular items rated are grouped into the first group, users with slightly less popular items rated are grouped into the second one, etc.</p> <p> TYPE: <code>Dict[str, float]</code> </p> <code>user_profiles</code> <p>one or more <code>Ratings</code> objects containing interactions of the profile of each user (e.g. the train set). It should be one for each split to evaluate!</p> <p> TYPE: <code>Union[list, Ratings]</code> </p> <code>original_ratings</code> <p><code>Ratings</code> object containing original interactions of the dataset that will be used to compute the popularity of each item (i.e. the number of times it is rated divided by the total number of users)</p> <p> TYPE: <code>Ratings</code> </p> <code>top_n</code> <p>it's a cutoff parameter, if specified the Gini index will be calculated considering only their first 'n' items of every recommendation list of all users. Default is None</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>pop_percentage</code> <p>How many (in percentage) most popular items must be considered. Default is 0.2</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.2</code> </p> Source code in <code>clayrs/evaluation/metrics/fairness_metrics.py</code> <pre><code>def __init__(self, user_groups: Dict[str, float], user_profiles: Union[list, Ratings], original_ratings: Ratings,\n             top_n: int = None, pop_percentage: float = 0.2):\n    if not 0 &lt; pop_percentage &lt;= 1:\n        raise ValueError('Incorrect percentage! Valid percentage range: 0 &lt; percentage &lt;= 1')\n\n    super().__init__(user_groups)\n    self._pop_by_item = get_item_popularity(original_ratings)\n\n    if not isinstance(user_profiles, list):\n        user_profiles = [user_profiles]\n    self._user_profiles = user_profiles\n    self.__top_n = top_n\n    self._pop_percentage = pop_percentage\n</code></pre>"},{"location":"evaluation/metrics/fairness_metrics/#clayrs.evaluation.metrics.fairness_metrics.DeltaGap.calculate_delta_gap","title":"<code>calculate_delta_gap(recs_gap, profile_gap)</code>  <code>staticmethod</code>","text":"<p>Compute the ratio between the recommendation gap and the user profiles gap</p> PARAMETER DESCRIPTION <code>recs_gap</code> <p>recommendation gap</p> <p> TYPE: <code>float</code> </p> <code>profile_gap</code> <p>user profiles gap</p> <p> TYPE: <code>float</code> </p> RETURNS DESCRIPTION <code>score</code> <p>delta gap measure</p> <p> TYPE: <code>float</code> </p> Source code in <code>clayrs/evaluation/metrics/fairness_metrics.py</code> <pre><code>@staticmethod\ndef calculate_delta_gap(recs_gap: float, profile_gap: float) -&gt; float:\n\"\"\"\n    Compute the ratio between the recommendation gap and the user profiles gap\n\n    Args:\n        recs_gap: recommendation gap\n        profile_gap: user profiles gap\n\n    Returns:\n        score: delta gap measure\n    \"\"\"\n    result = 0\n    if profile_gap != 0.0:\n        result = (recs_gap - profile_gap) / profile_gap\n\n    return result\n</code></pre>"},{"location":"evaluation/metrics/fairness_metrics/#clayrs.evaluation.metrics.fairness_metrics.DeltaGap.calculate_gap","title":"<code>calculate_gap(group, avg_pop_by_users)</code>  <code>staticmethod</code>","text":"<p>Compute the GAP (Group Average Popularity) formula</p> \\[ GAP = \\frac{\\sum_{u \\in U}\\cdot \\frac{\\sum_{i \\in i_u} pop_i}{|i_u|}}{|G|} \\] <p>Where:</p> <ul> <li>\\(G\\) is the set of users</li> <li>\\(i_u\\) is the set of items rated/recommended by/to user \\(u\\)</li> <li>\\(pop_i\\) is the popularity of item i</li> </ul> PARAMETER DESCRIPTION <code>group</code> <p>the set of users (user_id)</p> <p> TYPE: <code>Set[str]</code> </p> <code>avg_pop_by_users</code> <p>average popularity by user</p> <p> TYPE: <code>Dict[str, object]</code> </p> RETURNS DESCRIPTION <code>score</code> <p>gap score</p> <p> TYPE: <code>float</code> </p> Source code in <code>clayrs/evaluation/metrics/fairness_metrics.py</code> <pre><code>@staticmethod\ndef calculate_gap(group: Set[str], avg_pop_by_users: Dict[str, object]) -&gt; float:\nr\"\"\"\n    Compute the GAP (Group Average Popularity) formula\n\n    $$\n    GAP = \\frac{\\sum_{u \\in U}\\cdot \\frac{\\sum_{i \\in i_u} pop_i}{|i_u|}}{|G|}\n    $$\n\n    Where:\n\n    - $G$ is the set of users\n    - $i_u$ is the set of items rated/recommended by/to user $u$\n    - $pop_i$ is the popularity of item i\n\n    Args:\n        group: the set of users (user_id)\n        avg_pop_by_users: average popularity by user\n\n    Returns:\n        score (float): gap score\n    \"\"\"\n    total_pop = 0\n    for user in group:\n        if avg_pop_by_users.get(user):\n            total_pop += avg_pop_by_users[user]\n    return total_pop / len(group)\n</code></pre>"},{"location":"evaluation/metrics/fairness_metrics/#clayrs.evaluation.metrics.fairness_metrics.GiniIndex","title":"<code>GiniIndex(top_n=None)</code>","text":"<p>         Bases: <code>FairnessMetric</code></p> <p>The Gini Index metric measures inequality in recommendation lists. It's a system wide metric, so only its result it will be returned and not those of every user. The metric is calculated as such:</p> \\[ Gini_{sys} = \\frac{\\sum_i(2i - n - 1)x_i}{n\\cdot\\sum_i x_i} \\] <p>Where:</p> <ul> <li>\\(n\\) is the total number of distinct items that are being recommended</li> <li>\\(x_i\\) is the number of times that the item \\(i\\) has been recommended</li> </ul> <p>A perfectly equal recommender system should recommend every item the same number of times, in which case the Gini index would be equal to 0. The more the recsys is \"disegual\", the more the Gini Index is closer to 1</p> <p>If the 'top_n' parameter is specified, then the Gini index will measure inequality considering only the first n items of every recommendation list of all users</p> PARAMETER DESCRIPTION <code>top_n</code> <p>it's a cutoff parameter, if specified the Gini index will be calculated considering only the first 'n' items of every recommendation list of all users. Default is None</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> Source code in <code>clayrs/evaluation/metrics/fairness_metrics.py</code> <pre><code>def __init__(self, top_n: int = None):\n    self.__top_n = top_n\n</code></pre>"},{"location":"evaluation/metrics/fairness_metrics/#clayrs.evaluation.metrics.fairness_metrics.GroupFairnessMetric","title":"<code>GroupFairnessMetric(user_groups)</code>","text":"<p>         Bases: <code>FairnessMetric</code></p> <p>Abstract class for fairness metrics based on user groups</p> <p>It has some concrete methods useful for group divisions, since every subclass needs to split users into groups.</p> PARAMETER DESCRIPTION <code>user_groups</code> <p>Dict containing group names as keys and percentage of users as value, used to split users in groups. Users with more popular items rated are grouped into the first group, users with slightly less popular items rated are grouped into the second one, etc.</p> <p> TYPE: <code>Dict[str, float]</code> </p> Source code in <code>clayrs/evaluation/metrics/fairness_metrics.py</code> <pre><code>def __init__(self, user_groups: Dict[str, float]):\n    self.__user_groups = user_groups\n</code></pre>"},{"location":"evaluation/metrics/fairness_metrics/#clayrs.evaluation.metrics.fairness_metrics.GroupFairnessMetric.get_avg_pop_by_users","title":"<code>get_avg_pop_by_users(data, pop_by_items, group=None)</code>  <code>staticmethod</code>","text":"<p>Get the average popularity for each user in the <code>data</code> parameter.</p> <p>Average popularity of a single user \\(u\\) is defined as:</p> \\[ avg\\_pop_u = \\frac{\\sum_{i \\in i_u} pop_i}{|i_u|} \\] PARAMETER DESCRIPTION <code>data</code> <p>The <code>Ratings</code> object that will be used to compute average popularity of each user</p> <p> TYPE: <code>Ratings</code> </p> <code>pop_by_items</code> <p>popularity for each label ('label', 'popularity')</p> <p> TYPE: <code>Dict</code> </p> <code>group</code> <p>(optional) the set of users (user_id)</p> <p> TYPE: <code>Set[str]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Dict[str, float]</code> <p>Python dictionary containing as keys each user id and as values the average popularity of each user</p> Source code in <code>clayrs/evaluation/metrics/fairness_metrics.py</code> <pre><code>@staticmethod\ndef get_avg_pop_by_users(data: Ratings, pop_by_items: Dict, group: Set[str] = None) -&gt; Dict[str, float]:\nr\"\"\"\n    Get the average popularity for each user in the `data` parameter.\n\n    Average popularity of a single user $u$ is defined as:\n\n    $$\n    avg\\_pop_u = \\frac{\\sum_{i \\in i_u} pop_i}{|i_u|}\n    $$\n\n    Args:\n        data: The `Ratings` object that will be used to compute average popularity of each user\n        pop_by_items: popularity for each label ('label', 'popularity')\n        group: (optional) the set of users (user_id)\n\n    Returns:\n        Python dictionary containing as keys each user id and as values the average popularity of each user\n    \"\"\"\n    if group is None:\n        group = data.unique_user_id_column\n        group_int = data.unique_user_idx_column\n    else:\n        group_int = data.user_map.convert_seq_str2int(list(group))\n\n    avg_pop_by_users = []\n\n    for user_idx in group_int:\n        user_interactions_rows = data.get_user_interactions(user_idx, as_indices=True)\n        user_items = data.item_id_column[user_interactions_rows]\n\n        avg_pop_by_users.append(get_avg_pop(user_items, pop_by_items))\n\n    avg_pop_by_users = dict(zip(group, avg_pop_by_users))\n\n    return avg_pop_by_users\n</code></pre>"},{"location":"evaluation/metrics/fairness_metrics/#clayrs.evaluation.metrics.fairness_metrics.GroupFairnessMetric.split_user_in_groups","title":"<code>split_user_in_groups(score_frame, groups, pop_items)</code>  <code>staticmethod</code>","text":"<p>Users are split into groups based on the groups parameter, which contains names of the groups as keys, and percentage of how many user must contain a group as values. For example:</p> <pre><code>groups = {'popular_users': 0.3, 'medium_popular_users': 0.2, 'low_popular_users': 0.5}\n</code></pre> <p>Every user will be inserted in a group based on how many popular items the user has rated (in relation to the percentage of users we specified as value in the dictionary):</p> <ul> <li>users with many popular items will be inserted into the first group</li> <li>users with niche items rated will be inserted into one of the last groups.</li> </ul> <p>In general users are grouped by \\(Popularity\\_ratio\\) in a descending order. \\(Popularity\\_ratio\\) for a single user \\(u\\) is defined as:</p> \\[ Popularity\\_ratio_u = n\\_most\\_popular\\_items\\_rated_u / n\\_items\\_rated_u \\] <p>The most popular items are the first <code>pop_percentage</code>% items of all items ordered in a descending order by popularity.</p> <p>The popularity of an item is defined as the number of times it is rated in the <code>original_ratings</code> parameter divided by the total number of users in the <code>original_ratings</code>.</p> PARAMETER DESCRIPTION <code>score_frame</code> <p>the Ratings object</p> <p> TYPE: <code>Ratings</code> </p> <code>groups</code> <p>each key contains the name of the group and each value contains the percentage of the specified group. If the groups don't cover the entire user collection, the rest of the users are considered in a 'default_diverse' group</p> <p> TYPE: <code>Dict[str, float]</code> </p> <code>pop_items</code> <p>set of most popular item_id labels</p> <p> TYPE: <code>Set[str]</code> </p> RETURNS DESCRIPTION <code>Dict[str, Set[str]]</code> <p>A python dictionary containing as keys each group name and as values the set of user_id belonging to the particular group.</p> Source code in <code>clayrs/evaluation/metrics/fairness_metrics.py</code> <pre><code>@staticmethod\ndef split_user_in_groups(score_frame: Ratings, groups: Dict[str, float],\n                         pop_items: Set[str]) -&gt; Dict[str, Set[str]]:\nr\"\"\"\n    Users are split into groups based on the *groups* parameter, which contains names of the groups as keys,\n    and percentage of how many user must contain a group as values. For example:\n\n        groups = {'popular_users': 0.3, 'medium_popular_users': 0.2, 'low_popular_users': 0.5}\n\n    Every user will be inserted in a group based on how many popular items the user has rated (in relation to the\n    percentage of users we specified as value in the dictionary):\n\n    * users with many popular items will be inserted into the first group\n    * users with niche items rated will be inserted into one of the last groups.\n\n    In general users are grouped by $Popularity\\_ratio$ in a descending order. $Popularity\\_ratio$ for a\n    single user $u$ is defined as:\n\n    $$\n    Popularity\\_ratio_u = n\\_most\\_popular\\_items\\_rated_u / n\\_items\\_rated_u\n    $$\n\n    The *most popular items* are the first `pop_percentage`% items of all items ordered in a descending order by\n    popularity.\n\n    The popularity of an item is defined as the number of times it is rated in the `original_ratings` parameter\n    divided by the total number of users in the `original_ratings`.\n\n    Args:\n        score_frame: the Ratings object\n        groups: each key contains the name of the group and each value contains the\n            percentage of the specified group. If the groups don't cover the entire user collection,\n            the rest of the users are considered in a 'default_diverse' group\n        pop_items: set of most popular *item_id* labels\n\n    Returns:\n        A python dictionary containing as keys each group name and as values the set of *user_id* belonging to\n            the particular group.\n    \"\"\"\n    num_of_users = len(score_frame.unique_user_id_column)\n    if num_of_users &lt; len(groups):\n        raise NotEnoughUsers(\"You can't split in {} groups {} users! \"\n                             \"Try reducing number of groups\".format(len(groups), num_of_users))\n\n    for percentage_chosen in groups.values():\n        if not 0 &lt; percentage_chosen &lt;= 1:\n            raise ValueError('Incorrect percentage! Valid percentage range: 0 &lt; percentage &lt;= 1')\n    total = sum(groups.values())\n    if total &gt; 1:\n        raise ValueError(\"Incorrect percentage! Sum of percentage is &gt; than 1\")\n    elif total &lt; 1:\n        raise ValueError(\"Sum of percentage is &lt; than 1! Please add another group or redistribute percentages \"\n                         \"among already defined group to reach a total of 1!\")\n\n    pop_ratio_by_users = pop_ratio_by_user(score_frame, most_pop_items=pop_items)\n    pop_ratio_by_users = sorted(pop_ratio_by_users, key=pop_ratio_by_users.get, reverse=True)\n\n    groups_dict: Dict[str, Set[str]] = {}\n    last_index = 0\n    percentage = 0.0\n    for group_name in groups:\n        percentage += groups[group_name]\n        group_index = round(num_of_users * percentage)\n        if group_index == 0:\n            logger.warning('Not enough rows for group {}! It will be discarded'.format(group_name))\n        else:\n            groups_dict[group_name] = set(pop_ratio_by_users[last_index:group_index])\n            last_index = group_index\n    return groups_dict\n</code></pre>"},{"location":"evaluation/metrics/fairness_metrics/#clayrs.evaluation.metrics.fairness_metrics.PredictionCoverage","title":"<code>PredictionCoverage(catalog)</code>","text":"<p>         Bases: <code>FairnessMetric</code></p> <p>The Prediction Coverage metric measures in percentage how many distinct items are being recommended in relation to all available items. It's a system wise metric, so only its result it will be returned and not those of every user. The metric is calculated as such:</p> \\[ Prediction Coverage_{sys} = (\\frac{|I_p|}{|I|})\\cdot100 \\] <p>Where:</p> <ul> <li>\\(I\\) is the set of all available items</li> <li>\\(I_p\\) is the set of recommended items</li> </ul> <p>The \\(I\\) must be specified through the 'catalog' parameter</p> <p>Check the 'Beyond Accuracy: Evaluating Recommender Systems by Coverage and Serendipity' paper for more</p> PARAMETER DESCRIPTION <code>catalog</code> <p>set of item id of the catalog on which the prediction coverage must be computed</p> <p> TYPE: <code>Set[str]</code> </p> Source code in <code>clayrs/evaluation/metrics/fairness_metrics.py</code> <pre><code>def __init__(self, catalog: Set[str]):\n    self.__catalog = set(str(item_id) for item_id in catalog)\n</code></pre>"},{"location":"evaluation/metrics/plot_metrics/","title":"Plot metrics","text":"<p>Plot metrics save a plot in the chosen output directory</p>"},{"location":"evaluation/metrics/plot_metrics/#clayrs.evaluation.metrics.plot_metrics.LongTailDistr","title":"<code>LongTailDistr(out_dir='.', file_name='long_tail_distr', on='truth', format='png', overwrite=False)</code>","text":"<p>         Bases: <code>PlotMetric</code></p> <p>This metric generates the Long Tail Distribution plot and saves it in the output directory with the file name specified. The plot can be generated both for the truth set or the predictions set (based on the on parameter):</p> <ul> <li> <p>on = 'truth': in this case the long tail distribution is useful to see which are the most popular items (the   most rated ones)</p> </li> <li> <p>on = 'pred': in this case the long tail distribution is useful to see which are the most recommended items</p> </li> </ul> <p>The plot file will be saved as <code>out_dir/file_name.format</code></p> <p>Since multiple split could be evaluated at once, the overwrite parameter comes into play: if is set to False, file with the same name will be saved as <code>file_name (1).format</code>, <code>file_name (2).format</code>, etc. so that for every split a plot is generated without overwriting any file previously generated</p> PARAMETER DESCRIPTION <code>out_dir</code> <p>Directory where the plot will be saved. Default is '.', meaning that the plot will be saved in the same directory where the python script it's being executed</p> <p> TYPE: <code>str</code> DEFAULT: <code>'.'</code> </p> <code>file_name</code> <p>Name of the plot file. Default is 'long_tail_distr'</p> <p> TYPE: <code>str</code> DEFAULT: <code>'long_tail_distr'</code> </p> <code>on</code> <p>Set on which the Long Tail Distribution plot will be generated. Values accepted are 'truth' or 'pred'</p> <p> TYPE: <code>str</code> DEFAULT: <code>'truth'</code> </p> <code>format</code> <p>Format of the plot file. Could be 'jpg', 'svg', 'png'. Default is 'png'</p> <p> TYPE: <code>str</code> DEFAULT: <code>'png'</code> </p> <code>overwrite</code> <p>parameter which specifies if the plot saved must overwrite any file that as the same name ('file_name.format'). Default is False</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>exception raised when a invalid value for the 'on' parameter is specified</p> Source code in <code>clayrs/evaluation/metrics/plot_metrics.py</code> <pre><code>def __init__(self, out_dir: str = '.', file_name: str = 'long_tail_distr', on: str = 'truth', format: str = 'png',\n             overwrite: bool = False):\n    valid = {'truth', 'pred'}\n    self.__on = on.lower()\n\n    if self.__on not in valid:\n        raise ValueError(\"on={} is not supported! Long Tail can be calculated only on:\\n\"\n                         \"{}\".format(on, valid))\n    super().__init__(out_dir, file_name, format, overwrite)\n</code></pre>"},{"location":"evaluation/metrics/plot_metrics/#clayrs.evaluation.metrics.plot_metrics.PopRatioProfileVsRecs","title":"<code>PopRatioProfileVsRecs(user_groups, user_profiles, original_ratings, out_dir='.', file_name='pop_ratio_profile_vs_recs', pop_percentage=0.2, store_frame=False, format='png', overwrite=False)</code>","text":"<p>         Bases: <code>GroupFairnessMetric</code>, <code>PlotMetric</code></p> <p>This metric generates a plot where users are split into groups and, for every group, a boxplot comparing profile popularity ratio and recommendations popularity ratio is drawn</p> <p>Users are split into groups based on the user_groups parameter, which contains names of the groups as keys, and percentage of how many user must contain a group as values. For example:</p> <pre><code>user_groups = {'popular_users': 0.3, 'medium_popular_users': 0.2, 'low_popular_users': 0.5}\n</code></pre> <p>Every user will be inserted in a group based on how many popular items the user has rated (in relation to the percentage of users we specified as value in the dictionary):</p> <ul> <li>users with many popular items will be inserted into the first group</li> <li>users with niche items rated will be inserted into one of the last groups.</li> </ul> <p>In general users are grouped by \\(Popularity\\_ratio\\) in a descending order. \\(Popularity\\_ratio\\) for a single user \\(u\\) is defined as:</p> \\[ Popularity\\_ratio_u = n\\_most\\_popular\\_items\\_rated_u / n\\_items\\_rated_u \\] <p>The most popular items are the first <code>pop_percentage</code>% items of all items ordered in a descending order by popularity.</p> <p>The popularity of an item is defined as the number of times it is rated in the <code>original_ratings</code> parameter divided by the total number of users in the <code>original_ratings</code>.</p> <p>It can happen that for a particular user of a group no recommendation are available: in that case it will be skipped and it won't be considered in the \\(Popularity\\_ratio\\) computation of its group. In case no user of a group has recs available, a warning will be printed and the whole group won't be considered.</p> <p>The plot file will be saved as <code>out_dir/file_name.format</code></p> <p>Since multiple split could be evaluated at once, the <code>overwrite</code> parameter comes into play: if is set to False, file with the same name will be saved as <code>file_name (1).format</code>, <code>file_name (2).format</code>, etc. so that for every split a plot is generated without overwriting any file previously generated</p> <p>Thanks to the 'store_frame' parameter it's also possible to store a csv containing the calculations done in order to build every boxplot. Will be saved in the same directory and with the same file name as the plot itself (but with the .csv format):</p> <p>The csv will be saved as <code>out_dir/file_name.csv</code></p> <p>Please note: once computed, the DeltaGAP class needs to be re-instantiated in case you want to compute it again!</p> PARAMETER DESCRIPTION <code>user_groups</code> <p>Dict containing group names as keys and percentage of users as value, used to split users in groups. Users with more popular items rated are grouped into the first group, users with slightly less popular items rated are grouped into the second one, etc.</p> <p> TYPE: <code>Dict&lt;str, float&gt;</code> </p> <code>user_profiles</code> <p>one or more <code>Ratings</code> objects containing interactions of the profile of each user (e.g. the train set). It should be one for each split to evaluate!</p> <p> TYPE: <code>Union[list, Ratings]</code> </p> <code>original_ratings</code> <p><code>Ratings</code> object containing original interactions of the dataset that will be used to compute the popularity of each item (i.e. the number of times it is rated divided by the total number of users)</p> <p> TYPE: <code>Ratings</code> </p> <code>out_dir</code> <p>Directory where the plot will be saved. Default is '.', meaning that the plot will be saved in the same directory where the python script it's being executed</p> <p> TYPE: <code>str</code> DEFAULT: <code>'.'</code> </p> <code>file_name</code> <p>Name of the plot file. Default is 'pop_ratio_profile_vs_recs'</p> <p> TYPE: <code>str</code> DEFAULT: <code>'pop_ratio_profile_vs_recs'</code> </p> <code>pop_percentage</code> <p>How many (in percentage) 'most popular items' must be considered. Default is 0.2</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.2</code> </p> <code>store_frame</code> <p>True if you want to store calculations done in order to build every boxplot in a csv file, False otherwise. Default is set to False</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>format</code> <p>Format of the plot file. Could be 'jpg', 'svg', 'png'. Default is 'png'</p> <p> TYPE: <code>str</code> DEFAULT: <code>'png'</code> </p> <code>overwrite</code> <p>parameter which specifies if the plot saved must overwrite any file that as the same name ('file_name.format'). Default is False</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>clayrs/evaluation/metrics/plot_metrics.py</code> <pre><code>def __init__(self, user_groups: Dict[str, float],  user_profiles: Union[list, Ratings], original_ratings: Ratings,\n             out_dir: str = '.', file_name: str = 'pop_ratio_profile_vs_recs', pop_percentage: float = 0.2,\n             store_frame: bool = False, format: str = 'png', overwrite: bool = False):\n\n    PlotMetric.__init__(self, out_dir, file_name, format, overwrite)\n    GroupFairnessMetric.__init__(self, user_groups)\n\n    if not 0 &lt; pop_percentage &lt;= 1:\n        raise ValueError('Incorrect percentage! Valid percentage range: 0 &lt; percentage &lt;= 1')\n\n    self._pop_by_item = get_item_popularity(original_ratings)\n\n    if not isinstance(user_profiles, list):\n        user_profiles = [user_profiles]\n\n    self._user_profiles = user_profiles\n    self.__pop_percentage = pop_percentage\n    self.__user_groups = user_groups\n    self.__store_frame = store_frame\n</code></pre>"},{"location":"evaluation/metrics/plot_metrics/#clayrs.evaluation.metrics.plot_metrics.PopRecsCorrelation","title":"<code>PopRecsCorrelation(original_ratings, out_dir='.', file_name='pop_recs_correlation', mode='both', format='png', overwrite=False)</code>","text":"<p>         Bases: <code>PlotMetric</code></p> <p>This metric generates a plot which has as the X-axis the popularity of each item and as Y-axis the recommendation frequency, so that it can be easily seen the correlation between popular (niche) items and how many times are being recommended</p> <p>The popularity of an item is defined as the number of times it is rated in the <code>original_ratings</code> parameter divided by the total number of users in the <code>original_ratings</code>.</p> <p>The plot file will be saved as <code>out_dir/file_name.format</code></p> <p>Since multiple split could be evaluated at once, the overwrite parameter comes into play: if is set to False, file with the same name will be saved as <code>file_name (1).format</code>, <code>file_name (2).format</code>, etc. so that for every split a plot is generated without overwriting any file previously generated</p> <p>There exists cases in which some items are not recommended even once, so in the graph could appear zero recommendations. One could change this behaviour thanks to the 'mode' parameter:</p> <ul> <li> <p>mode='both': two graphs will be created, the first one containing eventual zero recommendations, the   second one where zero recommendations are excluded. This additional graph will be stored as   out_dir/file_name_no_zeros.format (the string '_no_zeros' will be added to the file_name chosen automatically)</p> </li> <li> <p>mode='w_zeros': only a graph containing eventual zero recommendations will be created</p> </li> <li> <p>mode='no_zeros': only a graph excluding eventual zero recommendations will be created. The graph will be   saved as out_dir/file_name_no_zeros.format (the string '_no_zeros' will be added to the file_name chosen   automatically)</p> </li> </ul> PARAMETER DESCRIPTION <code>original_ratings</code> <p><code>Ratings</code> object containing original interactions of the dataset that will be used to compute the popularity of each item (i.e. the number of times it is rated divided by the total number of users)</p> <p> TYPE: <code>Ratings</code> </p> <code>out_dir</code> <p>Directory where the plot will be saved. Default is '.', meaning that the plot will be saved in the same directory where the python script it's being executed</p> <p> TYPE: <code>str</code> DEFAULT: <code>'.'</code> </p> <code>file_name</code> <p>Name of the plot file. Default is 'pop_recs_correlation'</p> <p> TYPE: <code>str</code> DEFAULT: <code>'pop_recs_correlation'</code> </p> <code>mode</code> <p>Parameter which dictates which graph must be created. By default is 'both', so the graph with eventual zero recommendations as well as the graph excluding eventual zero recommendations will be created. Check the class documentation for more</p> <p> TYPE: <code>str</code> DEFAULT: <code>'both'</code> </p> <code>format</code> <p>Format of the plot file. Could be 'jpg', 'svg', 'png'. Default is 'png'</p> <p> TYPE: <code>str</code> DEFAULT: <code>'png'</code> </p> <code>overwrite</code> <p>parameter which specifies if the plot saved must overwrite any file that as the same name ('file_name.format'). Default is False</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>clayrs/evaluation/metrics/plot_metrics.py</code> <pre><code>def __init__(self, original_ratings: Ratings,\n             out_dir: str = '.',\n             file_name: str = 'pop_recs_correlation',\n             mode: str = 'both',\n             format: str = 'png', overwrite: bool = False):\n\n    valid = {'both', 'no_zeros', 'w_zeros'}\n    self.__mode = mode.lower()\n\n    if self.__mode not in valid:\n        raise ValueError(\"Mode {} is not supported! Modes available:\\n\"\n                         \"{}\".format(mode, valid))\n\n    self._pop_by_item = get_item_popularity(original_ratings)\n\n    super().__init__(out_dir, file_name, format, overwrite)\n</code></pre>"},{"location":"evaluation/metrics/plot_metrics/#clayrs.evaluation.metrics.plot_metrics.PopRecsCorrelation.build_no_zeros_plot","title":"<code>build_no_zeros_plot(popularity, recommendations)</code>","text":"<p>Method which builds and saves the plot excluding eventual zero recommendations It saves the plot as out_dir/filename_no_zeros.format, according to their value passed in the constructor. Note that the '_no_zeros' string is automatically added to the file_name chosen</p> PARAMETER DESCRIPTION <code>popularity</code> <p>x-axis values representing popularity of every item</p> <p> TYPE: <code>list</code> </p> <code>recommendations</code> <p>y-axis values representing number of times every item has been recommended</p> <p> TYPE: <code>list</code> </p> Source code in <code>clayrs/evaluation/metrics/plot_metrics.py</code> <pre><code>def build_no_zeros_plot(self, popularity: list, recommendations: list):\n\"\"\"\n    Method which builds and saves the plot **excluding** eventual *zero recommendations*\n    It saves the plot as *out_dir/filename_no_zeros.format*, according to their value passed in the constructor.\n    Note that the '_no_zeros' string is automatically added to the file_name chosen\n\n    Args:\n        popularity (list): x-axis values representing popularity of every item\n        recommendations (list): y-axis values representing number of times every item has been recommended\n    \"\"\"\n    title = 'Popularity Ratio - Recommendations Correlation (No zeros)'\n    fig = self.build_plot(popularity, recommendations, title)\n\n    file_name = self.file_name + '_no_zeros'\n\n    self.save_figure(fig, file_name)\n</code></pre>"},{"location":"evaluation/metrics/plot_metrics/#clayrs.evaluation.metrics.plot_metrics.PopRecsCorrelation.build_plot","title":"<code>build_plot(x, y, title)</code>","text":"<p>Method which builds a matplotlib plot given x-axis values, y-axis values and the title of the plot. X-axis label and Y-axis label are hard-coded as 'Popularity' and 'Recommendation frequency' respectively.</p> PARAMETER DESCRIPTION <code>x</code> <p>List containing x-axis values</p> <p> TYPE: <code>list</code> </p> <code>y</code> <p>List containing y-axis values</p> <p> TYPE: <code>list</code> </p> <code>title</code> <p>title of the plot</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>matplotlib.figure.Figure</code> <p>The matplotlib figure</p> Source code in <code>clayrs/evaluation/metrics/plot_metrics.py</code> <pre><code>def build_plot(self, x: list, y: list, title: str) -&gt; matplotlib.figure.Figure:\n\"\"\"\n    Method which builds a matplotlib plot given x-axis values, y-axis values and the title of the plot.\n    X-axis label and Y-axis label are hard-coded as 'Popularity' and 'Recommendation frequency' respectively.\n\n    Args:\n        x (list): List containing x-axis values\n        y (list): List containing y-axis values\n        title (str): title of the plot\n\n    Returns:\n        The matplotlib figure\n    \"\"\"\n    fig = plt.figure()\n    ax = fig.add_subplot()\n\n    ax.set(xlabel='Popularity Ratio', ylabel='Recommendation frequency',\n           title=title)\n\n    ax.scatter(x, y, marker='o', s=20, c='orange', edgecolors='black',\n               linewidths=0.05)\n\n    # automatic ticks but only integer ones\n    ax.yaxis.set_major_locator(plticker.MaxNLocator(integer=True))\n\n    return fig\n</code></pre>"},{"location":"evaluation/metrics/plot_metrics/#clayrs.evaluation.metrics.plot_metrics.PopRecsCorrelation.build_w_zeros_plot","title":"<code>build_w_zeros_plot(popularity, recommendations)</code>","text":"<p>Method which builds and saves the plot containing eventual zero recommendations It saves the plot as out_dir/filename.format, according to their value passed in the constructor</p> PARAMETER DESCRIPTION <code>popularity</code> <p>x-axis values representing popularity of every item</p> <p> TYPE: <code>list</code> </p> <code>recommendations</code> <p>y-axis values representing number of times every item has been recommended</p> <p> TYPE: <code>list</code> </p> Source code in <code>clayrs/evaluation/metrics/plot_metrics.py</code> <pre><code>def build_w_zeros_plot(self, popularity: list, recommendations: list):\n\"\"\"\n    Method which builds and saves the plot containing eventual *zero recommendations*\n    It saves the plot as *out_dir/filename.format*, according to their value passed in the constructor\n\n    Args:\n        popularity (list): x-axis values representing popularity of every item\n        recommendations (list): y-axis values representing number of times every item has been recommended\n    \"\"\"\n    title = 'Popularity Ratio - Recommendations Correlation'\n    fig = self.build_plot(popularity, recommendations, title)\n\n    file_name = self.file_name\n\n    self.save_figure(fig, file_name)\n</code></pre>"},{"location":"evaluation/metrics/ranking_metrics/","title":"Ranking metrics","text":"<p>Ranking metrics evaluate the quality of the recommendation lists</p>"},{"location":"evaluation/metrics/ranking_metrics/#clayrs.evaluation.metrics.ranking_metrics.Correlation","title":"<code>Correlation(method='pearson', top_n=None)</code>","text":"<p>         Bases: <code>RankingMetric</code></p> <p>The Correlation metric calculates the correlation between the ranking of a user and its ideal ranking. The currently correlation methods implemented are:</p> <ul> <li><code>pearson</code></li> <li><code>kendall</code></li> <li><code>spearman</code></li> </ul> <p>Every correlation method is implemented by the pandas library, so read its documentation for more</p> <p>The correlation metric is calculated as such for the single user:</p> \\[ Corr_u = Corr(ranking_u, ideal\\_ranking_u) \\] <p>Where:</p> <ul> <li>\\(ranking_u\\) is ranking of the user</li> <li>\\(ideal\\_ranking_u\\) is the ideal ranking for the user</li> </ul> <p>The ideal ranking is calculated based on the rating inside the ground truth of the user</p> <p>The Correlation metric calculated for the entire system is simply the average of every \\(Corr\\):</p> \\[ Corr_{sys} = \\frac{\\sum_{u} Corr_u}{|U|} \\] <p>Where:</p> <ul> <li>\\(Corr_u\\) is the correlation of the user \\(u\\)</li> <li>\\(U\\) is the set of all users</li> </ul> <p>The system average excludes NaN values.</p> <p>It's also possible to specify a cutoff parameter thanks to the 'top_n' parameter: if specified, only the first \\(n\\) results of the recommendation list will be used in order to calculate the correlation</p> PARAMETER DESCRIPTION <code>method</code> <p>The correlation method to use. It must be 'pearson', 'kendall' or 'spearman', otherwise a ValueError exception is raised. By default is 'pearson'</p> <p> TYPE: <code>str</code> DEFAULT: <code>'pearson'</code> </p> <code>top_n</code> <p>Cutoff parameter, if specified only the first n items of the recommendation list will be used in order to calculate the correlation</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>if an invalid method parameter is passed</p> Source code in <code>clayrs/evaluation/metrics/ranking_metrics.py</code> <pre><code>def __init__(self, method: str = 'pearson', top_n: int = None):\n    valid = {'pearson', 'kendall', 'spearman'}\n    self.__method = method.lower()\n\n    if self.__method not in valid:\n        raise ValueError(\"Method {} is not supported! Methods available:\\n\"\n                         \"{}\".format(method, valid))\n\n    self.__top_n = top_n\n</code></pre>"},{"location":"evaluation/metrics/ranking_metrics/#clayrs.evaluation.metrics.ranking_metrics.MAP","title":"<code>MAP(relevant_threshold=None)</code>","text":"<p>         Bases: <code>RankingMetric</code></p> <p>The \\(MAP\\) metric (Mean average Precision) is a ranking metric computed by first calculating the \\(AP\\) (Average Precision) for each user and then taking its mean.</p> <p>The \\(AP\\) is calculated as such for the single user:</p> \\[ AP_u = \\frac{1}{m_u}\\sum_{i=1}^{N_u}P(i)\\cdot rel(i) \\] <p>Where:</p> <ul> <li>\\(m_u\\) is the number of relevant items for the user \\(u\\)</li> <li>\\(N_u\\) is the number of recommended items for the user \\(u\\)</li> <li>\\(P(i)\\) is the precision computed at cutoff \\(i\\)</li> <li>\\(rel(i)\\) is an indicator variable that says whether the i-th item is relevant (\\(rel(i)=1\\)) or not (\\(rel(i)=0\\))</li> </ul> <p>After computing the \\(AP\\) for each user, we can compute the \\(MAP\\) for the whole system:</p> \\[ MAP_{sys} = \\frac{1}{|U|}\\sum_{u}AP_u \\] <p>This metric will return the \\(AP\\) computed for each user in the dataframe containing users results, and the \\(MAP\\) computed for the whole system in the dataframe containing system results</p> PARAMETER DESCRIPTION <code>relevant_threshold</code> <p>parameter needed to discern relevant items and non-relevant items for every user. If not specified, the mean rating score of every user will be used</p> <p> TYPE: <code>float</code> DEFAULT: <code>None</code> </p> Source code in <code>clayrs/evaluation/metrics/ranking_metrics.py</code> <pre><code>def __init__(self, relevant_threshold: float = None):\n    self.relevant_threshold = relevant_threshold\n</code></pre>"},{"location":"evaluation/metrics/ranking_metrics/#clayrs.evaluation.metrics.ranking_metrics.MAPAtK","title":"<code>MAPAtK(k, relevant_threshold=None)</code>","text":"<p>         Bases: <code>MAP</code></p> <p>The \\(MAP@K\\) metric (Mean average Precision At K) is a ranking metric computed by first calculating the \\(AP@K\\) (Average Precision At K) for each user and then taking its mean.</p> <p>The \\(AP@K\\) is calculated as such for the single user:</p> \\[ AP@K_u = \\frac{1}{m_u}\\sum_{i=1}^{K}P(i)\\cdot rel(i) \\] <p>Where:</p> <ul> <li>\\(m_u\\) is the number of relevant items for the user \\(u\\)</li> <li>\\(K\\) is the cutoff value</li> <li>\\(P(i)\\) is the precision computed at cutoff \\(i\\)</li> <li>\\(rel(i)\\) is an indicator variable that says whether the i-th item is relevant (\\(rel(i)=1\\)) or not (\\(rel(i)=0\\))</li> </ul> <p>After computing the \\(AP@K\\) for each user, we can compute the \\(MAP@K\\) for the whole system:</p> \\[ MAP@K_{sys} = \\frac{1}{|U|}\\sum_{u}AP@K_u \\] <p>This metric will return the \\(AP@K\\) computed for each user in the dataframe containing users results, and the \\(MAP@K\\) computed for the whole system in the dataframe containing system results</p> PARAMETER DESCRIPTION <code>k</code> <p>the cutoff parameter. It must be &gt;= 1, otherwise a ValueError exception is raised</p> <p> TYPE: <code>int</code> </p> <code>relevant_threshold</code> <p>parameter needed to discern relevant items and non-relevant items for every user. If not specified, the mean rating score of every user will be used</p> <p> TYPE: <code>float</code> DEFAULT: <code>None</code> </p> Source code in <code>clayrs/evaluation/metrics/ranking_metrics.py</code> <pre><code>def __init__(self, k: int, relevant_threshold: float = None):\n    super().__init__(relevant_threshold)\n    self.k = k\n</code></pre>"},{"location":"evaluation/metrics/ranking_metrics/#clayrs.evaluation.metrics.ranking_metrics.MRR","title":"<code>MRR(relevant_threshold=None)</code>","text":"<p>         Bases: <code>RankingMetric</code></p> <p>The MRR (Mean Reciprocal Rank) metric is a system wide metric, so only its result it will be returned and not those of every user. MRR is calculated as such:</p> \\[ MRR_{sys} = \\frac{1}{|Q|}\\cdot\\sum_{i=1}^{|Q|}\\frac{1}{rank(i)} \\] <p>Where:</p> <ul> <li>\\(Q\\) is the set of recommendation lists</li> <li>\\(rank(i)\\) is the position of the first relevant item in the i-th recommendation list</li> </ul> <p>The MRR metric needs to discern relevant items from the not relevant ones: in order to do that, one could pass a custom <code>relevant_threshold</code> parameter that will be applied to every user, so that if a rating of an item is &gt;= relevant_threshold, then it's relevant, otherwise it's not. If no <code>relevant_threshold</code> parameter is passed then, for every user, its mean rating score will be used</p> PARAMETER DESCRIPTION <code>relevant_threshold</code> <p>parameter needed to discern relevant items and non-relevant items for every user. If not specified, the mean rating score of every user will be used</p> <p> TYPE: <code>float</code> DEFAULT: <code>None</code> </p> Source code in <code>clayrs/evaluation/metrics/ranking_metrics.py</code> <pre><code>def __init__(self, relevant_threshold: float = None):\n    self.__relevant_threshold = relevant_threshold\n</code></pre>"},{"location":"evaluation/metrics/ranking_metrics/#clayrs.evaluation.metrics.ranking_metrics.MRR.calc_reciprocal_rank","title":"<code>calc_reciprocal_rank(user_predictions_items, user_truth_relevant_items)</code>","text":"<p>Method which calculates the RR (Reciprocal Rank) for a single user</p> PARAMETER DESCRIPTION <code>user_predictions_items</code> <p>list of ranked item ids for the user computed by the Recommender</p> <p> TYPE: <code>np.ndarray</code> </p> <code>user_truth_relevant_items</code> <p>list of relevant item ids for the user in its truth set</p> <p> TYPE: <code>np.ndarray</code> </p> Source code in <code>clayrs/evaluation/metrics/ranking_metrics.py</code> <pre><code>def calc_reciprocal_rank(self, user_predictions_items: np.ndarray, user_truth_relevant_items: np.ndarray):\n\"\"\"\n    Method which calculates the RR (Reciprocal Rank) for a single user\n\n    Args:\n        user_predictions_items: list of ranked item ids for the user computed by the Recommender\n        user_truth_relevant_items: list of relevant item ids for the user in its truth set\n    \"\"\"\n\n    common_idxs = npi.indices(user_truth_relevant_items, user_predictions_items, missing=-1)\n    non_missing_idxs = np.where(common_idxs != -1)[0]\n\n    reciprocal_rank = 0\n    if len(non_missing_idxs) != 0:\n        reciprocal_rank = 1 / (non_missing_idxs[0] + 1)  # [0][0] because where returns a tuple\n\n    return reciprocal_rank\n</code></pre>"},{"location":"evaluation/metrics/ranking_metrics/#clayrs.evaluation.metrics.ranking_metrics.MRRAtK","title":"<code>MRRAtK(k, relevant_threshold=None)</code>","text":"<p>         Bases: <code>MRR</code></p> <p>The MRR@K (Mean Reciprocal Rank at K) metric is a system wide metric, so only its result will be returned and not those of every user. MRR@K is calculated as such</p> \\[ MRR@K_{sys} = \\frac{1}{|Q|}\\cdot\\sum_{i=1}^{K}\\frac{1}{rank(i)} \\] <p>Where:</p> <ul> <li>\\(K\\) is the cutoff parameter</li> <li>\\(Q\\) is the set of recommendation lists</li> <li>\\(rank(i)\\) is the position of the first relevant item in the i-th recommendation list</li> </ul> PARAMETER DESCRIPTION <code>k</code> <p>the cutoff parameter. It must be &gt;= 1, otherwise a ValueError exception is raised</p> <p> TYPE: <code>int</code> </p> <code>relevant_threshold</code> <p>parameter needed to discern relevant items and non-relevant items for every user. If not specified, the mean rating score of every user will be used</p> <p> TYPE: <code>float</code> DEFAULT: <code>None</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>if an invalid cutoff parameter is passed (0 or negative)</p> Source code in <code>clayrs/evaluation/metrics/ranking_metrics.py</code> <pre><code>def __init__(self, k: int, relevant_threshold: float = None):\n    if k &lt; 1:\n        raise ValueError('k={} not valid! k must be &gt;= 1!'.format(k))\n    self.__k = k\n    super().__init__(relevant_threshold)\n</code></pre>"},{"location":"evaluation/metrics/ranking_metrics/#clayrs.evaluation.metrics.ranking_metrics.MRRAtK.calc_reciprocal_rank","title":"<code>calc_reciprocal_rank(user_predictions_items, user_truth_relevant_items)</code>","text":"<p>Method which calculates the RR (Reciprocal Rank) for a single user</p> PARAMETER DESCRIPTION <code>user_predictions_items</code> <p>list of ranked item ids for the user computed by the Recommender</p> <p> TYPE: <code>np.ndarray</code> </p> <code>user_truth_relevant_items</code> <p>list of relevant item ids for the user in its truth set</p> <p> TYPE: <code>np.ndarray</code> </p> Source code in <code>clayrs/evaluation/metrics/ranking_metrics.py</code> <pre><code>def calc_reciprocal_rank(self, user_predictions_items: np.ndarray, user_truth_relevant_items: np.ndarray):\n\"\"\"\n    Method which calculates the RR (Reciprocal Rank) for a single user\n\n    Args:\n        user_predictions_items: list of ranked item ids for the user computed by the Recommender\n        user_truth_relevant_items: list of relevant item ids for the user in its truth set\n    \"\"\"\n    user_predictions_cut = user_predictions_items[:self.k]\n\n    return super().calc_reciprocal_rank(user_predictions_cut, user_truth_relevant_items)\n</code></pre>"},{"location":"evaluation/metrics/ranking_metrics/#clayrs.evaluation.metrics.ranking_metrics.NDCG","title":"<code>NDCG(gains='linear', discount_log=np.log2)</code>","text":"<p>         Bases: <code>RankingMetric</code></p> <p>The NDCG (Normalized Discounted Cumulative Gain) metric is calculated for the single user by first computing the DCG score using the following formula:</p> \\[ DCG_{u}(scores_{u}) = \\sum_{r\\in scores_{u}}{\\frac{f(r)}{log_x(2 + i)}} \\] <p>Where:</p> <ul> <li>\\(scores_{u}\\) are the ground truth scores for predicted items, ordered according to the order of said items in the     ranking for the user \\(u\\)</li> <li>\\(f\\) is a gain function (linear or exponential, in particular)</li> <li>\\(x\\) is the base of the logarithm</li> <li>\\(i\\) is the index of the truth score \\(r\\) in the list of scores \\(scores_{u}\\)</li> </ul> <p>If \\(f\\) is \"linear\", then the truth score \\(r\\) is returned as is. Otherwise, in the \"exponential\" case, the following formula is applied to \\(r\\):</p> \\[ f(r) = 2^{r} - 1 \\] <p>The NDCG for a single user is then calculated using the following formula:</p> \\[ NDCG_u(scores_{u}) = \\frac{DCG_{u}(scores_{u})}{IDCG_{u}(scores_{u})} \\] <p>Where:</p> <ul> <li>\\(IDCG_{u}\\) is the DCG of the ideal ranking for the truth scores</li> </ul> <p>So the basic idea is to compare the actual ranking with the ideal one</p> <p>Finally, the NDCG of the entire system is calculated instead as such:</p> \\[ NDCG_{sys} = \\frac{\\sum_{u} NDCG_u}{|U|} \\] <p>Where:</p> <ul> <li>\\(NDCG_u\\) is the NDCG calculated for user :math:<code>u</code></li> <li>\\(U\\) is the set of all users</li> </ul> <p>The system average excludes NaN values.</p> PARAMETER DESCRIPTION <code>gains</code> <p>type of gain function to use when calculating the DCG score, the possible options are \"linear\" or \"exponential\"</p> <p> TYPE: <code>str</code> DEFAULT: <code>'linear'</code> </p> <code>discount_log</code> <p>logarithm function to use when calculating the DCG score, by default numpy logarithm in base 2 is used</p> <p> TYPE: <code>Callable</code> DEFAULT: <code>np.log2</code> </p> Source code in <code>clayrs/evaluation/metrics/ranking_metrics.py</code> <pre><code>def __init__(self, gains: str = \"linear\", discount_log: Callable = np.log2):\n    self.gains = gains\n    self.discount_log = discount_log\n\n    if self.gains == \"exponential\":\n        self.gains_fn = lambda r: 2 ** r - 1\n    elif self.gains == \"linear\":\n        self.gains_fn = lambda r: r\n    else:\n        raise ValueError(\"Invalid gains option.\")\n</code></pre>"},{"location":"evaluation/metrics/ranking_metrics/#clayrs.evaluation.metrics.ranking_metrics.NDCGAtK","title":"<code>NDCGAtK(k, gains='linear', discount_log=np.log2)</code>","text":"<p>         Bases: <code>NDCG</code></p> <p>The NDCG@K (Normalized Discounted Cumulative Gain at K) metric is calculated for the single user by using the framework implementation of the NDCG but considering \\(scores_{u}\\) cut at the first \\(k\\) predictions</p> PARAMETER DESCRIPTION <code>k</code> <p>the cutoff parameter</p> <p> TYPE: <code>int</code> </p> <code>gains</code> <p>type of gain function to use when calculating the DCG score, the possible options are \"linear\" or \"exponential\"</p> <p> TYPE: <code>str</code> DEFAULT: <code>'linear'</code> </p> <code>discount_log</code> <p>logarithm function to use when calculating the DCG score, by default numpy logarithm in base 2 is used</p> <p> TYPE: <code>Callable</code> DEFAULT: <code>np.log2</code> </p> Source code in <code>clayrs/evaluation/metrics/ranking_metrics.py</code> <pre><code>def __init__(self, k: int, gains: str = \"linear\", discount_log: Callable = np.log2):\n    super().__init__(gains, discount_log)\n\n    self._k = k\n</code></pre>"},{"location":"evaluation/statistical_tests/paired/","title":"Paired statistical tests","text":""},{"location":"evaluation/statistical_tests/paired/#clayrs.evaluation.statistical_test.PairedTest","title":"<code>PairedTest</code>","text":"<p>         Bases: <code>StatisticalTest</code></p>"},{"location":"evaluation/statistical_tests/paired/#clayrs.evaluation.statistical_test.PairedTest.perform","title":"<code>perform(df_list)</code>","text":"<p>Method which performs the chosen paired statistical test.</p> <p>Since it's a paired test, the final result is a pandas DataFrame which contains learning schemas compared in pair. For example if you call the <code>perform()</code> method by passing a list containing three different DataFrames, one for each learning schema to compare:</p> <pre><code># Ttest as example since it's a Paired Test\nTtest().perform([user_df1, user_df2, user_df3])\n</code></pre> <p>You will obtain a DataFrame comparing all different combinations:</p> <ul> <li>(system1, system2)</li> <li>(system1, system3)</li> <li>(system2, system3)</li> </ul> <p>The first value of each cell is the statistic, the second is the p-value</p> PARAMETER DESCRIPTION <code>df_list</code> <p>List containing DataFrames with several metrics to compare, preferably metrics computed for each user. One DataFrame corresponds to one learning schema</p> <p> TYPE: <code>List[pd.DataFrame]</code> </p> RETURNS DESCRIPTION <code>pd.DataFrame</code> <p>A Pandas DataFrame where each combination of learning schemas are compared in pair. The first value of each cell is the statistic, the second is the p-value</p> Source code in <code>clayrs/evaluation/statistical_test.py</code> <pre><code>def perform(self, df_list: List[pd.DataFrame]) -&gt; pd.DataFrame:\n\"\"\"\n    Method which performs the chosen paired statistical test.\n\n    Since it's a paired test, the final result is a pandas DataFrame which contains learning\n    schemas compared in pair.\n    For example if you call the `perform()` method by passing a list containing three different DataFrames, one for\n    each learning schema to compare:\n\n    ```python\n    # Ttest as example since it's a Paired Test\n    Ttest().perform([user_df1, user_df2, user_df3])\n    ```\n\n    You will obtain a DataFrame comparing all different combinations:\n\n    * (system1, system2)\n    * (system1, system3)\n    * (system2, system3)\n\n    The first value of each cell is the ***statistic***, the second is the ***p-value***\n\n    Args:\n        df_list: List containing DataFrames with several metrics to compare, preferably metrics computed for each\n            user. One DataFrame corresponds to one learning schema\n\n    Returns:\n        A Pandas DataFrame where each combination of learning schemas are compared in pair. The first value of each\n            cell is the ***statistic***, the second is the ***p-value***\n    \"\"\"\n\n    user_id_column = self.user_id_column\n    if user_id_column is None:\n        user_id_column = [\"user_id\" for _ in range(len(df_list))]\n    elif len(user_id_column) == 1:\n        user_id_column = [self.user_id_column[0] for _ in range(len(df_list))]\n    elif len(user_id_column) != len(df_list):\n        raise ValueError(\"You must either specify a single user_id column for all dataframes or a different \"\n                         \"user_id column for each DataFrame passed!\")\n\n    # if the user id column is the index we consider it as an additional column\n    df_list_reset = []\n    for user_id_col, df in zip(user_id_column, df_list):\n        if user_id_col not in df.columns:\n            if user_id_col == df.index.name:\n                df = df.reset_index()\n            else:\n                raise KeyError(f\"Column {user_id_col} not present neither in the columns nor as index!\")\n\n        df_list_reset.append(df)\n\n    data = defaultdict(lambda: defaultdict(dict))\n\n    # this will contain metrics that are in common at least by two system\n    global_metrics = set()\n\n    n_system_evaluated = 1\n    while len(df_list_reset) != 0:\n        df1 = df_list_reset.pop(0)\n        user_id_col1 = user_id_column.pop(0)\n        for i, (user_id_col2, other_df) in enumerate(zip(user_id_column, df_list_reset),\n                                                     start=n_system_evaluated + 1):\n\n            common_metrics = [column for column in df1.columns\n                              if column != user_id_col1 and column in other_df.columns]\n\n            common_rows = self._common_users(df1, user_id_col1, other_df, user_id_col2, common_metrics)\n\n            for metric in common_metrics:\n\n                global_metrics.add(metric)\n                # drop nan values since otherwise test may behave unexpectedly\n                metric_rows = common_rows[[f\"{metric}_x\", f\"{metric}_y\"]].dropna()\n\n                score_system1 = metric_rows[f\"{metric}_x\"]\n                score_system2 = metric_rows[f\"{metric}_y\"]\n\n                statistic, pvalue = self._perform_test(score_system1, score_system2)\n\n                data[(f\"system_{n_system_evaluated}\", f\"system_{i}\")][str(metric)][\"statistic\"] = statistic\n                data[(f\"system_{n_system_evaluated}\", f\"system_{i}\")][str(metric)][\"pvalue\"] = pvalue\n\n        n_system_evaluated += 1\n\n    # index will contain (sys1, sys2), (sys1, sys3), ... tuples\n    # formatted_data will be in the form\n    # {(NDCG, \"statistic\"): [..., ..., ..., ...], (NDCG, \"pvalue\"): [..., ..., ..., ...],\n    #  (Precision, \"statistic\"): [..., ..., ..., ...], (Precision, \"pvalue\"): [..., ..., ..., ...],\n    #  ...}\n    index = []\n    formatted_data = defaultdict(list)\n    for df_index_row, df_data_row in data.items():\n        index.append(df_index_row)\n\n        for metric_column in global_metrics:\n            stat, pval = np.nan, np.nan\n\n            stat_pval_dict = df_data_row.get(metric_column)\n            if stat_pval_dict is not None:\n                stat = stat_pval_dict[\"statistic\"]\n                pval = stat_pval_dict[\"pvalue\"]\n\n            formatted_data[(metric_column, \"statistic\")].append(stat)\n            formatted_data[(metric_column, \"pvalue\")].append(pval)\n\n    res = pd.DataFrame(formatted_data, index=index)\n    res.index.rename(\"sys_pair\", inplace=True)\n\n    return res\n</code></pre>"},{"location":"evaluation/statistical_tests/paired/#clayrs.evaluation.statistical_test.Ttest","title":"<code>Ttest</code>","text":"<p>         Bases: <code>PairedTest</code></p> <p>Calculate the T-test for the means of two independent samples of scores.</p> <p>This is a two-sided test for the null hypothesis that 2 independent samples have identical average (expected) values. This test assumes that the populations have identical variances by default.</p>"},{"location":"evaluation/statistical_tests/paired/#clayrs.evaluation.statistical_test.Wilcoxon","title":"<code>Wilcoxon</code>","text":"<p>         Bases: <code>PairedTest</code></p> <p>Compute the Wilcoxon rank-sum statistic for two samples.</p> <p>The Wilcoxon rank-sum test tests the null hypothesis that two sets of measurements are drawn from the same distribution. The alternative hypothesis is that values in one sample are more likely to be larger than the values in the other sample.</p>"},{"location":"first_steps/colab_examples/","title":"Colab examples","text":"<p>The GitHub repository hosts some IPython notebooks to get you start and running with the framework!</p> <p>To use them you could use Google colab:</p> <ul> <li>Go to Colab and open <code>File &gt; Open notebook</code></li> </ul> <p></p> <ul> <li>Then go to <code>GitHub</code> section, write swapUniba/ClayRS in the first text box and choose the example you want  to run!</li> </ul> <p></p>"},{"location":"first_steps/colab_examples/#available-examples","title":"Available examples","text":"<p>All the following use the Movielens 100k dataset</p> <ul> <li> <p><code>1_tfidf_centroid.ipynb</code>: the easiest example, a good starting point for newcomers of the framework. It guides you in how to represent via TfIdf technique a field of the raw source, how to instantiate a  <code>CentroidVector</code> algorithm and how to evaluate recommendations generated with several state-of-the-art metrics;</p> </li> <li> <p><code>2_embeddings_randomforest.ipynb</code>: a slightly more complex example, where several fields are represented with several techniques, including embedding techniques. For the recommendation phase a  <code>Random Forest</code> classifier is used;</p> </li> <li> <p><code>3_graph_pagerank.ipynb</code>: it will guide you on how to perform graph based recommendation via <code>ClayRS</code> (how to instantiate a graph, how to manipulate it, how to load exogenous properties). The Personalized PageRank algorithm is used in the recsys phase;</p> </li> <li> <p><code>4_evaluate_other_recs.ipynb</code>: a jolly example which shows how to export results (and intermediate results) obtained by <code>ClayRS</code>, but also how to evaluate external recommendation lists (i.e. recommendations generated via  other tools)</p> </li> </ul>"},{"location":"first_steps/installation/","title":"Installation","text":""},{"location":"first_steps/installation/#via-pip-recommended","title":"Via PIP recommended","text":"<p>ClayRS requires Python 3.7 or later, while package dependencies are in <code>requirements.txt</code> and are all installable via <code>pip</code>, as ClayRS itself.</p> <p>To install it execute the following command:</p> Latest <pre><code>pip install clayrs\n</code></pre> <p>This will automatically install compatible versions of all dependencies.</p> <p>Tip: We suggest installing ClayRS (or any python package, for that matters) in a virtual environment</p> <p>Virtual environments are special isolated environments where all the packages and versions you install only  apply to that specific environment. It\u2019s like a private island! \u2014 but for code.</p> <p>Read this Medium article for understanding all the advantages and the official python guide on how to set up one</p>"},{"location":"first_steps/quickstart/","title":"Quickstart","text":""},{"location":"first_steps/quickstart/#content-analyzer","title":"Content Analyzer","text":"<p>The first thing to do is to import the Content Analyzer module * We will access its methods and classes via dot notation <pre><code>import clayrs.content_analyzer as ca\n</code></pre></p> <p>Then, let's point to the source containing raw information to process <pre><code>raw_source = ca.JSONFile('items_info.json')\n</code></pre></p> <p>We can now start building the configuration for the items</p> <p>Info</p> <p>Note that same operations that can be specified for items could be also specified for users via the <code>ca.UserAnalyzerConfig</code> class</p> <pre><code># Configuration of item representation\nmovies_ca_config = ca.ItemAnalyzerConfig(\n    source=raw_source,\n    id='movielens_id', # (1) \n    output_directory='movies_codified/' # (2) \n)\n</code></pre> <ol> <li>The id in the raw source which uniquely identifies each item</li> <li>Directory which will contain items complexly represented</li> </ol> <p>Let's represent the plot field of each content with a TfIdf representation</p> <ul> <li> <p>Since the <code>preprocessing</code> parameter has been specified, then each field is first preprocessed with the specified operations <pre><code>movies_ca_config.add_single_config(\n    'plot',\n    ca.FieldConfig(ca.SkLearnTfIdf(),\n                   preprocessing=ca.NLTK(stopwords_removal=True,\n                                         lemmatization=True),\n                   id='tfidf')  # (1)\n)\n</code></pre></p> </li> <li> <p>User defined id for the representation</p> </li> </ul> <p>To finalize the Content Analyzer part, let's instantiate the <code>ContentAnalyzer</code> class by passing the built configuration and by calling its <code>fit()</code> method</p> <p><pre><code>ca.ContentAnalyzer(movies_ca_config).fit()\n</code></pre> The items will be created with the specified representations and serialized</p>"},{"location":"first_steps/quickstart/#recsys","title":"RecSys","text":"<p>Similarly above, we must first import the RecSys module <pre><code>import clayrs.recsys as rs\n</code></pre></p> <p>Then we load the rating frame from a TSV file</p> <p>Info</p> <p>In this case in our file the first three columns are user_id, item_id, score in this order</p> <ul> <li>If your file has a different structure you must specify how to map the column via parameters, check documentation   for more</li> </ul> <pre><code>ratings = ca.Ratings(ca.CSVFile('ratings.tsv', separator='\\t'))\n</code></pre> <p>Let's split with the KFold technique the loaded rating frame into train set and test set</p> <ul> <li>since <code>n_splits=2</code>, train_list will contain two train_sets and test_list will contain two test_sets <pre><code>train_list, test_list = rs.KFoldPartitioning(n_splits=2).split_all(ratings)\n</code></pre></li> </ul> <p>In order to recommend items to users, we must choose an algorithm to use</p> <ul> <li>In this case we are using the <code>CentroidVector</code> algorithm which will work by using the first representation specified for the plot field</li> <li>You can freely choose which representation to use among all representation codified for the fields in the Content Analyzer phase</li> </ul> <pre><code>centroid_vec = rs.CentroidVector(\n    {'plot': 'tfidf'}, # (1)\n\n    similarity=rs.CosineSimilarity()\n)\n</code></pre> <ol> <li>We can reference the representation specified for the 'plot' field with the assigned custom id in the Content Analyzer phase</li> </ol> <p>Let's now compute the top-10 ranking for each user of the train set</p> <ul> <li>By default the candidate items are those in the test set of the user, but you can change this behaviour with the <code>methodology</code> parameter</li> </ul> <p>Since we used the kfold technique, we iterate over all train sets and test sets <pre><code>result_list = []\n\nfor train_set, test_set in zip(train_list, test_list):\n\n  cbrs = rs.ContentBasedRS(centroid_vec, train_set, 'movies_codified/')\n  rank = cbrs.fit_rank(test_set, n_recs=10)\n\n  result_list.append(rank)\n</code></pre> <code>result_list</code> will contain two <code>Rank</code> objects in this case, one for each split</p>"},{"location":"first_steps/quickstart/#evaluation-module","title":"Evaluation module","text":"<p>Similarly to the Content Analyzer and RecSys module, we must first import the evaluation module <pre><code>import clayrs.evaluation as eva\n</code></pre></p> <p>The class responsible for evaluating recommendation lists is the <code>EvalModel</code> class. It needs the following parameters:</p> <ul> <li>A list of computed rank/predictions (in case multiple splits must be evaluated)</li> <li>A list of truths (in case multiple splits must be evaluated)</li> <li>List of metrics to compute</li> </ul> <p>Obviously the list of computed rank/predictions and list of truths must have the same length, and the rank/prediction in position \\(i\\) will be compared with the truth at position \\(i\\)</p> <pre><code>em = eva.EvalModel(\n    pred_list=result_list,\n    truth_list=test_list,\n    metric_list=[\n        eva.NDCG(),\n        eva.Precision(),\n        eva.RecallAtK(k=5)\n    ]\n)\n</code></pre> <p>Then simply call the <code>fit()</code> method of the instantiated object</p> <ul> <li>It will return two pandas DataFrame: the first one contains the metrics aggregated for the system, while the second contains the metrics computed for each user (where possible)</li> </ul> <pre><code>sys_result, users_result =  em.fit()\n</code></pre> <p>Note</p> <p>Note that the EvalModel is able to compute evaluation of recommendations generated by other tools/frameworks, check documentation for more</p>"},{"location":"recsys/experiment/","title":"Experiment class","text":""},{"location":"recsys/experiment/#clayrs.recsys.ContentBasedExperiment","title":"<code>ContentBasedExperiment(original_ratings, partitioning_technique, algorithm_list, items_directory, users_directory=None, metric_list=None, report=False, output_folder='experiment_result', overwrite_if_exists=False)</code>","text":"<p>         Bases: <code>Experiment</code></p> <p>The <code>Experiment</code> class for content based algorithms</p> <p>It provides an easy interface to perform a complete experiment by comparing different cb-algorithms, starting from splitting the dataset to evaluating predictions computed. It is also capable of producing a <code>yml</code> report for both the recsys phase and the evaluation phase.</p> <p>Both the evaluation phase and the report are optional and are produced only if specified.</p> <p>All the results (split, ranking, evaluation results, etc.) will be saved in the folder specified with the <code>output_folder</code> parameter. For each algorithm a different sub-folder will be created and named after it:</p> <ul> <li>If multiple instances of the same algorithm are present in the <code>algorithm_list</code>, sub-folders will be disambiguated     depending on the order of execution (<code>algName_1</code>, <code>algName_2</code>, <code>algName_3</code>, etc.)</li> </ul> <p>Info</p> <p>Please remember that by default if a folder with same name of the <code>output_folder</code> parameter is present, the experiment won't run and an exception will be raised. To overcome this, simply set the <code>overwrite_if_exists</code> parameter to <code>True</code> or change the <code>output_folder</code>.</p> <p>Examples:</p> <p>Suppose you want to compare:</p> <ul> <li>A <code>CentroidVector</code> algorithm</li> <li>The <code>SVC</code> classifier</li> <li>The <code>KNN</code> classifier</li> </ul> <p>For the three different configuration, an <code>HoldOut</code> partitioning technique should be used and results should be evaluated on \\(Precision\\) and \\(Recall\\)</p> <pre><code>from clayrs.utils import ContentBasedExperiment\nfrom clayrs import content_analyzer as ca\nfrom clayrs import content_analyzer as rs\n\noriginal_rat = ca.Ratings(ca.CSVFile(ratings_path))\n\nalg1 = rs.CentroidVector({'Plot': 'tfidf'},\n                         similarity=rs.CosineSimilarity()) # (1)\n\nalg2 = rs.ClassifierRecommender({'Plot': 'tfidf'},\n                                classifier=rs.SkSVC()) # (2)\n\nalg3 = rs.ClassifierRecommender({'Plot': 'tfidf'},\n                                 classifier=rs.SkKNN()) # (3)\n\na = ContentBasedExperiment(\n    original_ratings=rat,\n\n    partitioning_technique=rs.HoldOutPartitioning(),\n\n    algorithm_list=[alg1, alg2, alg3],\n\n    items_directory=movies_dir,\n\n    metric_list=[eva.Precision(), eva.Recall()]\n\n    output_folder=\"my_experiment\"\n)\n\na.rank()\n</code></pre> <ol> <li>Results will be saved in my_experiment/CentroidVector_1</li> <li>Results will be saved in my_experiment/ClassifierRecommender_1</li> <li>Results will be saved in my_experiment/ClassifierRecommender_2</li> </ol> PARAMETER DESCRIPTION <code>original_ratings</code> <p>Ratings object containing original interactions between users and items</p> <p> TYPE: <code>Ratings</code> </p> <code>partitioning_technique</code> <p>Partitioning object which specifies how the original ratings should be split</p> <p> TYPE: <code>Partitioning</code> </p> <code>algorithm_list</code> <p>List of Content Based algorithms for which the whole experiment will be conducted</p> <p> TYPE: <code>List[ContentBasedAlgorithm]</code> </p> <code>items_directory</code> <p>Path to the folder containing serialized complexly represented items</p> <p> TYPE: <code>str</code> </p> <code>users_directory</code> <p>Path to the folder containing serialized complexly represented items. Needed only if one or more algorithms in <code>algorithm_list</code> needs it</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>metric_list</code> <p>List of metric with which predictions computed by the CBRS will be evaluated</p> <p> TYPE: <code>List[Metric]</code> DEFAULT: <code>None</code> </p> <code>report</code> <p>If <code>True</code>, a <code>yml</code> report will be produced for the recsys phase. It will be also produced for the evaluation phase, but only if the <code>metric_list</code> parameter is set</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>output_folder</code> <p>Path of the folder where all the results of the experiment will be saved</p> <p> TYPE: <code>str</code> DEFAULT: <code>'experiment_result'</code> </p> <code>overwrite_if_exists</code> <p>If <code>True</code> and the path set in <code>output_folder</code> points to an already existent directory, it will be deleted and replaced by the folder containing the experiment results</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>clayrs/recsys/experiment.py</code> <pre><code>def __init__(self, original_ratings: Ratings,\n             partitioning_technique: Partitioning,\n             algorithm_list: List[ContentBasedAlgorithm],\n             items_directory: str,\n             users_directory: str = None,\n             metric_list: List[Metric] = None,\n             report: bool = False,\n             output_folder: str = \"experiment_result\",\n             overwrite_if_exists: bool = False):\n\n    super().__init__(original_ratings, partitioning_technique, algorithm_list, items_directory, users_directory,\n                     metric_list, report, output_folder, overwrite_if_exists)\n</code></pre>"},{"location":"recsys/experiment/#clayrs.recsys.experiment.ContentBasedExperiment.predict","title":"<code>predict(user_id_list=None, methodology=TestRatingsMethodology(), num_cpus=1, skip_alg_error=True)</code>","text":"<p>Method used to perform an experiment which involves score predictions.</p> <p>The method will first split the original ratings passed in the constructor in train and test set, then the Recommender System will be fit for each user in the train set.</p> <p>If the algorithm can't be fit for some users, a warning message is printed and no score predictions will be computed for said user</p> <p>Info</p> <p>BE CAREFUL: not all algorithms are able to perform score prediction. In case a pure ranking algorithm is asked to perform score prediction, the <code>NotPredictionAlg</code> will be raised. if the <code>skip_alg_error</code> is set to <code>True</code>, then said exception will be caught, a warning will be printed, and the experiment will go on with the next algorithm</p> <p>Via the <code>methodology</code> parameter you can perform different candidate item selection. By default, the <code>TestRatingsMethodology()</code> is used: so, for each user, items in its test set only will be considered for score prediction</p> PARAMETER DESCRIPTION <code>user_id_list</code> <p>List of users for which you want to compute the ranking. If None, the ranking will be computed for all users of the <code>test_set</code></p> <p> TYPE: <code>Optional[Union[List[str], List[int]]]</code> DEFAULT: <code>None</code> </p> <code>methodology</code> <p><code>Methodology</code> object which governs the candidate item selection. Default is <code>TestRatingsMethodology</code></p> <p> TYPE: <code>Optional[Methodology]</code> DEFAULT: <code>TestRatingsMethodology()</code> </p> <code>num_cpus</code> <p>number of processors that must be reserved for the method. Default is 0, meaning that the number of cpus will be automatically detected.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>skip_alg_error</code> <p>If set to <code>True</code>, a pure ranking algorithm will be skipped and the experiment will continue with the following algorithm. Otherwise, the <code>NotPredictionAlg</code> exception raised will be re-raised</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RAISES DESCRIPTION <code>NotPredictionAlg</code> <p>When a pure ranking algorithm is asked to perform score prediction and <code>skip_alg_error</code> == False</p> Source code in <code>clayrs/recsys/experiment.py</code> <pre><code>def predict(self,\n            user_id_list: Optional[Union[List[str], List[int]]] = None,\n            methodology: Optional[Methodology] = TestRatingsMethodology(),\n            num_cpus: int = 1,\n            skip_alg_error: bool = True) -&gt; None:\n\"\"\"\n    Method used to perform an experiment which involves ***score predictions***.\n\n    The method will first split the original ratings passed in the constructor in train and test set, then\n    the Recommender System will be fit for each user in the train set.\n\n    If the algorithm can't be fit for some users, a warning message is printed and no score predictions will be\n    computed for said user\n\n    !!! info\n\n        **BE CAREFUL**: not all algorithms are able to perform *score prediction*. In case a pure ranking algorithm\n        is asked to perform score prediction, the `NotPredictionAlg` will be raised. if the `skip_alg_error` is set\n        to `True`, then said exception will be caught, a warning will be printed, and the experiment will go on with\n        the next algorithm\n\n    Via the `methodology` parameter you can perform different candidate item selection. By default, the\n    `TestRatingsMethodology()` is used: so, for each user, items in its test set only will be considered for score\n    prediction\n\n    Args:\n        user_id_list: List of users for which you want to compute the ranking. If None, the ranking will be computed\n            for all users of the `test_set`\n        methodology: `Methodology` object which governs the candidate item selection. Default is\n            `TestRatingsMethodology`\n        num_cpus: number of processors that must be reserved for the method. Default is 0, meaning that\n            the number of cpus will be automatically detected.\n        skip_alg_error: If set to `True`, a pure ranking algorithm will be skipped and the experiment will continue\n            with the following algorithm. Otherwise, the `NotPredictionAlg` exception raised will be re-raised\n\n    Raises:\n        NotPredictionAlg: When a pure ranking algorithm is asked to perform score prediction and\n            `skip_alg_error` == False\n    \"\"\"\n    def cb_fit_predict(split_num, alg, train_set, test_set, dirname):\n        cbrs = ContentBasedRS(alg, train_set, self.items_directory)\n\n        predict_alg = cbrs.fit_predict(test_set, methodology=methodology, num_cpus=num_cpus,\n                                       user_list=user_id_list)\n\n        predict_alg.to_csv(f\"{self.output_folder}/{dirname}\", file_name=f\"rs_predict_split{split_num}\",\n                           ids_as_str=True)\n\n        if self.report:\n            Report(output_dir=f\"{self.output_folder}/{dirname}\").yaml(original_ratings=self.original_ratings,\n                                                                      partitioning_technique=self.pt,\n                                                                      recsys=cbrs)\n\n        return predict_alg\n\n    self.main_experiment(cb_fit_predict, skip_alg_error=skip_alg_error)\n</code></pre>"},{"location":"recsys/experiment/#clayrs.recsys.experiment.ContentBasedExperiment.rank","title":"<code>rank(n_recs=10, user_id_list=None, methodology=TestRatingsMethodology(), num_cpus=1)</code>","text":"<p>Method used to perform an experiment which involves rankings.</p> <p>The method will first split the original ratings passed in the constructor in train and test set, then the Recommender System will be fit for each user in the train. If the algorithm can't be fit for some users, a warning message is printed and no ranking will be computed for said user</p> <p>Via the <code>methodology</code> parameter you can perform different candidate item selection. By default, the <code>TestRatingsMethodology()</code> is used: so, for each user, items in its test set only will be considered as eligible for ranking</p> PARAMETER DESCRIPTION <code>n_recs</code> <p>Number of the top items that will be present in the ranking of each user. If <code>None</code> all candidate items will be returned for the user</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>10</code> </p> <code>user_id_list</code> <p>List of users for which you want to compute the ranking. If None, the ranking will be computed for all users of the <code>test_set</code></p> <p> TYPE: <code>Optional[Union[List[str], List[int]]]</code> DEFAULT: <code>None</code> </p> <code>methodology</code> <p><code>Methodology</code> object which governs the candidate item selection. Default is <code>TestRatingsMethodology</code></p> <p> TYPE: <code>Optional[Methodology]</code> DEFAULT: <code>TestRatingsMethodology()</code> </p> <code>num_cpus</code> <p>number of processors that must be reserved for the method. Default is 0, meaning that the number of cpus will be automatically detected</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> Source code in <code>clayrs/recsys/experiment.py</code> <pre><code>def rank(self,\n         n_recs: Optional[int] = 10,\n         user_id_list: Optional[Union[List[str], List[int]]] = None,\n         methodology: Optional[Methodology] = TestRatingsMethodology(),\n         num_cpus: int = 1):\n\"\"\"\n    Method used to perform an experiment which involves ***rankings***.\n\n    The method will first split the original ratings passed in the constructor in train and test set, then\n    the Recommender System will be fit for each user in the train.\n    If the algorithm can't be fit for some users, a warning message is printed and no ranking will be\n    computed for said user\n\n    Via the `methodology` parameter you can perform different candidate item selection. By default, the\n    `TestRatingsMethodology()` is used: so, for each user, items in its test set only will be considered as eligible\n    for ranking\n\n    Args:\n        n_recs: Number of the top items that will be present in the ranking of each user.\n            If `None` all candidate items will be returned for the user\n        user_id_list: List of users for which you want to compute the ranking. If None, the ranking will be computed\n            for all users of the `test_set`\n        methodology: `Methodology` object which governs the candidate item selection. Default is\n            `TestRatingsMethodology`\n        num_cpus: number of processors that must be reserved for the method. Default is 0, meaning that\n            the number of cpus will be automatically detected\n\n    \"\"\"\n    def cb_fit_rank(split_num, alg, train_set, test_set, dirname):\n        cbrs = ContentBasedRS(alg, train_set, self.items_directory)\n\n        predict_alg = cbrs.fit_rank(test_set, n_recs=n_recs, methodology=methodology,\n                                    user_list=user_id_list, num_cpus=num_cpus)\n\n        predict_alg.to_csv(f\"{self.output_folder}/{dirname}\", file_name=f\"rs_rank_split{split_num}\",\n                           ids_as_str=True)\n\n        if self.report:\n            Report(output_dir=f\"{self.output_folder}/{dirname}\").yaml(original_ratings=self.original_ratings,\n                                                                      partitioning_technique=self.pt,\n                                                                      recsys=cbrs)\n\n        return predict_alg\n\n    self.main_experiment(cb_fit_rank)\n</code></pre>"},{"location":"recsys/experiment/#clayrs.recsys.GraphBasedExperiment","title":"<code>GraphBasedExperiment(original_ratings, partitioning_technique, algorithm_list, items_directory=None, item_exo_properties=None, users_directory=None, user_exo_properties=None, link_label=None, metric_list=None, report=False, output_folder='experiment_result', overwrite_if_exists=False)</code>","text":"<p>         Bases: <code>Experiment</code></p> <p>The <code>Experiment</code> class for graph based algorithms</p> <p>It provides an easy interface to perform a complete experiment by comparing different gb-algorithms, starting from splitting the dataset to evaluating predictions computed.</p> <p>Every graph based algorithm expects a graph: that's why right before computing ranking/score predictions, a graph will be created depending on the current train and test split (if multiple are available):</p> <ul> <li>All the nodes from the original graph will be present, The interactions in the test set will be     missing (It won't be represented as a link between a user node and an item node)</li> </ul> <p>The class is also capable of producing a <code>yml</code> report for both the recsys phase and the evaluation phase.</p> <p>Both the evaluation phase and the report are optional and are produced only if specified.</p> <p>All the results (split, ranking, evaluation results, etc.) will be saved in the folder specified with the <code>output_folder</code> parameter. For each algorithm a different sub-folder will be created and named after it:</p> <ul> <li>If multiple instances of the same algorithm are present in the <code>algorithm_list</code>, sub-folders will be disambiguated     depending on the order of execution (<code>algName_1</code>, <code>algName_2</code>, <code>algName_3</code>, etc.)</li> </ul> <p>Info</p> <p>Please remember that by default if a folder with same name of the <code>output_folder</code> parameter is present, the experiment won't run and an exception will be raised. To overcome this, simply set the <code>overwrite_if_exists</code> parameter to <code>True</code> or change the <code>output_folder</code>.</p> <p>Examples:</p> <p>Suppose you want to compare:</p> <ul> <li>The <code>PageRank</code> algorithm with <code>alpha=0.8</code></li> <li>The <code>PageRank</code> algorithm with <code>alpha=0.9</code></li> <li>The <code>Personalized PageRank</code> algorithm</li> </ul> <p>For the three different configuration, a <code>KFold</code> partitioning technique with three splits should be used and results should be evaluated on \\(Precision\\), \\(Recall\\), \\(NDCG\\)</p> <pre><code>from clayrs.utils import GraphBasedExperiment\nfrom clayrs import content_analyzer as ca\nfrom clayrs import content_analyzer as rs\n\noriginal_rat = ca.Ratings(ca.CSVFile(ratings_path))\n\nalg1 = rs.NXPageRank(alpha=0.8) # (1)\n\nalg2 = rs.NXPageRank(alpha=0.9) # (2)\n\nalg3 = rs.NXPageRank(personalized=True) # (3)\n\na = GraphBasedExperiment(\n    original_ratings=rat,\n\n    partitioning_technique=rs.KFoldPartitioning(n_splits=3),\n\n    algorithm_list=[alg1, alg2, alg3],\n\n    items_directory=movies_dir,\n\n    metric_list=[eva.Precision(), eva.Recall()]\n\n    output_folder=\"my_experiment\"\n)\n\na.rank()\n</code></pre> <ol> <li>Results will be saved in my_experiment/NXPageRank_1</li> <li>Results will be saved in my_experiment/NXPageRank_2</li> <li>Results will be saved in my_experiment/NXPageRank_3</li> </ol> PARAMETER DESCRIPTION <code>original_ratings</code> <p>Ratings object containing original interactions between users and items</p> <p> TYPE: <code>Ratings</code> </p> <code>partitioning_technique</code> <p>Partitioning object which specifies how the original ratings should be split</p> <p> TYPE: <code>Partitioning</code> </p> <code>algorithm_list</code> <p>List of Graph Based algorithms for which the whole experiment will be conducted</p> <p> TYPE: <code>List[GraphBasedAlgorithm]</code> </p> <code>items_directory</code> <p>Path to the folder containing serialized complexly represented items with one or more exogenous property to load</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>item_exo_properties</code> <p>Set or Dict which contains representations to load from items. Use a <code>Set</code> if you want to load all properties from specific representations, or use a <code>Dict</code> if you want to choose which properties to load from specific representations</p> <p> TYPE: <code>Union[Dict, set]</code> DEFAULT: <code>None</code> </p> <code>users_directory</code> <p>Path to the folder containing serialized complexly represented users with one or more exogenous property to load</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>user_exo_properties</code> <p>Set or Dict which contains representations to load from items. Use a <code>Set</code> if you want to load all properties from specific representations, or use a <code>Dict</code> if you want to choose which properties to load from specific representations</p> <p> TYPE: <code>Union[Dict, set]</code> DEFAULT: <code>None</code> </p> <code>link_label</code> <p>If specified, each link between user and item nodes will be labeled with the given label. Default is None</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>metric_list</code> <p>List of metric with which predictions computed by the GBRS will be evaluated</p> <p> TYPE: <code>List[Metric]</code> DEFAULT: <code>None</code> </p> <code>report</code> <p>If <code>True</code>, a <code>yml</code> report will be produced for the recsys phase. It will be also produced for the evaluation phase, but only if the <code>metric_list</code> parameter is set</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>output_folder</code> <p>Path of the folder where all the results of the experiment will be saved</p> <p> TYPE: <code>str</code> DEFAULT: <code>'experiment_result'</code> </p> <code>overwrite_if_exists</code> <p>If <code>True</code> and the path set in <code>output_folder</code> points to an already existent directory, it will be deleted and replaced by the folder containing the experiment results</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>clayrs/recsys/experiment.py</code> <pre><code>def __init__(self, original_ratings: Ratings,\n             partitioning_technique: Partitioning,\n             algorithm_list: List[GraphBasedAlgorithm],\n             items_directory: str = None,\n             item_exo_properties: Union[Dict, set] = None,\n             users_directory: str = None,\n             user_exo_properties: Union[Dict, set] = None,\n             link_label: str = None,\n             metric_list: List[Metric] = None,\n             report: bool = False,\n             output_folder: str = \"experiment_result\",\n             overwrite_if_exists: bool = False):\n\n    super().__init__(original_ratings, partitioning_technique, algorithm_list, items_directory, users_directory,\n                     metric_list, report, output_folder, overwrite_if_exists)\n\n    self.item_exo_properties = item_exo_properties\n    self.user_exo_properties = user_exo_properties\n    self.link_label = link_label\n</code></pre>"},{"location":"recsys/experiment/#clayrs.recsys.experiment.GraphBasedExperiment.predict","title":"<code>predict(user_id_list=None, methodology=TestRatingsMethodology(), num_cpus=1, skip_alg_error=True)</code>","text":"<p>Method used to perform an experiment which involves score predictions.</p> <p>The method will first split the original ratings passed in the constructor in train and test set, then a graph will be built depending on them:</p> <ul> <li>All nodes of the original ratings will be present, but the links (interactions) that are present in the test set will be missing, so to make the training phase fair</li> </ul> <p>Info</p> <p>BE CAREFUL: not all algorithms are able to perform score prediction. In case a pure ranking algorithm is asked to perform score prediction, the <code>NotPredictionAlg</code> will be raised. if the <code>skip_alg_error</code> is set to <code>True</code>, then said exception will be caught, a warning will be printed, and the experiment will go on with the next algorithm</p> <p>Via the <code>methodology</code> parameter you can perform different candidate item selection. By default, the <code>TestRatingsMethodology()</code> is used: so, for each user, items in its test set only will be considered for score prediction</p> PARAMETER DESCRIPTION <code>user_id_list</code> <p>List of users for which you want to compute the ranking. If None, the ranking will be computed for all users of the <code>test_set</code></p> <p> TYPE: <code>Optional[Union[List[str], List[int]]]</code> DEFAULT: <code>None</code> </p> <code>methodology</code> <p><code>Methodology</code> object which governs the candidate item selection. Default is <code>TestRatingsMethodology</code></p> <p> TYPE: <code>Optional[Methodology]</code> DEFAULT: <code>TestRatingsMethodology()</code> </p> <code>num_cpus</code> <p>number of processors that must be reserved for the method. Default is 0, meaning that the number of cpus will be automatically detected.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>skip_alg_error</code> <p>If set to <code>True</code>, a pure ranking algorithm will be skipped and the experiment will continue with the following algorithm. Otherwise, the <code>NotPredictionAlg</code> exception raised will be re-raised</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RAISES DESCRIPTION <code>NotPredictionAlg</code> <p>When a pure ranking algorithm is asked to perform score prediction and <code>skip_alg_error</code> == False</p> Source code in <code>clayrs/recsys/experiment.py</code> <pre><code>def predict(self,\n            user_id_list: Optional[Union[List[str], List[int]]] = None,\n            methodology: Optional[Methodology] = TestRatingsMethodology(),\n            num_cpus: int = 1,\n            skip_alg_error: bool = True):\n\"\"\"\n    Method used to perform an experiment which involves ***score predictions***.\n\n    The method will first split the original ratings passed in the constructor in train and test set, then\n    a graph will be built depending on them:\n\n    * All nodes of the original ratings will be present, but the *links* (***interactions***) that are present in\n    the test set will be missing, so to make the training phase *fair*\n\n    !!! info\n\n        **BE CAREFUL**: not all algorithms are able to perform *score prediction*. In case a pure ranking algorithm\n        is asked to perform score prediction, the `NotPredictionAlg` will be raised. if the `skip_alg_error` is set\n        to `True`, then said exception will be caught, a warning will be printed, and the experiment will go on with\n        the next algorithm\n\n    Via the `methodology` parameter you can perform different candidate item selection. By default, the\n    `TestRatingsMethodology()` is used: so, for each user, items in its test set only will be considered for score\n    prediction\n\n    Args:\n        user_id_list: List of users for which you want to compute the ranking. If None, the ranking will be computed\n            for all users of the `test_set`\n        methodology: `Methodology` object which governs the candidate item selection. Default is\n            `TestRatingsMethodology`\n        num_cpus: number of processors that must be reserved for the method. Default is 0, meaning that\n            the number of cpus will be automatically detected.\n        skip_alg_error: If set to `True`, a pure ranking algorithm will be skipped and the experiment will continue\n            with the following algorithm. Otherwise, the `NotPredictionAlg` exception raised will be re-raised\n\n    Raises:\n        NotPredictionAlg: When a pure ranking algorithm is asked to perform score prediction\n            and `skip_alg_error` == False\n    \"\"\"\n    def gb_fit_predict(split_num, alg, train_set, test_set, dirname):\n\n        graph = NXFullGraph(self.original_ratings,\n                            item_contents_dir=self.items_directory,\n                            item_exo_properties=self.item_exo_properties,\n                            user_contents_dir=self.users_directory,\n                            user_exo_properties=self.user_exo_properties,\n                            link_label=self.link_label)\n\n        for user, item in zip(test_set.user_id_column, test_set.item_id_column):\n            graph.remove_link(UserNode(user), ItemNode(item))\n\n        gbrs = GraphBasedRS(alg, graph)\n\n        predict_alg = gbrs.predict(test_set, methodology=methodology,\n                                   num_cpus=num_cpus, user_list=user_id_list)\n\n        predict_alg.to_csv(f\"{self.output_folder}/{dirname}\", file_name=f\"rs_predict_split{split_num}\")\n\n        if self.report:\n            Report(output_dir=f\"{self.output_folder}/{dirname}\").yaml(original_ratings=self.original_ratings,\n                                                                      partitioning_technique=self.pt,\n                                                                      recsys=gbrs)\n\n        return predict_alg\n\n    self.main_experiment(gb_fit_predict, skip_alg_error=skip_alg_error)\n</code></pre>"},{"location":"recsys/experiment/#clayrs.recsys.experiment.GraphBasedExperiment.rank","title":"<code>rank(n_recs=10, user_id_list=None, methodology=TestRatingsMethodology(), num_cpus=1)</code>","text":"<p>Method used to perform an experiment which involves rankings.</p> <p>The method will first split the original ratings passed in the constructor in train and test set, then a graph will be built depending on them:</p> <ul> <li>All nodes of the original ratings will be present, but the links (interactions) that are present in the test set will be missing, so to make the training phase fair</li> </ul> <p>Via the <code>methodology</code> parameter you can perform different candidate item selection. By default, the <code>TestRatingsMethodology()</code> is used, so for each user, items in its test set only will be eligible for ranking</p> PARAMETER DESCRIPTION <code>n_recs</code> <p>Number of the top items that will be present in the ranking of each user. If <code>None</code> all candidate items will be returned for the user</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>10</code> </p> <code>user_id_list</code> <p>List of users for which you want to compute the ranking. If None, the ranking will be computed for all users of the <code>test_set</code></p> <p> TYPE: <code>Optional[Union[List[str], List[int]]]</code> DEFAULT: <code>None</code> </p> <code>methodology</code> <p><code>Methodology</code> object which governs the candidate item selection. Default is <code>TestRatingsMethodology</code></p> <p> TYPE: <code>Optional[Methodology]</code> DEFAULT: <code>TestRatingsMethodology()</code> </p> <code>num_cpus</code> <p>number of processors that must be reserved for the method. Default is 0, meaning that the number of cpus will be automatically detected.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> Source code in <code>clayrs/recsys/experiment.py</code> <pre><code>def rank(self,\n         n_recs: Optional[int] = 10,\n         user_id_list: Optional[Union[List[str], List[int]]] = None,\n         methodology: Optional[Methodology] = TestRatingsMethodology(),\n         num_cpus: int = 1):\n\"\"\"\n    Method used to perform an experiment which involves ***rankings***.\n\n    The method will first split the original ratings passed in the constructor in train and test set, then\n    a graph will be built depending on them:\n\n    * All nodes of the original ratings will be present, but the *links* (***interactions***) that are present in\n    the test set will be missing, so to make the training phase *fair*\n\n    Via the `methodology` parameter you can perform different candidate item selection. By default, the\n    `TestRatingsMethodology()` is used, so for each user, items in its test set only will be eligible for ranking\n\n    Args:\n        n_recs: Number of the top items that will be present in the ranking of each user.\n            If `None` all candidate items will be returned for the user\n        user_id_list: List of users for which you want to compute the ranking. If None, the ranking will be computed\n            for all users of the `test_set`\n        methodology: `Methodology` object which governs the candidate item selection. Default is\n            `TestRatingsMethodology`\n        num_cpus: number of processors that must be reserved for the method. Default is 0, meaning that\n            the number of cpus will be automatically detected.\n\n    \"\"\"\n    def gb_fit_rank(split_num, alg, train_set, test_set, dirname):\n\n        graph = NXFullGraph(self.original_ratings,\n                            item_contents_dir=self.items_directory,\n                            item_exo_properties=self.item_exo_properties,\n                            user_contents_dir=self.users_directory,\n                            user_exo_properties=self.user_exo_properties,\n                            link_label=self.link_label)\n\n        for user, item in zip(test_set.user_id_column, test_set.item_id_column):\n            graph.remove_link(UserNode(user), ItemNode(item))\n\n        gbrs = GraphBasedRS(alg, graph)\n\n        predict_alg = gbrs.rank(test_set, n_recs=n_recs, methodology=methodology,\n                                num_cpus=num_cpus, user_list=user_id_list)\n\n        predict_alg.to_csv(f\"{self.output_folder}/{dirname}\", file_name=f\"rs_rank_split{split_num}\")\n\n        if self.report:\n            Report(output_dir=f\"{self.output_folder}/{dirname}\").yaml(original_ratings=self.original_ratings,\n                                                                      partitioning_technique=self.pt,\n                                                                      recsys=gbrs)\n\n        return predict_alg\n\n    self.main_experiment(gb_fit_rank)\n</code></pre>"},{"location":"recsys/introduction/","title":"Introduction","text":"<p>Warning</p> <p>Docs are complete, but revision is still a Work in Progress. Sorry for any typos!</p>"},{"location":"recsys/introduction/#introduction","title":"Introduction","text":"<p>The Recommender System module lets you easily build a Content Based Recommender System (CBRS) or a Graph Based Recommender system (GBRS) with various algorithms.</p> <p>Info</p> <p>The Recsys module is grounded on contents created with the Content Analyzer</p> <p>The following will introduce you to the standard usage pipeline for this module, starting from importing the dataset to generating recommendation lists.</p>"},{"location":"recsys/introduction/#importing-the-dataset","title":"Importing the dataset","text":"<p>The Ratings class allows you to import rating from a source file (or also from an existent dataframe) into a custom object.</p> <p>If the source file contains users, items and ratings in this order, no additional parameters are needed, otherwise  the mapping must be explictly specified using:</p> <ul> <li>'user_id' column,</li> <li>'item_id' column,</li> <li>'score' column</li> </ul> <p>In this case the dataset we want to import is a CSV file with the following header:</p> <pre><code>user_id,item_id,rating,timestamp\n</code></pre> <p>As you can see the user id column, item id column and score column are the first three column and are already in sequential order, so no additional parameter is required to the <code>Ratings</code> class:</p> <pre><code>import clayrs.content_analyzer as ca\n\nratings_raw_source = ca.CSVFile('ratings.csv') # (1)\n\nratings = ca.Ratings(ratings_raw_source)\n</code></pre> <ol> <li>In this case our raw source is a CSV file, but ClayRS can also read from JSON files, DAT files and more</li> </ol>"},{"location":"recsys/introduction/#splitting-the-dataset","title":"Splitting the dataset","text":"<p>Once you imported the dataset, the first thing you may want to do is to split it with a Partitioning technique</p> <ul> <li>The output of any partitioning technique are two lists. The first containing all the train set produced by the partitioning technique (two train set in the below example), the other containing all the test set produced by the partitioning technique (two test set in the below example)</li> </ul> <pre><code>import clayrs.recsys as rs\n\n# kfold partitioning technique\nkf = rs.KFoldPartitioning(n_splits=2)\n\ntrain_list, test_list = kf.split_all(ratings) # (1)\n</code></pre> <ol> <li>You can pass to the <code>split_all()</code> method a specific <code>user_id_list</code> in case you only want to perform the splitting operation for a specific subset of users (e.g. select only users with more than x ratings)</li> </ol>"},{"location":"recsys/introduction/#defining-a-content-based-recommender-system","title":"Defining a Content Based Recommender System","text":"<p>A Content Based Recommender System needs an algorithm for ranking or predicting items to users. There are many available, in the following example we will use the CentroidVector algorithm:</p> <ul> <li>It computes the centroid vector of the features of items liked by the user</li> <li>It computes the similarity between the centroid vector and unrated items</li> </ul> <p>The items liked by a user are those having a rating higher or equal than a specific threshold. If the threshold is not specified, the average score of all items liked by the user is used.</p> <p>As already said, the Recommender System leverages the representations defined by the Content Analyzer. Suppose you have complexly represented the 'plot' with a simple TfIdf technique and assigned to this representation the <code>tfidf</code> id:</p> <p><pre><code>import clayrs.recsys as rs\n\ncentroid_vec = rs.CentroidVector(\n    {'plot': 'tfidf'},\n\n    similarity=rs.CosineSimilarity()\n)\n</code></pre> You can reference representation for a field also with an integer, in case you didn't assign any custom id during Content Analyzer phase.</p> <pre><code>centroid_vec = rs.CentroidVector(\n    {'plot': 0},  # (1)\n\n    similarity=rs.CosineSimilarity()\n)\n</code></pre> <ol> <li>This means that you want to use the first representation with which the 'plot' field was complexly represented</li> </ol> <p>Please note that multiple representations could be adopted for a single field, and also multiple representations for multiple fields can be combined together! Simply specify them in the <code>item_field</code> dict that must be passed to any Content Based algorithm:</p> <pre><code>centroid_vec = rs.CentroidVector(\n    {'plot': [0, 'glove-50', 'glove-100'],\n     'genre': ['tfidf', 'fasttext']},\n\n    similarity=rs.CosineSimilarity()\n)\n</code></pre> <p>After choosing the algorithm, you are ready to instantiate the <code>ContentBasedRS</code> class.</p> <p>A CBRS needs the following parameters:</p> <ul> <li>The recommendation  algorithm</li> <li>The train set</li> <li>The path of the items serialized by the Content Analyzer</li> </ul> <pre><code>train_set = test_list[0] # (1)\n\ncbrs = rs.ContentBasedRS(random_forests, train_set, 'movies_codified/')\n</code></pre> <ol> <li>Since every partitioning technique returns a list of train sets (here), in this way we are using only the first train set produced. Just below there's an example on how to produce recommendation for more than one split</li> </ol>"},{"location":"recsys/introduction/#defining-a-graph-based-recommender-system","title":"Defining a Graph Based Recommender System","text":"<p>A Graph Based Recommender System (GBRS) requires to first define a graph</p> <p>Ratings imported are used to create a Full Graph where property nodes (e.g. gender for users, budget for movies) can be linked to every node without any restriction</p> <p>The framework also allows to create a Bipartite Graph (a graph without property node) and a Tripartite Graph (where property nodes are only linked to item nodes)</p> <p>In order to load properties in the graph, we must specify where users and items are serialized and which properties to add (the following is the same for item_exo_properties):</p> <ul> <li> <p>If user_exo_properties is specified as a set, then the graph will try to load all properties from said exogenous representation <pre><code># example\n{'my_exo_id'}\n</code></pre></p> </li> <li> <p>If user_exo_properties is specified as a dict, then the graph will try to load said properties from said exogenous representation <pre><code># example\n{'my_exo_id': ['my_prop1', 'my_prop2']]}\n</code></pre></p> </li> </ul> <p>Let's now create the graph loading all properties:</p> <pre><code>full_graph = rs.NXFullGraph(ratings, \n                            user_contents_dir='users_codified/', # (1)\n                            item_contents_dir='movies_codified/', # (2)\n                            user_exo_properties={0}, # (3)\n                            item_exo_properties={'dbpedia'}, # (4)\n                            link_label='score')\n</code></pre> <ol> <li>Where users complexly represented have been serialized during Content Analyzer phase</li> <li>Where items complexly represented have been serialized during Content Analyzer phase</li> <li>This means that you want to use the first exogenous representation with which each user has been expanded</li> <li>You can also access exogenous representation with custom id, if specified during Content Analyzer phase</li> </ol> <p>The last step to perform before defining the GBRS is to instantiate an algorithm for ranking or predicting items to users.</p> <p>In the following example we use the Personalized PageRank algorithm:</p> <pre><code>pr = rs.NXPageRank(personalized=True)\n</code></pre> <p>Finally we can instantiate the GBRS!</p> <pre><code>gbrs = rs.GraphBasedRS(pr, full_graph)\n</code></pre>"},{"location":"recsys/introduction/#generating-recommendations","title":"Generating recommendations","text":"<p>Info</p> <p>The following procedure works both for CBRS and GBRS. In the following we will consider a cbrs as an example</p> <ul> <li>For GBRS there is no <code>fit()</code> method, only <code>rank()</code> or <code>predict()</code> method must be called</li> </ul> <p>Now the cbrs must be fit before we can compute the rank:</p> <ul> <li> <p>We could do this in two separate steps, by first calling the <code>fit(..)</code> method and then the <code>rank(...)</code> method </p> </li> <li> <p>Or by calling directly the <code>fit_rank(...)</code> method, which performs both in one step</p> </li> </ul> <p>In this case we choose the first method:</p> <pre><code>cbrs.fit()\n\ntest_set = test_list[0] # (1)\n\nrank = cbrs.rank(test_set, n_recs=10)  # top-10 recommendation for each user\n</code></pre> <ol> <li>Since every partitioning technique returns a list of test sets (here), in this way we are using only the first train set produced. Just below there's an example on how to produce recommendation for more than one split</li> </ol> <p>In case you perform a splitting of the dataset which returns a multiple train and test sets (KFold technique):</p> <pre><code>original_rat = ca.Ratings(ca.CSVFile(ratings_path))\n\ntrain_list, test_list = rs.KFoldPartitioning(n_splits=5).split_all(original_rat)\n\nalg = rs.CentroidVector()  # any cb algorithm\n\nfor train_set, test_set in zip(train_list, test_list):\n\n    cbrs = rs.ContentBasedRS(alg, train_set, items_path)\n    rank_to_append = cbrs.fit_rank(test_set)\n\n    result_list.append(rank_to_append)\n</code></pre> <p><code>result_list</code> will contain recommendation lists for each split</p>"},{"location":"recsys/introduction/#customizing-the-ranking-process","title":"Customizing the ranking process","text":"<p>You can customize the ranking process by changing the parameters of the <code>rank(...)</code> method</p> <ul> <li>You can choice for which users to produce recommendations:</li> </ul> <pre><code>rank = cbrs.rank(test_set, user_list=['u1', 'u23', 'u56'])\n</code></pre> <ul> <li>If a cut rank list for each user must be produced:</li> </ul> <pre><code>rank = cbrs.rank(test_set, n_recs=10)\n</code></pre> <ul> <li>If a different methodology must be used:</li> </ul> <p>Info</p> <p>A methodology lets you customize which items must be ranked for each user. For each target user \\(u\\), the following 4 different methodologies are available for defining those lists:</p> <ol> <li>TestRatings (default): the list of items to be evaluated consists of items rated by \\(u\\) in the test set</li> <li>TestItems: every item in the test set of every user except those in the training set of \\(u\\) will be predicted</li> <li>TrainingItems: every item in the training set of every user will be predicted except those in the training set of \\(u\\)</li> <li>AllItems: the whole set of items defined will be predicted, except those in the training set of \\(u\\)</li> </ol> <p>More information on this paper.</p> <p>By default the methodology used is the TestRatings methodology</p> <pre><code>rank = cbrs.rank(test_set, methodology=rs.TrainingItemsMethodology())\n</code></pre>"},{"location":"recsys/introduction/#generating-score-predictions","title":"Generating score predictions","text":"<p>Some algorithm (e.g. LinearPredictor algorithm) are able to predict the numeric rating that a user would give to unseen items.</p> <p>The usage is exactly the same of generating recommendations and  customizing the ranking process, the only thing that changes is the method to call:</p> <pre><code>score_prediction = cbrs.fit_predict(test_set)\n</code></pre> <p>or:</p> <pre><code>cbrs.fit()\n\nscore_prediction = cbrs.predict(test_set)\n</code></pre> <p>Note: if the <code>predict()</code> or the <code>fit_predict()</code> method is called for an algorithm that is not able to perform score prediction, the <code>NotPredictionAlg</code> exception is raised</p>"},{"location":"recsys/content_based/content_based_recsys/","title":"Content Based RecSys","text":""},{"location":"recsys/content_based/content_based_recsys/#clayrs.recsys.recsys.ContentBasedRS","title":"<code>ContentBasedRS(algorithm, train_set, items_directory, users_directory=None)</code>","text":"<p>         Bases: <code>RecSys</code></p> <p>Class for recommender systems which use the items' content in order to make predictions, some algorithms may also use users' content, so it's an optional parameter.</p> <p>Every CBRS differ from each other based the algorithm used.</p> <p>Examples:</p> <p>In case you perform a splitting of the dataset which returns a single train and test set (e.g. HoldOut technique):</p> Single split train<pre><code>from clayrs import recsys as rs\nfrom clayrs import content_analyzer as ca\n\noriginal_rat = ca.Ratings(ca.CSVFile(ratings_path))\n\n[train], [test] = rs.HoldOutPartitioning().split_all(original_rat)\n\nalg = rs.CentroidVector()  # any cb algorithm\n\ncbrs = rs.ContentBasedRS(alg, train, items_path)\n\nrank = cbrs.fit_rank(test, n_recs=10)\n</code></pre> <p>In case you perform a splitting of the dataset which returns a multiple train and test sets (KFold technique):</p> Multiple split train<pre><code>from clayrs import recsys as rs\nfrom clayrs import content_analyzer as ca\n\noriginal_rat = ca.Ratings(ca.CSVFile(ratings_path))\n\ntrain_list, test_list = rs.KFoldPartitioning(n_splits=5).split_all(original_rat)\n\nalg = rs.CentroidVector()  # any cb algorithm\n\nfor train_set, test_set in zip(train_list, test_list):\n\n    cbrs = rs.ContentBasedRS(alg, train_set, items_path)\n    rank_to_append = cbrs.fit_rank(test_set)\n\n    result_list.append(rank_to_append)\n</code></pre> <p><code>result_list</code> will contain recommendation lists for each split</p> PARAMETER DESCRIPTION <code>algorithm</code> <p>the content based algorithm that will be used in order to rank or make score prediction</p> <p> TYPE: <code>ContentBasedAlgorithm</code> </p> <code>train_set</code> <p>a Ratings object containing interactions between users and items</p> <p> TYPE: <code>Ratings</code> </p> <code>items_directory</code> <p>the path of the items serialized by the Content Analyzer</p> <p> TYPE: <code>str</code> </p> <code>users_directory</code> <p>the path of the users serialized by the Content Analyzer</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> Source code in <code>clayrs/recsys/recsys.py</code> <pre><code>def __init__(self,\n             algorithm: ContentBasedAlgorithm,\n             train_set: Ratings,\n             items_directory: str,\n             users_directory: str = None):\n\n    super().__init__(algorithm)\n    self.__train_set = train_set\n    self.__items_directory = items_directory\n    self.__users_directory = users_directory\n    self.fit_alg = None\n</code></pre>"},{"location":"recsys/content_based/content_based_recsys/#clayrs.recsys.recsys.ContentBasedRS.algorithm","title":"<code>algorithm: ContentBasedAlgorithm</code>  <code>property</code>","text":"<p>The content based algorithm chosen</p>"},{"location":"recsys/content_based/content_based_recsys/#clayrs.recsys.recsys.ContentBasedRS.items_directory","title":"<code>items_directory: str</code>  <code>property</code>","text":"<p>Path of the serialized items by the Content Analyzer</p>"},{"location":"recsys/content_based/content_based_recsys/#clayrs.recsys.recsys.ContentBasedRS.train_set","title":"<code>train_set: Ratings</code>  <code>property</code>","text":"<p>The train set of the Content Based RecSys</p>"},{"location":"recsys/content_based/content_based_recsys/#clayrs.recsys.recsys.ContentBasedRS.users_directory","title":"<code>users_directory: str</code>  <code>property</code>","text":"<p>Path of the serialized users by the Content Analyzer</p>"},{"location":"recsys/content_based/content_based_recsys/#clayrs.recsys.recsys.ContentBasedRS.fit","title":"<code>fit(num_cpus=1)</code>","text":"<p>Method which will fit the algorithm chosen for each user in the train set passed in the constructor</p> <p>If the algorithm can't be fit for some users, a warning message is printed showing the number of users for which the alg couldn't be fit</p> PARAMETER DESCRIPTION <code>num_cpus</code> <p>number of processors that must be reserved for the method. If set to <code>0</code>, all cpus available will be used. Be careful though: multiprocessing in python has a substantial memory overhead!</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> Source code in <code>clayrs/recsys/recsys.py</code> <pre><code>def fit(self, num_cpus: int = 1):\n\"\"\"\n    Method which will fit the algorithm chosen for each user in the train set passed in the constructor\n\n    If the algorithm can't be fit for some users, a warning message is printed showing the number of users\n    for which the alg couldn't be fit\n\n    Args:\n        num_cpus: number of processors that must be reserved for the method. If set to `0`, all cpus available will\n            be used. Be careful though: multiprocessing in python has a substantial memory overhead!\n\n    \"\"\"\n    self.fit_alg = self.algorithm.fit(train_set=self.train_set,\n                                      items_directory=self.items_directory,\n                                      num_cpus=num_cpus)\n\n    return self\n</code></pre>"},{"location":"recsys/content_based/content_based_recsys/#clayrs.recsys.recsys.ContentBasedRS.fit_predict","title":"<code>fit_predict(test_set, user_list=None, methodology=TestRatingsMethodology(), save_fit=False, num_cpus=1)</code>","text":"<p>Method used to both fit and calculate score prediction for all users in test set or all users in <code>user_list</code> parameter. The Recommender System will first be fit for each user in the <code>test_set</code> parameter or for each user inside the <code>user_list</code> parameter: the <code>user_list</code> parameter could contain users with their string id or with their mapped integer</p> <p>BE CAREFUL: not all algorithms are able to perform score prediction</p> <p>Via the <code>methodology</code> parameter you can perform different candidate item selection. By default, the <code>TestRatingsMethodology()</code> is used: so, for each user, items in its test set only will be considered for score prediction</p> <p>If the algorithm couldn't be fit for some users, they will be skipped and a warning message is printed showing the number of users for which the alg couldn't produce a ranking</p> <p>With the <code>save_fit</code> parameter you can decide if you want that you recommender system remains fit even after the complete execution of this method, in case you want to compute ranking/score prediction with other methodologies, or with a different <code>n_recs</code> parameter. Be mindful since it can be memory-expensive, thus by default this behaviour is disabled</p> PARAMETER DESCRIPTION <code>test_set</code> <p>Ratings object which represents the ground truth of the split considered</p> <p> TYPE: <code>Ratings</code> </p> <code>user_list</code> <p>List of users for which you want to compute score prediction. If None, the ranking will be computed for all users of the <code>test_set</code>. The list should contain user id as strings or user ids mapped to their integers</p> <p> TYPE: <code>List</code> DEFAULT: <code>None</code> </p> <code>methodology</code> <p><code>Methodology</code> object which governs the candidate item selection. Default is <code>TestRatingsMethodology</code>. If None, AllItemsMethodology() will be used</p> <p> TYPE: <code>Union[Methodology, None]</code> DEFAULT: <code>TestRatingsMethodology()</code> </p> <code>save_fit</code> <p>Boolean value which let you choose if the Recommender System should remain fit even after the complete execution of this method. Default is False</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>num_cpus</code> <p>number of processors that must be reserved for the method. If set to <code>0</code>, all cpus available will be used. Be careful though: multiprocessing in python has a substantial memory overhead!</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> RETURNS DESCRIPTION <code>Prediction</code> <p>Prediction object containing score prediction lists for all users of the test set or for all users in <code>user_list</code></p> Source code in <code>clayrs/recsys/recsys.py</code> <pre><code>def fit_predict(self, test_set: Ratings, user_list: List = None,\n                methodology: Union[Methodology, None] = TestRatingsMethodology(),\n                save_fit: bool = False, num_cpus: int = 1) -&gt; Prediction:\n\"\"\"\n    Method used to both fit and calculate score prediction for all users in test set or all users in `user_list`\n    parameter.\n    The Recommender System will first be fit for each user in the `test_set` parameter or for each\n    user inside the `user_list` parameter: the `user_list` parameter could contain users with their string id or\n    with their mapped integer\n\n    **BE CAREFUL**: not all algorithms are able to perform *score prediction*\n\n    Via the `methodology` parameter you can perform different candidate item selection. By default, the\n    `TestRatingsMethodology()` is used: so, for each user, items in its test set only will be considered for score\n    prediction\n\n    If the algorithm couldn't be fit for some users, they will be skipped and a warning message is printed showing\n    the number of users for which the alg couldn't produce a ranking\n\n    With the `save_fit` parameter you can decide if you want that you recommender system remains *fit* even after\n    the complete execution of this method, in case you want to compute ranking/score prediction with other\n    methodologies, or with a different `n_recs` parameter. Be mindful since it can be memory-expensive,\n    thus by default this behaviour is disabled\n\n    Args:\n        test_set: Ratings object which represents the ground truth of the split considered\n        user_list: List of users for which you want to compute score prediction. If None, the ranking\n            will be computed for all users of the `test_set`. The list should contain user id as strings or user ids\n            mapped to their integers\n        methodology: `Methodology` object which governs the candidate item selection. Default is\n            `TestRatingsMethodology`. If None, AllItemsMethodology() will be used\n        save_fit: Boolean value which let you choose if the Recommender System should remain fit even after the\n            complete execution of this method. Default is False\n        num_cpus: number of processors that must be reserved for the method. If set to `0`, all cpus available will\n            be used. Be careful though: multiprocessing in python has a substantial memory overhead!\n\n    Returns:\n        Prediction object containing score prediction lists for all users of the test set or for all users in\n            `user_list`\n    \"\"\"\n\n    logger.info(\"Don't worry if it looks stuck at first\")\n    logger.info(\"First iterations will stabilize the estimated remaining time\")\n\n    all_users = test_set.unique_user_idx_column\n    if user_list is not None:\n        all_users = np.array(user_list)\n        if np.issubdtype(all_users.dtype, str):\n            all_users = self.train_set.user_map.convert_seq_str2int(all_users)\n\n    all_users = set(all_users)\n\n    if methodology is None:\n        methodology = AllItemsMethodology()\n\n    methodology.setup(self.train_set, test_set)\n\n    fit_alg, pred = self.algorithm.fit_predict(self.train_set, test_set, user_idx_list=all_users,\n                                               items_directory=self.items_directory,\n                                               methodology=methodology, num_cpus=num_cpus, save_fit=save_fit)\n\n    self.fit_alg = fit_alg\n\n    # we should remove empty uir matrices otherwise vstack won't work due to dimensions mismatch\n    pred = [uir_pred for uir_pred in pred if len(uir_pred) != 0]\n\n    # can't vstack when pred is empty\n    if len(pred) == 0:\n        pred = Prediction.from_uir(np.array([]), user_map=test_set.user_map, item_map=test_set.item_map)\n        return pred\n\n    pred = Prediction.from_uir(np.vstack(pred), user_map=test_set.user_map, item_map=test_set.item_map)\n\n    self._yaml_report = {'mode': 'score_prediction', 'methodology': repr(methodology)}\n\n    return pred\n</code></pre>"},{"location":"recsys/content_based/content_based_recsys/#clayrs.recsys.recsys.ContentBasedRS.fit_rank","title":"<code>fit_rank(test_set, n_recs=10, user_list=None, methodology=TestRatingsMethodology(), save_fit=False, num_cpus=1)</code>","text":"<p>Method used to both fit and calculate ranking for all users in test set or all users in <code>user_list</code> parameter. The Recommender System will first be fit for each user in the <code>test_set</code> parameter or for each user inside the <code>user_list</code> parameter: the <code>user_list</code> parameter could contain users with their string id or with their mapped integer</p> <p>If the <code>n_recs</code> is specified, then the rank will contain the top-n items for the users. Otherwise, the rank will contain all unrated items of the particular users. By default the top-10 ranking is computed for each user</p> <p>Via the <code>methodology</code> parameter you can perform different candidate item selection. By default, the <code>TestRatingsMethodology()</code> is used: so, for each user, items in its test set only will be ranked</p> <p>If the algorithm couldn't be fit for some users, they will be skipped and a warning message is printed showing the number of users for which the alg couldn't produce a ranking</p> <p>With the <code>save_fit</code> parameter you can decide if you want that you recommender system remains fit even after the complete execution of this method, in case you want to compute ranking with other methodologies, or with a different <code>n_recs</code> parameter. Be mindful since it can be memory-expensive, thus by default this behaviour is disabled</p> PARAMETER DESCRIPTION <code>test_set</code> <p>Ratings object which represents the ground truth of the split considered</p> <p> TYPE: <code>Ratings</code> </p> <code>n_recs</code> <p>Number of the top items that will be present in the ranking of each user. If <code>None</code> all candidate items will be returned for the user. Default is 10 (top-10 for each user will be computed)</p> <p> TYPE: <code>int</code> DEFAULT: <code>10</code> </p> <code>user_list</code> <p>List of users for which you want to compute score prediction. If None, the ranking will be computed for all users of the <code>test_set</code>. The list should contain user id as strings or user ids mapped to their integers</p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>None</code> </p> <code>methodology</code> <p><code>Methodology</code> object which governs the candidate item selection. Default is <code>TestRatingsMethodology</code>. If None, AllItemsMethodology() will be used</p> <p> TYPE: <code>Union[Methodology, None]</code> DEFAULT: <code>TestRatingsMethodology()</code> </p> <code>save_fit</code> <p>Boolean value which let you choose if the Recommender System should remain fit even after the complete execution of this method. Default is False</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>num_cpus</code> <p>number of processors that must be reserved for the method. If set to <code>0</code>, all cpus available will be used. Be careful though: multiprocessing in python has a substantial memory overhead!</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> RAISES DESCRIPTION <code>NotFittedAlg</code> <p>Exception raised when this method is called without first calling the <code>fit</code> method</p> RETURNS DESCRIPTION <code>Rank</code> <p>Rank object containing recommendation lists for all users of the test set or for all users in <code>user_list</code></p> Source code in <code>clayrs/recsys/recsys.py</code> <pre><code>def fit_rank(self, test_set: Ratings, n_recs: int = 10, user_list: List[str] = None,\n             methodology: Union[Methodology, None] = TestRatingsMethodology(),\n             save_fit: bool = False, num_cpus: int = 1) -&gt; Rank:\n\"\"\"\n    Method used to both fit and calculate ranking for all users in test set or all users in `user_list`\n    parameter.\n    The Recommender System will first be fit for each user in the `test_set` parameter or for each\n    user inside the `user_list` parameter: the `user_list` parameter could contain users with their string id or\n    with their mapped integer\n\n    If the `n_recs` is specified, then the rank will contain the top-n items for the users.\n    Otherwise, the rank will contain all unrated items of the particular users.\n    By default the ***top-10*** ranking is computed for each user\n\n    Via the `methodology` parameter you can perform different candidate item selection. By default, the\n    `TestRatingsMethodology()` is used: so, for each user, items in its test set only will be ranked\n\n    If the algorithm couldn't be fit for some users, they will be skipped and a warning message is printed showing\n    the number of users for which the alg couldn't produce a ranking\n\n    With the `save_fit` parameter you can decide if you want that you recommender system remains *fit* even after\n    the complete execution of this method, in case you want to compute ranking with other methodologies, or\n    with a different `n_recs` parameter. Be mindful since it can be memory-expensive, thus by default this behaviour\n    is disabled\n\n    Args:\n        test_set: Ratings object which represents the ground truth of the split considered\n        n_recs: Number of the top items that will be present in the ranking of each user.\n            If `None` all candidate items will be returned for the user. Default is 10 (top-10 for each user\n            will be computed)\n        user_list: List of users for which you want to compute score prediction. If None, the ranking\n            will be computed for all users of the `test_set`. The list should contain user id as strings or user ids\n            mapped to their integers\n        methodology: `Methodology` object which governs the candidate item selection. Default is\n            `TestRatingsMethodology`. If None, AllItemsMethodology() will be used\n        save_fit: Boolean value which let you choose if the Recommender System should remain fit even after the\n            complete execution of this method. Default is False\n        num_cpus: number of processors that must be reserved for the method. If set to `0`, all cpus available will\n            be used. Be careful though: multiprocessing in python has a substantial memory overhead!\n\n    Raises:\n        NotFittedAlg: Exception raised when this method is called without first calling the `fit` method\n\n    Returns:\n        Rank object containing recommendation lists for all users of the test set or for all users in `user_list`\n    \"\"\"\n\n    logger.info(\"Don't worry if it looks stuck at first\")\n    logger.info(\"First iterations will stabilize the estimated remaining time\")\n\n    all_users = test_set.unique_user_idx_column\n    if user_list is not None:\n        all_users = np.array(user_list)\n        if np.issubdtype(all_users.dtype, str):\n            all_users = self.train_set.user_map.convert_seq_str2int(all_users)\n\n    all_users = set(all_users)\n\n    if methodology is None:\n        methodology = AllItemsMethodology()\n\n    methodology.setup(self.train_set, test_set)\n\n    fit_alg, rank = self.algorithm.fit_rank(self.train_set, test_set, user_idx_list=all_users,\n                                            items_directory=self.items_directory, n_recs=n_recs,\n                                            methodology=methodology, num_cpus=num_cpus, save_fit=save_fit)\n\n    self.fit_alg = fit_alg\n\n    # we should remove empty uir matrices otherwise vstack won't work due to dimensions mismatch\n    rank = [uir_rank for uir_rank in rank if len(uir_rank) != 0]\n\n    # can't vstack when rank is empty\n    if len(rank) == 0:\n        rank = Rank.from_uir(np.array([]), user_map=test_set.user_map, item_map=test_set.item_map)\n        return rank\n\n    rank = Rank.from_uir(np.vstack(rank), user_map=test_set.user_map, item_map=test_set.item_map)\n\n    self._yaml_report = {'mode': 'rank', 'n_recs': repr(n_recs), 'methodology': repr(methodology)}\n\n    return rank\n</code></pre>"},{"location":"recsys/content_based/content_based_recsys/#clayrs.recsys.recsys.ContentBasedRS.predict","title":"<code>predict(test_set, user_list=None, methodology=TestRatingsMethodology(), num_cpus=1)</code>","text":"<p>Method used to calculate score predictions for all users in test set or all users in <code>user_list</code> parameter. You must first call the <code>fit()</code> method before you can compute score predictions. The <code>user_list</code> parameter could contain users with their string id or with their mapped integer</p> <p>BE CAREFUL: not all algorithms are able to perform score prediction</p> <p>Via the <code>methodology</code> parameter you can perform different candidate item selection. By default, the <code>TestRatingsMethodology()</code> is used: so, for each user, items in its test set only will be considered for score prediction</p> <p>If the algorithm was not fit for some users, they will be skipped and a warning message is printed showing the number of users for which the alg couldn't produce a ranking</p> PARAMETER DESCRIPTION <code>test_set</code> <p>Ratings object which represents the ground truth of the split considered</p> <p> TYPE: <code>Ratings</code> </p> <code>user_list</code> <p>List of users for which you want to compute score prediction. If None, the ranking will be computed for all users of the <code>test_set</code>. The list should contain user id as strings or user ids mapped to their integers</p> <p> TYPE: <code>List</code> DEFAULT: <code>None</code> </p> <code>methodology</code> <p><code>Methodology</code> object which governs the candidate item selection. Default is <code>TestRatingsMethodology</code>. If None, AllItemsMethodology() will be used</p> <p> TYPE: <code>Union[Methodology, None]</code> DEFAULT: <code>TestRatingsMethodology()</code> </p> <code>num_cpus</code> <p>number of processors that must be reserved for the method. If set to <code>0</code>, all cpus available will be used. Be careful though: multiprocessing in python has a substantial memory overhead!</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> RAISES DESCRIPTION <code>NotFittedAlg</code> <p>Exception raised when this method is called without first calling the <code>fit</code> method</p> RETURNS DESCRIPTION <code>Prediction</code> <p>Prediction object containing score prediction lists for all users of the test set or for all users in <code>user_list</code></p> Source code in <code>clayrs/recsys/recsys.py</code> <pre><code>def predict(self, test_set: Ratings, user_list: List = None,\n            methodology: Union[Methodology, None] = TestRatingsMethodology(),\n            num_cpus: int = 1) -&gt; Prediction:\n\"\"\"\n    Method used to calculate score predictions for all users in test set or all users in `user_list` parameter.\n    You must first call the `fit()` method ***before*** you can compute score predictions.\n    The `user_list` parameter could contain users with their string id or with their mapped integer\n\n    **BE CAREFUL**: not all algorithms are able to perform *score prediction*\n\n    Via the `methodology` parameter you can perform different candidate item selection. By default, the\n    `TestRatingsMethodology()` is used: so, for each user, items in its test set only will be considered for score\n    prediction\n\n    If the algorithm was not fit for some users, they will be skipped and a warning message is printed showing the\n    number of users for which the alg couldn't produce a ranking\n\n    Args:\n        test_set: Ratings object which represents the ground truth of the split considered\n        user_list: List of users for which you want to compute score prediction. If None, the ranking\n            will be computed for all users of the `test_set`. The list should contain user id as strings or user ids\n            mapped to their integers\n        methodology: `Methodology` object which governs the candidate item selection. Default is\n            `TestRatingsMethodology`. If None, AllItemsMethodology() will be used\n        num_cpus: number of processors that must be reserved for the method. If set to `0`, all cpus available will\n            be used. Be careful though: multiprocessing in python has a substantial memory overhead!\n\n    Raises:\n        NotFittedAlg: Exception raised when this method is called without first calling the `fit` method\n\n    Returns:\n        Prediction object containing score prediction lists for all users of the test set or for all users in\n            `user_list`\n    \"\"\"\n\n    if self.fit_alg is None:\n        raise NotFittedAlg(\"Algorithm not fit! You must call the fit() method first, or fit_rank().\")\n\n    logger.info(\"Don't worry if it looks stuck at first\")\n    logger.info(\"First iterations will stabilize the estimated remaining time\")\n\n    all_users = test_set.unique_user_idx_column\n    if user_list is not None:\n        all_users = np.array(user_list)\n        if np.issubdtype(all_users.dtype, str):\n            all_users = self.train_set.user_map.convert_seq_str2int(all_users)\n\n    all_users = set(all_users)\n\n    if methodology is None:\n        methodology = AllItemsMethodology()\n\n    methodology.setup(self.train_set, test_set)\n\n    pred = self.algorithm.predict(self.fit_alg, self.train_set, test_set,\n                                  user_idx_list=all_users,\n                                  items_directory=self.items_directory,\n                                  methodology=methodology, num_cpus=num_cpus)\n\n    # we should remove empty uir matrices otherwise vstack won't work due to dimensions mismatch\n    pred = [uir_pred for uir_pred in pred if len(uir_pred) != 0]\n\n    # can't vstack when pred is empty\n    if len(pred) == 0:\n        pred = Prediction.from_uir(np.array([]), user_map=test_set.user_map, item_map=test_set.item_map)\n        return pred\n\n    pred = Prediction.from_uir(np.vstack(pred), user_map=test_set.user_map, item_map=test_set.item_map)\n\n    self._yaml_report = {'mode': 'score_prediction', 'methodology': repr(methodology)}\n\n    return pred\n</code></pre>"},{"location":"recsys/content_based/content_based_recsys/#clayrs.recsys.recsys.ContentBasedRS.rank","title":"<code>rank(test_set, n_recs=10, user_list=None, methodology=TestRatingsMethodology(), num_cpus=1)</code>","text":"<p>Method used to calculate ranking for all users in test set or all users in <code>user_list</code> parameter. You must first call the <code>fit()</code> method before you can compute the ranking. The <code>user_list</code> parameter could contain users with their string id or with their mapped integer</p> <p>If the <code>n_recs</code> is specified, then the rank will contain the top-n items for the users. Otherwise, the rank will contain all unrated items of the particular users. By default the top-10 ranking is computed for each user</p> <p>Via the <code>methodology</code> parameter you can perform different candidate item selection. By default, the <code>TestRatingsMethodology()</code> is used: so, for each user, items in its test set only will be ranked</p> <p>If the algorithm was not fit for some users, they will be skipped and a warning message is printed showing the number of users for which the alg couldn't produce a ranking</p> PARAMETER DESCRIPTION <code>test_set</code> <p>Ratings object which represents the ground truth of the split considered</p> <p> TYPE: <code>Ratings</code> </p> <code>n_recs</code> <p>Number of the top items that will be present in the ranking of each user. If <code>None</code> all candidate items will be returned for the user. Default is 10 (top-10 for each user will be computed)</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>10</code> </p> <code>user_list</code> <p>List of users for which you want to compute score prediction. If None, the ranking will be computed for all users of the <code>test_set</code>. The list should contain user id as strings or user ids mapped to their integers</p> <p> TYPE: <code>Union[List[str], List[int]]</code> DEFAULT: <code>None</code> </p> <code>methodology</code> <p><code>Methodology</code> object which governs the candidate item selection. Default is <code>TestRatingsMethodology</code>. If None, AllItemsMethodology() will be used</p> <p> TYPE: <code>Optional[Methodology]</code> DEFAULT: <code>TestRatingsMethodology()</code> </p> <code>num_cpus</code> <p>number of processors that must be reserved for the method. If set to <code>0</code>, all cpus available will be used. Be careful though: multiprocessing in python has a substantial memory overhead!</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> RAISES DESCRIPTION <code>NotFittedAlg</code> <p>Exception raised when this method is called without first calling the <code>fit</code> method</p> RETURNS DESCRIPTION <code>Rank</code> <p>Rank object containing recommendation lists for all users of the test set or for all users in <code>user_list</code></p> Source code in <code>clayrs/recsys/recsys.py</code> <pre><code>def rank(self, test_set: Ratings, n_recs: Optional[int] = 10, user_list: Union[List[str], List[int]] = None,\n         methodology: Optional[Methodology] = TestRatingsMethodology(),\n         num_cpus: int = 1) -&gt; Rank:\n\n\"\"\"\n    Method used to calculate ranking for all users in test set or all users in `user_list` parameter.\n    You must first call the `fit()` method ***before*** you can compute the ranking.\n    The `user_list` parameter could contain users with their string id or with their mapped integer\n\n    If the `n_recs` is specified, then the rank will contain the top-n items for the users.\n    Otherwise, the rank will contain all unrated items of the particular users.\n    By default the ***top-10*** ranking is computed for each user\n\n    Via the `methodology` parameter you can perform different candidate item selection. By default, the\n    `TestRatingsMethodology()` is used: so, for each user, items in its test set only will be ranked\n\n    If the algorithm was not fit for some users, they will be skipped and a warning message is printed showing the\n    number of users for which the alg couldn't produce a ranking\n\n    Args:\n        test_set: Ratings object which represents the ground truth of the split considered\n        n_recs: Number of the top items that will be present in the ranking of each user.\n            If `None` all candidate items will be returned for the user. Default is 10 (top-10 for each user\n            will be computed)\n        user_list: List of users for which you want to compute score prediction. If None, the ranking\n            will be computed for all users of the `test_set`. The list should contain user id as strings or user ids\n            mapped to their integers\n        methodology: `Methodology` object which governs the candidate item selection. Default is\n            `TestRatingsMethodology`. If None, AllItemsMethodology() will be used\n        num_cpus: number of processors that must be reserved for the method. If set to `0`, all cpus available will\n            be used. Be careful though: multiprocessing in python has a substantial memory overhead!\n\n    Raises:\n        NotFittedAlg: Exception raised when this method is called without first calling the `fit` method\n\n    Returns:\n        Rank object containing recommendation lists for all users of the test set or for all users in `user_list`\n    \"\"\"\n\n    if self.fit_alg is None:\n        raise NotFittedAlg(\"Algorithm not fit! You must call the fit() method first, or fit_rank().\")\n\n    logger.info(\"Don't worry if it looks stuck at first\")\n    logger.info(\"First iterations will stabilize the estimated remaining time\")\n\n    all_users = test_set.unique_user_idx_column\n    if user_list is not None:\n        all_users = np.array(user_list)\n        if np.issubdtype(all_users.dtype, str):\n            all_users = self.train_set.user_map.convert_seq_str2int(all_users)\n\n    all_users = set(all_users)\n\n    if methodology is None:\n        methodology = AllItemsMethodology()\n\n    methodology.setup(self.train_set, test_set)\n\n    rank = self.algorithm.rank(self.fit_alg, self.train_set, test_set,\n                               user_idx_list=all_users,\n                               items_directory=self.items_directory, n_recs=n_recs,\n                               methodology=methodology, num_cpus=num_cpus)\n\n    # we should remove empty uir matrices otherwise vstack won't work due to dimensions mismatch\n    rank = [uir_rank for uir_rank in rank if len(uir_rank) != 0]\n\n    # can't vstack when rank is empty\n    if len(rank) == 0:\n        rank = Rank.from_uir(np.array([]), user_map=test_set.user_map, item_map=test_set.item_map)\n        return rank\n\n    rank = Rank.from_uir(np.vstack(rank), user_map=test_set.user_map, item_map=test_set.item_map)\n\n    self._yaml_report = {'mode': 'rank', 'n_recs': repr(n_recs), 'methodology': repr(methodology)}\n\n    return rank\n</code></pre>"},{"location":"recsys/content_based/content_based_algorithms/centroid_vector/","title":"Centroid Vector","text":""},{"location":"recsys/content_based/content_based_algorithms/centroid_vector/#clayrs.recsys.content_based_algorithm.centroid_vector.CentroidVector","title":"<code>CentroidVector(item_field, similarity, threshold=None, embedding_combiner=Centroid())</code>","text":"<p>         Bases: <code>PerUserCBAlgorithm</code></p> <p>Class that implements a centroid-like recommender. It first gets the centroid of the items that the user liked. Then computes the similarity between the centroid and the item of which the ranking score must be predicted. It's a ranking algorithm, so it can't do score prediction</p> <ul> <li>It computes the centroid vector of the features of items liked by the user</li> <li>It computes the similarity between the centroid vector and the items of which the ranking score must be predicted</li> </ul> <p>The items liked by a user are those having a rating higher or equal than a specific threshold. If the threshold is not specified, the average score of all items liked by the user is used.</p> <p>Examples:</p> <ul> <li>Interested in only a field representation, <code>CosineSimilarity</code> as similarity, <code>threshold</code> \\(= 3\\) (Every item with rating \\(&gt;= 3\\) will be considered as positive)</li> </ul> <pre><code>&gt;&gt;&gt; from clayrs import recsys as rs\n&gt;&gt;&gt; alg = rs.CentroidVector({\"Plot\": 0}, rs.CosineSimilarity(), 3)\n</code></pre> <ul> <li>Interested in multiple field representations of the items, <code>CosineSimilarity</code> as similarity, <code>threshold</code> \\(= None\\) (Every item with rating \\(&gt;=\\) mean rating of the user will be considered as positive)</li> </ul> <pre><code>&gt;&gt;&gt; alg = rs.CentroidVector(\n&gt;&gt;&gt;                      item_field={\"Plot\": [0, \"tfidf\"],\n&gt;&gt;&gt;                                  \"Genre\": [0, 1],\n&gt;&gt;&gt;                                  \"Director\": \"doc2vec\"},\n&gt;&gt;&gt;                      similarity=rs.CosineSimilarity(),\n&gt;&gt;&gt;                      threshold=None)\n</code></pre> <p>Info</p> <p>After instantiating the <code>CentroidVector</code> algorithm, pass it in the initialization of a CBRS and the use its method to calculate ranking for single user or multiple users:</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; cbrs = rs.ContentBasedRS(algorithm=alg, ...)\n&gt;&gt;&gt; cbrs.fit_rank(...)\n&gt;&gt;&gt; # ...\n</code></pre> PARAMETER DESCRIPTION <code>item_field</code> <p>dict where the key is the name of the field that contains the content to use, value is the representation(s) id(s) that will be used for the said item. The value of a field can be a string or a list, use a list if you want to use multiple representations for a particular field.</p> <p> TYPE: <code>dict</code> </p> <code>similarity</code> <p><code>Similarity</code> object which handles how similarities between unseen items and centroid vector of positive items of the active user is computed</p> <p> TYPE: <code>Similarity</code> </p> <code>threshold</code> <p>Threshold for the ratings. If the rating is greater than the threshold, it will be considered as positive. If the threshold is not specified, the average score of all items rated by the user is used.</p> <p> TYPE: <code>float</code> DEFAULT: <code>None</code> </p> <code>embedding_combiner</code> <p><code>CombiningTechnique</code> used when embeddings representation must be used, but they are in a matrix form instead of a single vector (e.g. WordEmbedding representations have one vector for each word). By default, the <code>Centroid</code> of the rows of the matrix is computed</p> <p> TYPE: <code>CombiningTechnique</code> DEFAULT: <code>Centroid()</code> </p> Source code in <code>clayrs/recsys/content_based_algorithm/centroid_vector/centroid_vector.py</code> <pre><code>def __init__(self, item_field: dict, similarity: Similarity, threshold: float = None,\n             embedding_combiner: CombiningTechnique = Centroid()):\n    super().__init__(item_field, threshold)\n\n    self._similarity = similarity\n    self._emb_combiner = embedding_combiner\n    self._centroid: Optional[np.ndarray] = None\n    self._positive_rated_list: Optional[List] = None\n</code></pre>"},{"location":"recsys/content_based/content_based_algorithms/centroid_vector/#clayrs.recsys.content_based_algorithm.centroid_vector.centroid_vector.CentroidVector.fit_single_user","title":"<code>fit_single_user()</code>","text":"<p>The fit process for the CentroidVector consists in computing the centroid for the active user of the features of its positive items ONLY.</p> <p>This method uses extracted features of the positive items stored in a private attribute, so <code>process_rated()</code> must be called before this method.</p> <p>The built centroid will also be stored in a private attribute.</p> Source code in <code>clayrs/recsys/content_based_algorithm/centroid_vector/centroid_vector.py</code> <pre><code>def fit_single_user(self):\n\"\"\"\n    The fit process for the CentroidVector consists in computing the centroid for the active user of the features\n    of its positive items ONLY.\n\n    This method uses extracted features of the positive items stored in a private attribute, so\n    `process_rated()` must be called before this method.\n\n    The built centroid will also be stored in a private attribute.\n    \"\"\"\n    positive_rated_features_fused = self.fuse_representations(self._positive_rated_list, self._emb_combiner)\n    # reshape make the centroid bidimensional of shape (1, h) needed to compute faster similarities\n\n    self._centroid = positive_rated_features_fused.mean(axis=0).reshape(1, -1)\n\n    # we maintain original representation\n    if isinstance(positive_rated_features_fused, sparse.csr_matrix):\n        self._centroid = sparse.csr_matrix(self._centroid)\n\n    # we delete variable used to fit since will no longer be used\n    self._positive_rated_list = None\n</code></pre>"},{"location":"recsys/content_based/content_based_algorithms/centroid_vector/#clayrs.recsys.content_based_algorithm.centroid_vector.centroid_vector.CentroidVector.predict_single_user","title":"<code>predict_single_user(user_idx, train_ratings, available_loaded_items, filter_list)</code>","text":"<p>CentroidVector is not a score prediction algorithm, calling this method will raise the <code>NotPredictionAlg</code> exception!</p> RAISES DESCRIPTION <code>NotPredictionAlg</code> <p>exception raised since the CentroidVector algorithm is not a score prediction algorithm</p> Source code in <code>clayrs/recsys/content_based_algorithm/centroid_vector/centroid_vector.py</code> <pre><code>def predict_single_user(self, user_idx: int, train_ratings: Ratings, available_loaded_items: LoadedContentsDict,\n                        filter_list: List[str]) -&gt; np.ndarray:\n\"\"\"\n    CentroidVector is not a score prediction algorithm, calling this method will raise\n    the `NotPredictionAlg` exception!\n\n    Raises:\n        NotPredictionAlg: exception raised since the CentroidVector algorithm is not a score prediction algorithm\n    \"\"\"\n    raise NotPredictionAlg(\"CentroidVector is not a Score Prediction Algorithm!\")\n</code></pre>"},{"location":"recsys/content_based/content_based_algorithms/centroid_vector/#clayrs.recsys.content_based_algorithm.centroid_vector.centroid_vector.CentroidVector.process_rated","title":"<code>process_rated(user_idx, train_ratings, available_loaded_items)</code>","text":"<p>Function that extracts features from positive rated items ONLY of a user The extracted features will be used to fit the algorithm (build the centroid).</p> <p>Features extracted will be stored in a private attribute of the class.</p> <p>IF there are no rated items available locally or if there are only negative items, an exception is thrown.</p> PARAMETER DESCRIPTION <code>user_idx</code> <p>Mapped integer of the active user (the user for which we must fit the algorithm)</p> <p> TYPE: <code>int</code> </p> <code>train_ratings</code> <p><code>Ratings</code> object which contains the train set of each user</p> <p> TYPE: <code>Ratings</code> </p> <code>available_loaded_items</code> <p>The LoadedContents interface which contains loaded contents</p> <p> TYPE: <code>LoadedContentsDict</code> </p> RAISES DESCRIPTION <code>EmptyUserRatings</code> <p>Exception raised when the user does not appear in the train set</p> <code>NoRatedItems</code> <p>Exception raised when there isn't any item available locally rated by the user</p> <code>OnlyNegativeitems</code> <p>Exception raised when there are only negative items available locally for the user (Items that the user disliked)</p> Source code in <code>clayrs/recsys/content_based_algorithm/centroid_vector/centroid_vector.py</code> <pre><code>def process_rated(self, user_idx: int, train_ratings: Ratings, available_loaded_items: LoadedContentsDict):\n\"\"\"\n    Function that extracts features from positive rated items ONLY of a user\n    The extracted features will be used to fit the algorithm (build the centroid).\n\n    Features extracted will be stored in a private attribute of the class.\n\n    IF there are no rated items available locally or if there are only negative\n    items, an exception is thrown.\n\n    Args:\n        user_idx: Mapped integer of the active user (the user for which we must fit the algorithm)\n        train_ratings: `Ratings` object which contains the train set of each user\n        available_loaded_items: The LoadedContents interface which contains loaded contents\n\n    Raises:\n        EmptyUserRatings: Exception raised when the user does not appear in the train set\n        NoRatedItems: Exception raised when there isn't any item available locally\n            rated by the user\n        OnlyNegativeitems: Exception raised when there are only negative items available locally\n            for the user (Items that the user disliked)\n    \"\"\"\n    # a list since there could be duplicate interaction (eg bootstrap partitioning)\n    uir_user = train_ratings.get_user_interactions(user_idx)\n    rated_items_id = train_ratings.item_map.convert_seq_int2str(uir_user[:, 1].astype(int))\n\n    # a list since there could be duplicate interaction (eg bootstrap partitioning)\n    items_scores_dict = defaultdict(list)\n\n    for item_id, score in zip(rated_items_id, uir_user[:, 2]):\n        items_scores_dict[item_id].append(score)\n\n    items_scores_dict = dict(sorted(items_scores_dict.items()))  # sort dictionary based on key for reproducibility\n\n    # Create list of all the available items that are useful for the user\n    loaded_rated_items: List[Union[Content, None]] = available_loaded_items.get_list([item_id\n                                                                                      for item_id\n                                                                                      in rated_items_id])\n\n    # If threshold wasn't passed in the constructor, then we take the mean rating\n    # given by the user as its threshold\n    threshold = self.threshold\n    if threshold is None:\n        threshold = np.nanmean(uir_user[:, 2])\n\n    # we extract feature of each POSITIVE item sorted based on its key: IMPORTANT for reproducibility!!\n    # otherwise the matrix we feed to sklearn will have input item in different rows each run!\n    positive_rated_list = []\n    for item in loaded_rated_items:\n        if item is not None:\n\n            score_assigned = items_scores_dict[item.content_id]\n\n            for score in score_assigned:\n                if score &gt;= threshold:\n                    positive_rated_list.append(self.extract_features_item(item))\n\n    if len(uir_user) == 0:\n        raise EmptyUserRatings(\"The user selected doesn't have any ratings!\")\n\n    if len(loaded_rated_items) == 0 or (loaded_rated_items.count(None) == len(loaded_rated_items)):\n        raise NoRatedItems(\"User {} - No rated items available locally!\".format(user_idx))\n    if len(positive_rated_list) == 0:\n        raise OnlyNegativeItems(\"User {} - There are only negative items available locally!\")\n\n    self._positive_rated_list = positive_rated_list\n</code></pre>"},{"location":"recsys/content_based/content_based_algorithms/centroid_vector/#clayrs.recsys.content_based_algorithm.centroid_vector.centroid_vector.CentroidVector.rank_single_user","title":"<code>rank_single_user(user_idx, train_ratings, available_loaded_items, recs_number, filter_list)</code>","text":"<p>Rank the top-n recommended items for the active user, where the top-n items to rank are controlled by the <code>recs_number</code> and <code>filter_list</code> parameter:</p> <ul> <li>the former one is self-explanatory, the second is a list of items represented with their string ids. Must be necessarily strings and not their mapped integer since items are serialized following their string representation!</li> </ul> <p>If <code>recs_number</code> is <code>None</code>, all ranked items will be returned</p> <p>The filter list parameter is usually the result of the <code>filter_single()</code> method of a <code>Methodology</code> object</p> PARAMETER DESCRIPTION <code>user_idx</code> <p>Mapped integer of the active user</p> <p> TYPE: <code>int</code> </p> <code>train_ratings</code> <p><code>Ratings</code> object which contains the train set of each user</p> <p> TYPE: <code>Ratings</code> </p> <code>available_loaded_items</code> <p>The LoadedContents interface which contains loaded contents</p> <p> TYPE: <code>LoadedContentsDict</code> </p> <code>recs_number</code> <p>number of the top ranked items to return, if None all ranked items will be returned</p> <p> TYPE: <code>Optional[int]</code> </p> <code>filter_list</code> <p>list of the items to rank. Should contain string item ids</p> <p> TYPE: <code>List[str]</code> </p> RETURNS DESCRIPTION <code>np.ndarray</code> <p>uir matrix for a single user containing user and item idxs (integer representation) with the ranked score as third dimension sorted in a decreasing order</p> Source code in <code>clayrs/recsys/content_based_algorithm/centroid_vector/centroid_vector.py</code> <pre><code>def rank_single_user(self, user_idx: int, train_ratings: Ratings, available_loaded_items: LoadedContentsDict,\n                     recs_number: Optional[int], filter_list: List[str]) -&gt; np.ndarray:\n\"\"\"\n    Rank the top-n recommended items for the active user, where the top-n items to rank are controlled by the\n    `recs_number` and `filter_list` parameter:\n\n    * the former one is self-explanatory, the second is a list of items\n    represented with their string ids. Must be necessarily strings and not their mapped integer since items are\n    serialized following their string representation!\n\n    If `recs_number` is `None`, all ranked items will be returned\n\n    The filter list parameter is usually the result of the `filter_single()` method of a `Methodology` object\n\n    Args:\n        user_idx: Mapped integer of the active user\n        train_ratings: `Ratings` object which contains the train set of each user\n        available_loaded_items: The LoadedContents interface which contains loaded contents\n        recs_number: number of the top ranked items to return, if None all ranked items will be returned\n        filter_list: list of the items to rank. Should contain string item ids\n\n    Returns:\n        uir matrix for a single user containing user and item idxs (integer representation) with the ranked score\n            as third dimension sorted in a decreasing order\n    \"\"\"\n    uir_user = train_ratings.get_user_interactions(user_idx)\n    if len(uir_user) == 0:\n        raise EmptyUserRatings(\"The user selected doesn't have any ratings!\")\n\n    # Load items to predict\n    items_to_predict = available_loaded_items.get_list(filter_list)\n\n    # Extract features of the items to predict\n    idx_items_to_predict = []\n    features_items_to_predict = []\n    for item in items_to_predict:\n        if item is not None:\n            idx_items_to_predict.append(item.content_id)\n            features_items_to_predict.append(self.extract_features_item(item))\n\n    if len(idx_items_to_predict) == 0:\n        return np.array([])  # if no item to predict, empty rank is returned\n\n    idx_items_to_predict = train_ratings.item_map.convert_seq_str2int(idx_items_to_predict)\n\n    # Calculate predictions, they are the similarity of the new items with the centroid vector\n    features_fused = self.fuse_representations(features_items_to_predict, self._emb_combiner)\n    similarities = self._similarity.perform(self._centroid, features_fused).reshape(-1)  # 2d to 1d\n\n    sorted_scores_idxs = np.argsort(similarities)[::-1][:recs_number]\n    sorted_items = np.array(idx_items_to_predict)[sorted_scores_idxs]\n    sorted_scores = similarities[sorted_scores_idxs]\n\n    # we construct the output data\n    uir_rank = np.array([[user_idx, item_idx, score]\n                         for item_idx, score in zip(sorted_items, sorted_scores)])\n\n    return uir_rank\n</code></pre>"},{"location":"recsys/content_based/content_based_algorithms/centroid_vector/#similarities-implemented","title":"Similarities implemented","text":"<p>The following are similarities you can use in the <code>similarity</code> parameter of the <code>CentroidVector</code> class</p>"},{"location":"recsys/content_based/content_based_algorithms/centroid_vector/#clayrs.recsys.content_based_algorithm.centroid_vector.similarities.CosineSimilarity","title":"<code>CosineSimilarity()</code>","text":"<p>         Bases: <code>Similarity</code></p> <p>Computes cosine similarity</p> Source code in <code>clayrs/recsys/content_based_algorithm/centroid_vector/similarities.py</code> <pre><code>def __init__(self):\n    super().__init__()\n</code></pre>"},{"location":"recsys/content_based/content_based_algorithms/centroid_vector/#clayrs.recsys.content_based_algorithm.centroid_vector.similarities.CosineSimilarity.perform","title":"<code>perform(v1, v2)</code>","text":"<p>Calculates the cosine similarity between v1 and v2</p> PARAMETER DESCRIPTION <code>v1</code> <p>first numpy array</p> <p> TYPE: <code>Union[np.ndarray, sparse.csr_matrix]</code> </p> <code>v2</code> <p>second numpy array</p> <p> TYPE: <code>Union[np.ndarray, sparse.csr_matrix]</code> </p> Source code in <code>clayrs/recsys/content_based_algorithm/centroid_vector/similarities.py</code> <pre><code>def perform(self, v1: Union[np.ndarray, sparse.csr_matrix], v2: Union[np.ndarray, sparse.csr_matrix]):\n\"\"\"\n    Calculates the cosine similarity between v1 and v2\n\n    Args:\n        v1: first numpy array\n        v2: second numpy array\n    \"\"\"\n\n    return cosine_similarity(v1, v2, dense_output=True)\n</code></pre>"},{"location":"recsys/content_based/content_based_algorithms/classifier_recommender/","title":"Classifier Recommender","text":""},{"location":"recsys/content_based/content_based_algorithms/classifier_recommender/#clayrs.recsys.content_based_algorithm.classifier.classifier_recommender.ClassifierRecommender","title":"<code>ClassifierRecommender(item_field, classifier, threshold=None, embedding_combiner=Centroid())</code>","text":"<p>         Bases: <code>PerUserCBAlgorithm</code></p> <p>Class that implements recommendation through a specified <code>Classifier</code> object. It's a ranking algorithm, so it can't do score prediction.</p> <p>Examples:</p> <ul> <li>Interested in only a field representation, <code>DecisionTree</code> classifier from sklearn, <code>threshold</code> \\(= 3\\) (Every item with rating score \\(&gt;= 3\\) will be considered as positive)</li> </ul> <pre><code>&gt;&gt;&gt; from clayrs import recsys as rs\n&gt;&gt;&gt; alg = rs.ClassifierRecommender({\"Plot\": 0}, rs.SkDecisionTree(), 3)\n</code></pre> <ul> <li>Interested in only a field representation, <code>KNN</code> classifier with custom parameters from sklearn, <code>threshold</code> \\(= 3\\) (Every item with rating score \\(&gt;= 3\\) will be considered as positive)</li> </ul> <pre><code>&gt;&gt;&gt; alg = rs.ClassifierRecommender({\"Plot\": 0}, rs.SkKNN(n_neighbors=3), 0)\n</code></pre> <ul> <li>Interested in multiple field representations of the items, <code>KNN</code> classifier with custom parameters from sklearn, <code>threshold</code> \\(= None\\) (Every item with rating \\(&gt;=\\) mean rating of the user will be considered as positive)</li> </ul> <pre><code>&gt;&gt;&gt; alg = ClassifierRecommender(\n&gt;&gt;&gt;                             item_field={\"Plot\": [0, \"tfidf\"],\n&gt;&gt;&gt;                                         \"Genre\": [0, 1],\n&gt;&gt;&gt;                                         \"Director\": \"doc2vec\"},\n&gt;&gt;&gt;                             classifier=rs.SkKNN(n_neighbors=3),\n&gt;&gt;&gt;                             threshold=None)\n</code></pre> <p>Info</p> <p>After instantiating the ClassifierRecommender` algorithm, pass it in the initialization of a CBRS and the use its method to calculate ranking for single user or multiple users:</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; cbrs = rs.ContentBasedRS(algorithm=alg, ...)\n&gt;&gt;&gt; cbrs.fit_rank(...)\n&gt;&gt;&gt; # ...\n</code></pre> PARAMETER DESCRIPTION <code>item_field</code> <p>dict where the key is the name of the field that contains the content to use, value is the representation(s) id(s) that will be used for the said item. The value of a field can be a string or a list, use a list if you want to use multiple representations for a particular field.</p> <p> TYPE: <code>dict</code> </p> <code>classifier</code> <p>classifier that will be used. Can be one object of the Classifier class.</p> <p> TYPE: <code>Classifier</code> </p> <code>threshold</code> <p>Threshold for the ratings. If the rating is greater than the threshold, it will be considered as positive. If the threshold is not specified, the average score of all items rated by the user is used.</p> <p> TYPE: <code>float</code> DEFAULT: <code>None</code> </p> <code>embedding_combiner</code> <p><code>CombiningTechnique</code> used when embeddings representation must be used, but they are in a matrix form instead of a single vector (e.g. WordEmbedding representations have one vector for each word). By default, the <code>Centroid</code> of the rows of the matrix is computed</p> <p> TYPE: <code>CombiningTechnique</code> DEFAULT: <code>Centroid()</code> </p> Source code in <code>clayrs/recsys/content_based_algorithm/classifier/classifier_recommender.py</code> <pre><code>def __init__(self, item_field: dict, classifier: Classifier, threshold: float = None,\n             embedding_combiner: CombiningTechnique = Centroid()):\n    super().__init__(item_field, threshold)\n\n    self._classifier = classifier\n    self._embedding_combiner = embedding_combiner\n    self._labels: Optional[list] = None\n    self._items_features: Optional[list] = None\n</code></pre>"},{"location":"recsys/content_based/content_based_algorithms/classifier_recommender/#clayrs.recsys.content_based_algorithm.classifier.classifier_recommender.ClassifierRecommender.fit_single_user","title":"<code>fit_single_user()</code>","text":"<p>Fit the classifier specified in the constructor with the features and labels extracted with the <code>process_rated()</code> method.</p> <p>It uses private attributes to fit the classifier, so <code>process_rated()</code> must be called before this method.</p> Source code in <code>clayrs/recsys/content_based_algorithm/classifier/classifier_recommender.py</code> <pre><code>def fit_single_user(self):\n\"\"\"\n    Fit the classifier specified in the constructor with the features and labels\n    extracted with the `process_rated()` method.\n\n    It uses private attributes to fit the classifier, so `process_rated()` must be called\n    before this method.\n    \"\"\"\n    # Fuse the input if there are dicts, multiple representation, etc.\n    fused_features = self.fuse_representations(self._items_features, self._embedding_combiner)\n\n    self._classifier.fit(fused_features, self._labels)\n\n    # we delete variables used to fit since will no longer be used\n    self._items_features = None\n    self._labels = None\n</code></pre>"},{"location":"recsys/content_based/content_based_algorithms/classifier_recommender/#clayrs.recsys.content_based_algorithm.classifier.classifier_recommender.ClassifierRecommender.predict_single_user","title":"<code>predict_single_user(user_idx, train_ratings, available_loaded_items, filter_list=None)</code>","text":"<p>ClassifierRecommender is not a score prediction algorithm, calling this method will raise the <code>NotPredictionAlg</code> exception!</p> RAISES DESCRIPTION <code>NotPredictionAlg</code> <p>exception raised since the ClassifierRecommender algorithm is not a score prediction algorithm</p> Source code in <code>clayrs/recsys/content_based_algorithm/classifier/classifier_recommender.py</code> <pre><code>def predict_single_user(self, user_idx: int, train_ratings: Ratings, available_loaded_items: LoadedContentsDict,\n                        filter_list: List[str] = None) -&gt; np.ndarray:\n\"\"\"\n    ClassifierRecommender is not a score prediction algorithm, calling this method will raise\n    the `NotPredictionAlg` exception!\n\n    Raises:\n        NotPredictionAlg: exception raised since the ClassifierRecommender algorithm is not a\n            score prediction algorithm\n    \"\"\"\n    raise NotPredictionAlg(\"ClassifierRecommender is not a Score Prediction Algorithm!\")\n</code></pre>"},{"location":"recsys/content_based/content_based_algorithms/classifier_recommender/#clayrs.recsys.content_based_algorithm.classifier.classifier_recommender.ClassifierRecommender.process_rated","title":"<code>process_rated(user_idx, train_ratings, available_loaded_items)</code>","text":"<p>Function that extracts features from rated item and labels them. The extracted features will be later used to fit the classifier.</p> <p>Features and labels will be stored in private attributes of the class.</p> <p>IF there are no rated items available locally or if there are only positive/negative items, an exception is thrown.</p> PARAMETER DESCRIPTION <code>user_idx</code> <p>Mapped integer of the active user (the user for which we must fit the algorithm)</p> <p> TYPE: <code>int</code> </p> <code>train_ratings</code> <p><code>Ratings</code> object which contains the train set of each user</p> <p> TYPE: <code>Ratings</code> </p> <code>available_loaded_items</code> <p>The LoadedContents interface which contains loaded contents</p> <p> TYPE: <code>LoadedContentsDict</code> </p> RAISES DESCRIPTION <code>EmptyUserRatings</code> <p>Exception raised when the user does not appear in the train set</p> <code>NoRatedItems</code> <p>Exception raised when there isn't any item available locally rated by the user</p> <code>OnlyPositiveItems</code> <p>Exception raised when there are only positive items available locally for the user (Items that the user liked)</p> <code>OnlyNegativeitems</code> <p>Exception raised when there are only negative items available locally for the user (Items that the user disliked)</p> Source code in <code>clayrs/recsys/content_based_algorithm/classifier/classifier_recommender.py</code> <pre><code>def process_rated(self, user_idx: int, train_ratings: Ratings, available_loaded_items: LoadedContentsDict):\n\"\"\"\n    Function that extracts features from rated item and labels them.\n    The extracted features will be later used to fit the classifier.\n\n    Features and labels will be stored in private attributes of the class.\n\n    IF there are no rated items available locally or if there are only positive/negative\n    items, an exception is thrown.\n\n    Args:\n        user_idx: Mapped integer of the active user (the user for which we must fit the algorithm)\n        train_ratings: `Ratings` object which contains the train set of each user\n        available_loaded_items: The LoadedContents interface which contains loaded contents\n\n    Raises:\n        EmptyUserRatings: Exception raised when the user does not appear in the train set\n        NoRatedItems: Exception raised when there isn't any item available locally\n            rated by the user\n        OnlyPositiveItems: Exception raised when there are only positive items available locally\n            for the user (Items that the user liked)\n        OnlyNegativeitems: Exception raised when there are only negative items available locally\n            for the user (Items that the user disliked)\n    \"\"\"\n\n    uir_user = train_ratings.get_user_interactions(user_idx)\n    rated_items_id = train_ratings.item_map.convert_seq_int2str(uir_user[:, 1].astype(int))\n\n    # a list since there could be duplicate interaction (eg bootstrap partitioning)\n    items_scores_dict = defaultdict(list)\n\n    for item_id, score in zip(rated_items_id, uir_user[:, 2]):\n        items_scores_dict[item_id].append(score)\n\n    items_scores_dict = dict(sorted(items_scores_dict.items()))  # sort dictionary based on key for reproducibility\n\n    # Create list of all the available items that are useful for the user\n    loaded_rated_items: List[Union[Content, None]] = available_loaded_items.get_list([item_id\n                                                                                      for item_id\n                                                                                      in rated_items_id])\n\n    threshold = self.threshold\n    if threshold is None:\n        threshold = np.nanmean(uir_user[:, 2])\n\n    # Assign label and extract features from the rated items\n    labels = []\n    items_features = []\n\n    # we extract feature of each item sorted based on its key: IMPORTANT for reproducibility!!\n    # otherwise the matrix we feed to sklearn will have input item in different rows each run!\n    for item in loaded_rated_items:\n        if item is not None:\n\n            score_assigned = map(float, items_scores_dict[item.content_id])\n\n            for score in score_assigned:\n                items_features.append(self.extract_features_item(item))\n\n                if score &gt;= threshold:\n                    labels.append(1)\n                else:\n                    labels.append(0)\n\n    if len(uir_user[:, 1]) == 0:\n        raise EmptyUserRatings(\"The user selected doesn't have any ratings!\")\n\n    if len(items_features) == 0:\n        raise NoRatedItems(\"User {} - No rated item available locally!\".format(user_idx))\n    if 0 not in labels:\n        raise OnlyPositiveItems(\"User {} - There are only positive items available locally!\".format(user_idx))\n    elif 1 not in labels:\n        raise OnlyNegativeItems(\"User {} - There are only negative items available locally!\".format(user_idx))\n\n    self._labels = labels\n    self._items_features = items_features\n</code></pre>"},{"location":"recsys/content_based/content_based_algorithms/classifier_recommender/#clayrs.recsys.content_based_algorithm.classifier.classifier_recommender.ClassifierRecommender.rank_single_user","title":"<code>rank_single_user(user_idx, train_ratings, available_loaded_items, recs_number, filter_list)</code>","text":"<p>Rank the top-n recommended items for the active user, where the top-n items to rank are controlled by the <code>recs_number</code> and <code>filter_list</code> parameter:</p> <ul> <li>the former one is self-explanatory, the second is a list of items represented with their string ids. Must be necessarily strings and not their mapped integer since items are serialized following their string representation!</li> </ul> <p>If <code>recs_number</code> is <code>None</code>, all ranked items will be returned</p> <p>The filter list parameter is usually the result of the <code>filter_single()</code> method of a <code>Methodology</code> object</p> PARAMETER DESCRIPTION <code>user_idx</code> <p>Mapped integer of the active user</p> <p> TYPE: <code>int</code> </p> <code>train_ratings</code> <p><code>Ratings</code> object which contains the train set of each user</p> <p> TYPE: <code>Ratings</code> </p> <code>available_loaded_items</code> <p>The LoadedContents interface which contains loaded contents</p> <p> TYPE: <code>LoadedContentsDict</code> </p> <code>recs_number</code> <p>number of the top ranked items to return, if None all ranked items will be returned</p> <p> TYPE: <code>Optional[int]</code> </p> <code>filter_list</code> <p>list of the items to rank. Should contain string item ids</p> <p> TYPE: <code>List[str]</code> </p> RETURNS DESCRIPTION <code>np.ndarray</code> <p>uir matrix for a single user containing user and item idxs (integer representation) with the ranked score as third dimension sorted in a decreasing order</p> Source code in <code>clayrs/recsys/content_based_algorithm/classifier/classifier_recommender.py</code> <pre><code>def rank_single_user(self, user_idx: int, train_ratings: Ratings, available_loaded_items: LoadedContentsDict,\n                     recs_number: Optional[int], filter_list: List[str]) -&gt; np.ndarray:\n\"\"\"\n    Rank the top-n recommended items for the active user, where the top-n items to rank are controlled by the\n    `recs_number` and `filter_list` parameter:\n\n    * the former one is self-explanatory, the second is a list of items\n    represented with their string ids. Must be necessarily strings and not their mapped integer since items are\n    serialized following their string representation!\n\n    If `recs_number` is `None`, all ranked items will be returned\n\n    The filter list parameter is usually the result of the `filter_single()` method of a `Methodology` object\n\n    Args:\n        user_idx: Mapped integer of the active user\n        train_ratings: `Ratings` object which contains the train set of each user\n        available_loaded_items: The LoadedContents interface which contains loaded contents\n        recs_number: number of the top ranked items to return, if None all ranked items will be returned\n        filter_list: list of the items to rank. Should contain string item ids\n\n    Returns:\n        uir matrix for a single user containing user and item idxs (integer representation) with the ranked score\n            as third dimension sorted in a decreasing order\n    \"\"\"\n\n    uir_user = train_ratings.get_user_interactions(user_idx)\n    if len(uir_user) == 0:\n        raise EmptyUserRatings(\"The user selected doesn't have any ratings!\")\n\n    # Load items to predict\n    items_to_predict = available_loaded_items.get_list(filter_list)\n\n    # Extract features of the items to predict\n    idx_items_to_predict = []\n    features_items_to_predict = []\n    for item in items_to_predict:\n        if item is not None:\n            idx_items_to_predict.append(item.content_id)\n            features_items_to_predict.append(self.extract_features_item(item))\n\n    if len(idx_items_to_predict) == 0:\n        return np.array([])  # if no item to predict, empty rank is returned\n\n    idx_items_to_predict = train_ratings.item_map.convert_seq_str2int(idx_items_to_predict)\n\n    # Fuse the input if there are dicts, multiple representation, etc.\n    fused_features_items_to_pred = self.fuse_representations(features_items_to_predict,\n                                                             self._embedding_combiner)\n\n    class_prob = self._classifier.predict_proba(fused_features_items_to_pred)\n\n    # for each item we extract the probability that the item is liked (class 1)\n    sorted_scores_idxs = np.argsort(class_prob[:, 1])[::-1][:recs_number]\n    sorted_items = np.array(idx_items_to_predict)[sorted_scores_idxs]\n    sorted_scores = class_prob[:, 1][sorted_scores_idxs]\n\n    uir_rank = np.array([[user_idx, item_idx, score]\n                         for item_idx, score in zip(sorted_items, sorted_scores)])\n\n    return uir_rank\n</code></pre>"},{"location":"recsys/content_based/content_based_algorithms/classifier_recommender/#classifiers-implemented","title":"Classifiers Implemented","text":"<p>The following are the classifiers you can use in the <code>classifier</code> parameter of the <code>ClassifierRecommender</code> class</p>"},{"location":"recsys/content_based/content_based_algorithms/classifier_recommender/#clayrs.recsys.content_based_algorithm.classifier.classifiers.SkDecisionTree","title":"<code>SkDecisionTree(*, criterion='gini', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, class_weight=None, ccp_alpha=0.0)</code>","text":"<p>         Bases: <code>Classifier</code></p> <p>Class that implements the Decision Tree Classifier from sklearn. The parameters one could pass are the same ones you would pass instantiating the classifier directly from sklearn</p> <p>Sklearn documentation: here</p> Source code in <code>clayrs/recsys/content_based_algorithm/classifier/classifiers.py</code> <pre><code>def __init__(self, *,\n             criterion: Any = \"gini\",\n             splitter: Any = \"best\",\n             max_depth: Any = None,\n             min_samples_split: Any = 2,\n             min_samples_leaf: Any = 1,\n             min_weight_fraction_leaf: Any = 0.0,\n             max_features: Any = None,\n             random_state: Any = None,\n             max_leaf_nodes: Any = None,\n             min_impurity_decrease: Any = 0.0,\n             class_weight: Any = None,\n             ccp_alpha: Any = 0.0):\n    clf = DecisionTreeClassifier(criterion=criterion, splitter=splitter, max_depth=max_depth,\n                                 min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf,\n                                 min_weight_fraction_leaf=min_weight_fraction_leaf, max_features=max_features,\n                                 random_state=random_state, max_leaf_nodes=max_leaf_nodes,\n                                 min_impurity_decrease=min_impurity_decrease, class_weight=class_weight,\n                                 ccp_alpha=ccp_alpha)\n\n    super().__init__(clf, inspect.currentframe())\n</code></pre>"},{"location":"recsys/content_based/content_based_algorithms/classifier_recommender/#clayrs.recsys.content_based_algorithm.classifier.classifiers.SkGaussianProcess","title":"<code>SkGaussianProcess(kernel=None, *, optimizer='fmin_l_bfgs_b', n_restarts_optimizer=0, max_iter_predict=100, warm_start=False, copy_X_train=True, random_state=None, multi_class='one_vs_rest', n_jobs=None)</code>","text":"<p>         Bases: <code>Classifier</code></p> <p>Class that implements the Gaussian Process Classifier from sklearn. The parameters one could pass are the same ones you would pass instantiating the classifier directly from sklearn</p> <p>Sklearn documentation: here</p> Source code in <code>clayrs/recsys/content_based_algorithm/classifier/classifiers.py</code> <pre><code>def __init__(self, kernel: Any = None,\n             *,\n             optimizer: Any = \"fmin_l_bfgs_b\",\n             n_restarts_optimizer: Any = 0,\n             max_iter_predict: Any = 100,\n             warm_start: Any = False,\n             copy_X_train: Any = True,\n             random_state: Any = None,\n             multi_class: Any = \"one_vs_rest\",\n             n_jobs: Any = None):\n\n    clf = GaussianProcessClassifier(kernel=kernel, optimizer=optimizer, n_restarts_optimizer=n_restarts_optimizer,\n                                    max_iter_predict=max_iter_predict, warm_start=warm_start,\n                                    copy_X_train=copy_X_train, random_state=random_state,\n                                    multi_class=multi_class, n_jobs=n_jobs)\n\n    super().__init__(clf, inspect.currentframe())\n</code></pre>"},{"location":"recsys/content_based/content_based_algorithms/classifier_recommender/#clayrs.recsys.content_based_algorithm.classifier.classifiers.SkKNN","title":"<code>SkKNN(n_neighbors=5, *, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None)</code>","text":"<p>         Bases: <code>Classifier</code></p> <p>Class that implements the KNN Classifier from sklearn. The parameters one could pass are the same ones you would pass instantiating the classifier KNN directly from sklearn.</p> <p>Sklearn documentation: here</p> <p>Since KNN implementation of sklearn has <code>n_neighbors = 5</code> as default, it can throw an exception if less sample in the training data are provided, so we change dynamically the n_neighbors parameter according to the number of samples if the dataset is too small and if no manual <code>n_neighbors</code> is set</p> Source code in <code>clayrs/recsys/content_based_algorithm/classifier/classifiers.py</code> <pre><code>def __init__(self, n_neighbors: Any = 5,\n             *,\n             weights: Any = \"uniform\",\n             algorithm: Any = \"auto\",\n             leaf_size: Any = 30,\n             p: Any = 2,\n             metric: Any = \"minkowski\",\n             metric_params: Any = None,\n             n_jobs: Any = None):\n    clf = KNeighborsClassifier(n_neighbors=n_neighbors, weights=weights, algorithm=algorithm, leaf_size=leaf_size,\n                               p=p, metric=metric, metric_params=metric_params, n_jobs=n_jobs)\n\n    super().__init__(clf, inspect.currentframe())\n</code></pre>"},{"location":"recsys/content_based/content_based_algorithms/classifier_recommender/#clayrs.recsys.content_based_algorithm.classifier.classifiers.SkLogisticRegression","title":"<code>SkLogisticRegression(penalty='l2', *, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='lbfgs', max_iter=100, multi_class='auto', verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)</code>","text":"<p>         Bases: <code>Classifier</code></p> <p>Class that implements the Logistic Regression Classifier from sklearn. The parameters one could pass are the same ones you would pass instantiating the classifier directly from sklearn</p> <p>Sklearn documentation: here</p> Source code in <code>clayrs/recsys/content_based_algorithm/classifier/classifiers.py</code> <pre><code>def __init__(self, penalty: Any = \"l2\",\n             *,\n             dual: Any = False,\n             tol: Any = 1e-4,\n             C: Any = 1.0,\n             fit_intercept: Any = True,\n             intercept_scaling: Any = 1,\n             class_weight: Any = None,\n             random_state: Any = None,\n             solver: Any = \"lbfgs\",\n             max_iter: Any = 100,\n             multi_class: Any = \"auto\",\n             verbose: Any = 0,\n             warm_start: Any = False,\n             n_jobs: Any = None,\n             l1_ratio: Any = None):\n    clf = LogisticRegression(penalty=penalty, dual=dual, tol=tol, C=C, fit_intercept=fit_intercept,\n                             intercept_scaling=intercept_scaling, class_weight=class_weight,\n                             random_state=random_state, solver=solver, max_iter=max_iter,\n                             multi_class=multi_class, verbose=verbose, warm_start=warm_start,\n                             n_jobs=n_jobs, l1_ratio=l1_ratio)\n\n    super().__init__(clf, inspect.currentframe())\n</code></pre>"},{"location":"recsys/content_based/content_based_algorithms/classifier_recommender/#clayrs.recsys.content_based_algorithm.classifier.classifiers.SkRandomForest","title":"<code>SkRandomForest(n_estimators=100, *, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None)</code>","text":"<p>         Bases: <code>Classifier</code></p> <p>Class that implements the Random Forest Classifier from sklearn. The parameters one could pass are the same ones you would pass instantiating the classifier directly from sklearn</p> <p>Sklearn documentation: here</p> Source code in <code>clayrs/recsys/content_based_algorithm/classifier/classifiers.py</code> <pre><code>def __init__(self, n_estimators: Any = 100,\n             *,\n             criterion: Any = \"gini\",\n             max_depth: Any = None,\n             min_samples_split: Any = 2,\n             min_samples_leaf: Any = 1,\n             min_weight_fraction_leaf: Any = 0.0,\n             max_features: Any = \"auto\",\n             max_leaf_nodes: Any = None,\n             min_impurity_decrease: Any = 0.0,\n             bootstrap: Any = True,\n             oob_score: Any = False,\n             n_jobs: Any = None,\n             random_state: Any = None,\n             verbose: Any = 0,\n             warm_start: Any = False,\n             class_weight: Any = None,\n             ccp_alpha: Any = 0.0,\n             max_samples: Any = None):\n    clf = RandomForestClassifier(n_estimators=n_estimators, criterion=criterion, max_depth=max_depth,\n                                 min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf,\n                                 min_weight_fraction_leaf=min_weight_fraction_leaf, max_features=max_features,\n                                 max_leaf_nodes=max_leaf_nodes, min_impurity_decrease=min_impurity_decrease,\n                                 bootstrap=bootstrap, oob_score=oob_score, n_jobs=n_jobs, random_state=random_state,\n                                 verbose=verbose, warm_start=warm_start, class_weight=class_weight,\n                                 ccp_alpha=ccp_alpha, max_samples=max_samples)\n\n    super().__init__(clf, inspect.currentframe())\n</code></pre>"},{"location":"recsys/content_based/content_based_algorithms/classifier_recommender/#clayrs.recsys.content_based_algorithm.classifier.classifiers.SkSVC","title":"<code>SkSVC(*, C=1.0, kernel='rbf', degree=3, gamma='scale', coef0=0.0, shrinking=True, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape='ovr', break_ties=False, random_state=None)</code>","text":"<p>         Bases: <code>Classifier</code></p> <p>Class that implements the SVC Classifier from sklearn. The parameters one could pass are the same ones you would pass instantiating the classifier SVC directly from sklearn.</p> <p>Sklearn documentation: here</p> <p>The only parameter from sklearn that cannot be passed is the 'probability' parameter: it is set to True and cannot be changed</p> Source code in <code>clayrs/recsys/content_based_algorithm/classifier/classifiers.py</code> <pre><code>def __init__(self,\n             *,\n             C: Any = 1.0,\n             kernel: Any = \"rbf\",\n             degree: Any = 3,\n             gamma: Any = \"scale\",\n             coef0: Any = 0.0,\n             shrinking: Any = True,\n             tol: Any = 1e-3,\n             cache_size: Any = 200,\n             class_weight: Any = None,\n             verbose: Any = False,\n             max_iter: Any = -1,\n             decision_function_shape: Any = \"ovr\",\n             break_ties: Any = False,\n             random_state: Any = None):\n\n    # Force the probability parameter at True, otherwise SVC won't predict_proba\n    clf = SVC(C=C, kernel=kernel, degree=degree, gamma=gamma, coef0=coef0, shrinking=shrinking, tol=tol,\n              cache_size=cache_size, class_weight=class_weight, verbose=verbose, max_iter=max_iter,\n              decision_function_shape=decision_function_shape, break_ties=break_ties, random_state=random_state,\n              probability=True)\n\n    super().__init__(clf, inspect.currentframe())\n</code></pre>"},{"location":"recsys/content_based/content_based_algorithms/index_query/","title":"Index Query","text":""},{"location":"recsys/content_based/content_based_algorithms/index_query/#clayrs.recsys.content_based_algorithm.index_query.index_query.IndexQuery","title":"<code>IndexQuery(item_field, classic_similarity=True, threshold=None)</code>","text":"<p>         Bases: <code>PerUserCBAlgorithm</code></p> <p>Class for the search engine recommender using an index. It firsts builds a query using the representation(s) specified of the positive items, then uses the mentioned query to do an actual search inside the index: every item will have a score of \"closeness\" in relation to the query, we use this score to rank every item.</p> <p>Just be sure to use textual representation(s) to build a significant query and to make a significant search!</p> <p>Examples:</p> <ul> <li>Interested in only a field representation, classic tfidf similarity, <code>threshold</code> \\(= 3\\) (Every item with rating \\(&gt;= 3\\) will be considered as positive)</li> </ul> <pre><code>&gt;&gt;&gt; from clayrs import recsys as rs\n&gt;&gt;&gt; alg = rs.IndexQuery({\"Plot\": 0}, threshold=3)\n</code></pre> <ul> <li>Interested in multiple field representations of the items, BM25 similarity, <code>threshold</code> \\(= None\\) (Every item with rating \\(&gt;=\\) mean rating of the user will be considered as positive)</li> </ul> <pre><code>&gt;&gt;&gt; alg = rs.IndexQuery(\n&gt;&gt;&gt;                     item_field={\"Plot\": [0, \"original_text\"],\n&gt;&gt;&gt;                                 \"Genre\": [0, 1],\n&gt;&gt;&gt;                                 \"Director\": \"preprocessed_text\"},\n&gt;&gt;&gt;                     classic_similarity=False,\n&gt;&gt;&gt;                     threshold=3)\n</code></pre> <p>Info</p> <p>After instantiating the IndexQuery algorithm, pass it in the initialization of a CBRS and the use its method to calculate ranking for single user or multiple users:</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; cbrs = rs.ContentBasedRS(algorithm=alg, ...)\n&gt;&gt;&gt; cbrs.fit_rank(...)\n&gt;&gt;&gt; # ...\n</code></pre> PARAMETER DESCRIPTION <code>item_field</code> <p>dict where the key is the name of the field that contains the content to use, value is the representation(s) id(s) that will be used for the said item, just BE SURE to use textual representation(s). The value of a field can be a string or a list, use a list if you want to use multiple representations for a particular field.</p> <p> TYPE: <code>dict</code> </p> <code>classic_similarity</code> <p>True if you want to use the classic implementation of tfidf in Whoosh, False if you want BM25F</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>threshold</code> <p>Threshold for the ratings. If the rating is greater than the threshold, it will be considered as positive. If the threshold is not specified, the average score of all items rated by the user is used.</p> <p> TYPE: <code>float</code> DEFAULT: <code>None</code> </p> Source code in <code>clayrs/recsys/content_based_algorithm/index_query/index_query.py</code> <pre><code>def __init__(self, item_field: dict, classic_similarity: bool = True, threshold: float = None):\n    super().__init__(item_field, threshold)\n    self._string_query: Optional[str] = None\n    self._scores: Optional[list] = None\n    self._positive_user_docs: Optional[dict] = None\n    self._classic_similarity: bool = classic_similarity\n</code></pre>"},{"location":"recsys/content_based/content_based_algorithms/index_query/#clayrs.recsys.content_based_algorithm.index_query.index_query.IndexQuery.fit_single_user","title":"<code>fit_single_user()</code>","text":"<p>The fit process for the IndexQuery consists in building a query using the features of the positive items ONLY (items that the user liked). The terms relative to these 'positive' items are boosted by the rating he/she/it gave.</p> <p>This method uses extracted features of the positive items stored in a private attribute, so <code>process_rated()</code> must be called before this method.</p> <p>The built query will also be stored in a private attribute.</p> Source code in <code>clayrs/recsys/content_based_algorithm/index_query/index_query.py</code> <pre><code>def fit_single_user(self):\n\"\"\"\n    The fit process for the IndexQuery consists in building a query using the features of the positive items ONLY\n    (items that the user liked). The terms relative to these 'positive' items are boosted by the\n    rating he/she/it gave.\n\n    This method uses extracted features of the positive items stored in a private attribute, so\n    `process_rated()` must be called before this method.\n\n    The built query will also be stored in a private attribute.\n    \"\"\"\n    # For each field of each document one string (containing the name of the field and the data in it)\n    # is created and added to the query.\n    # Also each part of the query that refers to a document\n    # is boosted by the score given by the user to said document\n    string_query = \"(\"\n    for (doc_id, doc_data), score in zip(self._positive_user_docs, self._scores):\n        string_query += \"(\"\n        for field_name in doc_data:\n            if field_name == 'content_id':\n                continue\n            word_list = doc_data[field_name].split()\n            string_query += field_name + \":(\"\n            for term in word_list:\n                string_query += term + \" \"\n            string_query += \") \"\n        string_query += \")^\" + str(score) + \" \"\n    string_query += \") \"\n\n    self._string_query = string_query\n</code></pre>"},{"location":"recsys/content_based/content_based_algorithms/index_query/#clayrs.recsys.content_based_algorithm.index_query.index_query.IndexQuery.predict_single_user","title":"<code>predict_single_user(user_idx, train_ratings, available_loaded_items, filter_list)</code>","text":"<p>IndexQuery is not a Prediction Score Algorithm, so if this method is called, a NotPredictionAlg exception is raised</p> RAISES DESCRIPTION <code>NotPredictionAlg</code> <p>exception raised since the IndexQuery algorithm is not a score prediction algorithm</p> Source code in <code>clayrs/recsys/content_based_algorithm/index_query/index_query.py</code> <pre><code>def predict_single_user(self, user_idx: int, train_ratings: Ratings, available_loaded_items: LoadedContentsIndex,\n                        filter_list: List[str]) -&gt; np.ndarray:\n\"\"\"\n    IndexQuery is not a Prediction Score Algorithm, so if this method is called,\n    a NotPredictionAlg exception is raised\n\n    Raises:\n        NotPredictionAlg: exception raised since the IndexQuery algorithm is not a score prediction algorithm\n    \"\"\"\n    raise NotPredictionAlg(\"IndexQuery is not a Score Prediction Algorithm!\")\n</code></pre>"},{"location":"recsys/content_based/content_based_algorithms/index_query/#clayrs.recsys.content_based_algorithm.index_query.index_query.IndexQuery.process_rated","title":"<code>process_rated(user_idx, train_ratings, available_loaded_items)</code>","text":"<p>Function that extracts features from positive rated items ONLY of a user The extracted features will be used to fit the algorithm (build the query).</p> <p>Features extracted will be stored in private attributes of the class.</p> <p>IF there are no rated items available locally or if there are only positive/negative items, an exception is thrown.</p> PARAMETER DESCRIPTION <code>user_idx</code> <p>Mapped integer of the active user (the user for which we must fit the algorithm)</p> <p> TYPE: <code>int</code> </p> <code>train_ratings</code> <p><code>Ratings</code> object which contains the train set of each user</p> <p> TYPE: <code>Ratings</code> </p> <code>available_loaded_items</code> <p>The LoadedContents interface which contains loaded contents</p> <p> TYPE: <code>LoadedContentsIndex</code> </p> RAISES DESCRIPTION <code>EmptyUserRatings</code> <p>Exception raised when the user does not appear in the train set</p> <code>OnlyNegativeitems</code> <p>Exception raised when there are only negative items available locally for the user (Items that the user disliked)</p> Source code in <code>clayrs/recsys/content_based_algorithm/index_query/index_query.py</code> <pre><code>def process_rated(self, user_idx: int, train_ratings: Ratings, available_loaded_items: LoadedContentsIndex):\n\"\"\"\n    Function that extracts features from positive rated items ONLY of a user\n    The extracted features will be used to fit the algorithm (build the query).\n\n    Features extracted will be stored in private attributes of the class.\n\n    IF there are no rated items available locally or if there are only positive/negative\n    items, an exception is thrown.\n\n    Args:\n        user_idx: Mapped integer of the active user (the user for which we must fit the algorithm)\n        train_ratings: `Ratings` object which contains the train set of each user\n        available_loaded_items: The LoadedContents interface which contains loaded contents\n\n    Raises:\n        EmptyUserRatings: Exception raised when the user does not appear in the train set\n        OnlyNegativeitems: Exception raised when there are only negative items available locally\n            for the user (Items that the user disliked)\n    \"\"\"\n\n    uir_user = train_ratings.get_user_interactions(user_idx)\n    rated_items_id = train_ratings.item_map.convert_seq_int2str(uir_user[:, 1].astype(int))\n\n    # a list since there could be duplicate interaction (eg bootstrap partitioning)\n    items_scores_dict = defaultdict(list)\n\n    for item_id, score in zip(rated_items_id, uir_user[:, 2]):\n        items_scores_dict[item_id].append(score)\n\n    items_scores_dict = dict(sorted(items_scores_dict.items()))  # sort dictionary based on key for reproducibility\n\n    threshold = self.threshold\n    if threshold is None:\n        threshold = np.nanmean(uir_user[:, 2])\n\n    # Initializes positive_user_docs which is a list that has tuples with document_id as first element and\n    # a dictionary as second. The dictionary value has the name of the field as key\n    # and its contents as value. By doing so we obtain the data of the fields while\n    # also storing information regarding the field and the document where it was\n    scores = []\n    positive_user_docs = []\n\n    ix = available_loaded_items.get_contents_interface()\n\n    # we extract feature of each item sorted based on its key: IMPORTANT for reproducibility!!\n    # we must convert keys (which are strings) to the respective int idx to build the uir\n    for (item_id, (item_idx, score_list)) in zip(rated_items_id, items_scores_dict.items()):\n\n        score_assigned = map(float, score_list)\n\n        for score in score_assigned:\n            if score &gt;= threshold:\n                # {item_id: {\"item\": item_dictionary, \"score\": item_score}}\n                item_query = ix.query(item_id, results_number=1, classic_similarity=self._classic_similarity)\n                if len(item_query) != 0:\n                    item = item_query.pop(item_id).get('item')\n                    scores.append(score)\n                    positive_user_docs.append((item_idx, self._get_representations(item)))\n\n    if len(uir_user[:, 1]) == 0:\n        raise EmptyUserRatings(\"The user selected doesn't have any ratings!\")\n\n    if len(positive_user_docs) == 0:\n        raise OnlyNegativeItems(f\"User {user_idx} - There are no rated items available locally or there are only \"\n                                f\"negative items available locally!\")\n\n    self._positive_user_docs = positive_user_docs\n    self._scores = scores\n</code></pre>"},{"location":"recsys/content_based/content_based_algorithms/index_query/#clayrs.recsys.content_based_algorithm.index_query.index_query.IndexQuery.rank_single_user","title":"<code>rank_single_user(user_idx, train_ratings, available_loaded_items, recs_number, filter_list)</code>","text":"<p>Rank the top-n recommended items for the active user, where the top-n items to rank are controlled by the <code>recs_number</code> and <code>filter_list</code> parameter:</p> <ul> <li>the former one is self-explanatory, the second is a list of items represented with their string ids. Must be necessarily strings and not their mapped integer since items are serialized following their string representation!</li> </ul> <p>If <code>recs_number</code> is <code>None</code>, all ranked items will be returned</p> <p>The filter list parameter is usually the result of the <code>filter_single()</code> method of a <code>Methodology</code> object</p> PARAMETER DESCRIPTION <code>user_idx</code> <p>Mapped integer of the active user</p> <p> TYPE: <code>int</code> </p> <code>train_ratings</code> <p><code>Ratings</code> object which contains the train set of each user</p> <p> TYPE: <code>Ratings</code> </p> <code>available_loaded_items</code> <p>The LoadedContents interface which contains loaded contents</p> <p> TYPE: <code>LoadedContentsIndex</code> </p> <code>recs_number</code> <p>number of the top ranked items to return, if None all ranked items will be returned</p> <p> TYPE: <code>Optional[int]</code> </p> <code>filter_list</code> <p>list of the items to rank. Should contain string item ids</p> <p> TYPE: <code>List[str]</code> </p> RETURNS DESCRIPTION <code>np.ndarray</code> <p>uir matrix for a single user containing user and item idxs (integer representation) with the ranked score as third dimension sorted in a decreasing order</p> Source code in <code>clayrs/recsys/content_based_algorithm/index_query/index_query.py</code> <pre><code>def rank_single_user(self, user_idx: int, train_ratings: Ratings, available_loaded_items: LoadedContentsIndex,\n                     recs_number: Optional[int], filter_list: List[str]) -&gt; np.ndarray:\n\"\"\"\n    Rank the top-n recommended items for the active user, where the top-n items to rank are controlled by the\n    `recs_number` and `filter_list` parameter:\n\n    * the former one is self-explanatory, the second is a list of items\n    represented with their string ids. Must be necessarily strings and not their mapped integer since items are\n    serialized following their string representation!\n\n    If `recs_number` is `None`, all ranked items will be returned\n\n    The filter list parameter is usually the result of the `filter_single()` method of a `Methodology` object\n\n    Args:\n        user_idx: Mapped integer of the active user\n        train_ratings: `Ratings` object which contains the train set of each user\n        available_loaded_items: The LoadedContents interface which contains loaded contents\n        recs_number: number of the top ranked items to return, if None all ranked items will be returned\n        filter_list: list of the items to rank. Should contain string item ids\n\n    Returns:\n        uir matrix for a single user containing user and item idxs (integer representation) with the ranked score\n            as third dimension sorted in a decreasing order\n    \"\"\"\n    uir_user = train_ratings.get_user_interactions(user_idx)\n    if len(uir_user) == 0:\n        raise EmptyUserRatings(\"The user selected doesn't have any ratings!\")\n\n    user_seen_items = train_ratings.item_map.convert_seq_int2str(uir_user[:, 1].astype(int))\n    mask_list = self._build_mask_list(user_seen_items, filter_list)\n\n    ix = available_loaded_items.get_contents_interface()\n    score_docs = ix.query(self._string_query, recs_number, mask_list, filter_list, self._classic_similarity)\n\n    # we must convert keys (which are strings) to the respective int idx to build the uir\n    score_list_idxs = train_ratings.item_map.convert_seq_str2int(list(score_docs.keys()))\n\n    # we construct the output data\n    uir_rank = np.array([[user_idx, item_idx, score_docs[item_id]['score']]\n                         for item_idx, item_id in zip(score_list_idxs, score_docs)])\n\n    return uir_rank\n</code></pre>"},{"location":"recsys/content_based/content_based_algorithms/linear_predictor/","title":"Linear Predictor","text":""},{"location":"recsys/content_based/content_based_algorithms/linear_predictor/#clayrs.recsys.content_based_algorithm.regressor.linear_predictor.LinearPredictor","title":"<code>LinearPredictor(item_field, regressor, only_greater_eq=None, embedding_combiner=Centroid())</code>","text":"<p>         Bases: <code>PerUserCBAlgorithm</code></p> <p>Class that implements recommendation through a specified linear predictor. It's a score prediction algorithm, so it can predict what rating a user would give to an unseen item. As such, it's also a ranking algorithm (it simply ranks in descending order unseen items by the predicted rating)</p> <p>Examples:</p> <ul> <li>Interested in only a field representation, <code>LinearRegression</code> regressor from sklearn</li> </ul> <pre><code>&gt;&gt;&gt; from clayrs import recsys as rs\n&gt;&gt;&gt; alg = rs.LinearPredictor({\"Plot\": 0}, rs.SkLinearRegression())\n</code></pre> <ul> <li>Interested in only a field representation, <code>Ridge</code> regressor from sklearn with custom parameters</li> </ul> <pre><code>&gt;&gt;&gt; alg = rs.LinearPredictor({\"Plot\": 0}, rs.SkRidge(alpha=0.8))\n</code></pre> <ul> <li>Interested in multiple field representations of the items, Ridge regressor from sklearn with custom parameters, <code>only_greater_eq</code> \\(= 2\\) (Every item with rating \\(&gt;= 2\\) will be discarded and not considered in the ranking/score prediction task)</li> </ul> <pre><code>&gt;&gt;&gt; alg = rs.LinearPredictor(\n&gt;&gt;&gt;                         item_field={\"Plot\": [0, \"tfidf\"],\n&gt;&gt;&gt;                                     \"Genre\": [0, 1],\n&gt;&gt;&gt;                                     \"Director\": \"doc2vec\"},\n&gt;&gt;&gt;                         regressor=rs.SkRidge(alpha=0.8),\n&gt;&gt;&gt;                         only_greater_eq=2)\n</code></pre> <p>Info</p> <p>After instantiating the <code>LinearPredictor</code> algorithm, pass it in the initialization of a CBRS and the use its method to predict ratings or calculate ranking for a single user or multiple users:</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; cbrs = rs.ContentBasedRS(algorithm=alg, ...)\n&gt;&gt;&gt; cbrs.fit_predict(...)\n&gt;&gt;&gt; cbrs.fit_rank(...)\n&gt;&gt;&gt; # ...\n</code></pre> PARAMETER DESCRIPTION <code>item_field</code> <p>dict where the key is the name of the field that contains the content to use, value is the representation(s) id(s) that will be used for the said item. The value of a field can be a string or a list, use a list if you want to use multiple representations for a particular field.</p> <p> TYPE: <code>dict</code> </p> <code>regressor</code> <p>regressor that will be used. Can be one object of the <code>Regressor</code> class.</p> <p> TYPE: <code>Regressor</code> </p> <code>only_greater_eq</code> <p>Threshold for the ratings. Only items with rating greater or equal than the threshold will be considered, items with lower rating will be discarded. If None, no item will be filter out</p> <p> TYPE: <code>float</code> DEFAULT: <code>None</code> </p> <code>embedding_combiner</code> <p><code>CombiningTechnique</code> used when embeddings representation must be used, but they are in a matrix form instead of a single vector (e.g. WordEmbedding representations have one vector for each word). By default, the <code>Centroid</code> of the rows of the matrix is computed</p> <p> TYPE: <code>CombiningTechnique</code> DEFAULT: <code>Centroid()</code> </p> Source code in <code>clayrs/recsys/content_based_algorithm/regressor/linear_predictor.py</code> <pre><code>def __init__(self, item_field: dict, regressor: Regressor, only_greater_eq: float = None,\n             embedding_combiner: CombiningTechnique = Centroid()):\n    super().__init__(item_field, only_greater_eq)\n    self._regressor = regressor\n    self._labels: Optional[list] = None\n    self._items_features: Optional[list] = None\n    self._embedding_combiner = embedding_combiner\n</code></pre>"},{"location":"recsys/content_based/content_based_algorithms/linear_predictor/#clayrs.recsys.content_based_algorithm.regressor.linear_predictor.LinearPredictor.fit_single_user","title":"<code>fit_single_user()</code>","text":"<p>Fit the regressor specified in the constructor with the features and labels (rating scores) extracted with the <code>process_rated()</code> method.</p> <p>It uses private attributes to fit the regressor, so <code>process_rated()</code> must be called before this method.</p> Source code in <code>clayrs/recsys/content_based_algorithm/regressor/linear_predictor.py</code> <pre><code>def fit_single_user(self):\n\"\"\"\n    Fit the regressor specified in the constructor with the features and labels (rating scores)\n    extracted with the `process_rated()` method.\n\n    It uses private attributes to fit the regressor, so `process_rated()` must be called\n    before this method.\n    \"\"\"\n    # Fuse the input if there are dicts, multiple representation, etc.\n    fused_features = self.fuse_representations(self._items_features, self._embedding_combiner)\n\n    self._regressor.fit(fused_features, self._labels)\n\n    # we delete variables used to fit since will no longer be used\n    self._labels = None\n    self._items_features = None\n</code></pre>"},{"location":"recsys/content_based/content_based_algorithms/linear_predictor/#clayrs.recsys.content_based_algorithm.regressor.linear_predictor.LinearPredictor.predict_single_user","title":"<code>predict_single_user(user_idx, train_ratings, available_loaded_items, filter_list)</code>","text":"<p>Predicts how much a user will like unrated items.</p> <p>The filter list parameter is usually the result of the <code>filter_single()</code> method of a <code>Methodology</code> object, and is a list of items represented with their string ids. Must be necessarily strings and not their mapped integer since items are serialized following their string representation!</p> PARAMETER DESCRIPTION <code>user_idx</code> <p>Mapped integer of the active user</p> <p> TYPE: <code>int</code> </p> <code>train_ratings</code> <p><code>Ratings</code> object which contains the train set of each user</p> <p> TYPE: <code>Ratings</code> </p> <code>available_loaded_items</code> <p>The LoadedContents interface which contains loaded contents</p> <p> TYPE: <code>LoadedContentsDict</code> </p> <code>filter_list</code> <p>list of the items to rank. Should contain string item ids</p> <p> TYPE: <code>List[str]</code> </p> RETURNS DESCRIPTION <code>np.ndarray</code> <p>uir matrix for a single user containing user and item idxs (integer representation) with the predicted score as third dimension</p> Source code in <code>clayrs/recsys/content_based_algorithm/regressor/linear_predictor.py</code> <pre><code>def predict_single_user(self, user_idx: int, train_ratings: Ratings, available_loaded_items: LoadedContentsDict,\n                        filter_list: List[str]) -&gt; np.ndarray:\n\"\"\"\n    Predicts how much a user will like unrated items.\n\n    The filter list parameter is usually the result of the `filter_single()` method of a `Methodology` object, and\n    is a list of items represented with their string ids. Must be necessarily strings and not their mapped integer\n    since items are serialized following their string representation!\n\n    Args:\n        user_idx: Mapped integer of the active user\n        train_ratings: `Ratings` object which contains the train set of each user\n        available_loaded_items: The LoadedContents interface which contains loaded contents\n        filter_list: list of the items to rank. Should contain string item ids\n\n    Returns:\n        uir matrix for a single user containing user and item idxs (integer representation) with the predicted score\n            as third dimension\n    \"\"\"\n\n    idx_items_to_predict, score_labels = self._common_prediction_process(user_idx, train_ratings,\n                                                                         available_loaded_items,\n                                                                         filter_list)\n    if len(score_labels) != 0:\n        # Build the output data\n        uir_pred = np.array(\n            [[user_idx, item_idx, score] for item_idx, score in zip(idx_items_to_predict, score_labels)])\n    else:\n        uir_pred = np.array([])\n\n    return uir_pred\n</code></pre>"},{"location":"recsys/content_based/content_based_algorithms/linear_predictor/#clayrs.recsys.content_based_algorithm.regressor.linear_predictor.LinearPredictor.process_rated","title":"<code>process_rated(user_idx, train_ratings, available_loaded_items)</code>","text":"<p>Function that extracts features from rated item and labels them. The extracted features will be later used to fit the regressor.</p> <p>Features and labels (in this case the rating score) will be stored in private attributes of the class.</p> <p>IF there are no rated items available locally, an exception is thrown.</p> PARAMETER DESCRIPTION <code>user_idx</code> <p>Mapped integer of the active user (the user for which we must fit the algorithm)</p> <p> TYPE: <code>int</code> </p> <code>train_ratings</code> <p><code>Ratings</code> object which contains the train set of each user</p> <p> TYPE: <code>Ratings</code> </p> <code>available_loaded_items</code> <p>The LoadedContents interface which contains loaded contents</p> <p> TYPE: <code>LoadedContentsDict</code> </p> RAISES DESCRIPTION <code>EmptyUserRatings</code> <p>Exception raised when the user does not appear in the train set</p> <code>NoRatedItems</code> <p>Exception raised when there isn't any item available locally rated by the user</p> Source code in <code>clayrs/recsys/content_based_algorithm/regressor/linear_predictor.py</code> <pre><code>def process_rated(self, user_idx: int, train_ratings: Ratings, available_loaded_items: LoadedContentsDict):\n\"\"\"\n    Function that extracts features from rated item and labels them.\n    The extracted features will be later used to fit the regressor.\n\n    Features and labels (in this case the rating score) will be stored in private attributes of the class.\n\n    IF there are no rated items available locally, an exception is thrown.\n\n    Args:\n        user_idx: Mapped integer of the active user (the user for which we must fit the algorithm)\n        train_ratings: `Ratings` object which contains the train set of each user\n        available_loaded_items: The LoadedContents interface which contains loaded contents\n\n    Raises:\n        EmptyUserRatings: Exception raised when the user does not appear in the train set\n        NoRatedItems: Exception raised when there isn't any item available locally\n            rated by the user\n    \"\"\"\n    uir_user = train_ratings.get_user_interactions(user_idx)\n    rated_items_id = train_ratings.item_map.convert_seq_int2str(uir_user[:, 1].astype(int))\n\n    # a list since there could be duplicate interaction (eg bootstrap partitioning)\n    items_scores_dict = defaultdict(list)\n\n    for item_id, score in zip(rated_items_id, uir_user[:, 2]):\n        items_scores_dict[item_id].append(score)\n\n    items_scores_dict = dict(sorted(items_scores_dict.items()))  # sort dictionary based on key for reproducibility\n\n    # Create list of all the available items that are useful for the user\n    loaded_rated_items: List[Union[Content, None]] = available_loaded_items.get_list([item_id\n                                                                                      for item_id\n                                                                                      in rated_items_id])\n\n    # Assign label and extract features from the rated items\n    labels = []\n    items_features = []\n\n    for item in loaded_rated_items:\n        if item is not None:\n\n            score_assigned = map(float, items_scores_dict[item.content_id])\n\n            for score in score_assigned:\n                if self.threshold is None or score &gt;= self.threshold:\n                    items_features.append(self.extract_features_item(item))\n                    labels.append(score)\n\n    if len(uir_user[:, 1]) == 0:\n        raise EmptyUserRatings(\"The user selected doesn't have any ratings!\")\n\n    if len(items_features) == 0:\n        raise NoRatedItems(\"User {} - No rated item available locally!\".format(user_idx))\n\n    self._labels = labels\n    self._items_features = items_features\n</code></pre>"},{"location":"recsys/content_based/content_based_algorithms/linear_predictor/#clayrs.recsys.content_based_algorithm.regressor.linear_predictor.LinearPredictor.rank_single_user","title":"<code>rank_single_user(user_idx, train_ratings, available_loaded_items, recs_number, filter_list)</code>","text":"<p>Rank the top-n recommended items for the active user, where the top-n items to rank are controlled by the <code>recs_number</code> and <code>filter_list</code> parameter:</p> <ul> <li>the former one is self-explanatory, the second is a list of items represented with their string ids. Must be necessarily strings and not their mapped integer since items are serialized following their string representation!</li> </ul> <p>If <code>recs_number</code> is <code>None</code>, all ranked items will be returned</p> <p>The filter list parameter is usually the result of the <code>filter_single()</code> method of a <code>Methodology</code> object</p> PARAMETER DESCRIPTION <code>user_idx</code> <p>Mapped integer of the active user</p> <p> TYPE: <code>int</code> </p> <code>train_ratings</code> <p><code>Ratings</code> object which contains the train set of each user</p> <p> TYPE: <code>Ratings</code> </p> <code>available_loaded_items</code> <p>The LoadedContents interface which contains loaded contents</p> <p> TYPE: <code>LoadedContentsDict</code> </p> <code>recs_number</code> <p>number of the top ranked items to return, if None all ranked items will be returned</p> <p> TYPE: <code>Optional[int]</code> </p> <code>filter_list</code> <p>list of the items to rank. Should contain string item ids</p> <p> TYPE: <code>List[str]</code> </p> RETURNS DESCRIPTION <code>np.ndarray</code> <p>uir matrix for a single user containing user and item idxs (integer representation) with the ranked score as third dimension sorted in a decreasing order</p> Source code in <code>clayrs/recsys/content_based_algorithm/regressor/linear_predictor.py</code> <pre><code>def rank_single_user(self, user_idx: int, train_ratings: Ratings, available_loaded_items: LoadedContentsDict,\n                     recs_number: Optional[int], filter_list: List[str]) -&gt; np.ndarray:\n\"\"\"\n    Rank the top-n recommended items for the active user, where the top-n items to rank are controlled by the\n    `recs_number` and `filter_list` parameter:\n\n    * the former one is self-explanatory, the second is a list of items\n    represented with their string ids. Must be necessarily strings and not their mapped integer since items are\n    serialized following their string representation!\n\n    If `recs_number` is `None`, all ranked items will be returned\n\n    The filter list parameter is usually the result of the `filter_single()` method of a `Methodology` object\n\n    Args:\n        user_idx: Mapped integer of the active user\n        train_ratings: `Ratings` object which contains the train set of each user\n        available_loaded_items: The LoadedContents interface which contains loaded contents\n        recs_number: number of the top ranked items to return, if None all ranked items will be returned\n        filter_list: list of the items to rank. Should contain string item ids\n\n    Returns:\n        uir matrix for a single user containing user and item idxs (integer representation) with the ranked score\n            as third dimension sorted in a decreasing order\n    \"\"\"\n\n    # Predict the rating for the items and sort them in descending order\n    idx_items_to_predict, score_labels = self._common_prediction_process(user_idx, train_ratings,\n                                                                         available_loaded_items,\n                                                                         filter_list)\n\n    if len(score_labels) != 0:\n        sorted_scores_idxs = np.argsort(score_labels)[::-1][:recs_number]\n        sorted_items = np.array(idx_items_to_predict)[sorted_scores_idxs]\n        sorted_scores = score_labels[sorted_scores_idxs]\n\n        # we construct the output data\n        uir_rank = np.array([[user_idx, item_idx, score] for item_idx, score in zip(sorted_items, sorted_scores)])\n    else:\n        uir_rank = np.array([])\n\n    return uir_rank\n</code></pre>"},{"location":"recsys/content_based/content_based_algorithms/linear_predictor/#regressors-implemented","title":"Regressors Implemented","text":"<p>The following are the regressors you can use in the <code>regressor</code> parameter of the <code>LinearPredictor</code> class</p>"},{"location":"recsys/content_based/content_based_algorithms/linear_predictor/#clayrs.recsys.content_based_algorithm.regressor.regressors.SkARDRegression","title":"<code>SkARDRegression(*, n_iter=300, tol=0.001, alpha_1=1e-06, alpha_2=1e-06, lambda_1=1e-06, lambda_2=1e-06, compute_score=False, threshold_lambda=10000.0, fit_intercept=True, normalize='deprecated', copy_X=True, verbose=False)</code>","text":"<p>         Bases: <code>Regressor</code></p> <p>Class that implements the ARD regressor from sklearn. The parameters one could pass are the same ones you would pass instantiating the regressor ARD directly from sklearn.</p> <p>Sklearn documentation: here</p> Source code in <code>clayrs/recsys/content_based_algorithm/regressor/regressors.py</code> <pre><code>def __init__(self, *,\n             n_iter: Any = 300,\n             tol: Any = 1.0e-3,\n             alpha_1: Any = 1.0e-6,\n             alpha_2: Any = 1.0e-6,\n             lambda_1: Any = 1.0e-6,\n             lambda_2: Any = 1.0e-6,\n             compute_score: Any = False,\n             threshold_lambda: Any = 1.0e4,\n             fit_intercept: Any = True,\n             normalize: Any = \"deprecated\",\n             copy_X: Any = True,\n             verbose: Any = False):\n    model = ARDRegression(n_iter=n_iter, tol=tol, alpha_1=alpha_1, alpha_2=alpha_2, lambda_1=lambda_1,\n                          lambda_2=lambda_2, compute_score=compute_score, threshold_lambda=threshold_lambda,\n                          fit_intercept=fit_intercept, normalize=normalize, copy_X=copy_X, verbose=verbose)\n    super().__init__(model, inspect.currentframe())\n</code></pre>"},{"location":"recsys/content_based/content_based_algorithms/linear_predictor/#clayrs.recsys.content_based_algorithm.regressor.regressors.SkBayesianRidge","title":"<code>SkBayesianRidge(*, n_iter=300, tol=0.001, alpha_1=1e-06, alpha_2=1e-06, lambda_1=1e-06, lambda_2=1e-06, alpha_init=None, lambda_init=None, compute_score=False, fit_intercept=True, normalize='deprecated', copy_X=True, verbose=False)</code>","text":"<p>         Bases: <code>Regressor</code></p> <p>Class that implements the BayesianRidge regressor from sklearn. The parameters one could pass are the same ones you would pass instantiating the regressor BayesianRidge directly from sklearn.</p> <p>Sklearn documentation: here</p> Source code in <code>clayrs/recsys/content_based_algorithm/regressor/regressors.py</code> <pre><code>def __init__(self, *,\n             n_iter: Any = 300,\n             tol: Any = 1.0e-3,\n             alpha_1: Any = 1.0e-6,\n             alpha_2: Any = 1.0e-6,\n             lambda_1: Any = 1.0e-6,\n             lambda_2: Any = 1.0e-6,\n             alpha_init: Any = None,\n             lambda_init: Any = None,\n             compute_score: Any = False,\n             fit_intercept: Any = True,\n             normalize: Any = \"deprecated\",\n             copy_X: Any = True,\n             verbose: Any = False):\n    model = BayesianRidge(n_iter=n_iter, tol=tol, alpha_1=alpha_1, alpha_2=alpha_2, lambda_1=lambda_1,\n                          lambda_2=lambda_2, alpha_init=alpha_init, lambda_init=lambda_init,\n                          compute_score=compute_score, fit_intercept=fit_intercept, normalize=normalize,\n                          copy_X=copy_X, verbose=verbose)\n\n    super().__init__(model, inspect.currentframe())\n</code></pre>"},{"location":"recsys/content_based/content_based_algorithms/linear_predictor/#clayrs.recsys.content_based_algorithm.regressor.regressors.SkHuberRegressor","title":"<code>SkHuberRegressor(*, epsilon=1.35, max_iter=100, alpha=0.0001, warm_start=False, fit_intercept=True, tol=1e-05)</code>","text":"<p>         Bases: <code>Regressor</code></p> <p>Class that implements the Huber regressor from sklearn. The parameters one could pass are the same ones you would pass instantiating the regressor Huber directly from sklearn.</p> <p>Sklearn documentation: here</p> Source code in <code>clayrs/recsys/content_based_algorithm/regressor/regressors.py</code> <pre><code>def __init__(self, *,\n             epsilon: Any = 1.35,\n             max_iter: Any = 100,\n             alpha: Any = 0.0001,\n             warm_start: Any = False,\n             fit_intercept: Any = True,\n             tol: Any = 1e-05):\n    model = HuberRegressor(epsilon=epsilon, max_iter=max_iter, alpha=alpha,\n                           warm_start=warm_start, fit_intercept=fit_intercept, tol=tol)\n\n    super().__init__(model, inspect.currentframe())\n</code></pre>"},{"location":"recsys/content_based/content_based_algorithms/linear_predictor/#clayrs.recsys.content_based_algorithm.regressor.regressors.SkLinearRegression","title":"<code>SkLinearRegression(*, fit_intercept=True, normalize='deprecated', copy_X=True, n_jobs=None, positive=False)</code>","text":"<p>         Bases: <code>Regressor</code></p> <p>Class that implements the LinearRegression regressor from sklearn. The parameters one could pass are the same ones you would pass instantiating the regressor LinearRegression directly from sklearn.</p> <p>Sklearn documentation: here</p> Source code in <code>clayrs/recsys/content_based_algorithm/regressor/regressors.py</code> <pre><code>def __init__(self, *, fit_intercept: Any = True,\n             normalize: Any = \"deprecated\",\n             copy_X: Any = True,\n             n_jobs: Any = None,\n             positive: Any = False):\n    model = LinearRegression(fit_intercept=fit_intercept, normalize=normalize, copy_X=copy_X, n_jobs=n_jobs,\n                             positive=positive)\n\n    super().__init__(model, inspect.currentframe())\n</code></pre>"},{"location":"recsys/content_based/content_based_algorithms/linear_predictor/#clayrs.recsys.content_based_algorithm.regressor.regressors.SkPassiveAggressiveRegressor","title":"<code>SkPassiveAggressiveRegressor(*, C=1.0, fit_intercept=True, max_iter=1000, tol=0.001, early_stopping=False, validation_fraction=0.1, n_iter_no_change=5, shuffle=True, verbose=0, loss='epsilon_insensitive', epsilon=DEFAULT_EPSILON, random_state=None, warm_start=False, average=False)</code>","text":"<p>         Bases: <code>Regressor</code></p> <p>Class that implements the PassiveAggressive regressor from sklearn. The parameters one could pass are the same ones you would pass instantiating the regressor PassiveAggressive directly from sklearn.</p> <p>Sklearn documentation: here</p> Source code in <code>clayrs/recsys/content_based_algorithm/regressor/regressors.py</code> <pre><code>def __init__(self, *,\n             C: Any = 1.0,\n             fit_intercept: Any = True,\n             max_iter: Any = 1000,\n             tol: Any = 1e-3,\n             early_stopping: Any = False,\n             validation_fraction: Any = 0.1,\n             n_iter_no_change: Any = 5,\n             shuffle: Any = True,\n             verbose: Any = 0,\n             loss: Any = \"epsilon_insensitive\",\n             epsilon: Any = DEFAULT_EPSILON,\n             random_state: Any = None,\n             warm_start: Any = False,\n             average: Any = False):\n\n    model = PassiveAggressiveRegressor(C=C, fit_intercept=fit_intercept, max_iter=max_iter, tol=tol,\n                                       early_stopping=early_stopping, validation_fraction=validation_fraction,\n                                       n_iter_no_change=n_iter_no_change, shuffle=shuffle, verbose=verbose,\n                                       loss=loss, epsilon=epsilon, random_state=random_state, warm_start=warm_start,\n                                       average=average)\n    super().__init__(model, inspect.currentframe())\n</code></pre>"},{"location":"recsys/content_based/content_based_algorithms/linear_predictor/#clayrs.recsys.content_based_algorithm.regressor.regressors.SkRidge","title":"<code>SkRidge(alpha=1.0, *, fit_intercept=True, normalize='deprecated', copy_X=True, max_iter=None, tol=0.001, solver='auto', positive=False, random_state=None)</code>","text":"<p>         Bases: <code>Regressor</code></p> <p>Class that implements the Ridge regressor from sklearn. The parameters one could pass are the same ones you would pass instantiating the regressor Ridge directly from sklearn.</p> <p>Sklearn documentation: here</p> Source code in <code>clayrs/recsys/content_based_algorithm/regressor/regressors.py</code> <pre><code>def __init__(self, alpha: Any = 1.0,\n             *,\n             fit_intercept: Any = True,\n             normalize: Any = \"deprecated\",\n             copy_X: Any = True,\n             max_iter: Any = None,\n             tol: Any = 1e-3,\n             solver: Any = \"auto\",\n             positive: Any = False,\n             random_state: Any = None):\n    model = Ridge(alpha=alpha, fit_intercept=fit_intercept, normalize=normalize, copy_X=copy_X,\n                  max_iter=max_iter, tol=tol, solver=solver, positive=positive, random_state=random_state)\n\n    super().__init__(model, inspect.currentframe())\n</code></pre>"},{"location":"recsys/content_based/content_based_algorithms/linear_predictor/#clayrs.recsys.content_based_algorithm.regressor.regressors.SkSGDRegressor","title":"<code>SkSGDRegressor(loss='squared_error', *, penalty='l2', alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=1000, tol=0.001, shuffle=True, verbose=0, epsilon=DEFAULT_EPSILON, random_state=None, learning_rate='invscaling', eta0=0.01, power_t=0.25, early_stopping=False, validation_fraction=0.1, n_iter_no_change=5, warm_start=False, average=False)</code>","text":"<p>         Bases: <code>Regressor</code></p> <p>Class that implements the SGD regressor from sklearn. The parameters one could pass are the same ones you would pass instantiating the regressor SGD directly from sklearn.</p> <p>Sklearn documentation: here</p> Source code in <code>clayrs/recsys/content_based_algorithm/regressor/regressors.py</code> <pre><code>def __init__(self, loss: Any = \"squared_error\",\n             *,\n             penalty: Any = \"l2\",\n             alpha: Any = 0.0001,\n             l1_ratio: Any = 0.15,\n             fit_intercept: Any = True,\n             max_iter: Any = 1000,\n             tol: Any = 1e-3,\n             shuffle: Any = True,\n             verbose: Any = 0,\n             epsilon: Any = DEFAULT_EPSILON,\n             random_state: Any = None,\n             learning_rate: Any = \"invscaling\",\n             eta0: Any = 0.01,\n             power_t: Any = 0.25,\n             early_stopping: Any = False,\n             validation_fraction: Any = 0.1,\n             n_iter_no_change: Any = 5,\n             warm_start: Any = False,\n             average: Any = False):\n    model = SGDRegressor(loss=loss, penalty=penalty, alpha=alpha, l1_ratio=l1_ratio, fit_intercept=fit_intercept,\n                         max_iter=max_iter, tol=tol, shuffle=shuffle, verbose=verbose, epsilon=epsilon,\n                         random_state=random_state, learning_rate=learning_rate, eta0=eta0, power_t=power_t,\n                         early_stopping=early_stopping, validation_fraction=validation_fraction,\n                         n_iter_no_change=n_iter_no_change, warm_start=warm_start, average=average)\n    super().__init__(model, inspect.currentframe())\n</code></pre>"},{"location":"recsys/content_based/visual_based_algorithms/vbpr/","title":"Centroid Vector","text":""},{"location":"recsys/content_based/visual_based_algorithms/vbpr/#clayrs.recsys.visual_based_algorithm.vbpr.vbpr_algorithm.VBPR","title":"<code>VBPR(item_field, gamma_dim, theta_dim, batch_size, epochs, threshold=0, learning_rate=0.005, lambda_w=0.01, lambda_b_pos=0.01, lambda_b_neg=0.001, lambda_e=0, train_loss=fun.logsigmoid, optimizer_class=torch.optim.Adam, device=None, embedding_combiner=Centroid(), normalize=True, seed=None, additional_opt_parameters=None, additional_dl_parameters=None)</code>","text":"<p>         Bases: <code>ContentBasedAlgorithm</code></p> <p>Class that implements recommendation through the VBPR algorithm. It's a ranking algorithm, so it can't do score prediction.</p> <p>The VBPR algorithm expects features extracted from images and works on implicit feedback, but in theory you could use any embedding representation, and you can use explicit feedback which will be converted into implicit one thanks to the <code>threshold</code> parameter:</p> <ul> <li>All scores \\(&gt;= threshold\\) are considered positive scores</li> </ul> <p>For more details on VBPR algorithm, please check the relative paper here</p> PARAMETER DESCRIPTION <code>item_field</code> <p>dict where the key is the name of the field that contains the content to use, value is the representation(s) id(s) that will be used for the said item. The value of a field can be a string or a list, use a list if you want to use multiple representations for a particular field.</p> <p> TYPE: <code>dict</code> </p> <code>gamma_dim</code> <p>dimension of latent factors for non-visual parameters</p> <p> TYPE: <code>int</code> </p> <code>theta_dim</code> <p>dimension of latent factors for visual parameters</p> <p> TYPE: <code>int</code> </p> <code>batch_size</code> <p>dimension of each batch of the torch dataloader for the images features</p> <p> TYPE: <code>int</code> </p> <code>epochs</code> <p>number of training epochs</p> <p> TYPE: <code>int</code> </p> <code>threshold</code> <p>float value which is used to distinguish positive from negative items. If None, it will vary for each user, and it will be set to the average rating given by it</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>0</code> </p> <code>learning_rate</code> <p>learning rate for the torch optimizer</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.005</code> </p> <code>lambda_w</code> <p>weight assigned to the regularization of the loss on \\(\\gamma_u\\), \\(\\gamma_i\\), \\(\\theta_u\\)</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.01</code> </p> <code>lambda_b_pos</code> <p>weight assigned to the regularization of the loss on \\(\\beta_i\\) for the positive items</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.01</code> </p> <code>lambda_b_neg</code> <p>weight assigned to the regularization of the loss on \\(\\beta_i\\) for the negative items</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.001</code> </p> <code>lambda_e</code> <p>weight assigned to the regularization of the loss on \\(\\beta'\\), \\(E\\)</p> <p> TYPE: <code>float</code> DEFAULT: <code>0</code> </p> <code>train_loss</code> <p>loss function for the training phase. Default is logsigmoid</p> <p> TYPE: <code>Callable[[torch.Tensor], torch.Tensor]</code> DEFAULT: <code>fun.logsigmoid</code> </p> <code>optimizer_class</code> <p>optimizer torch class for the training phase. It will be instantiated using <code>additional_opt_parameters</code> if specified</p> <p> TYPE: <code>Type[torch.optim.Optimizer]</code> DEFAULT: <code>torch.optim.Adam</code> </p> <code>device</code> <p>device on which the training will be run. If None and a GPU is available, then the GPU is automatically selected as device to use. Otherwise, the cpu is used</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>embedding_combiner</code> <p><code>CombiningTechnique</code> used when embeddings representation must be used, but they are in a matrix form instead of a single vector (e.g. WordEmbedding representations have one vector for each word). By default, the <code>Centroid</code> of the rows of the matrix is computed</p> <p> TYPE: <code>CombiningTechnique</code> DEFAULT: <code>Centroid()</code> </p> <code>normalize</code> <p>Whether to normalize input features or not. If True, the input feature matrix is subtracted to its \\(min\\) and divided by its \\(max + 1e-10\\)</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>seed</code> <p>random state which will be used for weight initialization and sampling of the negative example</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>additional_opt_parameters</code> <p>kwargs for the optimizer. If you specify learning rate in this parameter, it will be overwritten by the local <code>learning_rate</code> parameter</p> <p> TYPE: <code>Dict[str, Any]</code> DEFAULT: <code>None</code> </p> <code>additional_dl_parameters</code> <p>kwargs for the dataloader. If you specify batch size in this parameter, it will be overwritten by the local <code>batch_size</code> parameter</p> <p> TYPE: <code>Dict[str, Any]</code> DEFAULT: <code>None</code> </p> Source code in <code>clayrs/recsys/visual_based_algorithm/vbpr/vbpr_algorithm.py</code> <pre><code>def __init__(self, item_field: dict,\n             gamma_dim: int, theta_dim: int, batch_size: int, epochs: int,\n             threshold: Optional[float] = 0,\n             learning_rate: float = 0.005,\n             lambda_w: float = 0.01, lambda_b_pos: float = 0.01, lambda_b_neg: float = 0.001, lambda_e: float = 0,\n             train_loss: Callable[[torch.Tensor], torch.Tensor] = fun.logsigmoid,\n             optimizer_class: Type[torch.optim.Optimizer] = torch.optim.Adam,\n             device: str = None,\n             embedding_combiner: CombiningTechnique = Centroid(),\n             normalize: bool = True,\n             seed: int = None,\n             additional_opt_parameters: Dict[str, Any] = None,\n             additional_dl_parameters: Dict[str, Any] = None):\n\n    super().__init__(item_field, threshold)\n\n    if additional_opt_parameters is None:\n        additional_opt_parameters = {}\n\n    if additional_dl_parameters is None:\n        additional_dl_parameters = {}\n\n    additional_opt_parameters[\"lr\"] = learning_rate\n    additional_dl_parameters[\"batch_size\"] = batch_size\n\n    self.device = device if device is not None else \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n\n    self.gamma_dim = gamma_dim\n    self.theta_dim = theta_dim\n\n    self.epochs = epochs\n    self.train_loss = train_loss\n    self.train_optimizer = optimizer_class\n    self.train_optimizer_parameters = additional_opt_parameters\n    self.normalize = normalize\n    self.lambda_w = lambda_w\n    self.lambda_b_pos = lambda_b_pos\n    self.lambda_b_neg = lambda_b_neg\n    self.lambda_e = lambda_e\n\n    self._embedding_combiner = embedding_combiner\n\n    self.seed = seed\n    self.dl_parameters = additional_dl_parameters\n</code></pre>"},{"location":"recsys/content_based/visual_based_algorithms/vbpr/#clayrs.recsys.visual_based_algorithm.vbpr.vbpr_algorithm.VBPR.fit","title":"<code>fit(train_set, items_directory, num_cpus=-1)</code>","text":"<p>Method which will fit the VBPR algorithm via neural training with torch</p> PARAMETER DESCRIPTION <code>train_set</code> <p><code>Ratings</code> object which contains the train set of each user</p> <p> TYPE: <code>Ratings</code> </p> <code>items_directory</code> <p>Path where complexly represented items are serialized by the Content Analyzer</p> <p> TYPE: <code>str</code> </p> <code>num_cpus</code> <p>number of processors that must be reserved for the method. If set to <code>0</code>, all cpus available will be used. Be careful though: multiprocessing in python has a substantial memory overhead!</p> <p> TYPE: <code>int</code> DEFAULT: <code>-1</code> </p> RETURNS DESCRIPTION <code>VBPRNetwork</code> <p>A fit VBPRNetwork object (torch module which implements the VBPR neural network)</p> Source code in <code>clayrs/recsys/visual_based_algorithm/vbpr/vbpr_algorithm.py</code> <pre><code>def fit(self, train_set: Ratings, items_directory: str, num_cpus: int = -1) -&gt; VBPRNetwork:\n\"\"\"\n    Method which will fit the VBPR algorithm via neural training with torch\n\n    Args:\n        train_set: `Ratings` object which contains the train set of each user\n        items_directory: Path where complexly represented items are serialized by the Content Analyzer\n        num_cpus: number of processors that must be reserved for the method. If set to `0`, all cpus available will\n            be used. Be careful though: multiprocessing in python has a substantial memory overhead!\n\n    Returns:\n        A fit VBPRNetwork object (torch module which implements the VBPR neural network)\n    \"\"\"\n\n    def _l2_loss(*tensors):\n        l2_loss = 0\n        for tensor in tensors:\n            l2_loss += tensor.pow(2).sum()\n        return l2_loss / 2\n\n    train_set = self._build_only_positive_ratings(train_set)\n\n    items_features = self._load_items_features(train_set, items_directory)\n\n    self._seed_all()\n\n    items_features = torch.tensor(items_features, device=self.device, dtype=torch.float)\n\n    model = VBPRNetwork(n_users=len(train_set.user_map),\n                        n_items=len(train_set.item_map),\n                        features_dim=items_features.shape[1],\n                        gamma_dim=self.gamma_dim,\n                        theta_dim=self.theta_dim,\n                        device=self.device)\n\n    optimizer = self.train_optimizer([\n        model.beta_items,\n        model.gamma_users,\n        model.gamma_items,\n        model.theta_users,\n        model.E,\n        model.beta_prime\n    ], **self.train_optimizer_parameters)\n\n    train_dataset = TriplesDataset(train_set, self.seed)\n\n    train_dl = torch.utils.data.DataLoader(train_dataset, **self.dl_parameters)\n\n    model.train()\n\n    logger.info(\"Starting VBPR training!\")\n    for epoch in range(self.epochs):\n\n        train_loss = 0\n        n_user_processed = 0\n\n        with get_progbar(train_dl) as pbar:\n\n            pbar.set_description(f\"Starting {epoch + 1}/{self.epochs} epoch...\")\n\n            for i, batch in enumerate(pbar):\n\n                user_idx = batch[0].long()\n                pos_idx = batch[1].long()\n                neg_idx = batch[2].long()\n\n                n_user_processed += len(user_idx)\n\n                positive_features = items_features[pos_idx]\n                negative_features = items_features[neg_idx]\n\n                model_input = (\n                    user_idx.to(self.device),\n                    pos_idx.to(self.device),\n                    neg_idx.to(self.device),\n                    positive_features.to(self.device),\n                    negative_features.to(self.device)\n                )\n\n                Xuij, (gamma_u, theta_u), (beta_i_pos, beta_i_neg), (gamma_i_pos, gamma_i_neg) = model(model_input)\n                loss = - self.train_loss(Xuij).sum()\n\n                reg = (\n                        _l2_loss(gamma_u, gamma_i_pos, gamma_i_neg, theta_u) * self.lambda_w\n                        + _l2_loss(beta_i_pos) * self.lambda_b_pos\n                        + _l2_loss(beta_i_neg) * self.lambda_b_neg\n                        + _l2_loss(model.E, model.beta_prime) * self.lambda_e\n                )\n\n                loss = loss + reg\n                train_loss += loss.item()\n\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n\n                if (i + 1) % 100 == 0 or (i + 1) == len(train_dl):\n                    pbar.set_description(f'[Epoch {epoch + 1}/{self.epochs}, '\n                                         f'Batch {i + 1}/{len(train_dl)}, '\n                                         f'Loss: {train_loss / n_user_processed:.3f}]')\n\n    logger.info(\"Training complete!\")\n\n    logger.info(\"Computing visual bias and theta items for faster ranking...\")\n    with torch.no_grad():\n        model.theta_items = items_features.mm(model.E.data).cpu()\n        model.visual_bias = items_features.mm(model.beta_prime.data).squeeze().cpu()\n        model.cpu()\n\n    logger.info(\"Done!\")\n\n    return model\n</code></pre>"},{"location":"recsys/content_based/visual_based_algorithms/vbpr/#clayrs.recsys.visual_based_algorithm.vbpr.vbpr_algorithm.VBPR.fit_predict","title":"<code>fit_predict(train_set, test_set, items_directory, user_idx_list, methodology, num_cpus, save_fit)</code>","text":"<p>VBPR is not a score prediction algorithm, calling this method will raise the <code>NotPredictionAlg</code> exception!</p> RAISES DESCRIPTION <code>NotPredictionAlg</code> <p>exception raised since the VBPR algorithm is not a score prediction algorithm</p> Source code in <code>clayrs/recsys/visual_based_algorithm/vbpr/vbpr_algorithm.py</code> <pre><code>def fit_predict(self, train_set: Ratings, test_set: Ratings, items_directory: str, user_idx_list: Set[int],\n                methodology: Methodology,\n                num_cpus: int, save_fit: bool) -&gt; Tuple[Optional[VBPRNetwork], List[np.ndarray]]:\n\"\"\"\n    VBPR is not a score prediction algorithm, calling this method will raise the `NotPredictionAlg` exception!\n\n    Raises:\n        NotPredictionAlg: exception raised since the VBPR algorithm is not a score prediction algorithm\n    \"\"\"\n\n    raise NotPredictionAlg(\"VBPR is not a Score Prediction Algorithm!\")\n</code></pre>"},{"location":"recsys/content_based/visual_based_algorithms/vbpr/#clayrs.recsys.visual_based_algorithm.vbpr.vbpr_algorithm.VBPR.fit_rank","title":"<code>fit_rank(train_set, test_set, items_directory, user_idx_list, n_recs, methodology, num_cpus, save_fit)</code>","text":"<p>Method used to both fit and calculate ranking for all users in <code>user_idx_list</code> parameter. The algorithm will first be fit considering all users in the <code>user_idx_list</code> which should contain user id mapped to their integer!</p> <p>With the <code>save_fit</code> parameter you can specify if you need the function to return the algorithm fit (in case you want to perform multiple calls to the <code>predict()</code> or <code>rank()</code> function). If set to True, the first value returned by this function will be the fit algorithm and the second will be the list of uir matrices with predictions for each user. Otherwise, if <code>save_fit</code> is False, the first value returned by this function will be <code>None</code></p> PARAMETER DESCRIPTION <code>train_set</code> <p><code>Ratings</code> object which contains the train set of each user</p> <p> TYPE: <code>Ratings</code> </p> <code>test_set</code> <p>Ratings object which represents the ground truth of the split considered</p> <p> TYPE: <code>Ratings</code> </p> <code>items_directory</code> <p>Path where complexly represented items are serialized by the Content Analyzer</p> <p> TYPE: <code>str</code> </p> <code>user_idx_list</code> <p>Set of user idx (int representation) for which a recommendation list must be generated. Users should be represented with their mapped integer!</p> <p> TYPE: <code>Set[int]</code> </p> <code>n_recs</code> <p>Number of the top items that will be present in the ranking of each user. If <code>None</code> all candidate items will be returned for the user. Default is 10 (top-10 for each user will be computed)</p> <p> TYPE: <code>Optional[int]</code> </p> <code>methodology</code> <p><code>Methodology</code> object which governs the candidate item selection. Default is <code>TestRatingsMethodology</code>. If None, AllItemsMethodology() will be used</p> <p> TYPE: <code>Methodology</code> </p> <code>save_fit</code> <p>Boolean value which let you choose if the fit algorithm should be saved and returned by this function. If True, the first value returned by this function is the fit algorithm. Otherwise, the first value will be None. The second value is always the list of predicted uir matrices</p> <p> TYPE: <code>bool</code> </p> <code>num_cpus</code> <p>number of processors that must be reserved for the method. If set to <code>0</code>, all cpus available will be used. Be careful though: multiprocessing in python has a substantial memory overhead!</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>Optional[VBPRNetwork]</code> <p>The first value is the fit VBPR algorithm (could be None if <code>save_fit == False</code>)</p> <code>List[np.ndarray]</code> <p>The second value is a list of predicted uir matrices all sorted in a decreasing order w.r.t. the ranking scores</p> Source code in <code>clayrs/recsys/visual_based_algorithm/vbpr/vbpr_algorithm.py</code> <pre><code>def fit_rank(self, train_set: Ratings, test_set: Ratings, items_directory: str, user_idx_list: Set[int],\n             n_recs: Optional[int], methodology: Methodology,\n             num_cpus: int, save_fit: bool) -&gt; Tuple[Optional[VBPRNetwork], List[np.ndarray]]:\n\"\"\"\n    Method used to both fit and calculate ranking for all users in `user_idx_list` parameter.\n    The algorithm will first be fit considering all users in the `user_idx_list` which should contain user id\n    mapped to their integer!\n\n    With the `save_fit` parameter you can specify if you need the function to return the algorithm fit (in case\n    you want to perform multiple calls to the `predict()` or `rank()` function). If set to True, the first value\n    returned by this function will be the fit algorithm and the second will be the list of uir matrices with\n    predictions for each user.\n    Otherwise, if `save_fit` is False, the first value returned by this function will be `None`\n\n    Args:\n        train_set: `Ratings` object which contains the train set of each user\n        test_set: Ratings object which represents the ground truth of the split considered\n        items_directory: Path where complexly represented items are serialized by the Content Analyzer\n        user_idx_list: Set of user idx (int representation) for which a recommendation list must be generated.\n            Users should be represented with their mapped integer!\n        n_recs: Number of the top items that will be present in the ranking of each user.\n            If `None` all candidate items will be returned for the user. Default is 10 (top-10 for each user\n            will be computed)\n        methodology: `Methodology` object which governs the candidate item selection. Default is\n            `TestRatingsMethodology`. If None, AllItemsMethodology() will be used\n        save_fit: Boolean value which let you choose if the fit algorithm should be saved and returned by this\n            function. If True, the first value returned by this function is the fit algorithm. Otherwise, the first\n            value will be None. The second value is always the list of predicted uir matrices\n        num_cpus: number of processors that must be reserved for the method. If set to `0`, all cpus available will\n            be used. Be careful though: multiprocessing in python has a substantial memory overhead!\n\n    Returns:\n        The first value is the fit VBPR algorithm (could be None if `save_fit == False`)\n\n        The second value is a list of predicted uir matrices all sorted in a decreasing order w.r.t.\n            the ranking scores\n    \"\"\"\n    vbpr_fit = self.fit(train_set, items_directory, num_cpus)\n    rank = self.rank(vbpr_fit, train_set, test_set, items_directory, user_idx_list, n_recs, methodology, num_cpus)\n\n    vbpr_fit = vbpr_fit if save_fit else None\n\n    return vbpr_fit, rank\n</code></pre>"},{"location":"recsys/content_based/visual_based_algorithms/vbpr/#clayrs.recsys.visual_based_algorithm.vbpr.vbpr_algorithm.VBPR.predict","title":"<code>predict(fit_alg, train_set, test_set, items_directory, user_idx_list, methodology, num_cpus)</code>","text":"<p>VBPR is not a score prediction algorithm, calling this method will raise the <code>NotPredictionAlg</code> exception!</p> RAISES DESCRIPTION <code>NotPredictionAlg</code> <p>exception raised since the VBPR algorithm is not a score prediction algorithm</p> Source code in <code>clayrs/recsys/visual_based_algorithm/vbpr/vbpr_algorithm.py</code> <pre><code>def predict(self, fit_alg: VBPRNetwork, train_set: Ratings, test_set: Ratings, items_directory: str,\n            user_idx_list: Set[int], methodology: Methodology,\n            num_cpus: int) -&gt; List[np.ndarray]:\n\"\"\"\n    VBPR is not a score prediction algorithm, calling this method will raise the `NotPredictionAlg` exception!\n\n    Raises:\n        NotPredictionAlg: exception raised since the VBPR algorithm is not a score prediction algorithm\n    \"\"\"\n\n    raise NotPredictionAlg(\"VBPR is not a Score Prediction Algorithm!\")\n</code></pre>"},{"location":"recsys/content_based/visual_based_algorithms/vbpr/#clayrs.recsys.visual_based_algorithm.vbpr.vbpr_algorithm.VBPR.rank","title":"<code>rank(fit_alg, train_set, test_set, items_directory, user_idx_list, n_recs, methodology, num_cpus)</code>","text":"<p>Method used to calculate ranking for all users in <code>user_idx_list</code> parameter. You must first call the <code>fit()</code> method before you can compute the ranking. The <code>user_idx_list</code> parameter should contain users with mapped to their integer!</p> <p>The representation of the fit VBPR algorithm is a <code>VBPRNetwork</code> object (torch module which implements the VBPR neural network)</p> <p>If the <code>n_recs</code> is specified, then the rank will contain the top-n items for the users. Otherwise, the rank will contain all unrated items of the particular users.</p> <p>Via the <code>methodology</code> parameter you can perform different candidate item selection. By default, the <code>TestRatingsMethodology()</code> is used: so, for each user, items in its test set only will be ranked</p> PARAMETER DESCRIPTION <code>fit_alg</code> <p>a fit <code>VBPRNetwork</code> object (torch module which implements the VBPR neural network)</p> <p> TYPE: <code>VBPRNetwork</code> </p> <code>train_set</code> <p><code>Ratings</code> object which contains the train set of each user</p> <p> TYPE: <code>Ratings</code> </p> <code>test_set</code> <p>Ratings object which represents the ground truth of the split considered</p> <p> TYPE: <code>Ratings</code> </p> <code>items_directory</code> <p>Path where complexly represented items are serialized by the Content Analyzer</p> <p> TYPE: <code>str</code> </p> <code>user_idx_list</code> <p>Set of user idx (int representation) for which a recommendation list must be generated. Users should be represented with their mapped integer!</p> <p> TYPE: <code>Set[int]</code> </p> <code>n_recs</code> <p>Number of the top items that will be present in the ranking of each user. If <code>None</code> all candidate items will be returned for the user. Default is 10 (top-10 for each user will be computed)</p> <p> TYPE: <code>Optional[int]</code> </p> <code>methodology</code> <p><code>Methodology</code> object which governs the candidate item selection. Default is <code>TestRatingsMethodology</code>. If None, AllItemsMethodology() will be used</p> <p> TYPE: <code>Methodology</code> </p> <code>num_cpus</code> <p>number of processors that must be reserved for the method. If set to <code>0</code>, all cpus available will be used. Be careful though: multiprocessing in python has a substantial memory overhead!</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>List[np.ndarray]</code> <p>List of uir matrices for each user, where each uir contains predicted interactions between users and unseen items sorted in a descending way w.r.t. the third dimension which is the ranked score</p> Source code in <code>clayrs/recsys/visual_based_algorithm/vbpr/vbpr_algorithm.py</code> <pre><code>def rank(self, fit_alg: VBPRNetwork, train_set: Ratings, test_set: Ratings, items_directory: str,\n         user_idx_list: Set[int], n_recs: Optional[int], methodology: Methodology,\n         num_cpus: int) -&gt; List[np.ndarray]:\n\"\"\"\n    Method used to calculate ranking for all users in `user_idx_list` parameter.\n    You must first call the `fit()` method ***before*** you can compute the ranking.\n    The `user_idx_list` parameter should contain users with mapped to their integer!\n\n    The representation of the fit VBPR algorithm is a `VBPRNetwork` object (torch module which implements the\n    VBPR neural network)\n\n    If the `n_recs` is specified, then the rank will contain the top-n items for the users.\n    Otherwise, the rank will contain all unrated items of the particular users.\n\n    Via the `methodology` parameter you can perform different candidate item selection. By default, the\n    `TestRatingsMethodology()` is used: so, for each user, items in its test set only will be ranked\n\n    Args:\n        fit_alg: a fit `VBPRNetwork` object (torch module which implements the VBPR neural network)\n        train_set: `Ratings` object which contains the train set of each user\n        test_set: Ratings object which represents the ground truth of the split considered\n        items_directory: Path where complexly represented items are serialized by the Content Analyzer\n        user_idx_list: Set of user idx (int representation) for which a recommendation list must be generated.\n            Users should be represented with their mapped integer!\n        n_recs: Number of the top items that will be present in the ranking of each user.\n            If `None` all candidate items will be returned for the user. Default is 10 (top-10 for each user\n            will be computed)\n        methodology: `Methodology` object which governs the candidate item selection. Default is\n            `TestRatingsMethodology`. If None, AllItemsMethodology() will be used\n        num_cpus: number of processors that must be reserved for the method. If set to `0`, all cpus available will\n            be used. Be careful though: multiprocessing in python has a substantial memory overhead!\n\n    Returns:\n        List of uir matrices for each user, where each uir contains predicted interactions between users and unseen\n            items sorted in a descending way w.r.t. the third dimension which is the ranked score\n    \"\"\"\n\n    def compute_single_rank(user_idx):\n        filter_list = methodology.filter_single(user_idx, train_set, test_set)\n        user_rank = fit_alg.return_scores(user_idx, filter_list)\n        user_uir = np.array((\n            np.full(len(user_rank), user_idx),\n            filter_list,\n            user_rank\n        )).T\n        # items are not sorted so we sort them (to have descending order, we invert the values of the user uir\n        # score column\n        sorted_user_uir = user_uir[(-user_uir[:, 2]).argsort()]\n        sorted_user_uir = sorted_user_uir[:n_recs]\n\n        return user_idx, sorted_user_uir\n\n    fit_alg.eval()\n\n    methodology.setup(train_set, test_set)\n\n    uir_rank_list = []\n    with get_iterator_parallel(num_cpus,\n                               compute_single_rank, user_idx_list,\n                               progress_bar=True, total=len(user_idx_list)) as pbar:\n\n        for user_idx, user_rank in pbar:\n            pbar.set_description(f\"Computing rank for user {user_idx}\")\n            uir_rank_list.append(user_rank)\n\n    return uir_rank_list\n</code></pre>"},{"location":"recsys/graph_based/feature_selection/","title":"Feature Selection","text":"<p>Via the <code>feature_selecter</code> function you are able to perform feature selection on a given graph, by keeping properties that are the most important according to a given feature selection algorithm. Check the documentation of the method for more and for a usage example</p>"},{"location":"recsys/graph_based/feature_selection/#clayrs.recsys.graphs.feature_selection.feature_selection_fn.feature_selector","title":"<code>feature_selector(graph, fs_algorithm_user=None, fs_algorithm_item=None, user_target_nodes=None, item_target_nodes=None, inplace=False)</code>","text":"<p>Given a FullGraph, this method performs feature selection on it and returns the \"reduced\" graph.</p> <p>You can choose to reduce only user properties (evaluate the <code>fs_algorithm_user</code> parameter), to reduce only item properties (evaluate the <code>fs_algorithm_item</code> parameter) or both (evaluate the <code>fs_algorithm_user</code> parameter and the <code>fs_algorithm_item</code> parameter). You can also choose different feature selection algorithms* for users and items.</p> <p>You can also define a custom list of user and item nodes:</p> <ul> <li>In this case only properties of those nodes will be considered during the feature selection process (instead of using properties of all users and items)</li> </ul> <p>This function changes a copy of the original graph by default, but you can change this behaviour with the <code>inplace</code> parameter.</p> <p>Examples:</p> <pre><code># create a full graph\nfull_graph = rs.NXFullGraph(ratings,\n                             user_contents_dir='users_codified/', # (1)\n                             item_contents_dir='movies_codified/', # (2)\n                             user_exo_properties={0}, # (3)\n                             item_exo_properties={'dbpedia'}, # (4)\n                             link_label='score')\n\n # perform feature selection by keeping only top 5 property labels\n # according to page rank algorithm\n fs_graph = rs.feature_selector(full_graph,\n                                fs_algorithm_item=rs.TopKPageRank(k=5))\n</code></pre> PARAMETER DESCRIPTION <code>graph</code> <p>Original graph on which feature selection will be performed</p> <p> TYPE: <code>FullDiGraph</code> </p> <code>fs_algorithm_user</code> <p>FeatureSelectionAlgorithm that will be performed on user properties. Can be different from <code>fs_algorithm_item</code></p> <p> TYPE: <code>FeatureSelectionAlgorithm</code> DEFAULT: <code>None</code> </p> <code>fs_algorithm_item</code> <p>FeatureSelectionAlgorithm that will be performed on item properties. Can be different from <code>fs_algorithm_user</code></p> <p> TYPE: <code>FeatureSelectionAlgorithm</code> DEFAULT: <code>None</code> </p> <code>user_target_nodes</code> <p>List of user nodes to consider in the feature selection process: only properties of user nodes in this list will be \"reduced\"</p> <p> TYPE: <code>list</code> DEFAULT: <code>None</code> </p> <code>item_target_nodes</code> <p>List of item nodes to consider in the feature selection process: only properties of item nodes in this list will be \"reduced\"</p> <p> TYPE: <code>list</code> DEFAULT: <code>None</code> </p> <code>inplace</code> <p>Boolean parameter that let you choose if changes must be performed on the original graph (<code>inplace=True</code>) or on its copy (<code>inplace=False</code>). Default is False</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>FullDiGraph</code> <p>Copy of the original graph from which the less important Property nodes (the ones having edges with less important property labels) will be removed</p> Source code in <code>clayrs/recsys/graphs/feature_selection/feature_selection_fn.py</code> <pre><code>def feature_selector(graph: FullDiGraph,\n                     fs_algorithm_user: FeatureSelectionAlgorithm = None,\n                     fs_algorithm_item: FeatureSelectionAlgorithm = None,\n                     user_target_nodes: Iterable[UserNode] = None,\n                     item_target_nodes: Iterable[ItemNode] = None,\n                     inplace: bool = False) -&gt; FullDiGraph:\n\"\"\"\n    Given a FullGraph, this method performs feature selection on it and returns the \"reduced\" graph.\n\n    You can choose to reduce only *user properties* (*evaluate the `fs_algorithm_user` parameter*),\n    to reduce only *item properties* (*evaluate the `fs_algorithm_item` parameter*) or both (*evaluate\n    the `fs_algorithm_user` parameter* and the `fs_algorithm_item` parameter*).\n    You can also choose different *feature selection algorithms* for users and items.\n\n    You can also define a custom list of user and item nodes:\n\n    * In this case only properties of those nodes will be considered during the feature selection process (instead of\n    using properties of all users and items)\n\n    This function changes a *copy* of the original graph by default, but you can change this behaviour with the\n    `inplace` parameter.\n\n    Examples:\n\n        ```python\n        # create a full graph\n        full_graph = rs.NXFullGraph(ratings,\n                                     user_contents_dir='users_codified/', # (1)\n                                     item_contents_dir='movies_codified/', # (2)\n                                     user_exo_properties={0}, # (3)\n                                     item_exo_properties={'dbpedia'}, # (4)\n                                     link_label='score')\n\n         # perform feature selection by keeping only top 5 property labels\n         # according to page rank algorithm\n         fs_graph = rs.feature_selector(full_graph,\n                                        fs_algorithm_item=rs.TopKPageRank(k=5))\n        ```\n\n    Args:\n        graph: Original graph on which feature selection will be performed\n        fs_algorithm_user: FeatureSelectionAlgorithm that will be performed on user properties. Can be different from\n            `fs_algorithm_item`\n        fs_algorithm_item: FeatureSelectionAlgorithm that will be performed on item properties. Can be different from\n            `fs_algorithm_user`\n        user_target_nodes (list): List of user nodes to consider in the feature selection process: only properties\n            of user nodes in this list will be \"reduced\"\n        item_target_nodes (list): List of item nodes to consider in the feature selection process: only properties\n            of item nodes in this list will be \"reduced\"\n        inplace: Boolean parameter that let you choose if changes must be performed on the original graph\n            (`inplace=True`) or on its copy (`inplace=False`). Default is False\n\n    Returns:\n        Copy of the original graph from which the less important Property nodes (the ones having edges with less\n            important property labels) will be removed\n    \"\"\"\n    if fs_algorithm_user is not None and user_target_nodes is None:\n        user_target_nodes = graph.user_nodes\n\n    if fs_algorithm_item is not None and item_target_nodes is None:\n        item_target_nodes = graph.item_nodes\n\n    property_labels_to_remove = list()\n    user_fs_failed = False\n    item_fs_failed = False\n\n    if fs_algorithm_user is not None:\n        logger.info(\"Performing Feature Selection on users\")\n        try:\n            user_props_to_remove = fs_algorithm_user.perform(graph, list(user_target_nodes), mode='to_remove')\n            property_labels_to_remove.extend(user_props_to_remove)\n        except FeatureSelectionException as e:\n            logger.warning(str(e) + \"!\\nUsers original properties will be kept\")\n            user_fs_failed = True\n\n    if fs_algorithm_item is not None:\n        logger.info(\"Performing Feature Selection on items\")\n        try:\n            item_props_to_remove = fs_algorithm_item.perform(graph, list(item_target_nodes), mode='to_remove')\n            property_labels_to_remove.extend(item_props_to_remove)\n        except FeatureSelectionException as e:\n            logger.warning(str(e) + \"!\\nItems original properties will be kept\")\n            item_fs_failed = True\n\n    # in case user feature selection or item feature selection failed\n    # if both failed the original graph is returned\n    # if only one of them failed, the original properties (either for items or users) are retrieved\n    if user_fs_failed and item_fs_failed:\n        logger.warning(\"Since both feature selection on items and feature selection on users failed or no fs algorithm\"\n                       \"has been defined,\\nthe original graph will be returned\")\n\n    if inplace is True:\n        graph_fs = _delete_property_nodes(graph, property_labels_to_remove)\n    else:\n        graph_copy = graph.copy()\n        graph_fs = _delete_property_nodes(graph_copy, property_labels_to_remove)\n\n    return graph_fs\n</code></pre>"},{"location":"recsys/graph_based/feature_selection/#feature-selection-algorithms","title":"Feature Selection algorithms","text":"<p>The following are the feature selection algorithms you can use in the <code>fs_algorithms_user</code>  and/or in the <code>fs_algorithm_item</code></p>"},{"location":"recsys/graph_based/feature_selection/#clayrs.recsys.graphs.feature_selection.feature_selection_alg.TopKPageRank","title":"<code>TopKPageRank(k=10, alpha=0.85, personalization=None, max_iter=100, tol=1e-06, nstart=None, weight=True, dangling=None)</code>","text":"<p>         Bases: <code>TopKFeatureSelection</code></p> <p>Computes the PageRank as FeatureSelection algorithm. Property labels of the original graph will be scored with their page rank score and only the top-k labels will be kept in the feature selected graph, while discarding the others</p> PARAMETER DESCRIPTION <code>k</code> <p>Top-k property labels to keep in the feature selected graph</p> <p> TYPE: <code>int</code> DEFAULT: <code>10</code> </p> <code>alpha</code> <p>Damping parameter for PageRank, default=0.85.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>0.85</code> </p> <code>personalization</code> <p>The \"personalization vector\" consisting of a dictionary with a key some subset of graph nodes and personalization value each of those. At least one personalization value must be non-zero. If not specfiied, a nodes personalization value will be zero. By default, a uniform distribution is used.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>None</code> </p> <code>max_iter</code> <p>Maximum number of iterations in power method eigenvalue solver.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>100</code> </p> <code>tol</code> <p>Error tolerance used to check convergence in power method solver.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>1e-06</code> </p> <code>nstart</code> <p>Starting value of PageRank iteration for each node.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>None</code> </p> <code>weight</code> <p>Edge data key to use as weight.  If None weights are set to 1.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>dangling</code> <p>The outedges to be assigned to any \"dangling\" nodes, i.e., nodes without any outedges. The dict key is the node the outedge points to and the dict value is the weight of that outedge. By default, dangling nodes are given outedges according to the personalization vector (uniform if not specified). This must be selected to result in an irreducible transition matrix (see notes under google_matrix). It may be common to have the dangling dict to be the same as the personalization dict.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>None</code> </p> Source code in <code>clayrs/recsys/graphs/feature_selection/feature_selection_alg.py</code> <pre><code>def __init__(self, k: int = 10, alpha: Any = 0.85, personalization: Any = None, max_iter: Any = 100,\n             tol: Any = 1.0e-6, nstart: Any = None, weight: bool = True, dangling: Any = None):\n    super().__init__(k)\n\n    self.alpha = alpha\n    self.personalization = personalization\n    self.max_iter = max_iter\n    self.tol = tol\n    self.nstart = nstart\n    self.weight = 'weight' if weight is True else None\n    self.dangling = dangling\n</code></pre>"},{"location":"recsys/graph_based/feature_selection/#clayrs.recsys.graphs.feature_selection.feature_selection_alg.TopKEigenVectorCentrality","title":"<code>TopKEigenVectorCentrality(k=10, max_iter=100, tol=1e-06, nstart=None, weight=False)</code>","text":"<p>         Bases: <code>TopKFeatureSelection</code></p> <p>Computes the Eigen Vector Centrality as FeatureSelection algorithm. Property labels of the original graph will be scored with their eigen vector centrality score and only the top-k labels will be kept in the feature selected graph, while discarding the others</p> PARAMETER DESCRIPTION <code>k</code> <p>Top-k property labels to keep in the feature selected graph</p> <p> TYPE: <code>int</code> DEFAULT: <code>10</code> </p> <code>max_iter</code> <p>Maximum number of iterations in power method.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>100</code> </p> <code>tol</code> <p>Error tolerance used to check convergence in power method iteration.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>1e-06</code> </p> <code>nstart</code> <p>Starting value of eigenvector iteration for each node.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>None</code> </p> <code>weight</code> <p>Boolean value which tells the algorithm if weight of the edges must be considered or not. Default is True</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>clayrs/recsys/graphs/feature_selection/feature_selection_alg.py</code> <pre><code>def __init__(self, k: int = 10, max_iter: Any = 100, tol: Any = 1.0e-6, nstart: Any = None, weight: bool = False):\n    super().__init__(k)\n\n    self.max_iter = max_iter\n    self.tol = tol\n    self.nstart = nstart\n    self.weight = 'weight' if weight is True else None\n</code></pre>"},{"location":"recsys/graph_based/feature_selection/#clayrs.recsys.graphs.feature_selection.feature_selection_alg.TopKDegreeCentrality","title":"<code>TopKDegreeCentrality(k=10)</code>","text":"<p>         Bases: <code>TopKFeatureSelection</code></p> <p>Computes the Degree Centrality as FeatureSelection algorithm. Property labels of the original graph will be scored with their degree centrality score and only the top-k labels will be kept in the feature selected graph, while discarding the others</p> Source code in <code>clayrs/recsys/graphs/feature_selection/feature_selection_alg.py</code> <pre><code>def __init__(self, k: int = 10):\n    super().__init__(k)\n</code></pre>"},{"location":"recsys/graph_based/graph_based_recsys/","title":"Graph Based RecSys","text":""},{"location":"recsys/graph_based/graph_based_recsys/#clayrs.recsys.recsys.GraphBasedRS","title":"<code>GraphBasedRS(algorithm, graph)</code>","text":"<p>         Bases: <code>RecSys</code></p> <p>Class for recommender systems which use a graph in order to make predictions</p> <p>Every GBRS differ from each other based the algorithm used.</p> <p>Examples:</p> <p>In case you perform a splitting of the dataset which returns a single train and test set (e.g. HoldOut technique):</p> Single split train<pre><code>from clayrs import recsys as rs\nfrom clayrs import content_analyzer as ca\n\noriginal_rat = ca.Ratings(ca.CSVFile(ratings_path))\n\n[train], [test] = rs.HoldOutPartitioning().split_all(original_rat)\n\nalg = rs.NXPageRank()  # any gb algorithm\n\ngraph = rs.NXBipartiteGraph(original_rat)\n\n# remove from the graph interaction of the test set\nfor user, item in zip(test.user_id_column, test.item_id_column):\n    user_node = rs.UserNode(user)\n    item_node = rs.ItemNode(item)\n\n    graph.remove_link(user_node, item_node)\n\ngbrs = rs.GraphBasedRS(alg, graph)\n\nrank = gbrs.rank(test, n_recs=10)\n</code></pre> <p>In case you perform a splitting of the dataset which returns a multiple train and test sets (KFold technique):</p> Multiple split train<pre><code>from clayrs import recsys as rs\nfrom clayrs import content_analyzer as ca\n\noriginal_rat = ca.Ratings(ca.CSVFile(ratings_path))\n\ntrain_list, test_list = rs.KFoldPartitioning(n_splits=5).split_all(original_rat)\n\nalg = rs.NXPageRank()  # any gb algorithm\n\nfor train_set, test_set in zip(train_list, test_list):\n\n    graph = rs.NXBipartiteGraph(original_rat)\n\n    # remove from the graph interaction of the test set\n    for user, item in zip(test_set.user_id_column, test_set.item_id_column):\n        user_node = rs.UserNode(user)\n        item_node = rs.ItemNode(item)\n\n        graph.remove_link(user_node, item_node)\n\n    gbrs = rs.GraphBasedRS(alg, graph)\n    rank_to_append = gbrs.rank(test_set)\n\n    result_list.append(rank_to_append)\n</code></pre> <p><code>result_list</code> will contain recommendation lists for each split</p> PARAMETER DESCRIPTION <code>algorithm</code> <p>the graph based algorithm that will be used in order to rank or make score prediction</p> <p> TYPE: <code>GraphBasedAlgorithm</code> </p> <code>graph</code> <p>A graph which models interactions of users and items</p> <p> TYPE: <code>FullDiGraph</code> </p> Source code in <code>clayrs/recsys/recsys.py</code> <pre><code>def __init__(self,\n             algorithm: GraphBasedAlgorithm,\n             graph: FullDiGraph):\n\n    self.__graph = graph\n    super().__init__(algorithm)\n</code></pre>"},{"location":"recsys/graph_based/graph_based_recsys/#clayrs.recsys.recsys.GraphBasedRS.algorithm","title":"<code>algorithm: GraphBasedAlgorithm</code>  <code>property</code>","text":"<p>The graph based algorithm chosen</p>"},{"location":"recsys/graph_based/graph_based_recsys/#clayrs.recsys.recsys.GraphBasedRS.graph","title":"<code>graph: FullDiGraph</code>  <code>property</code>","text":"<p>The graph containing interactions</p>"},{"location":"recsys/graph_based/graph_based_recsys/#clayrs.recsys.recsys.GraphBasedRS.users","title":"<code>users: Set[UserNode]</code>  <code>property</code>","text":"<p>Set of UserNode objects for each user of the graph</p>"},{"location":"recsys/graph_based/graph_based_recsys/#clayrs.recsys.recsys.GraphBasedRS.predict","title":"<code>predict(test_set, user_list=None, methodology=TestRatingsMethodology(), num_cpus=1)</code>","text":"<p>Method used to calculate score predictions for all users in test set or all users in <code>user_list</code> parameter. The <code>user_list</code> parameter could contain users with their string id or with their mapped integer</p> <p>BE CAREFUL: not all algorithms are able to perform score prediction</p> <p>Via the <code>methodology</code> parameter you can perform different candidate item selection. By default, the <code>TestRatingsMethodology()</code> is used: so for each user items in its test set only will be considered for score prediction</p> <p>If the algorithm couldn't perform score prediction for some users, they will be skipped and a warning message is printed showing the number of users for which the alg couldn't produce a score prediction</p> PARAMETER DESCRIPTION <code>test_set</code> <p>Ratings object which represents the ground truth of the split considered</p> <p> TYPE: <code>Ratings</code> </p> <code>user_list</code> <p>List of users for which you want to compute score prediction. If None, the ranking will be computed for all users of the <code>test_set</code>. The list should contain user id as strings or user ids mapped to their integers</p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>None</code> </p> <code>methodology</code> <p><code>Methodology</code> object which governs the candidate item selection. Default is <code>TestRatingsMethodology</code>. If None, AllItemsMethodology() will be used</p> <p> TYPE: <code>Union[Methodology, None]</code> DEFAULT: <code>TestRatingsMethodology()</code> </p> <code>num_cpus</code> <p>number of processors that must be reserved for the method. If set to <code>0</code>, all cpus available will be used. Be careful though: multiprocessing in python has a substantial memory overhead!</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> RETURNS DESCRIPTION <code>Prediction</code> <p>Prediction object containing score prediction lists for all users of the test set or for all users in <code>user_list</code></p> Source code in <code>clayrs/recsys/recsys.py</code> <pre><code>def predict(self, test_set: Ratings, user_list: List[str] = None,\n            methodology: Union[Methodology, None] = TestRatingsMethodology(),\n            num_cpus: int = 1) -&gt; Prediction:\n\"\"\"\n    Method used to calculate score predictions for all users in test set or all users in `user_list` parameter.\n    The `user_list` parameter could contain users with their string id or with their mapped integer\n\n    **BE CAREFUL**: not all algorithms are able to perform *score prediction*\n\n    Via the `methodology` parameter you can perform different candidate item selection. By default, the\n    `TestRatingsMethodology()` is used: so for each user items in its test set only will be considered for score\n    prediction\n\n    If the algorithm couldn't perform score prediction for some users, they will be skipped and a warning message is\n    printed showing the number of users for which the alg couldn't produce a score prediction\n\n    Args:\n        test_set: Ratings object which represents the ground truth of the split considered\n        user_list: List of users for which you want to compute score prediction. If None, the ranking\n            will be computed for all users of the `test_set`. The list should contain user id as strings or user ids\n            mapped to their integers\n        methodology: `Methodology` object which governs the candidate item selection. Default is\n            `TestRatingsMethodology`. If None, AllItemsMethodology() will be used\n        num_cpus: number of processors that must be reserved for the method. If set to `0`, all cpus available will\n            be used. Be careful though: multiprocessing in python has a substantial memory overhead!\n\n    Returns:\n        Prediction object containing score prediction lists for all users of the test set or for all users in\n            `user_list`\n    \"\"\"\n\n    train_set = self.graph.to_ratings(user_map=test_set.user_map, item_map=test_set.item_map)\n\n    logger.info(\"Don't worry if it looks stuck at first\")\n    logger.info(\"First iterations will stabilize the estimated remaining time\")\n\n    # in the graph recsys, each graph algorithm works with strings,\n    # so in case we should convert int to strings\n    all_users = test_set.unique_user_id_column\n    if user_list is not None:\n        all_users = np.array(user_list)\n        if np.issubdtype(all_users.dtype, int):\n            all_users = train_set.user_map.convert_seq_int2str(all_users)\n\n    all_users = set(all_users)\n\n    if methodology is None:\n        methodology = AllItemsMethodology()\n\n    methodology.setup(train_set, test_set)\n\n    pred = self.algorithm.predict(self.graph, train_set, test_set, all_users, methodology, num_cpus)\n    # we should remove empty uir matrices otherwise vstack won't work due to dimensions mismatch\n    pred = [uir_pred for uir_pred in pred if len(uir_pred) != 0]\n\n    # can't vstack when pred is empty\n    if len(pred) == 0:\n        pred = Prediction.from_uir(np.array([]), user_map=test_set.user_map, item_map=test_set.item_map)\n        return pred\n\n    pred = np.vstack(pred)\n    pred_users_idx = train_set.user_map.convert_seq_str2int(pred[:, 0])\n    pred_items_idx = train_set.item_map.convert_seq_str2int([item_node.value for item_node in pred[:, 1]])\n    pred[:, 0] = pred_users_idx\n    pred[:, 1] = pred_items_idx\n    pred = pred.astype(np.float64)\n    pred = Prediction.from_uir(pred, user_map=test_set.user_map, item_map=test_set.item_map)\n\n    self._yaml_report = {'graph': repr(self.graph), 'mode': 'score_prediction', 'methodology': repr(methodology)}\n\n    return pred\n</code></pre>"},{"location":"recsys/graph_based/graph_based_recsys/#clayrs.recsys.recsys.GraphBasedRS.rank","title":"<code>rank(test_set, n_recs=10, user_list=None, methodology=TestRatingsMethodology(), num_cpus=1)</code>","text":"<p>Method used to calculate ranking for all users in test set or all users in <code>user_list</code> parameter. The <code>user_list</code> parameter could contain users with their string id or with their mapped integer</p> <p>If the <code>n_recs</code> is specified, then the rank will contain the top-n items for the users. Otherwise, the rank will contain all unrated items of the particular users. By default the top-10 ranking is computed for each user</p> <p>Via the <code>methodology</code> parameter you can perform different candidate item selection. By default, the <code>TestRatingsMethodology()</code> is used: so, for each user, items in its test set only will be ranked</p> <p>If the algorithm couldn't produce a ranking for some users, they will be skipped and a warning message is printed showing the number of users for which the alg couldn't produce a ranking</p> PARAMETER DESCRIPTION <code>test_set</code> <p>Ratings object which represents the ground truth of the split considered</p> <p> TYPE: <code>Ratings</code> </p> <code>n_recs</code> <p>Number of the top items that will be present in the ranking of each user. If <code>None</code> all candidate items will be returned for the user. Default is 10 (top-10 for each user will be computed)</p> <p> TYPE: <code>int</code> DEFAULT: <code>10</code> </p> <code>user_list</code> <p>List of users for which you want to compute score prediction. If None, the ranking will be computed for all users of the <code>test_set</code>. The list should contain user id as strings or user ids mapped to their integers</p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>None</code> </p> <code>methodology</code> <p><code>Methodology</code> object which governs the candidate item selection. Default is <code>TestRatingsMethodology</code>. If None, AllItemsMethodology() will be used</p> <p> TYPE: <code>Union[Methodology, None]</code> DEFAULT: <code>TestRatingsMethodology()</code> </p> <code>num_cpus</code> <p>number of processors that must be reserved for the method. If set to <code>0</code>, all cpus available will be used. Be careful though: multiprocessing in python has a substantial memory overhead!</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> RETURNS DESCRIPTION <code>Rank</code> <p>Rank object containing recommendation lists for all users of the test set or for all users in <code>user_list</code></p> Source code in <code>clayrs/recsys/recsys.py</code> <pre><code>def rank(self, test_set: Ratings, n_recs: int = 10, user_list: List[str] = None,\n         methodology: Union[Methodology, None] = TestRatingsMethodology(),\n         num_cpus: int = 1) -&gt; Rank:\n\"\"\"\n    Method used to calculate ranking for all users in test set or all users in `user_list` parameter.\n    The `user_list` parameter could contain users with their string id or with their mapped integer\n\n    If the `n_recs` is specified, then the rank will contain the top-n items for the users.\n    Otherwise, the rank will contain all unrated items of the particular users.\n    By default the ***top-10*** ranking is computed for each user\n\n    Via the `methodology` parameter you can perform different candidate item selection. By default, the\n    `TestRatingsMethodology()` is used: so, for each user, items in its test set only will be ranked\n\n    If the algorithm couldn't produce a ranking for some users, they will be skipped and a warning message is\n    printed showing the number of users for which the alg couldn't produce a ranking\n\n    Args:\n        test_set: Ratings object which represents the ground truth of the split considered\n        n_recs: Number of the top items that will be present in the ranking of each user.\n            If `None` all candidate items will be returned for the user. Default is 10 (top-10 for each user\n            will be computed)\n        user_list: List of users for which you want to compute score prediction. If None, the ranking\n            will be computed for all users of the `test_set`. The list should contain user id as strings or user ids\n            mapped to their integers\n        methodology: `Methodology` object which governs the candidate item selection. Default is\n            `TestRatingsMethodology`. If None, AllItemsMethodology() will be used\n        num_cpus: number of processors that must be reserved for the method. If set to `0`, all cpus available will\n            be used. Be careful though: multiprocessing in python has a substantial memory overhead!\n\n    Returns:\n        Rank object containing recommendation lists for all users of the test set or for all users in `user_list`\n    \"\"\"\n\n    train_set = self.graph.to_ratings(user_map=test_set.user_map, item_map=test_set.item_map)\n\n    logger.info(\"Don't worry if it looks stuck at first\")\n    logger.info(\"First iterations will stabilize the estimated remaining time\")\n\n    # in the graph recsys, each graph algorithm works with strings,\n    # so in case we should convert int to strings\n    all_users = test_set.unique_user_id_column\n    if user_list is not None:\n        all_users = np.array(user_list)\n        if np.issubdtype(all_users.dtype, int):\n            all_users = train_set.user_map.convert_seq_int2str(all_users)\n\n    all_users = set(all_users)\n\n    if methodology is None:\n        methodology = AllItemsMethodology()\n\n    methodology.setup(train_set, test_set)\n\n    rank = self.algorithm.rank(self.graph, train_set, test_set, all_users, n_recs, methodology, num_cpus)\n    # we should remove empty uir matrices otherwise vstack won't work due to dimensions mismatch\n    rank = [uir_rank for uir_rank in rank if len(uir_rank) != 0]\n\n    # can't vstack when rank is empty\n    if len(rank) == 0:\n        rank = Rank.from_uir(np.array([]), user_map=test_set.user_map, item_map=test_set.item_map)\n        return rank\n\n    rank = np.vstack(rank)\n\n    # convert back strings and Nodes object to ints\n    rank_users_idx = train_set.user_map.convert_seq_str2int(rank[:, 0])\n    rank_items_idx = train_set.item_map.convert_seq_str2int([item_node.value for item_node in rank[:, 1]])\n    rank[:, 0] = rank_users_idx\n    rank[:, 1] = rank_items_idx\n    rank = rank.astype(np.float64)\n\n    rank = Rank.from_uir(rank, user_map=test_set.user_map, item_map=test_set.item_map)\n\n    if len(rank) == 0:\n        logger.warning(\"No items could be ranked for any users! Remember that items to rank must be present \"\n                       \"in the graph.\\n\"\n                       \"Try changing methodology!\")\n\n    elif len(rank.unique_user_id_column) != len(all_users):\n        logger.warning(f\"No items could be ranked for users {all_users - set(rank.user_id_column)}\\n\"\n                       f\"No nodes to rank for them found in the graph. Try changing methodology! \")\n\n    self._yaml_report = {'graph': repr(self.graph), 'mode': 'rank', 'n_recs': repr(n_recs),\n                         'methodology': repr(methodology)}\n\n    return rank\n</code></pre>"},{"location":"recsys/graph_based/graph_based_algorithms/nx_pagerank/","title":"Page Rank","text":""},{"location":"recsys/graph_based/graph_based_algorithms/nx_pagerank/#clayrs.recsys.graph_based_algorithm.page_rank.nx_page_rank.NXPageRank","title":"<code>NXPageRank(alpha=0.85, personalized=False, max_iter=100, tol=1e-06, nstart=None, weight=True, relevance_threshold=None, rel_items_weight=0.8, rel_items_prop_weight=None, default_nodes_weight=0.2)</code>","text":"<p>         Bases: <code>PageRank</code></p> <p>Page Rank algorithm based on the networkx implementation. Please note that it can only be used for instantiated NXGraphs</p> <p>The PageRank can be personalized, in this case the PageRank will be calculated with a personalization vector which depends by the specific user.</p> <p>Personalized PageRank</p> <p>In the PageRank, the random surfer has probability \\(\\alpha\\) of following one of the out links of the node it's in, and probability \\(1-\\alpha\\) of visiting another node which is not necessarily linked with an out link to the node it's in.</p> <ul> <li>In the classical Page Rank, all nodes have uniform probability of being picked by the random surfer when   not following the out links</li> <li>In the personalized Page Rank, we assign a different probability to certain nodes depending on heuristics   when not following the out links</li> </ul> <p>In the recommendation task, the idea is to assign a higher probability to item nodes which are relevant to the user, so that the Page Rank algorithm assigns higher score to item nodes close to relevant items for the user</p> <p>Several weighting schemas can be applied in order to customize the personalization vector of a user:</p> <ul> <li>80/20: 80% prob. that the random surfer ends up in a relevant item node, 20% that it will end up in any other node   (default)</li> <li>60/20/20: 60% prob. that the random surfer ends up in a relevant item node, 20% that it will end up in a property   node linked to a relevant item, 20% that it will end up in any other node</li> <li>40/40/20: 40% prob. that the random surfer ends up in a relevant item node, 40% that it will end up in a property   node linked to a relevant item, 20% that it will end up in any other node</li> <li>...</li> </ul> <p>It's important to note that the weight assigned are then normalized across each node category: this means that if 80% prob. is assigned to relevant items, then this probability is shared among all relevant items (i.e. 80% divided by the total number of relevant items for the user)</p> PARAMETER DESCRIPTION <code>alpha</code> <p>Damping parameter for PageRank, default=0.85.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>0.85</code> </p> <code>personalized</code> <p>Boolean value that specifies if the page rank must be calculated considering the user profile as personalization vector. Default is False</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>max_iter</code> <p>Maximum number of iterations in power method eigenvalue solver.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>100</code> </p> <code>tol</code> <p>Error tolerance used to check convergence in power method solver.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>1e-06</code> </p> <code>nstart</code> <p>Starting value of PageRank iteration for each node.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>None</code> </p> <code>weight</code> <p>Boolean value which tells the algorithm if weight of the edges must be considered or not. Default is True</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>relevance_threshold</code> <p>Threshold which separates relevant and non-relevant items for a user. Can be set globally, but if None the relevance threshold for each user is computed considering the mean rating given by the user in the train set</p> <p> TYPE: <code>float</code> DEFAULT: <code>None</code> </p> <code>rel_items_weight</code> <p>Probability that the random surfer will end up in a relevant item node when not following the out links of a node. This probability will be normalized and divided by the total number of relevant items for the user. If None, the <code>default_nodes_weight</code> probability will be assigned</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>0.8</code> </p> <code>rel_items_prop_weight</code> <p>Probability that the random surfer will end up in a property node linked to a relevant item when not following the out links of a node. This probability will be normalized and divided by the total number of property nodes linked to relevant items. If None, the <code>default_nodes_weight</code> probability will be assigned</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> <code>default_nodes_weight</code> <p>Probability that the random surfer will end up in a node which is not a relevant item or a property linked to a relevant item when not following the out links of a node. If <code>rel_items_weight</code> is None, then also relevant item nodes will be considered as default nodes. If <code>rel_items_prop_weight</code> is None, then also property nodes linked to relevant items will be considered as default nodes</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>0.2</code> </p> Source code in <code>clayrs/recsys/graph_based_algorithm/page_rank/nx_page_rank.py</code> <pre><code>def __init__(self, alpha: Any = 0.85,\n             personalized: bool = False,\n             max_iter: Any = 100,\n             tol: Any = 1.0e-6,\n             nstart: Any = None,\n             weight: bool = True,\n             relevance_threshold: float = None,\n             rel_items_weight: Optional[float] = 0.8,\n             rel_items_prop_weight: Optional[float] = None,\n             default_nodes_weight: Optional[float] = 0.2):\n\n    self.alpha = alpha\n    self.max_iter = max_iter\n    self.tol = tol\n    self.nstart = nstart\n    self.weight = weight\n\n    # basically when the user set a weight to be 0, the personalization vector\n    # should take that into consideration. But if the user set None, the weights will also be\n    # set automatically to 0, but in this case we should give them the 'default_nodes_weight'!\n    # That's why we have these other variables\n    self._strict_rel_items_weight = rel_items_weight == 0\n    self._strict_rel_items_prop_weight = rel_items_prop_weight == 0\n\n    super().__init__(personalized, relevance_threshold, rel_items_weight, rel_items_prop_weight,\n                     default_nodes_weight)\n</code></pre>"},{"location":"recsys/graph_based/graph_based_algorithms/nx_pagerank/#clayrs.recsys.graph_based_algorithm.page_rank.nx_page_rank.NXPageRank.rank","title":"<code>rank(graph, train_set, test_set, user_id_list, recs_number, methodology, num_cpus)</code>","text":"<p>Rank the top-n recommended items for the user. If the <code>recs_number</code> parameter is set to None, All unrated items for the user will be ranked among all those selected by the <code>methodology</code> parameter.</p> <p>The train set contains basically the interactions modelled in the graph, and it is needed by the methodology object</p> PARAMETER DESCRIPTION <code>graph</code> <p>A graph which models interactions of users and items</p> <p> TYPE: <code>NXBipartiteGraph</code> </p> <code>train_set</code> <p>a Ratings object containing interactions between users and items</p> <p> TYPE: <code>Ratings</code> </p> <code>recs_number</code> <p>number of the top ranked items to return, if None all ranked items will be returned</p> <p> TYPE: <code>Optional[int]</code> </p> <code>test_set</code> <p>Ratings object which represents the ground truth of the split considered</p> <p> TYPE: <code>Ratings</code> </p> <code>user_id_list</code> <p>List of users for which you want to compute ranking. The list should contain user id as strings and NOT user ids mapped to their integers</p> <p> TYPE: <code>Set[str]</code> </p> <code>recs_number</code> <p>number of the top ranked items to return, if None all ranked items will be returned</p> <p> TYPE: <code>Optional[int]</code> </p> <code>methodology</code> <p><code>Methodology</code> object which governs the candidate item selection. Default is <code>TestRatingsMethodology</code></p> <p> TYPE: <code>Methodology</code> </p> <code>num_cpus</code> <p>number of processors that must be reserved for the method. If set to <code>0</code>, all cpus available will be used. Be careful though: multiprocessing in python has a substantial memory overhead!</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>List[np.ndarray]</code> <p>List of uir matrices for each user, where each uir contains predicted interactions between users and unseen items sorted in a descending way w.r.t. the third dimension which is the ranked score</p> Source code in <code>clayrs/recsys/graph_based_algorithm/page_rank/nx_page_rank.py</code> <pre><code>def rank(self, graph: NXBipartiteGraph, train_set: Ratings, test_set: Ratings, user_id_list: Set[str],\n         recs_number: Optional[int], methodology: Methodology, num_cpus: int) -&gt; List[np.ndarray]:\n\"\"\"\n    Rank the top-n recommended items for the user. If the `recs_number` parameter is set to None,\n    All unrated items for the user will be ranked among all those selected by the `methodology` parameter.\n\n    The train set contains basically the interactions modelled in the graph, and it is needed by the methodology\n    object\n\n    Args:\n        graph: A graph which models interactions of users and items\n        train_set: a Ratings object containing interactions between users and items\n        recs_number: number of the top ranked items to return, if None all ranked items will be returned\n        test_set: Ratings object which represents the ground truth of the split considered\n        user_id_list: List of users for which you want to compute ranking. The list should contain user id as\n            strings and NOT user ids mapped to their integers\n        recs_number: number of the top ranked items to return, if None all ranked items will be returned\n        methodology: `Methodology` object which governs the candidate item selection. Default is\n            `TestRatingsMethodology`\n        num_cpus: number of processors that must be reserved for the method. If set to `0`, all cpus available will\n            be used. Be careful though: multiprocessing in python has a substantial memory overhead!\n\n    Returns:\n        List of uir matrices for each user, where each uir contains predicted interactions between users and unseen\n            items sorted in a descending way w.r.t. the third dimension which is the ranked score\n    \"\"\"\n\n    def compute_single_rank(user_tuple):\n\n        # nonlocal keyword allows to modify the score variable\n        nonlocal scores\n\n        user_id, user_idx = user_tuple\n        user_node = UserNode(user_id)\n\n        filter_list = set(ItemNode(item_to_rank)\n                          for item_to_rank in\n                          train_set.item_map.convert_seq_int2str(methodology.filter_single(user_idx,\n                                                                                           train_set,\n                                                                                           test_set)))\n\n        # run the pageRank\n        if self._personalized is True:\n\n            user_ratings = train_set.get_user_interactions(user_idx)\n            user_relevance_threshold = self._relevance_threshold or np.nanmean(user_ratings[:, 2])\n\n            pers_dict = {}\n\n            relevant_items = user_ratings[np.where(user_ratings[:, 2] &gt;= user_relevance_threshold)][:, 1]\n            relevant_items = [ItemNode(item_node)\n                              for item_node in train_set.item_map.convert_seq_int2str(relevant_items.astype(int))]\n\n            # If the prob is &gt; 0 then add relevant nodes to personalization vector. (rel_items_weight True)\n            # But also if the prob is 0 and the user explicitly set this prob to 0, add relevant nodes to\n            # personalization vector (strict_rel_items_weight is True)\n            # But if the prob is 0 and the user set None, we will skip this and add relevant nodes with the\n            # default_nodes_weight! (rel_items_weight not True and strict_rel_items_weight not True)\n            if self._rel_items_weight or self._strict_rel_items_weight:\n                pers_dict.update({item_node: self._rel_items_weight / len(relevant_items)\n                                  for item_node in relevant_items})\n\n            # If the prob is &gt; 0 then add relevant props to personalization vector. (rel_items_prop_weight True)\n            # But also if the prob is 0 and the user explicitly set this prob to 0, add relevant props to\n            # personalization vector (strict_rel_items_prop_weight is True)\n            # But if the prob is 0 and the user set None, we will skip this and add relevant props with the\n            # default_nodes_weight! (rel_items_prop_weight False and strict_rel_items_prop_weight False)\n            if self._rel_items_prop_weight or self._strict_rel_items_prop_weight:\n                relevant_props = set()\n                for item_node in relevant_items:\n                    relevant_item_properties = filter(lambda n: isinstance(n, PropertyNode),\n                                                      graph.get_successors(item_node))\n                    relevant_props.update(relevant_item_properties)\n\n                pers_dict.update({prop_node: self._rel_items_prop_weight / len(relevant_props)\n                                  for prop_node in relevant_props})\n\n            # all nodes that are not present up until now in the personalization vector, will be added\n            # with probability 'default_nodes_weight'\n            other_nodes = networkx_graph.nodes - pers_dict.keys()\n            pers_dict.update({node: self._default_nodes_weight / len(other_nodes) for node in other_nodes})\n\n            scores = nx.pagerank(networkx_graph, personalization=pers_dict, alpha=self.alpha,\n                                 max_iter=self.max_iter, tol=self.tol, nstart=self.nstart, weight=weight)\n\n        # if scores is None it means this is the first time we are running normal pagerank\n        # for all the other users the pagerank won't be computed again\n        elif scores is None:\n            scores = nx.pagerank(networkx_graph, alpha=self.alpha, max_iter=self.max_iter,\n                                 tol=self.tol, nstart=self.nstart, weight=weight)\n\n        # clean the results removing user nodes, selected user profile and eventually properties\n        user_scores = self.filter_result(graph, scores, filter_list, user_node)\n\n        if len(user_scores) == 0:\n            return user_id, np.array([])  # if no item to predict, empty rank is returned\n\n        user_scores_arr = np.array(list(user_scores.items()))\n\n        sorted_scores_idxs = np.argsort(user_scores_arr[:, 1])[::-1][:recs_number]\n        user_scores_arr = user_scores_arr[sorted_scores_idxs]\n\n        user_col = np.full((user_scores_arr.shape[0], 1), user_id)\n        uir_rank = np.append(user_col, user_scores_arr, axis=1)\n\n        return user_id, uir_rank\n\n    # scores will contain pagerank scores\n    scores = None\n    all_rank_uirs_list = []\n    weight = 'weight' if self.weight is True else None\n    networkx_graph = graph.to_networkx()\n    user_idxs_list = train_set.user_map.convert_seq_str2int(list(user_id_list))\n\n    with get_iterator_parallel(num_cpus,\n                               compute_single_rank, zip(user_id_list, user_idxs_list),\n                               progress_bar=True, total=len(user_id_list)) as pbar:\n\n        pbar.set_description(\"Prepping rank...\")\n\n        for user_id, user_rank in pbar:\n            all_rank_uirs_list.append(user_rank)\n            pbar.set_description(f\"Computing rank for user {user_id}\")\n\n    return all_rank_uirs_list\n</code></pre>"},{"location":"recsys/graph_based/graphs/nodes/","title":"Nodes categories","text":"<p>The followings are all the various category of nodes that can be added to a graph.</p> <p>Info</p> <p>Please note that there exists Bipartite Graph, Tripartite Graph and Full Graph, all with their peculiarities and restrictions.</p> <p>Check their documentation for more!</p>"},{"location":"recsys/graph_based/graphs/nodes/#clayrs.recsys.graphs.UserNode","title":"<code>UserNode(value)</code>","text":"<p>         Bases: <code>Node</code></p> <p>Class that represents 'user' nodes</p> PARAMETER DESCRIPTION <code>value</code> <p>the value to store in the node</p> <p> TYPE: <code>object</code> </p> Source code in <code>clayrs/recsys/graphs/graph.py</code> <pre><code>def __init__(self, value: str):\n    super().__init__(value)\n</code></pre>"},{"location":"recsys/graph_based/graphs/nodes/#clayrs.recsys.graphs.ItemNode","title":"<code>ItemNode(value)</code>","text":"<p>         Bases: <code>Node</code></p> <p>Class that represents 'item' nodes</p> PARAMETER DESCRIPTION <code>value</code> <p>the value to store in the node</p> <p> TYPE: <code>object</code> </p> Source code in <code>clayrs/recsys/graphs/graph.py</code> <pre><code>def __init__(self, value: str):\n    super().__init__(value)\n</code></pre>"},{"location":"recsys/graph_based/graphs/nodes/#clayrs.recsys.graphs.PropertyNode","title":"<code>PropertyNode(value)</code>","text":"<p>         Bases: <code>Node</code></p> <p>Class that represents 'property' nodes</p> PARAMETER DESCRIPTION <code>value</code> <p>the value to store in the node</p> <p> TYPE: <code>object</code> </p> Source code in <code>clayrs/recsys/graphs/graph.py</code> <pre><code>def __init__(self, value: str):\n    super().__init__(value)\n</code></pre>"},{"location":"recsys/graph_based/graphs/nx_bipartite/","title":"Bipartite Graph","text":""},{"location":"recsys/graph_based/graphs/nx_bipartite/#clayrs.recsys.graphs.nx_implementation.nx_bipartite_graphs.NXBipartiteGraph","title":"<code>NXBipartiteGraph(source_frame=None, link_label=None)</code>","text":"<p>         Bases: <code>BipartiteDiGraph</code></p> <p>Class that implements a Bipartite graph through networkx library.</p> <p>Info</p> <p>A Bipartite Graph is a graph which supports only User nodes and Item nodes. If you need to model also other node categories, consider using a Tripartite Graph or a Full Graph</p> <p>It creates a graph from an initial Rating object.</p> <p>Consider the following matrix representation of the Rating object <pre><code>    +------+-----------+-------+\n    | User |   Item    | Score |\n    +------+-----------+-------+\n    | u1   | Tenet     |     4 |\n    | u2   | Inception |     5 |\n    | ...  | ...       |   ... |\n    +------+-----------+-------+\n</code></pre></p> <p>The graph will be created with the following interactions:</p> <pre><code>             4\n        u1 -----&gt; Tenet\n             5\n        u2 -----&gt; Inception\n</code></pre> <p>where <code>u1</code> and <code>u2</code> become User nodes and <code>Tenet</code> and <code>Inception</code> become Item nodes, with the edge weighted depending on the score given</p> <p>If the <code>link_label</code> parameter is specified, then each link between users and items will be labeled with the label specified (e.g. <code>link_label='score'</code>):</p> <pre><code>        (4, 'score')\n    u1 -------------&gt; Tenet\n        (5, 'score')\n    u2 -------------&gt; Inception\n</code></pre> PARAMETER DESCRIPTION <code>source_frame</code> <p>the initial Ratings object needed to create the graph</p> <p> TYPE: <code>Ratings</code> DEFAULT: <code>None</code> </p> <code>link_label</code> <p>If specified, each link will be labeled with the given label. Default is None</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> Source code in <code>clayrs/recsys/graphs/nx_implementation/nx_bipartite_graphs.py</code> <pre><code>def __init__(self, source_frame: Ratings = None, link_label: str = None):\n\n    self._graph = nx.DiGraph()\n\n    if source_frame is not None:\n        not_none_dict = {}\n        if link_label is not None:\n            not_none_dict['label'] = link_label\n\n        user_column = source_frame.user_id_column\n        item_column = source_frame.item_id_column\n        score_column = source_frame.score_column\n        timestamp_column = source_frame.timestamp_column\n\n        if len(timestamp_column) != 0:\n            frame_iterator = zip(user_column, item_column, score_column, timestamp_column)\n        else:\n            frame_iterator = zip(user_column, item_column, score_column)\n\n        with get_progbar(frame_iterator, total=len(source_frame)) as progbar:\n            progbar.set_description(\"Creating User-&gt;Item links\")\n\n            if len(timestamp_column) != 0:\n                edges_with_attributes_gen = ((UserNode(interaction[0]), ItemNode(interaction[1]),\n\n                                              # {**x, **y} merges the dicts x and y\n                                              {**not_none_dict, **{'weight': interaction[2],\n                                                                   'timestamp': interaction[3]}}\n                                              )\n                                             for interaction in progbar)\n            else:\n                edges_with_attributes_gen = ((UserNode(interaction[0]), ItemNode(interaction[1]),\n\n                                              # {**x, **y} merges the dicts x and y\n                                              {**not_none_dict, **{'weight': interaction[2]}})\n                                             for interaction in progbar)\n\n            self._graph.add_edges_from(edges_with_attributes_gen)\n</code></pre>"},{"location":"recsys/graph_based/graphs/nx_bipartite/#clayrs.recsys.graphs.nx_implementation.nx_bipartite_graphs.NXBipartiteGraph.item_nodes","title":"<code>item_nodes: Set[ItemNode]</code>  <code>property</code>","text":"<p>Returns a set of all Item nodes in the graph</p>"},{"location":"recsys/graph_based/graphs/nx_bipartite/#clayrs.recsys.graphs.nx_implementation.nx_bipartite_graphs.NXBipartiteGraph.user_nodes","title":"<code>user_nodes: Set[UserNode]</code>  <code>property</code>","text":"<p>Returns a set of all User nodes in the graph</p>"},{"location":"recsys/graph_based/graphs/nx_bipartite/#clayrs.recsys.graphs.nx_implementation.nx_bipartite_graphs.NXBipartiteGraph.add_link","title":"<code>add_link(start_node, final_node, weight=None, label=None, timestamp=None)</code>","text":"<p>Creates a link connecting the <code>start_node</code> to the <code>final_node</code>. If two lists are passed, then the node in position \\(i\\) in the <code>start_node</code> list will be linked to the node in position \\(i\\) in the <code>final_node</code> list.</p> <p>If nodes to link do not exist, they will be added automatically to the graph. Please remember that since this is a Bipartite Graph, only User nodes and Item nodes can be added!</p> <p>A link can be weighted with the <code>weight</code> parameter and labeled with the <code>label</code> parameter. A timestamp can also be specified via <code>timestamp</code> parameter. All three are optional parameters, so they are not required</p> PARAMETER DESCRIPTION <code>start_node</code> <p>Single Node object or a list of Node objects. They will be the 'head' of the link, since it's a directed graph</p> <p> TYPE: <code>Union[Node, List[Node]]</code> </p> <code>final_node</code> <p>Single Node object or a list Node objects. They will be the 'tail' of the link, since it's a directed graph</p> <p> TYPE: <code>object</code> </p> <code>weight</code> <p>weight of the link, default is None (no weight)</p> <p> TYPE: <code>float</code> DEFAULT: <code>None</code> </p> <code>label</code> <p>label of the link, default is None (no label)</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>timestamp</code> <p>timestamp of the link, default is None (no timestamp)</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> Source code in <code>clayrs/recsys/graphs/nx_implementation/nx_bipartite_graphs.py</code> <pre><code>def add_link(self, start_node: Union[Node, List[Node]], final_node: Union[Node, List[Node]],\n             weight: float = None, label: str = None, timestamp: str = None):\n\"\"\"\n    Creates a link connecting the `start_node` to the `final_node`. If two lists are passed, then the node in\n    position $i$ in the `start_node` list will be linked to the node in position $i$ in the `final_node` list.\n\n    If nodes to link do not exist, they will be added automatically to the graph. Please remember that since this is\n    a Bipartite Graph, only *User nodes* and *Item nodes* can be added!\n\n    A link can be weighted with the `weight` parameter and labeled with the `label` parameter.\n    A timestamp can also be specified via `timestamp` parameter.\n    All three are optional parameters, so they are not required\n\n    Args:\n        start_node: Single Node object or a list of Node objects. They will be the 'head' of the link, since it's a\n            directed graph\n        final_node (object): Single Node object or a list Node objects. They will be the 'tail' of the link,\n            since it's a directed graph\n        weight: weight of the link, default is None (no weight)\n        label: label of the link, default is None (no label)\n        timestamp: timestamp of the link, default is None (no timestamp)\n    \"\"\"\n    if not isinstance(start_node, list):\n        start_node = [start_node]\n\n    if not isinstance(final_node, list):\n        final_node = [final_node]\n\n    self.add_node(start_node)\n    self.add_node(final_node)\n\n    not_none_dict = {}\n    if label is not None:\n        not_none_dict['label'] = label\n    if weight is not None:\n        not_none_dict['weight'] = weight\n    if timestamp is not None:\n        not_none_dict['timestamp'] = timestamp\n\n    self._graph.add_edges_from(zip(start_node, final_node),\n                               **not_none_dict)\n</code></pre>"},{"location":"recsys/graph_based/graphs/nx_bipartite/#clayrs.recsys.graphs.nx_implementation.nx_bipartite_graphs.NXBipartiteGraph.add_node","title":"<code>add_node(node)</code>","text":"<p>Adds one or multiple Node objects to the graph. Since this is a Bipartite Graph, only <code>User Node</code> and <code>Item Node</code> can be added!</p> <p>No duplicates are allowed, but different category nodes with same id are (e.g. <code>ItemNode('1')</code> and <code>UserNode('1')</code>)</p> PARAMETER DESCRIPTION <code>node</code> <p>Node(s) object(s) that needs to be added to the graph</p> <p> TYPE: <code>Union[Node, List[Node]]</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>Exception raised when one of the node to add to the graph is not a User or Item node</p> Source code in <code>clayrs/recsys/graphs/nx_implementation/nx_bipartite_graphs.py</code> <pre><code>def add_node(self, node: Union[Node, List[Node]]):\n\"\"\"\n    Adds one or multiple Node objects to the graph.\n    Since this is a Bipartite Graph, only `User Node` and `Item Node` can be added!\n\n    No duplicates are allowed, but different category nodes with same id are (e.g. `ItemNode('1')` and\n    `UserNode('1')`)\n\n    Args:\n        node: Node(s) object(s) that needs to be added to the graph\n\n    Raises:\n        ValueError: Exception raised when one of the node to add to the graph is not a User or Item node\n    \"\"\"\n    if not isinstance(node, list):\n        node = [node]\n\n    if any(not isinstance(n, (UserNode, ItemNode)) for n in node):\n        raise ValueError(\"You can only add UserNodes or ItemNodes to a bipartite graph!\")\n\n    self._graph.add_nodes_from(node)\n</code></pre>"},{"location":"recsys/graph_based/graphs/nx_bipartite/#clayrs.recsys.graphs.nx_implementation.nx_bipartite_graphs.NXBipartiteGraph.closeness_centrality","title":"<code>closeness_centrality()</code>","text":"<p>Calculate the closeness centrality for every node in the graph</p> RETURNS DESCRIPTION <code>Dict</code> <p>Dictionary containing the closeness centrality for each node in the graph</p> Source code in <code>clayrs/recsys/graphs/nx_implementation/nx_bipartite_graphs.py</code> <pre><code>def closeness_centrality(self) -&gt; Dict:\n\"\"\"\n    Calculate the closeness centrality for every node in the graph\n\n    Returns:\n        Dictionary containing the closeness centrality for each node in the graph\n    \"\"\"\n    return nx.closeness_centrality(self._graph)\n</code></pre>"},{"location":"recsys/graph_based/graphs/nx_bipartite/#clayrs.recsys.graphs.nx_implementation.nx_bipartite_graphs.NXBipartiteGraph.degree_centrality","title":"<code>degree_centrality()</code>","text":"<p>Calculate the degree centrality for every node in the graph</p> RETURNS DESCRIPTION <code>Dict</code> <p>Dictionary containing the degree centrality for each node in the graph</p> Source code in <code>clayrs/recsys/graphs/nx_implementation/nx_bipartite_graphs.py</code> <pre><code>def degree_centrality(self) -&gt; Dict:\n\"\"\"\n    Calculate the degree centrality for every node in the graph\n\n    Returns:\n        Dictionary containing the degree centrality for each node in the graph\n    \"\"\"\n    return nx.degree_centrality(self._graph)\n</code></pre>"},{"location":"recsys/graph_based/graphs/nx_bipartite/#clayrs.recsys.graphs.nx_implementation.nx_bipartite_graphs.NXBipartiteGraph.dispersion","title":"<code>dispersion()</code>","text":"<p>Calculate the dispersion for every node in the graph</p> RETURNS DESCRIPTION <code>Dict</code> <p>Dictionary containing the dispersion computed for each node in the graph</p> Source code in <code>clayrs/recsys/graphs/nx_implementation/nx_bipartite_graphs.py</code> <pre><code>def dispersion(self) -&gt; Dict:\n\"\"\"\n    Calculate the dispersion for every node in the graph\n\n    Returns:\n        Dictionary containing the dispersion computed for each node in the graph\n    \"\"\"\n    return nx.dispersion(self._graph)\n</code></pre>"},{"location":"recsys/graph_based/graphs/nx_bipartite/#clayrs.recsys.graphs.nx_implementation.nx_bipartite_graphs.NXBipartiteGraph.get_link_data","title":"<code>get_link_data(start_node, final_node)</code>","text":"<p>Get link data such as weight, label, timestamp. between the <code>start_node</code> and the <code>final_node</code>. Returns None if said link doesn't exists</p> <p>Remember that this is a directed graph so the result differs if 'start_node' and 'final_node' are switched.</p> PARAMETER DESCRIPTION <code>start_node</code> <p>Node object from where the link starts</p> <p> TYPE: <code>Node</code> </p> <code>final_node</code> <p>Node object to where the link ends</p> <p> TYPE: <code>Node</code> </p> Source code in <code>clayrs/recsys/graphs/nx_implementation/nx_bipartite_graphs.py</code> <pre><code>def get_link_data(self, start_node: Node, final_node: Node):\n\"\"\"\n    Get link data such as weight, label, timestamp. between the `start_node` and the `final_node`.\n    Returns None if said link doesn't exists\n\n    Remember that this is a directed graph so the result differs if 'start_node' and 'final_node'\n    are switched.\n\n    Args:\n        start_node: Node object from where the link starts\n        final_node: Node object to where the link ends\n    \"\"\"\n    return self._graph.get_edge_data(start_node, final_node)\n</code></pre>"},{"location":"recsys/graph_based/graphs/nx_bipartite/#clayrs.recsys.graphs.nx_implementation.nx_bipartite_graphs.NXBipartiteGraph.get_predecessors","title":"<code>get_predecessors(node)</code>","text":"<p>Returns a list containing the predecessors of the node passed. Raises TypeError exception if the node doesn't exists in the graph.</p> <p>Taken from networkx library:</p> <p>A predecessor of n is a node m such that there exists a directed edge from m to n</p> <p>For example: <pre><code># GRAPH:\n\nI1 &lt;-- U1\n\u2191\nU2\n</code></pre></p> <pre><code>&gt;&gt;&gt; graph.get_predecessors(ItemNode('I1'))\n[User U1, User U2]\n</code></pre> PARAMETER DESCRIPTION <code>node</code> <p>Node for which we want to know the predecessors</p> <p> TYPE: <code>Node</code> </p> RAISES DESCRIPTION <code>TypeError</code> <p>Exception raised when the node it's not in the graph</p> Source code in <code>clayrs/recsys/graphs/nx_implementation/nx_bipartite_graphs.py</code> <pre><code>def get_predecessors(self, node: Node) -&gt; List[Node]:\n\"\"\"\n    Returns a list containing the *predecessors* of the node passed.\n    Raises TypeError exception if the node doesn't exists in the graph.\n\n    Taken from networkx library:\n\n    &gt; A predecessor of n is a node m such that there exists a directed\n    edge from m to n\n\n    For example:\n    ```\n    # GRAPH:\n\n    I1 &lt;-- U1\n    \u2191\n    U2\n    ```\n\n    ```python\n    &gt;&gt;&gt; graph.get_predecessors(ItemNode('I1'))\n    [User U1, User U2]\n    ```\n\n    Args:\n        node: Node for which we want to know the predecessors\n\n    Raises:\n        TypeError: Exception raised when the node it's not in the graph\n    \"\"\"\n    try:\n        return list(self._graph.predecessors(node))\n    except nx.NetworkXError:\n        raise TypeError(\"The node specified is not in the graph!\")\n</code></pre>"},{"location":"recsys/graph_based/graphs/nx_bipartite/#clayrs.recsys.graphs.nx_implementation.nx_bipartite_graphs.NXBipartiteGraph.get_successors","title":"<code>get_successors(node)</code>","text":"<p>Returns a list containing the successors of the node passed. Returns None if the node doesn't exists in the graph.</p> <p>Taken from networkx library:</p> <p>A successor of n is a node m such that there exists a directed edge from n to m</p> <p>For example: <pre><code>U1 --&gt; I2\n\u2193\nI1\n</code></pre></p> <pre><code>&gt;&gt;&gt; graph.get_successors(UserNode('U1'))\n[Item I1, Item I2]\n</code></pre> PARAMETER DESCRIPTION <code>node</code> <p>Node for which we want to know the successors</p> <p> TYPE: <code>Node</code> </p> RAISES DESCRIPTION <code>TypeError</code> <p>Exception raised when the node it's not in the graph</p> Source code in <code>clayrs/recsys/graphs/nx_implementation/nx_bipartite_graphs.py</code> <pre><code>def get_successors(self, node: Node) -&gt; List[Node]:\n\"\"\"\n    Returns a list containing the successors of the node passed.\n    Returns None if the node doesn't exists in the graph.\n\n    Taken from networkx library:\n    &gt; A successor of n is a node m such that there exists a directed\n    edge from n to m\n\n    For example:\n    ```\n    U1 --&gt; I2\n    \u2193\n    I1\n    ```\n\n    ```python\n\n    &gt;&gt;&gt; graph.get_successors(UserNode('U1'))\n    [Item I1, Item I2]\n    ```\n\n    Args:\n        node: Node for which we want to know the successors\n\n    Raises:\n        TypeError: Exception raised when the node it's not in the graph\n    \"\"\"\n    try:\n        return list(self._graph.successors(node))\n    except nx.NetworkXError:\n        raise TypeError(\"The node specified is not in the graph!\")\n</code></pre>"},{"location":"recsys/graph_based/graphs/nx_bipartite/#clayrs.recsys.graphs.nx_implementation.nx_bipartite_graphs.NXBipartiteGraph.node_exists","title":"<code>node_exists(node)</code>","text":"<p>Returns True if the node passed exists in the graph, False otherwise</p> PARAMETER DESCRIPTION <code>node</code> <p>Node to check whether it's present in the graph or not</p> <p> TYPE: <code>Node</code> </p> Source code in <code>clayrs/recsys/graphs/nx_implementation/nx_bipartite_graphs.py</code> <pre><code>def node_exists(self, node: Node) -&gt; bool:\n\"\"\"\n    Returns True if the node passed exists in the graph, False otherwise\n\n    Args:\n        node: Node to check whether it's present in the graph or not\n    \"\"\"\n    r = self._graph.nodes.get(node)\n    return r is not None\n</code></pre>"},{"location":"recsys/graph_based/graphs/nx_bipartite/#clayrs.recsys.graphs.nx_implementation.nx_bipartite_graphs.NXBipartiteGraph.remove_link","title":"<code>remove_link(start_node, final_node)</code>","text":"<p>Removes the link connecting the <code>start_node</code> to the <code>final_node</code>. If there's no link between the two nodes, then a warning is printed</p> PARAMETER DESCRIPTION <code>start_node</code> <p>head node of the link to remove</p> <p> TYPE: <code>Node</code> </p> <code>final_node</code> <p>tail node of the link to remove</p> <p> TYPE: <code>Node</code> </p> Source code in <code>clayrs/recsys/graphs/nx_implementation/nx_bipartite_graphs.py</code> <pre><code>def remove_link(self, start_node: Node, final_node: Node):\n\"\"\"\n    Removes the link connecting the `start_node` to the `final_node`.\n    If there's no link between the two nodes, then a warning is printed\n\n    Args:\n        start_node: *head* node of the link to remove\n        final_node: *tail* node of the link to remove\n    \"\"\"\n    try:\n        self._graph.remove_edge(start_node, final_node)\n    except nx.NetworkXError:\n        logger.warning(\"No link exists between the start node and the final node!\\n\"\n                       \"No link will be removed\")\n</code></pre>"},{"location":"recsys/graph_based/graphs/nx_bipartite/#clayrs.recsys.graphs.nx_implementation.nx_bipartite_graphs.NXBipartiteGraph.remove_node","title":"<code>remove_node(node_to_remove)</code>","text":"<p>Removes one or multiple nodes from the graph. If one of the nodes to remove is not present in the graph, it is silently ignored</p> PARAMETER DESCRIPTION <code>node_to_remove</code> <p>Single Node object or a list of Node objects to remove from the graph</p> <p> TYPE: <code>Union[Node, List[Node]]</code> </p> Source code in <code>clayrs/recsys/graphs/nx_implementation/nx_bipartite_graphs.py</code> <pre><code>def remove_node(self, node_to_remove: Union[Node, List[Node]]):\n\"\"\"\n    Removes one or multiple nodes from the graph.\n    If one of the nodes to remove is not present in the graph, it is silently ignored\n\n    Args:\n        node_to_remove: Single Node object or a list of Node objects to remove from the graph\n    \"\"\"\n    if not isinstance(node_to_remove, list):\n        node_to_remove = [node_to_remove]\n\n    self._graph.remove_nodes_from(node_to_remove)\n</code></pre>"},{"location":"recsys/graph_based/graphs/nx_bipartite/#clayrs.recsys.graphs.nx_implementation.nx_bipartite_graphs.NXBipartiteGraph.to_networkx","title":"<code>to_networkx()</code>","text":"<p>Returns underlying networkx implementation of the graph</p> Source code in <code>clayrs/recsys/graphs/nx_implementation/nx_bipartite_graphs.py</code> <pre><code>def to_networkx(self) -&gt; nx.DiGraph:\n\"\"\"\n    Returns underlying networkx implementation of the graph\n    \"\"\"\n    return self._graph\n</code></pre>"},{"location":"recsys/graph_based/graphs/nx_full/","title":"Full Graph","text":"<p>Please remember that this class is a subclass of NXTripartiteGraph, so it inherits all its methods. You can check their documentation as well!</p>"},{"location":"recsys/graph_based/graphs/nx_full/#clayrs.recsys.graphs.nx_implementation.nx_full_graphs.NXFullGraph","title":"<code>NXFullGraph(source_frame=None, item_exo_properties=None, item_contents_dir=None, user_exo_properties=None, user_contents_dir=None, link_label=None)</code>","text":"<p>         Bases: <code>NXTripartiteGraph</code>, <code>FullDiGraph</code></p> <p>Class that implements a Full graph through networkx library.</p> <p>Info</p> <p>A Full Graph is a graph which doesn't impose any particular restriction</p> <p>It creates a graph from an initial Rating object.</p> <p>Consider the following matrix representation of the Rating object <pre><code>    +------+-----------+-------+\n    | User |   Item    | Score |\n    +------+-----------+-------+\n    | u1   | Tenet     |     4 |\n    | u2   | Inception |     5 |\n    | ...  | ...       |   ... |\n    +------+-----------+-------+\n</code></pre></p> <p>The graph will be created with the following interactions:</p> <pre><code>             4\n        u1 -----&gt; Tenet\n             5\n        u2 -----&gt; Inception\n</code></pre> <p>where <code>u1</code> and <code>u2</code> become User nodes and <code>Tenet</code> and <code>Inception</code> become Item nodes, with the edge weighted depending on the score given</p> <p>If the <code>link_label</code> parameter is specified, then each link between users and items will be labeled with the label specified (e.g. <code>link_label='score'</code>):</p> <pre><code>        (4, 'score')\n    u1 -------------&gt; Tenet\n        (5, 'score')\n    u2 -------------&gt; Inception\n</code></pre> <p>Then the framework tries to load 'Tenet' and 'Inception' from the <code>item_contents_dir</code> and 'u1' and 'u2' from <code>user_contents_dir</code> if they are specified and if it succeeds, adds in the graph their loaded properties as specified in the <code>item_exo_properties</code> parameter and <code>user_exo_properties</code>.</p> <p>Load exogenous properties</p> <p>In order to load properties in the graph, we must specify where users (and/or) items are serialized and which properties to add (the following is the same for item_exo_properties):</p> <ul> <li>If user_exo_properties is specified as a set, then the graph will try to load all properties from said exogenous representation</li> </ul> <pre><code>{'my_exo_id'}\n</code></pre> <ul> <li>If user_exo_properties is specified as a dict, then the graph will try to load said properties from said exogenous representation</li> </ul> <pre><code>{'my_exo_id': ['my_prop1', 'my_prop2']]}\n</code></pre> PARAMETER DESCRIPTION <code>source_frame</code> <p>The initial Ratings object needed to create the graph</p> <p> TYPE: <code>Ratings</code> DEFAULT: <code>None</code> </p> <code>item_exo_properties</code> <p>Set or Dict which contains representations to load from items. Use a <code>Set</code> if you want to load all properties from specific representations, or use a <code>Dict</code> if you want to choose which properties to load from specific representations</p> <p> TYPE: <code>Union[Dict, set]</code> DEFAULT: <code>None</code> </p> <code>item_contents_dir</code> <p>The path containing items serialized with the Content Analyzer</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>user_exo_properties</code> <p>Set or Dict which contains representations to load from items. Use a <code>Set</code> if you want to load all properties from specific representations, or use a <code>Dict</code> if you want to choose which properties to load from specific representations</p> <p> TYPE: <code>Union[Dict, set]</code> DEFAULT: <code>None</code> </p> <code>user_contents_dir</code> <p>The path containing users serialized with the Content Analyzer</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>link_label</code> <p>If specified, each link will be labeled with the given label. Default is None</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> Source code in <code>clayrs/recsys/graphs/nx_implementation/nx_full_graphs.py</code> <pre><code>def __init__(self, source_frame: Ratings = None,\n             item_exo_properties: Union[Dict, set] = None,\n             item_contents_dir: str = None,\n             user_exo_properties: Union[Dict, set] = None,\n             user_contents_dir: str = None,\n             link_label: str = None):\n\n    NXTripartiteGraph.__init__(self, source_frame, item_exo_properties, item_contents_dir, link_label)\n\n    if user_exo_properties and not user_contents_dir:\n        logger.warning(\"`user_exo_properties` parameter set but `user_contents_dir` is None! \"\n                       \"No property will be loaded\")\n    elif not user_exo_properties and user_contents_dir:\n        logger.warning(\"`user_contents_dir` parameter set but `user_exo_properties` is None! \"\n                       \"No property will be loaded\")\n\n    if source_frame is not None and user_contents_dir is not None and user_exo_properties is not None:\n        self.add_node_with_prop([UserNode(user_id) for user_id in source_frame.unique_user_id_column],\n                                user_exo_properties,\n                                user_contents_dir)\n</code></pre>"},{"location":"recsys/graph_based/graphs/nx_full/#clayrs.recsys.graphs.nx_implementation.nx_full_graphs.NXFullGraph.add_link","title":"<code>add_link(start_node, final_node, weight=None, label=None, timestamp=None)</code>","text":"<p>Creates a weighted link connecting the 'start_node' to the 'final_node' Both nodes must be present in the graph before calling this method</p> <p>'weight' and 'label' are optional parameters, if not specified default values will be used.</p> PARAMETER DESCRIPTION <code>start_node</code> <p>starting node of the link</p> <p> TYPE: <code>object</code> </p> <code>final_node</code> <p>ending node of the link</p> <p> TYPE: <code>object</code> </p> <code>weight</code> <p>weight of the link, default is 0.5</p> <p> TYPE: <code>float</code> DEFAULT: <code>None</code> </p> <code>label</code> <p>label of the link, default is 'score_label'</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> Source code in <code>clayrs/recsys/graphs/nx_implementation/nx_full_graphs.py</code> <pre><code>def add_link(self, start_node: Union[Node, List[Node]], final_node: Union[Node, List[Node]],\n             weight: float = None, label: str = None, timestamp: str = None):\n\"\"\"\n    Creates a weighted link connecting the 'start_node' to the 'final_node'\n    Both nodes must be present in the graph before calling this method\n\n    'weight' and 'label' are optional parameters, if not specified default values\n    will be used.\n\n    Args:\n        start_node (object): starting node of the link\n        final_node (object): ending node of the link\n        weight (float): weight of the link, default is 0.5\n        label (str): label of the link, default is 'score_label'\n    \"\"\"\n    if not isinstance(start_node, list):\n        start_node = [start_node]\n\n    if not isinstance(final_node, list):\n        final_node = [final_node]\n\n    self.add_node(start_node)\n    self.add_node(final_node)\n\n    not_none_dict = {}\n    if label is not None:\n        not_none_dict['label'] = label\n    if weight is not None:\n        not_none_dict['weight'] = weight\n    if timestamp is not None:\n        not_none_dict['timestamp'] = timestamp\n\n    self._graph.add_edges_from(zip(start_node, final_node),\n                               **not_none_dict)\n</code></pre>"},{"location":"recsys/graph_based/graphs/nx_full/#clayrs.recsys.graphs.nx_implementation.nx_full_graphs.NXFullGraph.add_node","title":"<code>add_node(node)</code>","text":"<p>Adds one or multiple Node objects to the graph. Since this is a Full Graph, any category of node is allowed</p> <p>No duplicates are allowed, but different category nodes with same id are (e.g. <code>ItemNode('1')</code> and <code>UserNode('1')</code>)</p> PARAMETER DESCRIPTION <code>node</code> <p>Node(s) object(s) that needs to be added to the graph</p> <p> TYPE: <code>Union[Node, List[Node]]</code> </p> Source code in <code>clayrs/recsys/graphs/nx_implementation/nx_full_graphs.py</code> <pre><code>def add_node(self, node: Union[Node, List[Node]]):\n\"\"\"\n    Adds one or multiple Node objects to the graph.\n    Since this is a Full Graph, any category of node is allowed\n\n    No duplicates are allowed, but different category nodes with same id are (e.g. `ItemNode('1')` and\n    `UserNode('1')`)\n\n    Args:\n        node: Node(s) object(s) that needs to be added to the graph\n    \"\"\"\n    if not isinstance(node, list):\n        node = [node]\n\n    self._graph.add_nodes_from(node)\n</code></pre>"},{"location":"recsys/graph_based/graphs/nx_full/#clayrs.recsys.graphs.nx_implementation.nx_full_graphs.NXFullGraph.add_node_with_prop","title":"<code>add_node_with_prop(node, exo_properties, contents_dir, content_filename=None)</code>","text":"<p>Adds one or multiple Node objects and its/their properties to the graph Since this is a Full Graph, no restriction are imposed and you can add any category of node together with its properties.</p> <p>In order to load properties in the graph, we must specify where contents are serialized and which properties to add (the following is the same for item_exo_properties):</p> <ul> <li>If exo_properties is specified as a set, then the graph will try to load all properties from said exogenous representation</li> </ul> <pre><code>{'my_exo_id'}\n</code></pre> <ul> <li>If exo_properties is specified as a dict, then the graph will try to load said properties from said exogenous representation</li> </ul> <pre><code>{'my_exo_id': ['my_prop1', 'my_prop2']]}\n</code></pre> <p>In case you want your node to have a different id from serialized contents, via the <code>content_filename</code> parameter you can specify what is the filename of the node that you are adding, e.g.</p> <pre><code>item_to_add = ItemNode('different_id')\n\n# content_filename is 'item_serialized_1.xz'\n\ngraph.add_node_with_prop(item_to_add, ..., content_filename='item_serialized_1')\n</code></pre> <p>In case you are adding a list of nodes, you can specify the filename for each node in the list.</p> PARAMETER DESCRIPTION <code>node</code> <p>Node(s) object(s) that needs to be added to the graph along with their properties</p> <p> TYPE: <code>Union[Node, List[Node]]</code> </p> <code>exo_properties</code> <p>Set or Dict which contains representations to load from items. Use a <code>Set</code> if you want to load all properties from specific representations, or use a <code>Dict</code> if you want to choose which properties to load from specific representations</p> <p> TYPE: <code>Union[Dict, set]</code> </p> <code>contents_dir</code> <p>The path containing items serialized with the Content Analyzer</p> <p> TYPE: <code>str</code> </p> <code>content_filename</code> <p>Filename(s) of the node(s) to add</p> <p> TYPE: <code>Union[str, List[str]]</code> DEFAULT: <code>None</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>Exception raised when one of the node to add to the graph with their properties is not an ItemNode</p> Source code in <code>clayrs/recsys/graphs/nx_implementation/nx_full_graphs.py</code> <pre><code>def add_node_with_prop(self, node: Union[Node, List[Node]], exo_properties: Union[Dict, set],\n                       contents_dir: str,\n                       content_filename: Union[str, List[str]] = None):\n\"\"\"\n    Adds one or multiple Node objects and its/their properties to the graph\n    Since this is a Full Graph, no restriction are imposed and you can add any category of node together with its\n    properties.\n\n    In order to load properties in the graph, we must specify where contents are serialized and ***which\n    properties to add*** (the following is the same for *item_exo_properties*):\n\n    *   If *exo_properties* is specified as a **set**, then the graph will try to load **all properties**\n    from **said exogenous representation**\n\n    ```python\n    {'my_exo_id'}\n    ```\n\n    *   If *exo_properties* is specified as a **dict**, then the graph will try to load **said properties**\n    from **said exogenous representation**\n\n    ```python\n    {'my_exo_id': ['my_prop1', 'my_prop2']]}\n    ```\n\n    In case you want your node to have a different id from serialized contents, via the `content_filename` parameter\n    you can specify what is the filename of the node that you are adding, e.g.\n\n    ```\n    item_to_add = ItemNode('different_id')\n\n    # content_filename is 'item_serialized_1.xz'\n\n    graph.add_node_with_prop(item_to_add, ..., content_filename='item_serialized_1')\n    ```\n\n    In case you are adding a list of nodes, you can specify the filename for each node in the list.\n\n    Args:\n        node: Node(s) object(s) that needs to be added to the graph along with their properties\n        exo_properties: Set or Dict which contains representations to load from items. Use a `Set` if you want\n            to load all properties from specific representations, or use a `Dict` if you want to choose which\n            properties to load from specific representations\n        contents_dir: The path containing items serialized with the Content Analyzer\n        content_filename: Filename(s) of the node(s) to add\n\n    Raises:\n        ValueError: Exception raised when one of the node to add to the graph with their properties is not\n            an ItemNode\n    \"\"\"\n    def node_prop_link_generator():\n        for n, id in zip(progbar, content_filename):\n            item: Content = loaded_items.get(id)\n\n            if item is not None:\n                exo_props = self._get_exo_props(exo_properties, item)\n\n                single_item_prop_edges = [(n,\n                                           PropertyNode(prop_dict[prop]),\n                                           {'label': prop})\n                                          for prop_dict in exo_props for prop in prop_dict]\n            else:\n                single_item_prop_edges = []\n\n            yield from single_item_prop_edges\n\n    if not isinstance(node, list):\n        node = [node]\n\n    if isinstance(exo_properties, set):\n        exo_properties = dict.fromkeys(exo_properties, None)\n\n    if content_filename is None:\n        content_filename = [n.value for n in node]\n\n    if not isinstance(content_filename, list):\n        content_filename = [content_filename]\n\n    loaded_items = LoadedContentsDict(contents_dir, contents_to_load=set(content_filename))\n    with get_progbar(node) as progbar:\n        progbar.set_description(\"Creating Node-&gt;Properties links\")\n\n        self._graph.add_edges_from((tuple_to_add for tuple_to_add in node_prop_link_generator()))\n</code></pre>"},{"location":"recsys/graph_based/graphs/nx_tripartite/","title":"Tripartite Graph","text":"<p>Please remember that this class is a subclass of NXBipartiteGraph, so it inherits all its methods. You can check their documentation as well!</p>"},{"location":"recsys/graph_based/graphs/nx_tripartite/#clayrs.recsys.graphs.nx_implementation.nx_tripartite_graphs.NXTripartiteGraph","title":"<code>NXTripartiteGraph(source_frame=None, item_exo_properties=None, item_contents_dir=None, link_label=None)</code>","text":"<p>         Bases: <code>NXBipartiteGraph</code>, <code>TripartiteDiGraph</code></p> <p>Class that implements a Tripartite graph through networkx library.</p> <p>Info</p> <p>A Tripartite Graph is a graph which supports User nodes, Item nodes and Property nodes, but the latter can only be linked to Item nodes. If you need maximum flexibility, consider using a Full Graph</p> <p>It creates a graph from an initial Rating object.</p> <p>Consider the following matrix representation of the Rating object <pre><code>    +------+-----------+-------+\n    | User |   Item    | Score |\n    +------+-----------+-------+\n    | u1   | Tenet     |     4 |\n    | u2   | Inception |     5 |\n    | ...  | ...       |   ... |\n    +------+-----------+-------+\n</code></pre></p> <p>The graph will be created with the following interactions:</p> <pre><code>             4\n        u1 -----&gt; Tenet\n             5\n        u2 -----&gt; Inception\n</code></pre> <p>where <code>u1</code> and <code>u2</code> become User nodes and <code>Tenet</code> and <code>Inception</code> become Item nodes, with the edge weighted depending on the score given</p> <p>If the <code>link_label</code> parameter is specified, then each link between users and items will be labeled with the label specified (e.g. <code>link_label='score'</code>):</p> <pre><code>        (4, 'score')\n    u1 -------------&gt; Tenet\n        (5, 'score')\n    u2 -------------&gt; Inception\n</code></pre> <p>Then the framework tries to load 'Tenet' and 'Inception' from the <code>item_contents_dir</code> if it is specified and if succeeds, adds in the graph their loaded properties as specified in the <code>item_exo_properties</code> parameter.</p> <p>Load exogenous properties</p> <p>In order to load properties in the graph, we must specify where items are serialized and which properties to add:</p> <ul> <li>If item_exo_properties is specified as a set, then the graph will try to load all properties from said exogenous representation</li> </ul> <pre><code>{'my_exo_id'}\n</code></pre> <ul> <li>If item_exo_properties is specified as a dict, then the graph will try to load said properties from said exogenous representation</li> </ul> <pre><code>{'my_exo_id': ['my_prop1', 'my_prop2']]}\n</code></pre> PARAMETER DESCRIPTION <code>source_frame</code> <p>The initial Ratings object needed to create the graph</p> <p> TYPE: <code>Ratings</code> DEFAULT: <code>None</code> </p> <code>item_exo_properties</code> <p>Set or Dict which contains representations to load from items. Use a <code>Set</code> if you want to load all properties from specific representations, or use a <code>Dict</code> if you want to choose which properties to load from specific representations</p> <p> TYPE: <code>Union[Dict, set]</code> DEFAULT: <code>None</code> </p> <code>item_contents_dir</code> <p>The path containing items serialized with the Content Analyzer</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>link_label</code> <p>If specified, each link will be labeled with the given label. Default is None</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> Source code in <code>clayrs/recsys/graphs/nx_implementation/nx_tripartite_graphs.py</code> <pre><code>def __init__(self, source_frame: Ratings = None,\n             item_exo_properties: Union[Dict, set] = None,\n             item_contents_dir: str = None,\n             link_label: str = None):\n\n    NXBipartiteGraph.__init__(self, source_frame, link_label)\n\n    if item_exo_properties and not item_contents_dir:\n        logger.warning(\"`item_exo_properties` parameter set but `item_contents_dir` is None! \"\n                       \"No property will be loaded\")\n    elif not item_exo_properties and item_contents_dir:\n        logger.warning(\"`item_contents_dir` parameter set but `item_exo_properties` is None! \"\n                       \"No property will be loaded\")\n\n    if source_frame is not None and item_contents_dir is not None and item_exo_properties is not None:\n        self.add_node_with_prop([ItemNode(item_id) for item_id in source_frame.unique_item_id_column],\n                                item_exo_properties,\n                                item_contents_dir)\n</code></pre>"},{"location":"recsys/graph_based/graphs/nx_tripartite/#clayrs.recsys.graphs.nx_implementation.nx_tripartite_graphs.NXTripartiteGraph.property_nodes","title":"<code>property_nodes: Set[PropertyNode]</code>  <code>property</code>","text":"<p>Returns a set of all Property nodes in the graph</p>"},{"location":"recsys/graph_based/graphs/nx_tripartite/#clayrs.recsys.graphs.nx_implementation.nx_tripartite_graphs.NXTripartiteGraph.add_link","title":"<code>add_link(start_node, final_node, weight=None, label=None, timestamp=None)</code>","text":"<p>Creates a link connecting the <code>start_node</code> to the <code>final_node</code>. If two lists are passed, then the node in position \\(i\\) in the <code>start_node</code> list will be linked to the node in position \\(i\\) in the <code>final_node</code> list.</p> <p>If nodes to link do not exist, they will be added automatically to the graph. Please remember that since this is a Tripartite Graph, only User nodes, Item nodes and Property nodes can be added! And Property nodes can only be linked to Item nodes!</p> <p>A link can be weighted with the <code>weight</code> parameter and labeled with the <code>label</code> parameter. A timestamp can also be specified via <code>timestamp</code> parameter. All three are optional parameters, so they are not required</p> PARAMETER DESCRIPTION <code>start_node</code> <p>Single Node object or a list of Node objects. They will be the 'head' of the link, since it's a directed graph</p> <p> TYPE: <code>Union[Node, List[Node]]</code> </p> <code>final_node</code> <p>Single Node object or a list Node objects. They will be the 'tail' of the link, since it's a directed graph</p> <p> TYPE: <code>object</code> </p> <code>weight</code> <p>weight of the link, default is None (no weight)</p> <p> TYPE: <code>float</code> DEFAULT: <code>None</code> </p> <code>label</code> <p>label of the link, default is None (no label)</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>timestamp</code> <p>timestamp of the link, default is None (no timestamp)</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>Exception raised when Property nodes are tried to be linked with non-Item nodes</p> Source code in <code>clayrs/recsys/graphs/nx_implementation/nx_tripartite_graphs.py</code> <pre><code>def add_link(self, start_node: Union[Node, List[Node]], final_node: Union[Node, List[Node]],\n             weight: float = None, label: str = None, timestamp: str = None):\n\"\"\"\n    Creates a link connecting the `start_node` to the `final_node`. If two lists are passed, then the node in\n    position $i$ in the `start_node` list will be linked to the node in position $i$ in the `final_node` list.\n\n    If nodes to link do not exist, they will be added automatically to the graph. Please remember that since this is\n    a Tripartite Graph, only *User nodes*, *Item nodes* and *Property nodes* can be added! And *Property nodes* can\n    only be linked to *Item nodes*!\n\n    A link can be weighted with the `weight` parameter and labeled with the `label` parameter.\n    A timestamp can also be specified via `timestamp` parameter.\n    All three are optional parameters, so they are not required\n\n    Args:\n        start_node: Single Node object or a list of Node objects. They will be the 'head' of the link, since it's a\n            directed graph\n        final_node (object): Single Node object or a list Node objects. They will be the 'tail' of the link,\n            since it's a directed graph\n        weight: weight of the link, default is None (no weight)\n        label: label of the link, default is None (no label)\n        timestamp: timestamp of the link, default is None (no timestamp)\n\n    Raises:\n        ValueError: Exception raised when Property nodes are tried to be linked with non-Item nodes\n    \"\"\"\n\n    def is_not_valid_link(start_n: Node, final_n: Node):\n        return (isinstance(final_n, PropertyNode) and not isinstance(start_n, ItemNode)) or \\\n               (isinstance(start_n, PropertyNode) and not isinstance(final_n, ItemNode))\n\n    if not isinstance(start_node, list):\n        start_node = [start_node]\n\n    if not isinstance(final_node, list):\n        final_node = [final_node]\n\n    if any(is_not_valid_link(start_n, final_n) for start_n, final_n in zip(start_node, final_node)):\n        raise ValueError(\"Only item nodes can be linked to property nodes in a Tripartite Graph!\")\n\n    self.add_node(start_node)\n    self.add_node(final_node)\n\n    not_none_dict = {}\n    if label is not None:\n        not_none_dict['label'] = label\n    if weight is not None:\n        not_none_dict['weight'] = weight\n    if timestamp is not None:\n        not_none_dict['timestamp'] = timestamp\n\n    self._graph.add_edges_from(zip(start_node, final_node),\n                               **not_none_dict)\n</code></pre>"},{"location":"recsys/graph_based/graphs/nx_tripartite/#clayrs.recsys.graphs.nx_implementation.nx_tripartite_graphs.NXTripartiteGraph.add_node","title":"<code>add_node(node)</code>","text":"<p>Adds one or multiple Node objects to the graph. Since this is a Tripartite Graph, only <code>User Node</code>, <code>Item Node</code> and <code>Property Node</code> can be added!</p> <p>No duplicates are allowed, but different category nodes with same id are (e.g. <code>ItemNode('1')</code> and <code>UserNode('1')</code>)</p> PARAMETER DESCRIPTION <code>node</code> <p>Node(s) object(s) that needs to be added to the graph</p> <p> TYPE: <code>Union[Node, List[Node]]</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>Exception raised when one of the node to add to the graph is not a User, Item or Property node</p> Source code in <code>clayrs/recsys/graphs/nx_implementation/nx_tripartite_graphs.py</code> <pre><code>def add_node(self, node: Union[Node, List[Node]]):\n\"\"\"\n    Adds one or multiple Node objects to the graph.\n    Since this is a Tripartite Graph, only `User Node`, `Item Node` and `Property Node` can be added!\n\n    No duplicates are allowed, but different category nodes with same id are (e.g. `ItemNode('1')` and\n    `UserNode('1')`)\n\n    Args:\n        node: Node(s) object(s) that needs to be added to the graph\n\n    Raises:\n        ValueError: Exception raised when one of the node to add to the graph is not a User, Item or Property node\n    \"\"\"\n    if not isinstance(node, list):\n        node = [node]\n\n    if any(not isinstance(n, (UserNode, ItemNode, PropertyNode)) for n in node):\n        raise ValueError(\"You can only add UserNodes or ItemNodes to a bipartite graph!\")\n\n    self._graph.add_nodes_from(node)\n</code></pre>"},{"location":"recsys/graph_based/graphs/nx_tripartite/#clayrs.recsys.graphs.nx_implementation.nx_tripartite_graphs.NXTripartiteGraph.add_node_with_prop","title":"<code>add_node_with_prop(node, item_exo_properties, item_contents_dir, item_filename=None)</code>","text":"<p>Adds one or multiple Node objects and its/their properties to the graph. Since this is a Tripartite Graph, only <code>Item Node</code> are allowed to have properties!</p> <p>In order to load properties in the graph, we must specify where items are serialized and which properties to add:</p> <ul> <li>If item_exo_properties is specified as a set, then the graph will try to load all properties from said exogenous representation</li> </ul> <pre><code>{'my_exo_id'}\n</code></pre> <ul> <li>If item_exo_properties is specified as a dict, then the graph will try to load said properties from said exogenous representation</li> </ul> <pre><code>{'my_exo_id': ['my_prop1', 'my_prop2']]}\n</code></pre> <p>In case you want your node to have a different id from serialized contents, via the <code>item_filename</code> parameter you can specify what is the filename of the node that you are adding, e.g.</p> <pre><code>item_to_add = ItemNode('different_id')\n\n# item_filename is 'item_serialized_1.xz'\n\ngraph.add_node_with_prop(item_to_add, ..., item_filename='item_serialized_1')\n</code></pre> <p>In case you are adding a list of nodes, you can specify the filename for each node in the list.</p> PARAMETER DESCRIPTION <code>node</code> <p>Node(s) object(s) that needs to be added to the graph along with their properties</p> <p> TYPE: <code>Union[ItemNode, List[ItemNode]]</code> </p> <code>item_exo_properties</code> <p>Set or Dict which contains representations to load from items. Use a <code>Set</code> if you want to load all properties from specific representations, or use a <code>Dict</code> if you want to choose which properties to load from specific representations</p> <p> TYPE: <code>Union[Dict, set]</code> </p> <code>item_contents_dir</code> <p>The path containing items serialized with the Content Analyzer</p> <p> TYPE: <code>str</code> </p> <code>item_filename</code> <p>Filename(s) of the node(s) to add</p> <p> TYPE: <code>Union[str, List[str]]</code> DEFAULT: <code>None</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>Exception raised when one of the node to add to the graph with their properties is not an ItemNode</p> Source code in <code>clayrs/recsys/graphs/nx_implementation/nx_tripartite_graphs.py</code> <pre><code>def add_node_with_prop(self, node: Union[ItemNode, List[ItemNode]], item_exo_properties: Union[Dict, set],\n                       item_contents_dir: str,\n                       item_filename: Union[str, List[str]] = None):\n\"\"\"\n    Adds one or multiple Node objects and its/their properties to the graph.\n    Since this is a Tripartite Graph, only `Item Node` are allowed to have properties!\n\n    In order to load properties in the graph, we must specify where items are serialized and ***which\n    properties to add***:\n\n    *   If *item_exo_properties* is specified as a **set**, then the graph will try to load **all properties**\n    from **said exogenous representation**\n\n    ```python\n    {'my_exo_id'}\n    ```\n\n    *   If *item_exo_properties* is specified as a **dict**, then the graph will try to load **said properties**\n    from **said exogenous representation**\n\n    ```python\n    {'my_exo_id': ['my_prop1', 'my_prop2']]}\n    ```\n\n    In case you want your node to have a different id from serialized contents, via the `item_filename` parameter\n    you can specify what is the filename of the node that you are adding, e.g.\n\n    ```\n    item_to_add = ItemNode('different_id')\n\n    # item_filename is 'item_serialized_1.xz'\n\n    graph.add_node_with_prop(item_to_add, ..., item_filename='item_serialized_1')\n    ```\n\n    In case you are adding a list of nodes, you can specify the filename for each node in the list.\n\n    Args:\n        node: Node(s) object(s) that needs to be added to the graph along with their properties\n        item_exo_properties: Set or Dict which contains representations to load from items. Use a `Set` if you want\n            to load all properties from specific representations, or use a `Dict` if you want to choose which\n            properties to load from specific representations\n        item_contents_dir: The path containing items serialized with the Content Analyzer\n        item_filename: Filename(s) of the node(s) to add\n\n    Raises:\n        ValueError: Exception raised when one of the node to add to the graph with their properties is not\n            an ItemNode\n    \"\"\"\n    def node_prop_link_generator():\n        for n, id in zip(progbar, item_filename):\n            item: Content = loaded_items.get(id)\n\n            if item is not None:\n                exo_props = self._get_exo_props(item_exo_properties, item)\n\n                single_item_prop_edges = [(n,\n                                           PropertyNode(prop_dict[prop]),\n                                           {'label': prop})\n                                          for prop_dict in exo_props for prop in prop_dict]\n\n            else:\n                single_item_prop_edges = []\n\n            yield from single_item_prop_edges\n\n    if not isinstance(node, list):\n        node = [node]\n\n    if any(not isinstance(n, ItemNode) for n in node):\n        raise ValueError(\"Only item nodes can be linked to property nodes in a Tripartite Graph!\")\n\n    if isinstance(item_exo_properties, set):\n        item_exo_properties = dict.fromkeys(item_exo_properties, None)\n\n    if item_filename is None:\n        item_filename = [n.value for n in node]\n\n    if not isinstance(item_filename, list):\n        item_filename = [item_filename]\n\n    loaded_items = LoadedContentsDict(item_contents_dir, contents_to_load=set(item_filename))\n    with get_progbar(node) as progbar:\n\n        progbar.set_description(\"Creating Item-&gt;Properties links\")\n        self._graph.add_edges_from((tuple_to_add for tuple_to_add in node_prop_link_generator()))\n</code></pre>"},{"location":"recsys/methodology/abstract_methodology/","title":"Abstract methodology class","text":""},{"location":"recsys/methodology/abstract_methodology/#clayrs.recsys.methodology.Methodology","title":"<code>Methodology(only_greater_eq=None)</code>","text":"<p>         Bases: <code>ABC</code></p> <p>Class which, given a train set and a test set, has the task to calculate which items must be used in order to generate a recommendation list</p> <p>The methodologies here implemented follow the 'Precision-Oriented Evaluation of Recommender Systems: An Algorithmic Comparison' paper</p> Source code in <code>clayrs/recsys/methodology.py</code> <pre><code>def __init__(self, only_greater_eq: float = None):\n\n    self._threshold = only_greater_eq\n\n    # items arr is an array with all items id mapped to their integer\n    self._items_arr: Optional[np.ndarray] = None\n    # query vector is the vector with same length of _items_arr used as boolean query vector\n    # position in which a True appears will be taken from _items_arr, position set to False will not\n    self._query_vector: Optional[np.ndarray] = None\n</code></pre>"},{"location":"recsys/methodology/abstract_methodology/#clayrs.recsys.methodology.Methodology.filter_all","title":"<code>filter_all(train_set, test_set, result_as_dict=False, ids_as_str=True)</code>","text":"<p>Concrete method which calculates for all users of the test set which items must be used in order to generate a recommendation list</p> <p>It takes in input a train set and a test set and returns a single DataFrame or a python dictionary containing, for every user, all items which must be recommended based on the methodology chosen.</p> PARAMETER DESCRIPTION <code>train_set</code> <p><code>Ratings</code> object which contains the train set of every user</p> <p> TYPE: <code>Ratings</code> </p> <code>test_set</code> <p><code>Ratings</code> object which contains the test set of every user</p> <p> TYPE: <code>Ratings</code> </p> <code>result_as_dict</code> <p>If True the output of the method will be a generator of a dictionary that contains users as keys and numpy arrays with items as values. If <code>ids_as_str</code> is set to True, users and items will be present with their string id, otherwise will be present with their mapped integer</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ids_as_str</code> <p>If True, the result will contain users and items represented with their string id. Otherwise, will be present with their mapped integer</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>Union[pd.DataFrame, Union[Dict[str, np.ndarray], Dict[int, np.ndarray]]]</code> <p>A DataFrame or a python dictionary which contains all items which must be recommended to every user based on the methodology chosen.</p> Source code in <code>clayrs/recsys/methodology.py</code> <pre><code>def filter_all(self, train_set: Ratings, test_set: Ratings,\n               result_as_dict: bool = False,\n               ids_as_str: bool = True) -&gt; Union[pd.DataFrame,\n                                                 Union[Dict[str, np.ndarray], Dict[int, np.ndarray]]]:\n\"\"\"\n    Concrete method which calculates for all users of the *test set* which items must be used in order to\n    generate a recommendation list\n\n    It takes in input a *train set* and a *test set* and returns a single DataFrame or a python\n    dictionary containing, for every user, all items which must be recommended based on the methodology chosen.\n\n    Args:\n        train_set: `Ratings` object which contains the train set of every user\n        test_set: `Ratings` object which contains the test set of every user\n        result_as_dict: If True the output of the method will be a generator of a dictionary that contains\n            users as keys and numpy arrays with items as values. If `ids_as_str` is set to True, users and items\n            will be present with their string id, otherwise will be present with their mapped integer\n        ids_as_str: If True, the result will contain users and items represented with their string id. Otherwise,\n            will be present with their mapped integer\n\n    Returns:\n        A DataFrame or a python dictionary which contains all items which must be recommended to\n            every user based on the methodology chosen.\n    \"\"\"\n    user_list = test_set.unique_user_idx_column\n    user_int2str = train_set.user_map.convert_int2str\n    item_seq_int2str = train_set.item_map.convert_seq_int2str\n\n    with get_progbar(user_list) as pbar:\n        pbar.set_description(f\"Filtering items based on {str(self)}\")\n\n        if ids_as_str:\n            filtered = {user_int2str(user_idx): item_seq_int2str(self.filter_single(user_idx, train_set, test_set).astype(int))\n                        for user_idx in pbar}\n        else:\n            filtered = {user_idx: self.filter_single(user_idx, train_set, test_set)\n                        for user_idx in pbar}\n\n    if not result_as_dict:\n\n        will_be_frame = {\"user_id\": [], \"item_id\": []}\n        for user_id, filter_list in filtered.items():\n\n            will_be_frame[\"user_id\"].append(np.full(filter_list.shape, user_id))\n            will_be_frame[\"item_id\"].append(filter_list)\n\n        will_be_frame[\"user_id\"] = np.hstack(will_be_frame[\"user_id\"])\n        will_be_frame[\"item_id\"] = np.hstack(will_be_frame[\"item_id\"])\n\n        filtered = pd.DataFrame.from_dict(will_be_frame)\n\n    return filtered\n</code></pre>"},{"location":"recsys/methodology/abstract_methodology/#clayrs.recsys.methodology.Methodology.filter_single","title":"<code>filter_single(user_idx, train_set, test_set)</code>  <code>abstractmethod</code>","text":"<p>Abstract method in which must be specified how to calculate which items must be part of the recommendation list of a single user</p> Source code in <code>clayrs/recsys/methodology.py</code> <pre><code>@abstractmethod\ndef filter_single(self, user_idx: int, train_set: Ratings, test_set: Ratings) -&gt; np.ndarray:\n\"\"\"\n    Abstract method in which must be specified how to calculate which items must be part of the recommendation list\n    of a single user\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"recsys/methodology/abstract_methodology/#clayrs.recsys.methodology.Methodology.setup","title":"<code>setup(train_set, test_set)</code>  <code>abstractmethod</code>","text":"<p>Method to call before calling <code>filter_all()</code> or <code>filter_single()</code>. It is used to set up numpy arrays which will filter items according to the methodology chosen.</p> <p>This method has side effect, meaning that it will return a <code>Methodology</code> object which has been set up but will also change the <code>Methodology</code> object that has called this method</p> PARAMETER DESCRIPTION <code>train_set</code> <p><code>Ratings</code> object which contains the train set of every user</p> <p> TYPE: <code>Ratings</code> </p> <code>test_set</code> <p><code>Ratings</code> object which contains the test set of every user</p> <p> TYPE: <code>Ratings</code> </p> RETURNS DESCRIPTION <code>Methodology</code> <p>The set-up Methodology object</p> Source code in <code>clayrs/recsys/methodology.py</code> <pre><code>@abstractmethod\ndef setup(self, train_set: Ratings, test_set: Ratings) -&gt; Methodology:\n\"\"\"\n    Method to call before calling `filter_all()` or `filter_single()`.\n    It is used to set up numpy arrays which will filter items according to the methodology chosen.\n\n    This method has side effect, meaning that it will return a `Methodology` object which has been set up but will\n    also change the `Methodology` object that has called this method\n\n    Args:\n        train_set: `Ratings` object which contains the train set of every user\n        test_set: `Ratings` object which contains the test set of every user\n\n    Returns:\n        The set-up Methodology object\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"recsys/methodology/all_items/","title":"All Items methodology","text":""},{"location":"recsys/methodology/all_items/#clayrs.recsys.AllItemsMethodology","title":"<code>AllItemsMethodology(items_list=None)</code>","text":"<p>         Bases: <code>Methodology</code></p> <p>Class which, given a train set and a test set, has the task to calculate which items must be used in order to generate a recommendation list</p> <p>With AllItemsMethodology, given a user \\(u\\), items to recommend for \\(u\\) are all items that appear in <code>items_list</code> parameter excluding those items that appear in the train set of \\(u\\)</p> <p>If <code>items_list</code> is None, then the union of items that appear in the train and test set will be considered</p> PARAMETER DESCRIPTION <code>items_list</code> <p>Items set that must appear in the recommendation list of every user. If None, all items that appear in the train and test set will be considered</p> <p> TYPE: <code>Union[Sequence[str], Sequence[int]]</code> DEFAULT: <code>None</code> </p> Source code in <code>clayrs/recsys/methodology.py</code> <pre><code>def __init__(self, items_list: Union[Sequence[str], Sequence[int]] = None):\n    self.items_list = items_list\n    if items_list is not None:\n        self.items_list = np.array(items_list)\n\n    super(AllItemsMethodology, self).__init__(None)\n</code></pre>"},{"location":"recsys/methodology/all_items/#clayrs.recsys.methodology.AllItemsMethodology.filter_single","title":"<code>filter_single(user_idx, train_set, test_set)</code>","text":"<p>Method that returns items that needs to be part of the recommendation list of a single user. Since it's the AllItems Methodology, all items that appear in the <code>items_list</code> parameter of the constructor will be returned, except for those that appear in the train set of the user passed as parameter</p> PARAMETER DESCRIPTION <code>user_idx</code> <p>User idx (meaning its mapped integer) of which we want to calculate items that must appear in its recommendation list</p> <p> TYPE: <code>int</code> </p> <code>train_set</code> <p><code>Ratings</code> object which contains the train set of every user</p> <p> TYPE: <code>Ratings</code> </p> <code>test_set</code> <p><code>Ratings</code> object which contains the test set of every user</p> <p> TYPE: <code>Ratings</code> </p> Source code in <code>clayrs/recsys/methodology.py</code> <pre><code>def filter_single(self, user_idx: int, train_set: Ratings, test_set: Ratings) -&gt; np.ndarray:\n\"\"\"\n    Method that returns items that needs to be part of the recommendation list of a single user.\n    Since it's the AllItems Methodology, all items that appear in the `items_list` parameter of the constructor\n    will be returned, except for those that appear in the *train set* of the user passed as parameter\n\n    Args:\n        user_idx: User idx (meaning its mapped integer) of which we want to calculate items that must appear in its\n            recommendation list\n        train_set: `Ratings` object which contains the train set of every user\n        test_set: `Ratings` object which contains the test set of every user\n    \"\"\"\n    already_seen_items_it = pd.unique(train_set.get_user_interactions(user_idx)[:, 1].astype(int))\n\n    self._query_vector[already_seen_items_it] = False\n    result = self._items_arr[self._query_vector]\n    self._query_vector[self.items_list] = True\n\n    return result.astype(int)\n</code></pre>"},{"location":"recsys/methodology/test_items/","title":"Test Items methodology","text":""},{"location":"recsys/methodology/test_items/#clayrs.recsys.TestItemsMethodology","title":"<code>TestItemsMethodology(only_greater_eq=None)</code>","text":"<p>         Bases: <code>Methodology</code></p> <p>Class which, given a train set and a test set, has the task to calculate which items must be used in order to generate a recommendation list</p> <p>With TestItemsMethodology, given a user \\(u\\), items to recommend for \\(u\\) are all items that appear in the test set of every user excluding those items that appear in the train set of \\(u\\)</p> <p>If the <code>only_greater_eq</code> parameter is set, then only items with rating score \\(&gt;=\\) only_greater_eq will be returned</p> PARAMETER DESCRIPTION <code>only_greater_eq</code> <p>float which acts as a filter, if specified only items with rating score \\(&gt;=\\) only_greater_eq will be returned</p> <p> TYPE: <code>float</code> DEFAULT: <code>None</code> </p> Source code in <code>clayrs/recsys/methodology.py</code> <pre><code>def __init__(self, only_greater_eq: float = None):\n    super(TestItemsMethodology, self).__init__(only_greater_eq)\n\n    self._filtered_test_set_items: Optional[np.ndarray] = None\n</code></pre>"},{"location":"recsys/methodology/test_items/#clayrs.recsys.methodology.TestItemsMethodology.filter_single","title":"<code>filter_single(user_idx, train_set, test_set)</code>","text":"<p>Method that returns items that need to be part of the recommendation list of a single user. Since it's the TestItems Methodology, all items that appear in the test set of every user will be returned, except for those that appear in the train set of the user passed as parameter</p> PARAMETER DESCRIPTION <code>user_idx</code> <p>User idx (meaning its mapped integer) of which we want to calculate items that must appear in its recommendation list</p> <p> TYPE: <code>int</code> </p> <code>train_set</code> <p><code>Ratings</code> object which contains the train set of every user</p> <p> TYPE: <code>Ratings</code> </p> <code>test_set</code> <p><code>Ratings</code> object which contains the test set of every user</p> <p> TYPE: <code>Ratings</code> </p> Source code in <code>clayrs/recsys/methodology.py</code> <pre><code>def filter_single(self, user_idx: int, train_set: Ratings, test_set: Ratings) -&gt; np.ndarray:\n\"\"\"\n    Method that returns items that need to be part of the recommendation list of a single user.\n    Since it's the TestItems Methodology, all items that appear in the *test set* of every user will be returned,\n    except for those that appear in the *train set* of the user passed as parameter\n\n    Args:\n        user_idx: User idx (meaning its mapped integer) of which we want to calculate items that must appear in its\n            recommendation list\n        train_set: `Ratings` object which contains the train set of every user\n        test_set: `Ratings` object which contains the test set of every user\n    \"\"\"\n    already_seen_items_it = pd.unique(train_set.get_user_interactions(user_idx)[:, 1].astype(int))\n\n    self._query_vector[already_seen_items_it] = False\n\n    result = self._items_arr[self._query_vector]\n\n    self._query_vector[self._filtered_test_set_items] = True\n\n    return result.astype(int)\n</code></pre>"},{"location":"recsys/methodology/test_ratings/","title":"Test Ratings methodology","text":""},{"location":"recsys/methodology/test_ratings/#clayrs.recsys.TestRatingsMethodology","title":"<code>TestRatingsMethodology(only_greater_eq=None)</code>","text":"<p>         Bases: <code>Methodology</code></p> <p>Class which, given a train set and a test set, has the task to calculate which items must be used in order to generate a recommendation list</p> <p>With TestRatingsMethodology, given a user \\(u\\), items to recommend for \\(u\\) are simply those items that appear in its test set</p> <p>If the <code>only_greater_eq</code> parameter is set, then only items with rating score \\(&gt;=\\) only_greater_eq will be returned</p> PARAMETER DESCRIPTION <code>only_greater_eq</code> <p>float which acts as a filter, if specified only items with rating score \\(&gt;=\\) only_greater_eq will be returned</p> <p> TYPE: <code>float</code> DEFAULT: <code>None</code> </p> Source code in <code>clayrs/recsys/methodology.py</code> <pre><code>def __init__(self, only_greater_eq: float = None):\n    super(TestRatingsMethodology, self).__init__(only_greater_eq)\n</code></pre>"},{"location":"recsys/methodology/test_ratings/#clayrs.recsys.methodology.TestRatingsMethodology.filter_single","title":"<code>filter_single(user_idx, train_set, test_set)</code>","text":"<p>Method that returns items that need to be part of the recommendation list of a single user. Since it's the TestRatings Methodology, only items that appear in the test set of the user will be returned.</p> PARAMETER DESCRIPTION <code>user_idx</code> <p>User idx (meaning its mapped integer) of which we want to calculate items that must appear in its recommendation list</p> <p> TYPE: <code>int</code> </p> <code>train_set</code> <p><code>Ratings</code> object which contains the train set of every user</p> <p> TYPE: <code>Ratings</code> </p> <code>test_set</code> <p><code>Ratings</code> object which contains the test set of every user</p> <p> TYPE: <code>Ratings</code> </p> Source code in <code>clayrs/recsys/methodology.py</code> <pre><code>def filter_single(self, user_idx: int, train_set: Ratings, test_set: Ratings) -&gt; np.ndarray:\n\"\"\"\n    Method that returns items that need to be part of the recommendation list of a single user.\n    Since it's the TestRatings Methodology, only items that appear in the *test set* of the user will be returned.\n\n    Args:\n        user_idx: User idx (meaning its mapped integer) of which we want to calculate items that must appear in its\n            recommendation list\n        train_set: `Ratings` object which contains the train set of every user\n        test_set: `Ratings` object which contains the test set of every user\n    \"\"\"\n    uir_user = test_set.get_user_interactions(user_idx)\n\n    if self._threshold is not None:\n        result = pd.unique(uir_user[:, 1][np.where(uir_user[:, 2] &gt;= self._threshold)])\n    else:\n        # TestRatings just returns the test set of the user\n        result = pd.unique(uir_user[:, 1])\n\n    return result.astype(int)\n</code></pre>"},{"location":"recsys/methodology/training_items/","title":"Training Items methodology","text":""},{"location":"recsys/methodology/training_items/#clayrs.recsys.TrainingItemsMethodology","title":"<code>TrainingItemsMethodology(only_greater_eq=None)</code>","text":"<p>         Bases: <code>Methodology</code></p> <p>Class which, given a train set and a test set, has the task to calculate which items must be used in order to generate a recommendation list</p> <p>With TrainingItemsMethodology, given a user \\(u\\), items to recommend for \\(u\\) are all items that appear in the 'train set' of every user excluding those items that appear in the 'train set' of \\(u\\)</p> <p>If the <code>only_greater_eq</code> parameter is set, then only items with rating score \\(&gt;=\\) only_greater_eq will be returned</p> PARAMETER DESCRIPTION <code>only_greater_eq</code> <p>float which acts as a filter, if specified only items with rating score \\(&gt;=\\) only_greater_eq will be returned</p> <p> TYPE: <code>float</code> DEFAULT: <code>None</code> </p> Source code in <code>clayrs/recsys/methodology.py</code> <pre><code>def __init__(self, only_greater_eq: float = None):\n    super(TrainingItemsMethodology, self).__init__(only_greater_eq)\n\n    self._filtered_train_set_items: Optional[Set] = None\n</code></pre>"},{"location":"recsys/methodology/training_items/#clayrs.recsys.methodology.TrainingItemsMethodology.filter_single","title":"<code>filter_single(user_idx, train_set, test_set)</code>","text":"<p>Method that returns items that needs to be part of the recommendation list of a single user. Since it's the TrainingItems Methodology, all items that appear in the train set of every user will be returned, except for those that appear in the train set of the user passed as parameter</p> PARAMETER DESCRIPTION <code>user_idx</code> <p>User idx (meaning its mapped integer) of which we want to calculate items that must appear in its recommendation list</p> <p> TYPE: <code>int</code> </p> <code>train_set</code> <p><code>Ratings</code> object which contains the train set of every user</p> <p> TYPE: <code>Ratings</code> </p> <code>test_set</code> <p><code>Ratings</code> object which contains the test set of every user</p> <p> TYPE: <code>Ratings</code> </p> Source code in <code>clayrs/recsys/methodology.py</code> <pre><code>def filter_single(self, user_idx: int, train_set: Ratings, test_set: Ratings) -&gt; np.ndarray:\n\"\"\"\n    Method that returns items that needs to be part of the recommendation list of a single user.\n    Since it's the TrainingItems Methodology, all items that appear in the *train set* of every user will be\n    returned, except for those that appear in the *train set* of the user passed as parameter\n\n    Args:\n        user_idx: User idx (meaning its mapped integer) of which we want to calculate items that must appear in its\n            recommendation list\n        train_set: `Ratings` object which contains the train set of every user\n        test_set: `Ratings` object which contains the test set of every user\n    \"\"\"\n    already_seen_items_it = pd.unique(train_set.get_user_interactions(user_idx)[:, 1].astype(int))\n\n    self._query_vector[already_seen_items_it] = False\n\n    result = self._items_arr[self._query_vector]\n\n    self._query_vector[self._filtered_train_set_items] = True\n\n    return result.astype(int)\n</code></pre>"},{"location":"recsys/partitioning/abstract_partitioning/","title":"Abstract Partitioning class","text":""},{"location":"recsys/partitioning/abstract_partitioning/#clayrs.recsys.partitioning.Partitioning","title":"<code>Partitioning(skip_user_error=True)</code>","text":"<p>         Bases: <code>ABC</code></p> <p>Abstract class for partitioning technique. Each class must implement the <code>split_single()</code> method which specify how data for a single user will be split</p> PARAMETER DESCRIPTION <code>skip_user_error</code> <p>If set to True, users for which data can't be split will be skipped and only a warning will be logged at the end of the split process specifying n\u00b0 of users skipped. Otherwise, a <code>ValueError</code> exception is raised</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> Source code in <code>clayrs/recsys/partitioning.py</code> <pre><code>def __init__(self, skip_user_error: bool = True):\n    self.__skip_user_error = skip_user_error\n</code></pre>"},{"location":"recsys/partitioning/abstract_partitioning/#clayrs.recsys.partitioning.Partitioning.split_all","title":"<code>split_all(ratings_to_split, user_list=None)</code>","text":"<p>Concrete method that splits, for every user in the user column of <code>ratings_to_split</code>, the original ratings into train set and test set. If a <code>user_list</code> parameter is set, the method will do the splitting only for the users specified inside the list (Users can be specified as strings or with their mapped integer).</p> <p>The method returns two lists:</p> <ul> <li>The first contains all train set for each split (if the partitioning technique returns more than one split e.g. KFold)</li> <li>The second contains all test set for each split (if the partitioning technique returns more than one split e.g. KFold)</li> </ul> <p>Obviously the two lists will have the same length, and to the train set in position \\(i\\) corresponds the truth set at position \\(i\\)</p> PARAMETER DESCRIPTION <code>ratings_to_split</code> <p><code>Ratings</code> object which contains the interactions of the users that must be split into train set and test set</p> <p> TYPE: <code>Ratings</code> </p> <code>user_list</code> <p>The Set of users for which splitting will be done. If set, splitting will be performed only for users inside the list. Otherwise, splitting will be performed for all users in <code>ratings_to_split</code> parameter. User can be specified with their string id or with their mapped integer</p> <p> TYPE: <code>Union[Set[int], Set[str]]</code> DEFAULT: <code>None</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>if <code>skip_user_error=True</code> in the constructor and for at least one user splitting can't be performed</p> Source code in <code>clayrs/recsys/partitioning.py</code> <pre><code>def split_all(self, ratings_to_split: Ratings,\n              user_list: Union[Set[int], Set[str]] = None) -&gt; Tuple[List[Ratings], List[Ratings]]:\n\"\"\"\n    Concrete method that splits, for every user in the user column of `ratings_to_split`, the original ratings\n    into *train set* and *test set*.\n    If a `user_list` parameter is set, the method will do the splitting only for the users\n    specified inside the list (Users can be specified as *strings* or with their mapped *integer*).\n\n    The method returns two lists:\n\n    * The first contains all train set for each split (if the partitioning technique returns more than one split\n    e.g. KFold)\n    * The second contains all test set for each split (if the partitioning technique returns more than one split\n    e.g. KFold)\n\n    Obviously the two lists will have the same length, and to the *train set* in position $i$ corresponds the\n    *truth set* at position $i$\n\n    Args:\n        ratings_to_split: `Ratings` object which contains the interactions of the users that must be split\n            into *train set* and *test set*\n        user_list: The Set of users for which splitting will be done. If set, splitting will be performed only\n            for users inside the list. Otherwise, splitting will be performed for all users in `ratings_to_split`\n            parameter. User can be specified with their string id or with their mapped integer\n\n    Raises:\n        ValueError: if `skip_user_error=True` in the constructor and for at least one user splitting\n            can't be performed\n    \"\"\"\n\n    # convert user list to list of int if necessary (strings are passed)\n    if user_list is not None:\n        all_users = np.array(list(user_list))\n        if np.issubdtype(all_users.dtype, str):\n            all_users = ratings_to_split.user_map.convert_seq_str2int(all_users)\n\n        all_users = set(all_users)\n    else:\n        all_users = set(ratings_to_split.unique_user_idx_column)\n\n    # {\n    #   0: {'train': [u1_uir, u2_uir]},\n    #       'test': [u1_uir, u2_uir]},\n    #\n    #   1: {'train': [u1_uir, u2_uir]},\n    #       'test': [u1_uir, u2_uir]\n    #  }\n    train_test_dict = defaultdict(lambda: defaultdict(list))\n    error_count = 0\n\n    with get_progbar(all_users) as pbar:\n\n        pbar.set_description(\"Performing {}\".format(str(self)))\n        for user_idx in pbar:\n            user_ratings = ratings_to_split.get_user_interactions(user_idx)\n            try:\n                user_train_list, user_test_list = self.split_single(user_ratings)\n                for split_number, (single_train, single_test) in enumerate(zip(user_train_list, user_test_list)):\n\n                    train_test_dict[split_number]['train'].append(single_train)\n                    train_test_dict[split_number]['test'].append(single_test)\n\n            except ValueError as e:\n                if self.skip_user_error:\n                    error_count += 1\n                    continue\n                else:\n                    raise e from None\n\n    if error_count &gt; 0:\n        logger.warning(f\"{error_count} users will be skipped because partitioning couldn't be performed\\n\"\n                       f\"Change this behavior by setting `skip_user_error` to True\")\n\n    train_list = [Ratings.from_uir(np.vstack(train_test_dict[split]['train']),\n                                   ratings_to_split.user_map, ratings_to_split.item_map)\n                  for split in train_test_dict]\n\n    test_list = [Ratings.from_uir(np.vstack(train_test_dict[split]['test']),\n                                  ratings_to_split.user_map, ratings_to_split.item_map)\n                 for split in train_test_dict]\n\n    return train_list, test_list\n</code></pre>"},{"location":"recsys/partitioning/abstract_partitioning/#clayrs.recsys.partitioning.Partitioning.split_single","title":"<code>split_single(uir_user)</code>  <code>abstractmethod</code>","text":"<p>Abstract method in which each partitioning technique must specify how to split data for a single user</p> PARAMETER DESCRIPTION <code>uir_user</code> <p>uir matrix containing interactions of a single user</p> <p> TYPE: <code>np.ndarray</code> </p> RETURNS DESCRIPTION <code>List[np.ndarray]</code> <p>The first list contains a uir matrix for each split constituting the train set of the user</p> <code>List[np.ndarray]</code> <p>The second list contains a uir matrix for each split constituting the test set of the user</p> Source code in <code>clayrs/recsys/partitioning.py</code> <pre><code>@abc.abstractmethod\ndef split_single(self, uir_user: np.ndarray) -&gt; Tuple[List[np.ndarray], List[np.ndarray]]:\n\"\"\"\n    Abstract method in which each partitioning technique must specify how to split data for a single user\n\n    Args:\n        uir_user: uir matrix containing interactions of a single user\n\n    Returns:\n        The first list contains a uir matrix for each split constituting the *train set* of the user\n\n        The second list contains a uir matrix for each split constituting the *test set* of the user\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"recsys/partitioning/bootstrap/","title":"KFold partitioning technique","text":""},{"location":"recsys/partitioning/bootstrap/#clayrs.recsys.BootstrapPartitioning","title":"<code>BootstrapPartitioning(random_state=None, skip_user_error=True)</code>","text":"<p>         Bases: <code>Partitioning</code></p> <p>Class that performs Bootstrap Partitioning.</p> <p>The bootstrap partitioning consists in executing \\(n\\) extractions with replacement for each user from the original interaction frame, where \\(n\\) is the length of the user interactions:</p> <ul> <li>The sampled data will be part of the train set</li> <li>All the data which is part of the original dataset but was not sampled will be part of the test set</li> </ul> <p>Info</p> <p>The bootstrap partitioning can change the original data distribution, since during the extraction phase you could sample the same data more than once</p> PARAMETER DESCRIPTION <code>random_state</code> <p>Controls the shuffling applied to the data before applying the split. Pass an int for reproducible output across multiple function calls.</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>skip_user_error</code> <p>If set to True, users for which data can't be split will be skipped and only a warning will be logged at the end of the split process specifying n\u00b0 of users skipped. Otherwise, a <code>ValueError</code> exception is raised</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> Source code in <code>clayrs/recsys/partitioning.py</code> <pre><code>def __init__(self, random_state: int = None, skip_user_error: bool = True):\n    super().__init__(skip_user_error)\n\n    self.__random_state = random_state\n</code></pre>"},{"location":"recsys/partitioning/bootstrap/#clayrs.recsys.partitioning.BootstrapPartitioning.split_single","title":"<code>split_single(uir_user)</code>","text":"<p>Method which splits train set and test set the ratings of a single user by performing \\(n\\) extraction with replacement of the user interactions, where \\(n\\) is the number of its interactions. The interactions which are not sampled will be part of the test set</p> PARAMETER DESCRIPTION <code>uir_user</code> <p>uir matrix containing interactions of a single user</p> <p> TYPE: <code>np.ndarray</code> </p> RETURNS DESCRIPTION <code>List[np.ndarray]</code> <p>The first list contains a uir matrix for each split constituting the train set of the user</p> <code>List[np.ndarray]</code> <p>The second list contains a uir matrix for each split constituting the test set of the user</p> Source code in <code>clayrs/recsys/partitioning.py</code> <pre><code>def split_single(self, uir_user: np.ndarray) -&gt; Tuple[List[np.ndarray], List[np.ndarray]]:\n\"\"\"\n    Method which splits *train set* and *test set* the ratings of a single user by performing $n$ extraction with\n    replacement of the user interactions, where $n$ is the number of its interactions.\n    The interactions which are not sampled will be part of the *test set*\n\n    Args:\n        uir_user: uir matrix containing interactions of a single user\n\n    Returns:\n        The first list contains a uir matrix for each split constituting the *train set* of the user\n\n        The second list contains a uir matrix for each split constituting the *test set* of the user\n    \"\"\"\n\n    interactions_train = resample(uir_user,\n                                  replace=True,\n                                  n_samples=len(uir_user[:, 0]),\n                                  random_state=self.__random_state)\n\n    interactions_test = np.array([interaction\n                                  for interaction in uir_user\n                                  if not any(np.array_equal(interaction, interaction_train, equal_nan=True)\n                                             for interaction_train in interactions_train)])\n\n    user_train_list = [interactions_train]\n    user_test_list = [interactions_test]\n\n    if len(interactions_test) == 0:\n        raise ValueError(\"The test set for the user is empty! Try increasing the number of its interactions!\")\n\n    return user_train_list, user_test_list\n</code></pre>"},{"location":"recsys/partitioning/hold_out/","title":"HoldOut partitioning technique","text":""},{"location":"recsys/partitioning/hold_out/#clayrs.recsys.HoldOutPartitioning","title":"<code>HoldOutPartitioning(train_set_size=None, test_set_size=None, shuffle=True, random_state=None, skip_user_error=True)</code>","text":"<p>         Bases: <code>Partitioning</code></p> <p>Class that performs Hold-Out partitioning</p> PARAMETER DESCRIPTION <code>train_set_size</code> <p>Should be between 0.0 and 1.0 and represent the proportion of the ratings to hold in the train set for each user. If int, represents the absolute number of train samples. If None, the value is automatically set to the complement of the test size.</p> <p> TYPE: <code>Union[float, int, None]</code> DEFAULT: <code>None</code> </p> <code>test_set_size</code> <p>If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the test split. If int, represents the absolute number of test samples. If None, the value is set to the complement of the train size. If <code>train_size</code> is also None, it will be set to 0.25.</p> <p> TYPE: <code>Union[float, int, None]</code> DEFAULT: <code>None</code> </p> <code>random_state</code> <p>Controls the shuffling applied to the data before applying the split. Pass an int for reproducible output across multiple function calls.</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>shuffle</code> <p>Whether to shuffle the data before splitting.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>skip_user_error</code> <p>If set to True, users for which data can't be split will be skipped and only a warning will be logged at the end of the split process specifying n\u00b0 of users skipped. Otherwise, a <code>ValueError</code> exception is raised</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> Source code in <code>clayrs/recsys/partitioning.py</code> <pre><code>def __init__(self, train_set_size: Union[float, int, None] = None, test_set_size: Union[float, int, None] = None,\n             shuffle: bool = True, random_state: int = None,\n             skip_user_error: bool = True):\n\n    if train_set_size is not None and train_set_size &lt; 0:\n        raise ValueError(\"train_set_size must be a positive number\")\n\n    if test_set_size is not None and test_set_size &lt; 0:\n        raise ValueError(\"test_set_size must be a positive number\")\n\n    if isinstance(train_set_size, float) and train_set_size &gt; 1.0:\n        raise ValueError(\"train_set_size must be between 0.0 and 1.0\")\n\n    if isinstance(test_set_size, float) and test_set_size &gt; 1.0:\n        raise ValueError(\"test_set_size must be between 0.0 and 1.0\")\n\n    if isinstance(train_set_size, float) and isinstance(test_set_size, float) and \\\n            (train_set_size + test_set_size) &gt; 1.0:\n        raise ValueError(\"train_set_size and test_set_size percentages must not sum to a value greater than 1.0\")\n\n    self.__train_set_size = train_set_size\n    self.__test_set_size = test_set_size\n    self.__random_state = random_state\n    self.__shuffle = shuffle\n\n    super().__init__(skip_user_error)\n</code></pre>"},{"location":"recsys/partitioning/hold_out/#clayrs.recsys.partitioning.HoldOutPartitioning.split_single","title":"<code>split_single(uir_user)</code>","text":"<p>Method which splits train set and test set the ratings of a single user by holding in the train set of the user interactions accoring to the parameters set in the constructor</p> PARAMETER DESCRIPTION <code>uir_user</code> <p>uir matrix containing interactions of a single user</p> <p> TYPE: <code>np.ndarray</code> </p> RETURNS DESCRIPTION <code>List[np.ndarray]</code> <p>The first list contains a uir matrix for each split constituting the train set of the user</p> <code>List[np.ndarray]</code> <p>The second list contains a uir matrix for each split constituting the test set of the user</p> Source code in <code>clayrs/recsys/partitioning.py</code> <pre><code>def split_single(self, uir_user: np.ndarray) -&gt; Tuple[List[np.ndarray], List[np.ndarray]]:\n\"\"\"\n    Method which splits *train set* and *test set* the ratings of a single user by holding in the train set of the\n    user interactions accoring to the parameters set in the constructor\n\n    Args:\n        uir_user: uir matrix containing interactions of a single user\n\n    Returns:\n        The first list contains a uir matrix for each split constituting the *train set* of the user\n\n        The second list contains a uir matrix for each split constituting the *test set* of the user\n    \"\"\"\n    uir_train, uir_test = train_test_split(uir_user,\n                                           train_size=self.__train_set_size,\n                                           test_size=self.__test_set_size,\n                                           shuffle=self.__shuffle,\n                                           random_state=self.__random_state)\n\n    user_train_list = [uir_train]\n    user_test_list = [uir_test]\n\n    return user_train_list, user_test_list\n</code></pre>"},{"location":"recsys/partitioning/kfold/","title":"KFold partitioning technique","text":""},{"location":"recsys/partitioning/kfold/#clayrs.recsys.KFoldPartitioning","title":"<code>KFoldPartitioning(n_splits=2, shuffle=True, random_state=None, skip_user_error=True)</code>","text":"<p>         Bases: <code>Partitioning</code></p> <p>Class that performs K-Fold partitioning</p> PARAMETER DESCRIPTION <code>n_splits</code> <p>Number of splits. Must be at least 2</p> <p> TYPE: <code>int</code> DEFAULT: <code>2</code> </p> <code>shuffle</code> <p>Whether to shuffle the data before splitting into batches. Note that the samples within each split will not be shuffled.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>random_state</code> <p>When <code>shuffle</code> is True, <code>random_state</code> affects the ordering of the indices, which controls the randomness of each fold. Otherwise, this parameter has no effect. Pass an int for reproducible output across multiple function calls.</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>skip_user_error</code> <p>If set to True, users for which data can't be split will be skipped and only a warning will be logged at the end of the split process specifying n\u00b0 of users skipped. Otherwise, a <code>ValueError</code> exception is raised</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> Source code in <code>clayrs/recsys/partitioning.py</code> <pre><code>def __init__(self, n_splits: int = 2, shuffle: bool = True, random_state: int = None,\n             skip_user_error: bool = True):\n    self.__kf = KFold(n_splits=n_splits, shuffle=shuffle, random_state=random_state)\n\n    super(KFoldPartitioning, self).__init__(skip_user_error)\n</code></pre>"},{"location":"recsys/partitioning/kfold/#clayrs.recsys.partitioning.KFoldPartitioning.split_single","title":"<code>split_single(uir_user)</code>","text":"<p>Method which splits in \\(k\\) splits both in train set and test set the ratings of a single user</p> PARAMETER DESCRIPTION <code>uir_user</code> <p>uir matrix containing interactions of a single user</p> <p> TYPE: <code>np.ndarray</code> </p> RETURNS DESCRIPTION <code>List[np.ndarray]</code> <p>The first list contains a uir matrix for each split constituting the train set of the user</p> <code>List[np.ndarray]</code> <p>The second list contains a uir matrix for each split constituting the test set of the user</p> Source code in <code>clayrs/recsys/partitioning.py</code> <pre><code>def split_single(self, uir_user: np.ndarray) -&gt; Tuple[List[np.ndarray], List[np.ndarray]]:\n\"\"\"\n    Method which splits in $k$ splits both in *train set* and *test set* the ratings of a single user\n\n    Args:\n        uir_user: uir matrix containing interactions of a single user\n\n    Returns:\n        The first list contains a uir matrix for each split constituting the *train set* of the user\n\n        The second list contains a uir matrix for each split constituting the *test set* of the user\n    \"\"\"\n    split_result = self.__kf.split(uir_user)\n\n    user_train_list = []\n    user_test_list = []\n\n    # split_result contains index of the ratings which must constitutes train set and test set\n    for train_set_indexes, test_set_indexes in split_result:\n        user_interactions_train = uir_user[train_set_indexes]\n        user_interactions_test = uir_user[test_set_indexes]\n\n        user_train_list.append(user_interactions_train)\n        user_test_list.append(user_interactions_test)\n\n    return user_train_list, user_test_list\n</code></pre>"},{"location":"utils/report/","title":"Report class","text":""},{"location":"utils/report/#clayrs.utils.Report","title":"<code>Report(output_dir='.', ca_report_filename='ca_report', rs_report_filename='rs_report', eva_report_filename='eva_report')</code>","text":"<p>Class which will generate a YAML report for the whole experiment (or a part of it) depending on the objects passed to the <code>yaml()</code> function.</p> <p>A report will be generated for each module used (<code>Content Analyzer</code>, <code>RecSys</code>, <code>Evaluation</code>).</p> PARAMETER DESCRIPTION <code>output_dir</code> <p>Path of the folder where reports generated will be saved</p> <p> TYPE: <code>str</code> DEFAULT: <code>'.'</code> </p> <code>ca_report_filename</code> <p>Filename of the Content Analyzer report</p> <p> TYPE: <code>str</code> DEFAULT: <code>'ca_report'</code> </p> <code>rs_report_filename</code> <p>Filename of the Recsys report</p> <p> TYPE: <code>str</code> DEFAULT: <code>'rs_report'</code> </p> <code>eva_report_filename</code> <p>Filename of the evaluation report</p> <p> TYPE: <code>str</code> DEFAULT: <code>'eva_report'</code> </p> Source code in <code>clayrs/utils/report.py</code> <pre><code>def __init__(self, output_dir: str = '.',\n             ca_report_filename: str = 'ca_report',\n             rs_report_filename: str = 'rs_report',\n             eva_report_filename: str = 'eva_report'):\n\n    self._output_dir = output_dir\n    self._ca_report_filename = ca_report_filename\n    self._rs_report_filename = rs_report_filename\n    self._eva_report_filename = eva_report_filename\n</code></pre>"},{"location":"utils/report/#clayrs.utils.report.Report.yaml","title":"<code>yaml(content_analyzer=None, original_ratings=None, partitioning_technique=None, recsys=None, eval_model=None)</code>","text":"<p>Main module responsible of generating the <code>YAML</code> reports based on the objects passed to this function:</p> <ul> <li>If <code>content_analyzer</code> is set, then the report for the Content Analyzer will be produced</li> <li>If one between <code>original_ratings</code>, <code>partitioning_technique</code>, <code>recsys</code> is set, then the report for the recsys module will be produced.</li> <li>If <code>eval_model</code> is set, then the report for the evaluation module will be produced</li> </ul> <p>PLEASE NOTE: by setting the <code>recsys</code> parameter, the last experiment conducted will be documented! If no experiment is conducted in the current run, then a <code>ValueError</code> exception is raised!</p> <ul> <li>Same goes for the <code>eval_model</code></li> </ul> <p>Examples:</p> <ul> <li>Generate a report for the Content Analyzer module</li> </ul> <pre><code>&gt;&gt;&gt; from clayrs import content_analyzer as ca\n&gt;&gt;&gt; from clayrs import utils as ut\n&gt;&gt;&gt; # movies_ca_config = ...  # user defined configuration\n&gt;&gt;&gt; content_a = ca.ContentAnalyzer(movies_config)\n&gt;&gt;&gt; content_a.fit()  # generate and serialize contents\n&gt;&gt;&gt; ut.Report().yaml(content_analyzer=content_a)  # generate yaml\n</code></pre> <ul> <li>Generate a partial report for the RecSys module</li> </ul> <pre><code>&gt;&gt;&gt; from clayrs import utils as ut\n&gt;&gt;&gt; from clayrs import recsys as rs\n&gt;&gt;&gt; ratings = ca.Ratings(ca.CSVFile(ratings_path))\n&gt;&gt;&gt; pt = rs.HoldOutPartitioning()\n&gt;&gt;&gt; [train], [test] = pt.split_all(ratings)\n&gt;&gt;&gt; ut.Report().yaml(original_ratings=ratings, partitioning_technique=pt)\n</code></pre> <ul> <li>Generate a full report for the RecSys module and evaluation module</li> </ul> <pre><code>&gt;&gt;&gt; from clayrs import utils as ut\n&gt;&gt;&gt; from clayrs import recsys as rs\n&gt;&gt;&gt; from clayrs import evaluation as eva\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Generate recommendations\n&gt;&gt;&gt; ratings = ca.Ratings(ca.CSVFile(ratings_path))\n&gt;&gt;&gt; pt = rs.HoldOutPartitioning()\n&gt;&gt;&gt; [train], [test] = pt.split_all(ratings)\n&gt;&gt;&gt; alg = rs.CentroidVector()\n&gt;&gt;&gt; cbrs = rs.ContentBasedRS(alg, train_set=train, items_directory=items_path)\n&gt;&gt;&gt; rank = cbrs.fit_rank(test, n_recs=10)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Evaluate recommendations and generate report\n&gt;&gt;&gt; em = eva.EvalModel([rank], [test], metric_list=[eva.Precision(), eva.Recall()])\n&gt;&gt;&gt; ut.Report().yaml(original_ratings=ratings,\n&gt;&gt;&gt;                  partitioning_technique=pt,\n&gt;&gt;&gt;                  recsys=cbrs,\n&gt;&gt;&gt;                  eval_model=em)\n</code></pre> PARAMETER DESCRIPTION <code>content_analyzer</code> <p><code>ContentAnalyzer</code> object used to generate complex representation in the experiment</p> <p> TYPE: <code>ContentAnalyzer</code> DEFAULT: <code>None</code> </p> <code>original_ratings</code> <p><code>Ratings</code> object representing the original dataset</p> <p> TYPE: <code>Ratings</code> DEFAULT: <code>None</code> </p> <code>partitioning_technique</code> <p><code>Partitioning</code> object used to split the original dataset</p> <p> TYPE: <code>Partitioning</code> DEFAULT: <code>None</code> </p> <code>recsys</code> <p><code>RecSys</code> object used to produce recommendations/score predictions. Please note that the latest experiment run will be documented. If no experiment is run, then an exception is thrown</p> <p> TYPE: <code>RecSys</code> DEFAULT: <code>None</code> </p> <code>eval_model</code> <p><code>EvalModel</code> object used to evaluate predictions generated. Please note that the latest evaluation run will be documented. If no evaluation is run, then an exception is thrown</p> <p> TYPE: <code>EvalModel</code> DEFAULT: <code>None</code> </p> Source code in <code>clayrs/utils/report.py</code> <pre><code>def yaml(self, content_analyzer: ContentAnalyzer = None,\n         original_ratings: Ratings = None,\n         partitioning_technique: Partitioning = None,\n         recsys: RecSys = None,\n         eval_model: EvalModel = None):\n\"\"\"\n    Main module responsible of generating the `YAML` reports based on the objects passed to this function:\n\n    * If `content_analyzer` is set, then the report for the Content Analyzer will be produced\n    * If one between `original_ratings`, `partitioning_technique`, `recsys` is set, then the report for the recsys\n    module will be produced.\n    * If `eval_model` is set, then the report for the evaluation module will be produced\n\n    **PLEASE NOTE**: by setting the `recsys` parameter, the last experiment conducted will be documented! If no\n    experiment is conducted in the current run, then a `ValueError` exception is raised!\n\n    * Same goes for the `eval_model`\n\n    Examples:\n\n        * Generate a report for the Content Analyzer module\n        &gt;&gt;&gt; from clayrs import content_analyzer as ca\n        &gt;&gt;&gt; from clayrs import utils as ut\n        &gt;&gt;&gt; # movies_ca_config = ...  # user defined configuration\n        &gt;&gt;&gt; content_a = ca.ContentAnalyzer(movies_config)\n        &gt;&gt;&gt; content_a.fit()  # generate and serialize contents\n        &gt;&gt;&gt; ut.Report().yaml(content_analyzer=content_a)  # generate yaml\n\n        * Generate a partial report for the RecSys module\n        &gt;&gt;&gt; from clayrs import utils as ut\n        &gt;&gt;&gt; from clayrs import recsys as rs\n        &gt;&gt;&gt; ratings = ca.Ratings(ca.CSVFile(ratings_path))\n        &gt;&gt;&gt; pt = rs.HoldOutPartitioning()\n        &gt;&gt;&gt; [train], [test] = pt.split_all(ratings)\n        &gt;&gt;&gt; ut.Report().yaml(original_ratings=ratings, partitioning_technique=pt)\n\n        * Generate a full report for the RecSys module and evaluation module\n        &gt;&gt;&gt; from clayrs import utils as ut\n        &gt;&gt;&gt; from clayrs import recsys as rs\n        &gt;&gt;&gt; from clayrs import evaluation as eva\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Generate recommendations\n        &gt;&gt;&gt; ratings = ca.Ratings(ca.CSVFile(ratings_path))\n        &gt;&gt;&gt; pt = rs.HoldOutPartitioning()\n        &gt;&gt;&gt; [train], [test] = pt.split_all(ratings)\n        &gt;&gt;&gt; alg = rs.CentroidVector()\n        &gt;&gt;&gt; cbrs = rs.ContentBasedRS(alg, train_set=train, items_directory=items_path)\n        &gt;&gt;&gt; rank = cbrs.fit_rank(test, n_recs=10)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Evaluate recommendations and generate report\n        &gt;&gt;&gt; em = eva.EvalModel([rank], [test], metric_list=[eva.Precision(), eva.Recall()])\n        &gt;&gt;&gt; ut.Report().yaml(original_ratings=ratings,\n        &gt;&gt;&gt;                  partitioning_technique=pt,\n        &gt;&gt;&gt;                  recsys=cbrs,\n        &gt;&gt;&gt;                  eval_model=em)\n\n    Args:\n        content_analyzer: `ContentAnalyzer` object used to generate complex representation in the experiment\n        original_ratings: `Ratings` object representing the original dataset\n        partitioning_technique: `Partitioning` object used to split the original dataset\n        recsys: `RecSys` object used to produce recommendations/score predictions. Please note that the latest\n            experiment run will be documented. If no experiment is run, then an exception is thrown\n        eval_model: `EvalModel` object used to evaluate predictions generated. Please note that the latest\n            evaluation run will be documented. If no evaluation is run, then an exception is thrown\n    \"\"\"\n\n    def represent_none(self, _):\n        return self.represent_scalar('tag:yaml.org,2002:null', 'null')\n\n    def dump_yaml(output_dir, data):\n        with open(output_dir, 'w') as yaml_file:\n            pyaml.dump(data, yaml_file, sort_dicts=False, safe=True,)\n\n    # None values will be represented as 'null' in yaml file.\n    # without this, they will simply be represented as an empty string\n    pyaml.add_representer(type(None), represent_none)\n\n    if content_analyzer is not None:\n        ca_dict = self._report_ca_module(content_analyzer)\n\n        # create folder if it doesn't exist\n        Path(self.output_dir).mkdir(parents=True, exist_ok=True)\n\n        output_dir = os.path.join(self.output_dir, f'{self._ca_report_filename}.yml')\n        dump_yaml(output_dir, ca_dict)\n\n    if original_ratings is not None or partitioning_technique is not None or recsys is not None:\n        rs_dict = self._report_rs_module(original_ratings, partitioning_technique, recsys)\n\n        # create folder if it doesn't exist\n        Path(self.output_dir).mkdir(parents=True, exist_ok=True)\n\n        output_dir = os.path.join(self.output_dir, f'{self._rs_report_filename}.yml')\n        dump_yaml(output_dir, rs_dict)\n\n    if eval_model is not None:\n        eva_dict = self._report_eva_module(eval_model)\n\n        # create folder if it doesn't exist\n        Path(self.output_dir).mkdir(parents=True, exist_ok=True)\n\n        output_dir = os.path.join(self.output_dir, f'{self._eva_report_filename}.yml')\n        dump_yaml(output_dir, eva_dict)\n</code></pre>"}]}