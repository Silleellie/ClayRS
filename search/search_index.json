{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Warning Docs are still a WIP Welcome to ClayRS's documentation! ClayRS is a python framework for (mainly) content-based recommender systems which allows you to perform several operations, starting from a raw representation of users and items to building and evaluating a recommender system. It also supports graph-based recommendation with feature selection algorithms and graph manipulation methods. The framework has three main modules, which you can also use individually: Given a raw source, the Content Analyzer : Creates and serializes contents, Using the chosen configuration The RecSys module allows to: Instantiate a recommender system Using items and users serialized by the Content Analyzer Make score prediction or recommend items for the active user(s) The EvalModel has the task of evaluating a recommender system, using several state-of-the-art metrics Code examples for all three modules will follow in the Usage section","title":"Home"},{"location":"#welcome-to-clayrss-documentation","text":"ClayRS is a python framework for (mainly) content-based recommender systems which allows you to perform several operations, starting from a raw representation of users and items to building and evaluating a recommender system. It also supports graph-based recommendation with feature selection algorithms and graph manipulation methods. The framework has three main modules, which you can also use individually: Given a raw source, the Content Analyzer : Creates and serializes contents, Using the chosen configuration The RecSys module allows to: Instantiate a recommender system Using items and users serialized by the Content Analyzer Make score prediction or recommend items for the active user(s) The EvalModel has the task of evaluating a recommender system, using several state-of-the-art metrics Code examples for all three modules will follow in the Usage section","title":"Welcome to ClayRS's documentation!"},{"location":"content_analyzer/introduction/","text":"Introduction ClayRS is a python framework for (mainly) content-based recommender systems which allows you to perform several operations, starting from a raw representation of users and items to building and evaluating a recommender system. It also supports graph-based recommendation with feature selection algorithms and graph manipulation methods. The framework has three main modules, which you can also use individually: Given a raw source, the Content Analyzer : * Creates and serializes contents, * Using the chosen configuration The RecSys module allows to: * Instantiate a recommender system * Using items and users serialized by the Content Analyzer * Make score prediction or recommend items for the active user(s) The EvalModel has the task of evaluating a recommender system, using several state-of-the-art metrics Code examples for all three modules will follow in the Usage section","title":"Introduction"},{"location":"content_analyzer/introduction/#introduction","text":"ClayRS is a python framework for (mainly) content-based recommender systems which allows you to perform several operations, starting from a raw representation of users and items to building and evaluating a recommender system. It also supports graph-based recommendation with feature selection algorithms and graph manipulation methods. The framework has three main modules, which you can also use individually: Given a raw source, the Content Analyzer : * Creates and serializes contents, * Using the chosen configuration The RecSys module allows to: * Instantiate a recommender system * Using items and users serialized by the Content Analyzer * Make score prediction or recommend items for the active user(s) The EvalModel has the task of evaluating a recommender system, using several state-of-the-art metrics Code examples for all three modules will follow in the Usage section","title":"Introduction"},{"location":"content_analyzer/ratings/","text":"Ratings Ratings Class that imports the ratings Parameters: Name Type Description Default source RawInformationSource Source from which the ratings will be imported required rating_configs list<RatingsFieldConfig> required from_id_column str Name of the field containing the reference to the person who gave the rating (for example, the user id) required to_id_column str Name of the field containing the reference to the item that a person rated required timestamp_column str Name of the field containing the timestamp None output_directory str Name of the directory where the acquired ratings will be stored required score_combiner str Metric to use to combine the scores required Source code in clayrs/content_analyzer/ratings_manager/ratings.py 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 class Ratings : \"\"\" Class that imports the ratings Args: source (RawInformationSource): Source from which the ratings will be imported rating_configs (list<RatingsFieldConfig>): from_id_column (str): Name of the field containing the reference to the person who gave the rating (for example, the user id) to_id_column (str): Name of the field containing the reference to the item that a person rated timestamp_column (str): Name of the field containing the timestamp output_directory (str): Name of the directory where the acquired ratings will be stored score_combiner (str): Metric to use to combine the scores \"\"\" def __init__ ( self , source : RawInformationSource , user_id_column : Union [ str , int ] = 0 , item_id_column : Union [ str , int ] = 1 , score_column : Union [ str , int ] = 2 , timestamp_column : Union [ str , int ] = None , score_processor : RatingProcessor = None ): self . _ratings_dict = self . _import_ratings ( source , user_id_column , item_id_column , score_column , timestamp_column , score_processor ) @property @functools . lru_cache ( maxsize = 128 ) def user_id_column ( self ) -> list : return [ interaction . user_id for interaction in self ] @property @functools . lru_cache ( maxsize = 128 ) def item_id_column ( self ) -> list : return [ interaction . item_id for interaction in self ] @property @functools . lru_cache ( maxsize = 128 ) def score_column ( self ) -> list : return [ interaction . score for interaction in self ] @property @functools . lru_cache ( maxsize = 128 ) def timestamp_column ( self ) -> list : return [ interaction . timestamp for interaction in self if interaction . timestamp is not None ] @Handler_ScoreNotFloat def _import_ratings ( self , source : RawInformationSource , user_column : Union [ str , int ], item_column : Union [ str , int ], score_column : Union [ str , int ], timestamp_column : Union [ str , int ], score_processor : RatingProcessor ): \"\"\" Imports the ratings from the source and stores in a dataframe Returns: ratings_frame: pd.DataFrame \"\"\" ratings_dict = defaultdict ( list ) with get_progbar ( source ) as pbar : pbar . set_description ( desc = \"Importing ratings\" ) for row in pbar : user_id = self . _get_field_data ( user_column , row ) item_id = self . _get_field_data ( item_column , row ) score = self . _get_field_data ( score_column , row ) timestamp = None if score_processor is not None : score = score_processor . fit ( score ) else : score = float ( score ) if timestamp_column is not None : timestamp = self . _get_field_data ( timestamp_column , row ) ratings_dict [ user_id ] . append ( Interaction ( user_id , item_id , score , timestamp )) # re-hashing return dict ( ratings_dict ) def get_user_interactions ( self , user_id : str , head : int = None ): return self . _ratings_dict [ user_id ][: head ] def filter_ratings ( self , user_list : Iterable [ str ]): filtered_ratings_generator = (( user , self . _ratings_dict [ user ]) for user in user_list ) return self . from_dict ( filtered_ratings_generator ) def take_head_all ( self , head : int ): ratings_cut_generator = (( user_id , user_ratings [: head ]) for user_id , user_ratings in zip ( self . _ratings_dict . keys (), self . _ratings_dict . values ())) return self . from_dict ( ratings_cut_generator ) # @Handler_ScoreNotFloat # def add_score_column(self, score_column: Union[str, int], column_name: str, # score_processor: RatingProcessor = None): # # col_to_add = [self._get_field_data(score_column, row) for row in self._source] # # if score_processor: # col_to_add = score_processor.fit(col_to_add) # else: # col_to_add = [float(score) for score in col_to_add] # # self._ratings_frame[column_name] = col_to_add # # start_ratings_user = 0 # for user_id, user_ratings in zip(self._ratings_dict.keys(), self._ratings_dict.values()): # first_range_val = start_ratings_user # second_range_val = start_ratings_user + len(user_ratings) # # score_to_add = col_to_add[first_range_val:second_range_val] # new_ratings = [rating_tuple + (added_score,) for rating_tuple, added_score in # zip(user_ratings, score_to_add)] # self._ratings_dict[user_id] = new_ratings # # start_ratings_user += len(user_ratings) def to_dataframe ( self ): will_be_frame = { 'user_id' : self . user_id_column , 'item_id' : self . item_id_column , 'score' : self . score_column } if len ( self . timestamp_column ) != 0 : will_be_frame [ 'timestamp' ] = self . timestamp_column return pd . DataFrame ( will_be_frame ) def to_csv ( self , output_directory : str = '.' , file_name : str = 'ratings_frame' , overwrite : bool = False ): Path ( output_directory ) . mkdir ( parents = True , exist_ok = True ) file_name = get_valid_filename ( output_directory , file_name , 'csv' , overwrite ) frame = self . to_dataframe () frame . to_csv ( os . path . join ( output_directory , file_name ), index = False , header = True ) @staticmethod def _get_field_data ( field_name : Union [ str , int ], row : Dict ): try : if isinstance ( field_name , str ): data = row [ field_name ] else : row_keys = list ( row . keys ()) key = row_keys [ field_name ] data = row [ key ] except KeyError : raise KeyError ( \"Column {} not found in the raw source\" . format ( field_name )) except IndexError : raise IndexError ( \"Column index {} not present in the raw source\" . format ( field_name )) return str ( data ) @classmethod def from_dataframe ( cls , interaction_frame : pd . DataFrame , user_column : Union [ str , int ] = 0 , item_column : Union [ str , int ] = 1 , score_column : Union [ str , int ] = 2 , timestamp_column : Union [ str , int ] = None ): def get_value_row_df ( row , column , dtype ): try : if isinstance ( column , str ): value = row [ column ] else : # it's an int, so we get the column id and then we get the corresponding value in the row key_dict = interaction_frame . columns [ column ] value = row [ key_dict ] except ( KeyError , IndexError ) as e : if isinstance ( e , KeyError ): raise KeyError ( f \"Column { column } not found in interaction frame!\" ) else : raise IndexError ( f \"Column { column } not found in interaction frame!\" ) return dtype ( value ) if value is not None else None obj = cls . __new__ ( cls ) # Does not call __init__ super ( Ratings , obj ) . __init__ () # Don't forget to call any polymorphic base class initializers ratings_dict = defaultdict ( list ) if not interaction_frame . empty : for row in interaction_frame . to_dict ( orient = 'records' ): user_id = get_value_row_df ( row , user_column , str ) item_id = get_value_row_df ( row , item_column , str ) score = get_value_row_df ( row , score_column , float ) timestamp = get_value_row_df ( row , timestamp_column , str ) if timestamp_column is not None else None ratings_dict [ user_id ] . append ( Interaction ( user_id , item_id , score , timestamp )) obj . _ratings_dict = dict ( ratings_dict ) return obj @classmethod def from_list ( cls , interaction_list : Union [ List [ Interaction ], Generator ]): obj = cls . __new__ ( cls ) # Does not call __init__ super ( Ratings , obj ) . __init__ () # Don't forget to call any polymorphic base class initializers ratings_dict = defaultdict ( list ) for interaction in interaction_list : ratings_dict [ interaction . user_id ] . append ( interaction ) obj . _ratings_dict = dict ( ratings_dict ) return obj @classmethod def from_dict ( cls , interaction_dict : Union [ Dict [ str , List [ Interaction ]], Generator ]): obj = cls . __new__ ( cls ) # Does not call __init__ super ( Ratings , obj ) . __init__ () # Don't forget to call any polymorphic base class initializers obj . _ratings_dict = dict ( interaction_dict ) return obj def __len__ ( self ): # all columns have same length, so only one is needed in order # to check what is the length return len ( self . user_id_column ) def __str__ ( self ): return str ( self . to_dataframe ()) def __repr__ ( self ): return repr ( self . _ratings_dict ) def __iter__ ( self ): yield from itertools . chain . from_iterable ( self . _ratings_dict . values ()) RatingsLowMemory Source code in clayrs/content_analyzer/ratings_manager/ratings.py 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 class RatingsLowMemory : def __init__ ( self , source : RawInformationSource , user_id_column : Union [ str , int ] = 0 , item_id_column : Union [ str , int ] = 1 , score_column : Union [ str , int ] = 2 , timestamp_column : Union [ str , int ] = None , score_processor : RatingProcessor = None ): rat = pd . DataFrame ( source , dtype = str ) self . _ratings_dict = self . _import_ratings ( rat , user_id_column , item_id_column , score_column , timestamp_column , score_processor ) @property @functools . lru_cache ( maxsize = 128 ) def user_id_column ( self ) -> list : return self . _ratings_dict . index . get_level_values ( 'user_id' ) . tolist () @property @functools . lru_cache ( maxsize = 128 ) def item_id_column ( self ) -> list : return self . _ratings_dict . index . get_level_values ( 'item_id' ) . tolist () @property @functools . lru_cache ( maxsize = 128 ) def score_column ( self ) -> list : return self . _ratings_dict [ 'score' ] . tolist () @property @functools . lru_cache ( maxsize = 128 ) def timestamp_column ( self ) -> list : timestamp_list = self . _ratings_dict [ 'timestamp' ] . tolist () return timestamp_list if all ( timestamp is not None for timestamp in timestamp_list ) else [] @Handler_ScoreNotFloat def _import_ratings ( self , rat : pd . DataFrame , user_column : Union [ str , int ], item_column : Union [ str , int ], score_column : Union [ str , int ], timestamp_column : Union [ str , int ], score_processor : RatingProcessor ): \"\"\" Imports the ratings from the source and stores in a dataframe Returns: ratings_frame: pd.DataFrame \"\"\" if isinstance ( user_column , int ): user_column = rat . columns [ user_column ] if isinstance ( item_column , int ): item_column = rat . columns [ item_column ] if isinstance ( score_column , int ): score_column = rat . columns [ score_column ] if isinstance ( timestamp_column , int ): timestamp_column = rat . columns [ timestamp_column ] elif timestamp_column is None : rat [ 'timestamp' ] = None timestamp_column = 'timestamp' index = pd . MultiIndex . from_tuples ( zip ( rat [ user_column ] . values , rat [ item_column ] . values ), names = [ \"user_id\" , \"item_id\" ]) rat = rat [[ score_column , timestamp_column ]] . set_index ( index ) rat . columns = [ 'score' , 'timestamp' ] rat [ 'score' ] = pd . to_numeric ( rat [ 'score' ]) return rat def get_user_interactions ( self , user_id : str , head : int = None ): user_rat = self . _ratings_dict . loc [ user_id ][: head ] user_rat = [ Interaction ( user_id , index_item , row [ 0 ], row [ 1 ]) for index_item , row in zip ( user_rat . index , user_rat . values )] return user_rat def filter_ratings ( self , user_list : Iterable [ str ]): filtered_df = self . _ratings_dict . loc [ ( self . _ratings_dict . index . get_level_values ( 'user_id' ) . isin ( set ( user_list )))] filtered_df = filtered_df . reset_index ( drop = False ) return self . from_dataframe ( filtered_df , user_column = 'user_id' , item_column = 'item_id' , score_column = 'score' , timestamp_column = 'timestamp' ) def take_head_all ( self , head : int ): filtered_df = self . _ratings_dict . groupby ( level = 'user_id' ) . head ( head ) filtered_df = filtered_df . reset_index ( drop = False ) return self . from_dataframe ( filtered_df , user_column = 'user_id' , item_column = 'item_id' , score_column = 'score' , timestamp_column = 'timestamp' ) @classmethod def from_dataframe ( cls , interaction_frame : pd . DataFrame , user_column : Union [ str , int ] = 0 , item_column : Union [ str , int ] = 1 , score_column : Union [ str , int ] = 2 , timestamp_column : Union [ str , int ] = None ): obj = cls . __new__ ( cls ) # Does not call __init__ super ( RatingsLowMemory , obj ) . __init__ () # Don't forget to call any polymorphic base class initializers ratings_dict = cls . _import_ratings ( obj , interaction_frame , user_column , item_column , score_column , timestamp_column , None ) obj . _ratings_dict = ratings_dict return obj @classmethod def from_list ( cls , interaction_list : List [ Interaction ]): obj = cls . __new__ ( cls ) # Does not call __init__ super ( RatingsLowMemory , obj ) . __init__ () # Don't forget to call any polymorphic base class initializers user_column_iterator = ( interaction . user_id for interaction in interaction_list ) item_column_iterator = ( interaction . item_id for interaction in interaction_list ) index = pd . MultiIndex . from_tuples ( zip ( user_column_iterator , item_column_iterator ), names = [ \"user_id\" , \"item_id\" ]) score_timestamp_iterator = ({ 'score' : interaction . score , 'timestamp' : interaction . timestamp } for interaction in interaction_list ) rat = pd . DataFrame ( score_timestamp_iterator , index = index ) obj . _ratings_dict = rat return obj @classmethod def from_dict ( cls , interaction_dict : Dict [ str , List [ Interaction ]]): obj = cls . __new__ ( cls ) # Does not call __init__ super ( RatingsLowMemory , obj ) . __init__ () # Don't forget to call any polymorphic base class initializers obj . _ratings_dict = dict ( interaction_dict ) return obj def __iter__ ( self ): yield from itertools . chain . from_iterable ( self . get_user_interactions ( user_id ) for user_id in self . _ratings_dict . index . get_level_values ( 'user_id' ) . unique ()) def __len__ ( self ): return len ( self . _ratings_dict )","title":"Ratings"},{"location":"content_analyzer/ratings/#ratings","text":"","title":"Ratings"},{"location":"content_analyzer/ratings/#clayrs.content_analyzer.ratings_manager.ratings.Ratings","text":"Class that imports the ratings Parameters: Name Type Description Default source RawInformationSource Source from which the ratings will be imported required rating_configs list<RatingsFieldConfig> required from_id_column str Name of the field containing the reference to the person who gave the rating (for example, the user id) required to_id_column str Name of the field containing the reference to the item that a person rated required timestamp_column str Name of the field containing the timestamp None output_directory str Name of the directory where the acquired ratings will be stored required score_combiner str Metric to use to combine the scores required Source code in clayrs/content_analyzer/ratings_manager/ratings.py 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 class Ratings : \"\"\" Class that imports the ratings Args: source (RawInformationSource): Source from which the ratings will be imported rating_configs (list<RatingsFieldConfig>): from_id_column (str): Name of the field containing the reference to the person who gave the rating (for example, the user id) to_id_column (str): Name of the field containing the reference to the item that a person rated timestamp_column (str): Name of the field containing the timestamp output_directory (str): Name of the directory where the acquired ratings will be stored score_combiner (str): Metric to use to combine the scores \"\"\" def __init__ ( self , source : RawInformationSource , user_id_column : Union [ str , int ] = 0 , item_id_column : Union [ str , int ] = 1 , score_column : Union [ str , int ] = 2 , timestamp_column : Union [ str , int ] = None , score_processor : RatingProcessor = None ): self . _ratings_dict = self . _import_ratings ( source , user_id_column , item_id_column , score_column , timestamp_column , score_processor ) @property @functools . lru_cache ( maxsize = 128 ) def user_id_column ( self ) -> list : return [ interaction . user_id for interaction in self ] @property @functools . lru_cache ( maxsize = 128 ) def item_id_column ( self ) -> list : return [ interaction . item_id for interaction in self ] @property @functools . lru_cache ( maxsize = 128 ) def score_column ( self ) -> list : return [ interaction . score for interaction in self ] @property @functools . lru_cache ( maxsize = 128 ) def timestamp_column ( self ) -> list : return [ interaction . timestamp for interaction in self if interaction . timestamp is not None ] @Handler_ScoreNotFloat def _import_ratings ( self , source : RawInformationSource , user_column : Union [ str , int ], item_column : Union [ str , int ], score_column : Union [ str , int ], timestamp_column : Union [ str , int ], score_processor : RatingProcessor ): \"\"\" Imports the ratings from the source and stores in a dataframe Returns: ratings_frame: pd.DataFrame \"\"\" ratings_dict = defaultdict ( list ) with get_progbar ( source ) as pbar : pbar . set_description ( desc = \"Importing ratings\" ) for row in pbar : user_id = self . _get_field_data ( user_column , row ) item_id = self . _get_field_data ( item_column , row ) score = self . _get_field_data ( score_column , row ) timestamp = None if score_processor is not None : score = score_processor . fit ( score ) else : score = float ( score ) if timestamp_column is not None : timestamp = self . _get_field_data ( timestamp_column , row ) ratings_dict [ user_id ] . append ( Interaction ( user_id , item_id , score , timestamp )) # re-hashing return dict ( ratings_dict ) def get_user_interactions ( self , user_id : str , head : int = None ): return self . _ratings_dict [ user_id ][: head ] def filter_ratings ( self , user_list : Iterable [ str ]): filtered_ratings_generator = (( user , self . _ratings_dict [ user ]) for user in user_list ) return self . from_dict ( filtered_ratings_generator ) def take_head_all ( self , head : int ): ratings_cut_generator = (( user_id , user_ratings [: head ]) for user_id , user_ratings in zip ( self . _ratings_dict . keys (), self . _ratings_dict . values ())) return self . from_dict ( ratings_cut_generator ) # @Handler_ScoreNotFloat # def add_score_column(self, score_column: Union[str, int], column_name: str, # score_processor: RatingProcessor = None): # # col_to_add = [self._get_field_data(score_column, row) for row in self._source] # # if score_processor: # col_to_add = score_processor.fit(col_to_add) # else: # col_to_add = [float(score) for score in col_to_add] # # self._ratings_frame[column_name] = col_to_add # # start_ratings_user = 0 # for user_id, user_ratings in zip(self._ratings_dict.keys(), self._ratings_dict.values()): # first_range_val = start_ratings_user # second_range_val = start_ratings_user + len(user_ratings) # # score_to_add = col_to_add[first_range_val:second_range_val] # new_ratings = [rating_tuple + (added_score,) for rating_tuple, added_score in # zip(user_ratings, score_to_add)] # self._ratings_dict[user_id] = new_ratings # # start_ratings_user += len(user_ratings) def to_dataframe ( self ): will_be_frame = { 'user_id' : self . user_id_column , 'item_id' : self . item_id_column , 'score' : self . score_column } if len ( self . timestamp_column ) != 0 : will_be_frame [ 'timestamp' ] = self . timestamp_column return pd . DataFrame ( will_be_frame ) def to_csv ( self , output_directory : str = '.' , file_name : str = 'ratings_frame' , overwrite : bool = False ): Path ( output_directory ) . mkdir ( parents = True , exist_ok = True ) file_name = get_valid_filename ( output_directory , file_name , 'csv' , overwrite ) frame = self . to_dataframe () frame . to_csv ( os . path . join ( output_directory , file_name ), index = False , header = True ) @staticmethod def _get_field_data ( field_name : Union [ str , int ], row : Dict ): try : if isinstance ( field_name , str ): data = row [ field_name ] else : row_keys = list ( row . keys ()) key = row_keys [ field_name ] data = row [ key ] except KeyError : raise KeyError ( \"Column {} not found in the raw source\" . format ( field_name )) except IndexError : raise IndexError ( \"Column index {} not present in the raw source\" . format ( field_name )) return str ( data ) @classmethod def from_dataframe ( cls , interaction_frame : pd . DataFrame , user_column : Union [ str , int ] = 0 , item_column : Union [ str , int ] = 1 , score_column : Union [ str , int ] = 2 , timestamp_column : Union [ str , int ] = None ): def get_value_row_df ( row , column , dtype ): try : if isinstance ( column , str ): value = row [ column ] else : # it's an int, so we get the column id and then we get the corresponding value in the row key_dict = interaction_frame . columns [ column ] value = row [ key_dict ] except ( KeyError , IndexError ) as e : if isinstance ( e , KeyError ): raise KeyError ( f \"Column { column } not found in interaction frame!\" ) else : raise IndexError ( f \"Column { column } not found in interaction frame!\" ) return dtype ( value ) if value is not None else None obj = cls . __new__ ( cls ) # Does not call __init__ super ( Ratings , obj ) . __init__ () # Don't forget to call any polymorphic base class initializers ratings_dict = defaultdict ( list ) if not interaction_frame . empty : for row in interaction_frame . to_dict ( orient = 'records' ): user_id = get_value_row_df ( row , user_column , str ) item_id = get_value_row_df ( row , item_column , str ) score = get_value_row_df ( row , score_column , float ) timestamp = get_value_row_df ( row , timestamp_column , str ) if timestamp_column is not None else None ratings_dict [ user_id ] . append ( Interaction ( user_id , item_id , score , timestamp )) obj . _ratings_dict = dict ( ratings_dict ) return obj @classmethod def from_list ( cls , interaction_list : Union [ List [ Interaction ], Generator ]): obj = cls . __new__ ( cls ) # Does not call __init__ super ( Ratings , obj ) . __init__ () # Don't forget to call any polymorphic base class initializers ratings_dict = defaultdict ( list ) for interaction in interaction_list : ratings_dict [ interaction . user_id ] . append ( interaction ) obj . _ratings_dict = dict ( ratings_dict ) return obj @classmethod def from_dict ( cls , interaction_dict : Union [ Dict [ str , List [ Interaction ]], Generator ]): obj = cls . __new__ ( cls ) # Does not call __init__ super ( Ratings , obj ) . __init__ () # Don't forget to call any polymorphic base class initializers obj . _ratings_dict = dict ( interaction_dict ) return obj def __len__ ( self ): # all columns have same length, so only one is needed in order # to check what is the length return len ( self . user_id_column ) def __str__ ( self ): return str ( self . to_dataframe ()) def __repr__ ( self ): return repr ( self . _ratings_dict ) def __iter__ ( self ): yield from itertools . chain . from_iterable ( self . _ratings_dict . values ())","title":"Ratings"},{"location":"content_analyzer/ratings/#clayrs.content_analyzer.ratings_manager.ratings.RatingsLowMemory","text":"Source code in clayrs/content_analyzer/ratings_manager/ratings.py 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 class RatingsLowMemory : def __init__ ( self , source : RawInformationSource , user_id_column : Union [ str , int ] = 0 , item_id_column : Union [ str , int ] = 1 , score_column : Union [ str , int ] = 2 , timestamp_column : Union [ str , int ] = None , score_processor : RatingProcessor = None ): rat = pd . DataFrame ( source , dtype = str ) self . _ratings_dict = self . _import_ratings ( rat , user_id_column , item_id_column , score_column , timestamp_column , score_processor ) @property @functools . lru_cache ( maxsize = 128 ) def user_id_column ( self ) -> list : return self . _ratings_dict . index . get_level_values ( 'user_id' ) . tolist () @property @functools . lru_cache ( maxsize = 128 ) def item_id_column ( self ) -> list : return self . _ratings_dict . index . get_level_values ( 'item_id' ) . tolist () @property @functools . lru_cache ( maxsize = 128 ) def score_column ( self ) -> list : return self . _ratings_dict [ 'score' ] . tolist () @property @functools . lru_cache ( maxsize = 128 ) def timestamp_column ( self ) -> list : timestamp_list = self . _ratings_dict [ 'timestamp' ] . tolist () return timestamp_list if all ( timestamp is not None for timestamp in timestamp_list ) else [] @Handler_ScoreNotFloat def _import_ratings ( self , rat : pd . DataFrame , user_column : Union [ str , int ], item_column : Union [ str , int ], score_column : Union [ str , int ], timestamp_column : Union [ str , int ], score_processor : RatingProcessor ): \"\"\" Imports the ratings from the source and stores in a dataframe Returns: ratings_frame: pd.DataFrame \"\"\" if isinstance ( user_column , int ): user_column = rat . columns [ user_column ] if isinstance ( item_column , int ): item_column = rat . columns [ item_column ] if isinstance ( score_column , int ): score_column = rat . columns [ score_column ] if isinstance ( timestamp_column , int ): timestamp_column = rat . columns [ timestamp_column ] elif timestamp_column is None : rat [ 'timestamp' ] = None timestamp_column = 'timestamp' index = pd . MultiIndex . from_tuples ( zip ( rat [ user_column ] . values , rat [ item_column ] . values ), names = [ \"user_id\" , \"item_id\" ]) rat = rat [[ score_column , timestamp_column ]] . set_index ( index ) rat . columns = [ 'score' , 'timestamp' ] rat [ 'score' ] = pd . to_numeric ( rat [ 'score' ]) return rat def get_user_interactions ( self , user_id : str , head : int = None ): user_rat = self . _ratings_dict . loc [ user_id ][: head ] user_rat = [ Interaction ( user_id , index_item , row [ 0 ], row [ 1 ]) for index_item , row in zip ( user_rat . index , user_rat . values )] return user_rat def filter_ratings ( self , user_list : Iterable [ str ]): filtered_df = self . _ratings_dict . loc [ ( self . _ratings_dict . index . get_level_values ( 'user_id' ) . isin ( set ( user_list )))] filtered_df = filtered_df . reset_index ( drop = False ) return self . from_dataframe ( filtered_df , user_column = 'user_id' , item_column = 'item_id' , score_column = 'score' , timestamp_column = 'timestamp' ) def take_head_all ( self , head : int ): filtered_df = self . _ratings_dict . groupby ( level = 'user_id' ) . head ( head ) filtered_df = filtered_df . reset_index ( drop = False ) return self . from_dataframe ( filtered_df , user_column = 'user_id' , item_column = 'item_id' , score_column = 'score' , timestamp_column = 'timestamp' ) @classmethod def from_dataframe ( cls , interaction_frame : pd . DataFrame , user_column : Union [ str , int ] = 0 , item_column : Union [ str , int ] = 1 , score_column : Union [ str , int ] = 2 , timestamp_column : Union [ str , int ] = None ): obj = cls . __new__ ( cls ) # Does not call __init__ super ( RatingsLowMemory , obj ) . __init__ () # Don't forget to call any polymorphic base class initializers ratings_dict = cls . _import_ratings ( obj , interaction_frame , user_column , item_column , score_column , timestamp_column , None ) obj . _ratings_dict = ratings_dict return obj @classmethod def from_list ( cls , interaction_list : List [ Interaction ]): obj = cls . __new__ ( cls ) # Does not call __init__ super ( RatingsLowMemory , obj ) . __init__ () # Don't forget to call any polymorphic base class initializers user_column_iterator = ( interaction . user_id for interaction in interaction_list ) item_column_iterator = ( interaction . item_id for interaction in interaction_list ) index = pd . MultiIndex . from_tuples ( zip ( user_column_iterator , item_column_iterator ), names = [ \"user_id\" , \"item_id\" ]) score_timestamp_iterator = ({ 'score' : interaction . score , 'timestamp' : interaction . timestamp } for interaction in interaction_list ) rat = pd . DataFrame ( score_timestamp_iterator , index = index ) obj . _ratings_dict = rat return obj @classmethod def from_dict ( cls , interaction_dict : Dict [ str , List [ Interaction ]]): obj = cls . __new__ ( cls ) # Does not call __init__ super ( RatingsLowMemory , obj ) . __init__ () # Don't forget to call any polymorphic base class initializers obj . _ratings_dict = dict ( interaction_dict ) return obj def __iter__ ( self ): yield from itertools . chain . from_iterable ( self . get_user_interactions ( user_id ) for user_id in self . _ratings_dict . index . get_level_values ( 'user_id' ) . unique ()) def __len__ ( self ): return len ( self . _ratings_dict )","title":"RatingsLowMemory"},{"location":"evaluation/eval_model/","text":"Eval Model class EvalModel Class for evaluating a recommender system. The Evaluation module needs the following parameters: A list of computed rank/predictions (in case multiple splits must be evaluated) A list of truths (in case multiple splits must be evaluated) List of metrics to compute Obviously the list of computed rank/predictions and list of truths must have the same length, and the rank/prediction in position \\(i\\) will be compared with the truth at position \\(i\\) Examples: >>> import clayrs.evaluation as eva >>> >>> em = eva . EvalModel ( >>> pred_list = rank_list , >>> truth_list = truth_list , >>> metric_list = [ >>> eva . NDCG (), >>> eva . Precision () >>> eva . RecallAtK ( k = 5 , sys_average = 'micro' ) >>> ] >>> ) Then call the fit() method of the instantiated EvalModel to perform the actual evaluation Parameters: Name Type Description Default pred_list Union [ List [ Prediction ], List [ Rank ]] Recommendations list to evaluate. It's a list in case multiple splits must be evaluated. Both Rank objects (where items are ordered and the score is not relevant) or Prediction objects (where the score predicted is the predicted rating for the user regarding a certain item) can be evaluated required truth_list List [ Ratings ] Ground truths list used to compare recommendations. It's a list in case multiple splits must be evaluated. required metric_list List [ Metric ] List of metrics that will be used to evaluate recommendation list specified required Source code in clayrs/evaluation/eval_model.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 class EvalModel : \"\"\" Class for evaluating a recommender system. The Evaluation module needs the following parameters: * A list of computed rank/predictions (in case multiple splits must be evaluated) * A list of truths (in case multiple splits must be evaluated) * List of metrics to compute Obviously the list of computed rank/predictions and list of truths must have the same length, and the rank/prediction in position $i$ will be compared with the truth at position $i$ Examples: >>> import clayrs.evaluation as eva >>> >>> em = eva.EvalModel( >>> pred_list=rank_list, >>> truth_list=truth_list, >>> metric_list=[ >>> eva.NDCG(), >>> eva.Precision() >>> eva.RecallAtK(k=5, sys_average='micro') >>> ] >>> ) Then call the fit() method of the instantiated EvalModel to perform the actual evaluation Args: pred_list: Recommendations list to evaluate. It's a list in case multiple splits must be evaluated. Both Rank objects (where items are ordered and the score is not relevant) or Prediction objects (where the score predicted is the predicted rating for the user regarding a certain item) can be evaluated truth_list: Ground truths list used to compare recommendations. It's a list in case multiple splits must be evaluated. metric_list: List of metrics that will be used to evaluate recommendation list specified Raises: ValueError in case the pred_list and truth_list are empty or have different length \"\"\" def __init__ ( self , pred_list : Union [ List [ Prediction ], List [ Rank ]], truth_list : List [ Ratings ], metric_list : List [ Metric ]): if len ( pred_list ) == 0 and len ( truth_list ) == 0 : raise ValueError ( \"List containing predictions and list containing ground truths are empty!\" ) elif len ( pred_list ) != len ( truth_list ): raise ValueError ( \"List containing predictions and list containing ground truths must have the same length!\" ) self . _pred_list = pred_list self . _truth_list = truth_list self . _metric_list = metric_list self . _yaml_report_result = None @property def pred_list ( self ) -> Union [ List [ Prediction ], List [ Rank ]]: \"\"\" List containing recommendations frame Returns: The list containing recommendations frame \"\"\" return self . _pred_list @property def truth_list ( self ) -> List [ Ratings ]: \"\"\" List containing ground truths Returns: The list containing ground truths \"\"\" return self . _truth_list @property def metric_list ( self ) -> List [ Metric ]: \"\"\" List of metrics used to evaluate recommendation lists Returns: The list containing all metrics \"\"\" return self . _metric_list def append_metric ( self , metric : Metric ): \"\"\" Append a metric to the metric list that will be used to evaluate recommendation lists Args: metric: Metric to append to the metric list \"\"\" self . _metric_list . append ( metric ) def fit ( self , user_id_list : list = None ) -> Tuple [ pd . DataFrame , pd . DataFrame ]: \"\"\" This method performs the actual evaluation of the recommendation frames passed as input in the constructor of the class In case you want to perform evaluation for selected users, specify their ids parameter of this method. Otherwise, all users in the recommendation frames will be considered in the evaluation process Examples: >>> import clayrs.evaluation as eva >>> selected_users = ['u1', 'u22', 'u3'] # (1) >>> em = eva.EvalModel( >>> pred_list, >>> truth_list, >>> metric_list=[eva.Precision(), eva.Recall()] >>> ) >>> em.fit(selected_users) The method returns two pandas DataFrame: one containing ***system results*** for every metric in the metric list, one containing ***users results*** for every metric eligible Returns: The first DataFrame contains the **system result** for every metric inside the metric_list The second DataFrame contains every **user results** for every metric eligible inside the metric_list \"\"\" logger . info ( 'Performing evaluation on metrics chosen' ) pred_list = self . _pred_list truth_list = self . _truth_list if user_id_list is not None : user_id_list_set = set ([ str ( user_id ) for user_id in user_id_list ]) pred_list = [ pred . filter_ratings ( user_id_list_set ) for pred in self . _pred_list ] truth_list = [ truth . filter_ratings ( user_id_list_set ) for truth in self . _truth_list ] sys_result , users_result = MetricEvaluator ( pred_list , truth_list ) . eval_metrics ( self . metric_list ) # we save the sys result for report yaml self . _yaml_report_result = sys_result . to_dict ( orient = 'index' ) return sys_result , users_result def __repr__ ( self ): return f 'EvalModel(pred_list= { self . _pred_list } , truth_list= { self . _truth_list } ,' \\ f ' metric_list= { self . _metric_list } ' append_metric ( metric ) Append a metric to the metric list that will be used to evaluate recommendation lists Parameters: Name Type Description Default metric Metric Metric to append to the metric list required Source code in clayrs/evaluation/eval_model.py 97 98 99 100 101 102 103 104 def append_metric ( self , metric : Metric ): \"\"\" Append a metric to the metric list that will be used to evaluate recommendation lists Args: metric: Metric to append to the metric list \"\"\" self . _metric_list . append ( metric ) fit ( user_id_list = None ) This method performs the actual evaluation of the recommendation frames passed as input in the constructor of the class In case you want to perform evaluation for selected users, specify their ids parameter of this method. Otherwise, all users in the recommendation frames will be considered in the evaluation process Examples: >>> import clayrs.evaluation as eva >>> selected_users = [ 'u1' , 'u22' , 'u3' ] # (1) >>> em = eva . EvalModel ( >>> pred_list , >>> truth_list , >>> metric_list = [ eva . Precision (), eva . Recall ()] >>> ) >>> em . fit ( selected_users ) The method returns two pandas DataFrame: one containing system results for every metric in the metric list, one containing users results for every metric eligible Returns: Type Description pd . DataFrame The first DataFrame contains the system result for every metric inside the metric_list pd . DataFrame The second DataFrame contains every user results for every metric eligible inside the metric_list Source code in clayrs/evaluation/eval_model.py 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 def fit ( self , user_id_list : list = None ) -> Tuple [ pd . DataFrame , pd . DataFrame ]: \"\"\" This method performs the actual evaluation of the recommendation frames passed as input in the constructor of the class In case you want to perform evaluation for selected users, specify their ids parameter of this method. Otherwise, all users in the recommendation frames will be considered in the evaluation process Examples: >>> import clayrs.evaluation as eva >>> selected_users = ['u1', 'u22', 'u3'] # (1) >>> em = eva.EvalModel( >>> pred_list, >>> truth_list, >>> metric_list=[eva.Precision(), eva.Recall()] >>> ) >>> em.fit(selected_users) The method returns two pandas DataFrame: one containing ***system results*** for every metric in the metric list, one containing ***users results*** for every metric eligible Returns: The first DataFrame contains the **system result** for every metric inside the metric_list The second DataFrame contains every **user results** for every metric eligible inside the metric_list \"\"\" logger . info ( 'Performing evaluation on metrics chosen' ) pred_list = self . _pred_list truth_list = self . _truth_list if user_id_list is not None : user_id_list_set = set ([ str ( user_id ) for user_id in user_id_list ]) pred_list = [ pred . filter_ratings ( user_id_list_set ) for pred in self . _pred_list ] truth_list = [ truth . filter_ratings ( user_id_list_set ) for truth in self . _truth_list ] sys_result , users_result = MetricEvaluator ( pred_list , truth_list ) . eval_metrics ( self . metric_list ) # we save the sys result for report yaml self . _yaml_report_result = sys_result . to_dict ( orient = 'index' ) return sys_result , users_result metric_list () property List of metrics used to evaluate recommendation lists Returns: Type Description List [ Metric ] The list containing all metrics Source code in clayrs/evaluation/eval_model.py 87 88 89 90 91 92 93 94 95 @property def metric_list ( self ) -> List [ Metric ]: \"\"\" List of metrics used to evaluate recommendation lists Returns: The list containing all metrics \"\"\" return self . _metric_list pred_list () property List containing recommendations frame Returns: Type Description Union [ List [ Prediction ], List [ Rank ]] The list containing recommendations frame Source code in clayrs/evaluation/eval_model.py 67 68 69 70 71 72 73 74 75 @property def pred_list ( self ) -> Union [ List [ Prediction ], List [ Rank ]]: \"\"\" List containing recommendations frame Returns: The list containing recommendations frame \"\"\" return self . _pred_list truth_list () property List containing ground truths Returns: Type Description List [ Ratings ] The list containing ground truths Source code in clayrs/evaluation/eval_model.py 77 78 79 80 81 82 83 84 85 @property def truth_list ( self ) -> List [ Ratings ]: \"\"\" List containing ground truths Returns: The list containing ground truths \"\"\" return self . _truth_list","title":"Ttest"},{"location":"evaluation/eval_model/#eval-model-class","text":"","title":"Eval Model class"},{"location":"evaluation/eval_model/#clayrs.evaluation.eval_model.EvalModel","text":"Class for evaluating a recommender system. The Evaluation module needs the following parameters: A list of computed rank/predictions (in case multiple splits must be evaluated) A list of truths (in case multiple splits must be evaluated) List of metrics to compute Obviously the list of computed rank/predictions and list of truths must have the same length, and the rank/prediction in position \\(i\\) will be compared with the truth at position \\(i\\) Examples: >>> import clayrs.evaluation as eva >>> >>> em = eva . EvalModel ( >>> pred_list = rank_list , >>> truth_list = truth_list , >>> metric_list = [ >>> eva . NDCG (), >>> eva . Precision () >>> eva . RecallAtK ( k = 5 , sys_average = 'micro' ) >>> ] >>> ) Then call the fit() method of the instantiated EvalModel to perform the actual evaluation Parameters: Name Type Description Default pred_list Union [ List [ Prediction ], List [ Rank ]] Recommendations list to evaluate. It's a list in case multiple splits must be evaluated. Both Rank objects (where items are ordered and the score is not relevant) or Prediction objects (where the score predicted is the predicted rating for the user regarding a certain item) can be evaluated required truth_list List [ Ratings ] Ground truths list used to compare recommendations. It's a list in case multiple splits must be evaluated. required metric_list List [ Metric ] List of metrics that will be used to evaluate recommendation list specified required Source code in clayrs/evaluation/eval_model.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 class EvalModel : \"\"\" Class for evaluating a recommender system. The Evaluation module needs the following parameters: * A list of computed rank/predictions (in case multiple splits must be evaluated) * A list of truths (in case multiple splits must be evaluated) * List of metrics to compute Obviously the list of computed rank/predictions and list of truths must have the same length, and the rank/prediction in position $i$ will be compared with the truth at position $i$ Examples: >>> import clayrs.evaluation as eva >>> >>> em = eva.EvalModel( >>> pred_list=rank_list, >>> truth_list=truth_list, >>> metric_list=[ >>> eva.NDCG(), >>> eva.Precision() >>> eva.RecallAtK(k=5, sys_average='micro') >>> ] >>> ) Then call the fit() method of the instantiated EvalModel to perform the actual evaluation Args: pred_list: Recommendations list to evaluate. It's a list in case multiple splits must be evaluated. Both Rank objects (where items are ordered and the score is not relevant) or Prediction objects (where the score predicted is the predicted rating for the user regarding a certain item) can be evaluated truth_list: Ground truths list used to compare recommendations. It's a list in case multiple splits must be evaluated. metric_list: List of metrics that will be used to evaluate recommendation list specified Raises: ValueError in case the pred_list and truth_list are empty or have different length \"\"\" def __init__ ( self , pred_list : Union [ List [ Prediction ], List [ Rank ]], truth_list : List [ Ratings ], metric_list : List [ Metric ]): if len ( pred_list ) == 0 and len ( truth_list ) == 0 : raise ValueError ( \"List containing predictions and list containing ground truths are empty!\" ) elif len ( pred_list ) != len ( truth_list ): raise ValueError ( \"List containing predictions and list containing ground truths must have the same length!\" ) self . _pred_list = pred_list self . _truth_list = truth_list self . _metric_list = metric_list self . _yaml_report_result = None @property def pred_list ( self ) -> Union [ List [ Prediction ], List [ Rank ]]: \"\"\" List containing recommendations frame Returns: The list containing recommendations frame \"\"\" return self . _pred_list @property def truth_list ( self ) -> List [ Ratings ]: \"\"\" List containing ground truths Returns: The list containing ground truths \"\"\" return self . _truth_list @property def metric_list ( self ) -> List [ Metric ]: \"\"\" List of metrics used to evaluate recommendation lists Returns: The list containing all metrics \"\"\" return self . _metric_list def append_metric ( self , metric : Metric ): \"\"\" Append a metric to the metric list that will be used to evaluate recommendation lists Args: metric: Metric to append to the metric list \"\"\" self . _metric_list . append ( metric ) def fit ( self , user_id_list : list = None ) -> Tuple [ pd . DataFrame , pd . DataFrame ]: \"\"\" This method performs the actual evaluation of the recommendation frames passed as input in the constructor of the class In case you want to perform evaluation for selected users, specify their ids parameter of this method. Otherwise, all users in the recommendation frames will be considered in the evaluation process Examples: >>> import clayrs.evaluation as eva >>> selected_users = ['u1', 'u22', 'u3'] # (1) >>> em = eva.EvalModel( >>> pred_list, >>> truth_list, >>> metric_list=[eva.Precision(), eva.Recall()] >>> ) >>> em.fit(selected_users) The method returns two pandas DataFrame: one containing ***system results*** for every metric in the metric list, one containing ***users results*** for every metric eligible Returns: The first DataFrame contains the **system result** for every metric inside the metric_list The second DataFrame contains every **user results** for every metric eligible inside the metric_list \"\"\" logger . info ( 'Performing evaluation on metrics chosen' ) pred_list = self . _pred_list truth_list = self . _truth_list if user_id_list is not None : user_id_list_set = set ([ str ( user_id ) for user_id in user_id_list ]) pred_list = [ pred . filter_ratings ( user_id_list_set ) for pred in self . _pred_list ] truth_list = [ truth . filter_ratings ( user_id_list_set ) for truth in self . _truth_list ] sys_result , users_result = MetricEvaluator ( pred_list , truth_list ) . eval_metrics ( self . metric_list ) # we save the sys result for report yaml self . _yaml_report_result = sys_result . to_dict ( orient = 'index' ) return sys_result , users_result def __repr__ ( self ): return f 'EvalModel(pred_list= { self . _pred_list } , truth_list= { self . _truth_list } ,' \\ f ' metric_list= { self . _metric_list } '","title":"EvalModel"},{"location":"evaluation/eval_model/#clayrs.evaluation.eval_model.EvalModel.append_metric","text":"Append a metric to the metric list that will be used to evaluate recommendation lists Parameters: Name Type Description Default metric Metric Metric to append to the metric list required Source code in clayrs/evaluation/eval_model.py 97 98 99 100 101 102 103 104 def append_metric ( self , metric : Metric ): \"\"\" Append a metric to the metric list that will be used to evaluate recommendation lists Args: metric: Metric to append to the metric list \"\"\" self . _metric_list . append ( metric )","title":"append_metric()"},{"location":"evaluation/eval_model/#clayrs.evaluation.eval_model.EvalModel.fit","text":"This method performs the actual evaluation of the recommendation frames passed as input in the constructor of the class In case you want to perform evaluation for selected users, specify their ids parameter of this method. Otherwise, all users in the recommendation frames will be considered in the evaluation process Examples: >>> import clayrs.evaluation as eva >>> selected_users = [ 'u1' , 'u22' , 'u3' ] # (1) >>> em = eva . EvalModel ( >>> pred_list , >>> truth_list , >>> metric_list = [ eva . Precision (), eva . Recall ()] >>> ) >>> em . fit ( selected_users ) The method returns two pandas DataFrame: one containing system results for every metric in the metric list, one containing users results for every metric eligible Returns: Type Description pd . DataFrame The first DataFrame contains the system result for every metric inside the metric_list pd . DataFrame The second DataFrame contains every user results for every metric eligible inside the metric_list Source code in clayrs/evaluation/eval_model.py 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 def fit ( self , user_id_list : list = None ) -> Tuple [ pd . DataFrame , pd . DataFrame ]: \"\"\" This method performs the actual evaluation of the recommendation frames passed as input in the constructor of the class In case you want to perform evaluation for selected users, specify their ids parameter of this method. Otherwise, all users in the recommendation frames will be considered in the evaluation process Examples: >>> import clayrs.evaluation as eva >>> selected_users = ['u1', 'u22', 'u3'] # (1) >>> em = eva.EvalModel( >>> pred_list, >>> truth_list, >>> metric_list=[eva.Precision(), eva.Recall()] >>> ) >>> em.fit(selected_users) The method returns two pandas DataFrame: one containing ***system results*** for every metric in the metric list, one containing ***users results*** for every metric eligible Returns: The first DataFrame contains the **system result** for every metric inside the metric_list The second DataFrame contains every **user results** for every metric eligible inside the metric_list \"\"\" logger . info ( 'Performing evaluation on metrics chosen' ) pred_list = self . _pred_list truth_list = self . _truth_list if user_id_list is not None : user_id_list_set = set ([ str ( user_id ) for user_id in user_id_list ]) pred_list = [ pred . filter_ratings ( user_id_list_set ) for pred in self . _pred_list ] truth_list = [ truth . filter_ratings ( user_id_list_set ) for truth in self . _truth_list ] sys_result , users_result = MetricEvaluator ( pred_list , truth_list ) . eval_metrics ( self . metric_list ) # we save the sys result for report yaml self . _yaml_report_result = sys_result . to_dict ( orient = 'index' ) return sys_result , users_result","title":"fit()"},{"location":"evaluation/eval_model/#clayrs.evaluation.eval_model.EvalModel.metric_list","text":"List of metrics used to evaluate recommendation lists Returns: Type Description List [ Metric ] The list containing all metrics Source code in clayrs/evaluation/eval_model.py 87 88 89 90 91 92 93 94 95 @property def metric_list ( self ) -> List [ Metric ]: \"\"\" List of metrics used to evaluate recommendation lists Returns: The list containing all metrics \"\"\" return self . _metric_list","title":"metric_list()"},{"location":"evaluation/eval_model/#clayrs.evaluation.eval_model.EvalModel.pred_list","text":"List containing recommendations frame Returns: Type Description Union [ List [ Prediction ], List [ Rank ]] The list containing recommendations frame Source code in clayrs/evaluation/eval_model.py 67 68 69 70 71 72 73 74 75 @property def pred_list ( self ) -> Union [ List [ Prediction ], List [ Rank ]]: \"\"\" List containing recommendations frame Returns: The list containing recommendations frame \"\"\" return self . _pred_list","title":"pred_list()"},{"location":"evaluation/eval_model/#clayrs.evaluation.eval_model.EvalModel.truth_list","text":"List containing ground truths Returns: Type Description List [ Ratings ] The list containing ground truths Source code in clayrs/evaluation/eval_model.py 77 78 79 80 81 82 83 84 85 @property def truth_list ( self ) -> List [ Ratings ]: \"\"\" List containing ground truths Returns: The list containing ground truths \"\"\" return self . _truth_list","title":"truth_list()"},{"location":"evaluation/introduction/","text":"Warning Docs are still a WIP Introduction The Evaluation module has the task of evaluating a recommender system, using several state-of-the-art metrics The usage pipeline it's pretty simple, all the work is done by the EvalModel class. Suppose you want to evaluate recommendation lists using NDCG , macro Precision , micro Precision@5 , you need to instantiate the EvalModel class with the following parameters: A list of computed rank/predictions (in case multiple splits must be evaluated) A list of truths (in case multiple splits must be evaluated) List of metrics to compute Obviously the list of computed rank/predictions and list of truths must have the same length, and the rank/prediction in position \\(i\\) will be compared with the truth at position \\(i\\) Usage example In this case rank_list and truth_list are results obtained from the RecSys module of the framework import clayrs.evaluation as eva em = eva . EvalModel ( pred_list = rank_list , truth_list = truth_list , metric_list = [ eva . NDCG (), eva . Precision (), # (1) eva . RecallAtK ( k = 5 , sys_average = 'micro' ) ] ) If not specified, by default system average is computed as macro Info Precision , Recall , and in general all classification metrics requires a threshold which separates relevant items from non-relevant. If a threshold is specified, then it is fixed for all users If no threshold is specified, the mean rating score of each user will be used Check documentation for more Then simply call the fit () method of the instantiated object It will return two pandas DataFrame: the first one contains the metrics aggregated for the system, while the second contains the metrics computed for each user (where possible) sys_result , users_result = em . fit () Evaluating external recommendation lists The evaluation module is completely independent from the Recsys and Content Analyzer module: that means that we can easily evaluate recommendation lists computed by other frameworks/tools! Let's suppose we have recommendations (and related truths) generated via other tools in a csv format. We first import them into the framework and then pass them to the EvalModel class import clayrs.content_analyzer as ca csv_rank_1 = ca . CSVFile ( 'rank_split_1.csv' ) csv_truth_1 = ca . CSVFile ( 'truth_split_1.csv' ) csv_rank_2 = ca . CSVFile ( 'rank_split_2.csv' ) csv_truth_2 = ca . CSVFile ( 'truth_split_2.csv' ) # Importing split 1 (1) rank_1 = ca . Rank ( csv_rank_1 ) truth_1 = ca . Ratings ( csv_truth_1 ) # Importing split 2 (2) rank_2 = ca . Rank ( csv_rank_2 ) truth_2 = ca . Ratings ( csv_truth_2 ) # since multiple splits, we wrap ranks and truths in lists imported_ranks = [ rank_1 , rank_2 ] imported_truths = [ truth_1 , truth_2 ] Remember that this instantiation to the Rank/Ratings class assumes a certain order of the columns of your raw source. Otherwise, you need to manually map columns. Check documentation for more Remember that this instantiation to the Rank/Ratings class assumes a certain order of the columns of your raw source. Otherwise, you need to manually map columns. Check documentation for more Then simply evaluate them exactly in the same way as shown before! import clayrs.evaluation as eva em = eva . EvalModel ( pred_list = imported_ranks , truth_list = imported_truths , metric_list = [ # ... Choose your own metrics ] ) sys_results_df , users_results_df = em . fit () Perform a statistical test ClayRS lets you also compare different learning schemas by performing statistical tests: Simply instantiate the desired test and call its perform () method. The parameter it expects is the list of user_results dataframe obtained in the evaluation step, one for each learning schema to compare. ttest = eva . Ttest () all_comb_df = ttest . perform ([ user_result1 , user_result2 , user_result3 ]) Info In this case since the Ttest it's a paired test, the final result is a pandas DataFrame which contains learning schemas compared in pair: (system1, system2) (system1, system3) (system2, system3)","title":"Introduction"},{"location":"evaluation/introduction/#introduction","text":"The Evaluation module has the task of evaluating a recommender system, using several state-of-the-art metrics The usage pipeline it's pretty simple, all the work is done by the EvalModel class. Suppose you want to evaluate recommendation lists using NDCG , macro Precision , micro Precision@5 , you need to instantiate the EvalModel class with the following parameters: A list of computed rank/predictions (in case multiple splits must be evaluated) A list of truths (in case multiple splits must be evaluated) List of metrics to compute Obviously the list of computed rank/predictions and list of truths must have the same length, and the rank/prediction in position \\(i\\) will be compared with the truth at position \\(i\\)","title":"Introduction"},{"location":"evaluation/introduction/#usage-example","text":"In this case rank_list and truth_list are results obtained from the RecSys module of the framework import clayrs.evaluation as eva em = eva . EvalModel ( pred_list = rank_list , truth_list = truth_list , metric_list = [ eva . NDCG (), eva . Precision (), # (1) eva . RecallAtK ( k = 5 , sys_average = 'micro' ) ] ) If not specified, by default system average is computed as macro Info Precision , Recall , and in general all classification metrics requires a threshold which separates relevant items from non-relevant. If a threshold is specified, then it is fixed for all users If no threshold is specified, the mean rating score of each user will be used Check documentation for more Then simply call the fit () method of the instantiated object It will return two pandas DataFrame: the first one contains the metrics aggregated for the system, while the second contains the metrics computed for each user (where possible) sys_result , users_result = em . fit ()","title":"Usage example"},{"location":"evaluation/introduction/#evaluating-external-recommendation-lists","text":"The evaluation module is completely independent from the Recsys and Content Analyzer module: that means that we can easily evaluate recommendation lists computed by other frameworks/tools! Let's suppose we have recommendations (and related truths) generated via other tools in a csv format. We first import them into the framework and then pass them to the EvalModel class import clayrs.content_analyzer as ca csv_rank_1 = ca . CSVFile ( 'rank_split_1.csv' ) csv_truth_1 = ca . CSVFile ( 'truth_split_1.csv' ) csv_rank_2 = ca . CSVFile ( 'rank_split_2.csv' ) csv_truth_2 = ca . CSVFile ( 'truth_split_2.csv' ) # Importing split 1 (1) rank_1 = ca . Rank ( csv_rank_1 ) truth_1 = ca . Ratings ( csv_truth_1 ) # Importing split 2 (2) rank_2 = ca . Rank ( csv_rank_2 ) truth_2 = ca . Ratings ( csv_truth_2 ) # since multiple splits, we wrap ranks and truths in lists imported_ranks = [ rank_1 , rank_2 ] imported_truths = [ truth_1 , truth_2 ] Remember that this instantiation to the Rank/Ratings class assumes a certain order of the columns of your raw source. Otherwise, you need to manually map columns. Check documentation for more Remember that this instantiation to the Rank/Ratings class assumes a certain order of the columns of your raw source. Otherwise, you need to manually map columns. Check documentation for more Then simply evaluate them exactly in the same way as shown before! import clayrs.evaluation as eva em = eva . EvalModel ( pred_list = imported_ranks , truth_list = imported_truths , metric_list = [ # ... Choose your own metrics ] ) sys_results_df , users_results_df = em . fit ()","title":"Evaluating external recommendation lists"},{"location":"evaluation/introduction/#perform-a-statistical-test","text":"ClayRS lets you also compare different learning schemas by performing statistical tests: Simply instantiate the desired test and call its perform () method. The parameter it expects is the list of user_results dataframe obtained in the evaluation step, one for each learning schema to compare. ttest = eva . Ttest () all_comb_df = ttest . perform ([ user_result1 , user_result2 , user_result3 ]) Info In this case since the Ttest it's a paired test, the final result is a pandas DataFrame which contains learning schemas compared in pair: (system1, system2) (system1, system3) (system2, system3)","title":"Perform a statistical test"},{"location":"evaluation/metrics/classification_metrics/","text":"Classification metrics A classification metric uses confusion matrix terminology (true positive, false positive, true negative, false negative) to classify each item predicted, and in general it needs a way to discern relevant items from non-relevant items for users FMeasure Bases: ClassificationMetric The FMeasure metric combines Precision and Recall into a single metric. It is calculated as such for the single user : \\[ FMeasure_u = (1 + \\beta^2) \\cdot \\frac{P_u \\cdot R_u}{(\\beta^2 \\cdot P_u) + R_u} \\] Where: \\(P_u\\) is the Precision calculated for the user u \\(R_u\\) is the Recall calculated for the user u \\(\\beta\\) is a real factor which could weight differently Recall or Precision based on its value: \\(\\beta = 1\\) : Equally weight Precision and Recall \\(\\beta > 1\\) : Weight Recall more \\(\\beta < 1\\) : Weight Precision more A famous FMeasure is the F1 Metric, where \\(\\beta = 1\\) , which basically is the harmonic mean of recall and precision: \\[ F1_u = \\frac{2 \\cdot P_u \\cdot R_u}{P_u + R_u} \\] The FMeasure metric is calculated as such for the entire system , depending if 'macro' average or 'micro' average has been chosen: \\[ FMeasure_{sys} - micro = (1 + \\beta^2) \\cdot \\frac{P_u \\cdot R_u}{(\\beta^2 \\cdot P_u) + R_u} \\] \\[ FMeasure_{sys} - macro = \\frac{\\sum_{u \\in U} FMeasure_u}{|U|} \\] Parameters: Name Type Description Default beta float real factor which could weight differently Recall or Precision based on its value. Default is 1 1 relevant_threshold float parameter needed to discern relevant items and non-relevant items for every user. If not specified, the mean rating score of every user will be used None sys_average str specify how the system average must be computed. Default is 'macro' 'macro' Source code in clayrs/evaluation/metrics/classification_metrics.py 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 class FMeasure ( ClassificationMetric ): r \"\"\" The FMeasure metric combines Precision and Recall into a single metric. It is calculated as such for the **single user**: $$ FMeasure_u = (1 + \\beta^2) \\cdot \\frac{P_u \\cdot R_u}{(\\beta^2 \\cdot P_u) + R_u} $$ Where: - $P_u$ is the Precision calculated for the user *u* - $R_u$ is the Recall calculated for the user *u* - $\\beta$ is a real factor which could weight differently Recall or Precision based on its value: - $\\beta = 1$: Equally weight Precision and Recall - $\\beta > 1$: Weight Recall more - $\\beta < 1$: Weight Precision more A famous FMeasure is the F1 Metric, where $\\beta = 1$, which basically is the harmonic mean of recall and precision: $$ F1_u = \\frac{2 \\cdot P_u \\cdot R_u}{P_u + R_u} $$ The FMeasure metric is calculated as such for the **entire system**, depending if 'macro' average or 'micro' average has been chosen: $$ FMeasure_{sys} - micro = (1 + \\beta^2) \\cdot \\frac{P_u \\cdot R_u}{(\\beta^2 \\cdot P_u) + R_u} $$ $$ FMeasure_{sys} - macro = \\frac{\\sum_{u \\in U} FMeasure_u}{|U|} $$ Args: beta (float): real factor which could weight differently Recall or Precision based on its value. Default is 1 relevant_threshold (float): parameter needed to discern relevant items and non-relevant items for every user. If not specified, the mean rating score of every user will be used sys_average (str): specify how the system average must be computed. Default is 'macro' \"\"\" def __init__ ( self , beta : float = 1 , relevant_threshold : float = None , sys_average : str = 'macro' , precision : [ Callable ] = np . float64 ): super () . __init__ ( relevant_threshold , sys_average , precision ) self . __beta = beta @property def beta ( self ): return self . __beta def __str__ ( self ): return \"F {} - {} \" . format ( self . beta , self . sys_avg ) def __repr__ ( self ): return f \"FMeasure(beta= { self . beta } , relevant_threshold= { self . relevant_threshold } , \" \\ f \"sys_average= { self . sys_avg } , precision= { self . precision } )\" def _calc_metric ( self , confusion_matrix : np . ndarray ): prec = Precision () . _calc_metric ( confusion_matrix ) reca = Recall () . _calc_metric ( confusion_matrix ) beta_2 = self . beta ** 2 num = prec * reca den = ( beta_2 * prec ) + reca fbeta = self . precision (( 1 + beta_2 ) * ( den and num / den or 0 )) # safediv between num and den return fbeta def _perform_single_user ( self , user_prediction : List [ Interaction ], user_truth_relevant_items : Set [ str ]): tp = len ([ prediction_interaction for prediction_interaction in user_prediction if prediction_interaction . item_id in user_truth_relevant_items ]) fp = len ( user_prediction ) - tp fn = len ( user_truth_relevant_items ) - tp useful_confusion_matrix_user = np . array ([[ tp , fp ], [ fn , 0 ]], dtype = np . int32 ) return self . _calc_metric ( useful_confusion_matrix_user ), useful_confusion_matrix_user FMeasureAtK Bases: FMeasure The FMeasure@K metric combines Precision@K and Recall@K into a single metric. It is calculated as such for the single user : \\[ FMeasure_u = (1 + \\beta^2) \\cdot \\frac{P@K_u \\cdot R@K_u}{(\\beta^2 \\cdot P@K_u) + R@K_u} \\] Where: \\(P@K_u\\) is the Precision at K calculated for the user u \\(R@K_u\\) is the Recall at K calculated for the user u \\(\\beta\\) is a real factor which could weight differently Recall or Precision based on its value: \\(\\beta = 1\\) : Equally weight Precision and Recall \\(\\beta > 1\\) : Weight Recall more \\(\\beta < 1\\) : Weight Precision more A famous FMeasure@K is the F1@K Metric, where :math: \\beta = 1 , which basically is the harmonic mean of recall and precision: \\[ F1@K_u = \\frac{2 \\cdot P@K_u \\cdot R@K_u}{P@K_u + R@K_u} \\] The FMeasure@K metric is calculated as such for the entire system , depending if 'macro' average or 'micro' average has been chosen: \\[ FMeasure@K_{sys} - micro = (1 + \\beta^2) \\cdot \\frac{P@K_u \\cdot R@K_u}{(\\beta^2 \\cdot P@K_u) + R@K_u} \\] \\[ FMeasure@K_{sys} - macro = \\frac{\\sum_{u \\in U} FMeasure@K_u}{|U|} \\] Parameters: Name Type Description Default k int cutoff parameter. Will be used for the computation of Precision@K and Recall@K required beta float real factor which could weight differently Recall or Precision based on its value. Default is 1 1 relevant_threshold float parameter needed to discern relevant items and non-relevant items for every user. If not specified, the mean rating score of every user will be used None sys_average str specify how the system average must be computed. Default is 'macro' 'macro' Source code in clayrs/evaluation/metrics/classification_metrics.py 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 class FMeasureAtK ( FMeasure ): r \"\"\" The FMeasure@K metric combines Precision@K and Recall@K into a single metric. It is calculated as such for the **single user**: $$ FMeasure_u = (1 + \\beta^2) \\cdot \\frac{P@K_u \\cdot R@K_u}{(\\beta^2 \\cdot P@K_u) + R@K_u} $$ Where: - $P@K_u$ is the Precision at K calculated for the user *u* - $R@K_u$ is the Recall at K calculated for the user *u* - $\\beta$ is a real factor which could weight differently Recall or Precision based on its value: - $\\beta = 1$: Equally weight Precision and Recall - $\\beta > 1$: Weight Recall more - $\\beta < 1$: Weight Precision more A famous FMeasure@K is the F1@K Metric, where :math:`\\beta = 1`, which basically is the harmonic mean of recall and precision: $$ F1@K_u = \\frac{2 \\cdot P@K_u \\cdot R@K_u}{P@K_u + R@K_u} $$ The FMeasure@K metric is calculated as such for the **entire system**, depending if 'macro' average or 'micro' average has been chosen: $$ FMeasure@K_{sys} - micro = (1 + \\beta^2) \\cdot \\frac{P@K_u \\cdot R@K_u}{(\\beta^2 \\cdot P@K_u) + R@K_u} $$ $$ FMeasure@K_{sys} - macro = \\frac{\\sum_{u \\in U} FMeasure@K_u}{|U|} $$ Args: k (int): cutoff parameter. Will be used for the computation of Precision@K and Recall@K beta (float): real factor which could weight differently Recall or Precision based on its value. Default is 1 relevant_threshold (float): parameter needed to discern relevant items and non-relevant items for every user. If not specified, the mean rating score of every user will be used sys_average (str): specify how the system average must be computed. Default is 'macro' \"\"\" def __init__ ( self , k : int , beta : int = 1 , relevant_threshold : float = None , sys_average : str = 'macro' ): super () . __init__ ( beta , relevant_threshold , sys_average ) if k < 1 : raise ValueError ( 'k= {} not valid! k must be >= 1!' . format ( k )) self . __k = k @property def k ( self ): return self . __k def __str__ ( self ): return \"F {} @ {} - {} \" . format ( self . beta , self . k , self . sys_avg ) def __repr__ ( self ): return f \"FMeasureAtK(k= { self . k } , beta= { self . beta } , relevant_threshold= { self . relevant_threshold } , \" \\ f \"sys_average= { self . sys_avg } , precision= { self . precision } )\" def _perform_single_user ( self , user_prediction : List [ Interaction ], user_truth_relevant_items : Set [ str ]): user_prediction_cut = user_prediction [: self . k ] tp = len ([ prediction_interaction for prediction_interaction in user_prediction_cut if prediction_interaction . item_id in user_truth_relevant_items ]) fp = len ( user_prediction_cut ) - tp fn = len ( user_truth_relevant_items ) - tp useful_confusion_matrix_user = np . array ([[ tp , fp ], [ fn , 0 ]], dtype = np . int32 ) return self . _calc_metric ( useful_confusion_matrix_user ), useful_confusion_matrix_user Precision Bases: ClassificationMetric The Precision metric is calculated as such for the single user : \\[ Precision_u = \\frac{tp_u}{tp_u + fp_u} \\] Where: \\(tp_u\\) is the number of items which are in the recommendation list of the user and have a rating >= relevant_threshold in its 'ground truth' \\(fp_u\\) is the number of items which are in the recommendation list of the user and have a rating < relevant_threshold in its 'ground truth' And it is calculated as such for the entire system , depending if 'macro' average or 'micro' average has been chosen: \\[ Precision_{sys} - micro = \\frac{\\sum_{u \\in U} tp_u}{\\sum_{u \\in U} tp_u + \\sum_{u \\in U} fp_u} \\] \\[ Precision_{sys} - macro = \\frac{\\sum_{u \\in U} Precision_u}{|U|} \\] Parameters: Name Type Description Default relevant_threshold float parameter needed to discern relevant items and non-relevant items for every user. If not specified, the mean rating score of every user will be used None sys_average str specify how the system average must be computed. Default is 'macro' 'macro' Source code in clayrs/evaluation/metrics/classification_metrics.py 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 class Precision ( ClassificationMetric ): r \"\"\" The Precision metric is calculated as such for the **single user**: $$ Precision_u = \\frac{tp_u}{tp_u + fp_u} $$ Where: - $tp_u$ is the number of items which are in the recommendation list of the user and have a rating >= relevant_threshold in its 'ground truth' - $fp_u$ is the number of items which are in the recommendation list of the user and have a rating < relevant_threshold in its 'ground truth' And it is calculated as such for the **entire system**, depending if 'macro' average or 'micro' average has been chosen: $$ Precision_{sys} - micro = \\frac{\\sum_{u \\in U} tp_u}{\\sum_{u \\in U} tp_u + \\sum_{u \\in U} fp_u} $$ $$ Precision_{sys} - macro = \\frac{\\sum_{u \\in U} Precision_u}{|U|} $$ Args: relevant_threshold: parameter needed to discern relevant items and non-relevant items for every user. If not specified, the mean rating score of every user will be used sys_average: specify how the system average must be computed. Default is 'macro' \"\"\" def __init__ ( self , relevant_threshold : float = None , sys_average : str = 'macro' , precision : [ Callable ] = np . float64 ): super ( Precision , self ) . __init__ ( relevant_threshold , sys_average , precision ) def __str__ ( self ): return \"Precision - {} \" . format ( self . sys_avg ) def __repr__ ( self ): return f \"Precision(relevant_threshold= { self . relevant_threshold } , sys_average= { self . sys_avg } , \" \\ f \"precision= { self . precision } )\" def _calc_metric ( self , confusion_matrix : np . ndarray ): tp = confusion_matrix [ 0 , 0 ] fp = confusion_matrix [ 0 , 1 ] return self . precision (( tp + fp ) and tp / ( tp + fp ) or 0 ) # safediv between tp and (tp + fp) def _perform_single_user ( self , user_prediction : List [ Interaction ], user_truth_relevant_items : Set [ str ]): tp = len ([ prediction_interaction for prediction_interaction in user_prediction if prediction_interaction . item_id in user_truth_relevant_items ]) fp = len ( user_prediction ) - tp # we do not compute the full confusion matrix for the user useful_confusion_matrix_user = np . array ([[ tp , fp ], [ 0 , 0 ]], dtype = np . int32 ) return self . _calc_metric ( useful_confusion_matrix_user ), useful_confusion_matrix_user PrecisionAtK Bases: Precision The Precision@K metric is calculated as such for the single user : \\[ Precision@K_u = \\frac{tp@K_u}{tp@K_u + fp@K_u} \\] Where: \\(tp@K_u\\) is the number of items which are in the recommendation list of the user cutoff to the first K items and have a rating >= relevant_threshold in its 'ground truth' \\(tp@K_u\\) is the number of items which are in the recommendation list of the user cutoff to the first K items and have a rating < relevant_threshold in its 'ground truth' And it is calculated as such for the entire system , depending if 'macro' average or 'micro' average has been chosen: \\[ Precision@K_{sys} - micro = \\frac{\\sum_{u \\in U} tp@K_u}{\\sum_{u \\in U} tp@K_u + \\sum_{u \\in U} fp@K_u} \\] \\[ Precision@K_{sys} - macro = \\frac{\\sum_{u \\in U} Precision@K_u}{|U|} \\] Parameters: Name Type Description Default k int cutoff parameter. Only the first k items of the recommendation list will be considered required relevant_threshold float parameter needed to discern relevant items and non-relevant items for every user. If not specified, the mean rating score of every user will be used None sys_average str specify how the system average must be computed. Default is 'macro' 'macro' Source code in clayrs/evaluation/metrics/classification_metrics.py 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 class PrecisionAtK ( Precision ): r \"\"\" The Precision@K metric is calculated as such for the **single user**: $$ Precision@K_u = \\frac{tp@K_u}{tp@K_u + fp@K_u} $$ Where: - $tp@K_u$ is the number of items which are in the recommendation list of the user **cutoff to the first K items** and have a rating >= relevant_threshold in its 'ground truth' - $tp@K_u$ is the number of items which are in the recommendation list of the user **cutoff to the first K items** and have a rating < relevant_threshold in its 'ground truth' And it is calculated as such for the **entire system**, depending if 'macro' average or 'micro' average has been chosen: $$ Precision@K_{sys} - micro = \\frac{\\sum_{u \\in U} tp@K_u}{\\sum_{u \\in U} tp@K_u + \\sum_{u \\in U} fp@K_u} $$ $$ Precision@K_{sys} - macro = \\frac{\\sum_{u \\in U} Precision@K_u}{|U|} $$ Args: k (int): cutoff parameter. Only the first k items of the recommendation list will be considered relevant_threshold (float): parameter needed to discern relevant items and non-relevant items for every user. If not specified, the mean rating score of every user will be used sys_average (str): specify how the system average must be computed. Default is 'macro' \"\"\" def __init__ ( self , k : int , relevant_threshold : float = None , sys_average : str = 'macro' , precision : [ Callable ] = np . float64 ): super () . __init__ ( relevant_threshold , sys_average , precision ) if k < 1 : raise ValueError ( 'k= {} not valid! k must be >= 1!' . format ( k )) self . __k = k @property def k ( self ): return self . __k def __str__ ( self ): return \"Precision@ {} - {} \" . format ( self . k , self . sys_avg ) def __repr__ ( self ): return f \"PrecisionAtK(k= { self . k } , relevant_threshold= { self . relevant_threshold } , sys_average= { self . sys_avg } , \" \\ f \"precision= { self . precision } )\" def _perform_single_user ( self , user_prediction : List [ Interaction ], user_truth_relevant_items : Set [ str ]): user_prediction_cut = user_prediction [: self . k ] tp = len ([ prediction_interaction for prediction_interaction in user_prediction_cut if prediction_interaction . item_id in user_truth_relevant_items ]) fp = len ( user_prediction_cut ) - tp useful_confusion_matrix_user = np . array ([[ tp , fp ], [ 0 , 0 ]], dtype = np . int32 ) return self . _calc_metric ( useful_confusion_matrix_user ), useful_confusion_matrix_user RPrecision Bases: Precision The R-Precision metric is calculated as such for the single user : \\[ R-Precision_u = \\frac{tp@R_u}{tp@R_u + fp@R_u} \\] Where: \\(R\\) it's the number of relevant items for the user u \\(tp@R_u\\) is the number of items which are in the recommendation list of the user cutoff to the first R items and have a rating >= relevant_threshold in its 'ground truth' \\(tp@R_u\\) is the number of items which are in the recommendation list of the user cutoff to the first R items and have a rating < relevant_threshold in its 'ground truth' And it is calculated as such for the entire system , depending if 'macro' average or 'micro' average has been chosen: \\[ Precision@K_{sys} - micro = \\frac{\\sum_{u \\in U} tp@R_u}{\\sum_{u \\in U} tp@R_u + \\sum_{u \\in U} fp@R_u} \\] \\[ Precision@K_{sys} - macro = \\frac{\\sum_{u \\in U} R-Precision_u}{|U|} \\] Parameters: Name Type Description Default relevant_threshold parameter needed to discern relevant items and non-relevant items for every user. If not specified, the mean rating score of every user will be used required sys_average specify how the system average must be computed. Default is 'macro' required Source code in clayrs/evaluation/metrics/classification_metrics.py 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 class RPrecision ( Precision ): r \"\"\" The R-Precision metric is calculated as such for the **single user**: $$ R-Precision_u = \\frac{tp@R_u}{tp@R_u + fp@R_u} $$ Where: - $R$ it's the number of relevant items for the user *u* - $tp@R_u$ is the number of items which are in the recommendation list of the user **cutoff to the first R items** and have a rating >= relevant_threshold in its 'ground truth' - $tp@R_u$ is the number of items which are in the recommendation list of the user **cutoff to the first R items** and have a rating < relevant_threshold in its 'ground truth' And it is calculated as such for the **entire system**, depending if 'macro' average or 'micro' average has been chosen: $$ Precision@K_{sys} - micro = \\frac{\\sum_{u \\in U} tp@R_u}{\\sum_{u \\in U} tp@R_u + \\sum_{u \\in U} fp@R_u} $$ $$ Precision@K_{sys} - macro = \\frac{\\sum_{u \\in U} R-Precision_u}{|U|} $$ Args: relevant_threshold: parameter needed to discern relevant items and non-relevant items for every user. If not specified, the mean rating score of every user will be used sys_average: specify how the system average must be computed. Default is 'macro' \"\"\" def __str__ ( self ): return \"R-Precision - {} \" . format ( self . sys_avg ) def __repr__ ( self ): return f \"RPrecision(relevant_threshold= { self . relevant_threshold } , sys_average= { self . sys_avg } , \" \\ f \"precision= { self . precision } )\" def _perform_single_user ( self , user_prediction : List [ Interaction ], user_truth_relevant_items : Set [ str ]): r = len ( user_truth_relevant_items ) user_prediction_cut = user_prediction [: r ] tp = len ([ prediction_interaction for prediction_interaction in user_prediction_cut if prediction_interaction . item_id in user_truth_relevant_items ]) fp = len ( user_prediction_cut ) - tp useful_confusion_matrix_user = np . array ([[ tp , fp ], [ 0 , 0 ]], dtype = np . int32 ) return self . _calc_metric ( useful_confusion_matrix_user ), useful_confusion_matrix_user Recall Bases: ClassificationMetric The Recall metric is calculated as such for the single user : \\[ Recall_u = \\frac{tp_u}{tp_u + fn_u} \\] Where: \\(tp_u\\) is the number of items which are in the recommendation list of the user and have a rating >= relevant_threshold in its 'ground truth' \\(fn_u\\) is the number of items which are NOT in the recommendation list of the user and have a rating >= relevant_threshold in its 'ground truth' And it is calculated as such for the entire system , depending if 'macro' average or 'micro' average has been chosen: \\[ Recall_{sys} - micro = \\frac{\\sum_{u \\in U} tp_u}{\\sum_{u \\in U} tp_u + \\sum_{u \\in U} fn_u} \\] \\[ Recall_{sys} - macro = \\frac{\\sum_{u \\in U} Recall_u}{|U|} \\] Source code in clayrs/evaluation/metrics/classification_metrics.py 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 class Recall ( ClassificationMetric ): r \"\"\" The Recall metric is calculated as such for the **single user**: $$ Recall_u = \\frac{tp_u}{tp_u + fn_u} $$ Where: - $tp_u$ is the number of items which are in the recommendation list of the user and have a rating >= relevant_threshold in its 'ground truth' - $fn_u$ is the number of items which are NOT in the recommendation list of the user and have a rating >= relevant_threshold in its 'ground truth' And it is calculated as such for the **entire system**, depending if 'macro' average or 'micro' average has been chosen: $$ Recall_{sys} - micro = \\frac{\\sum_{u \\in U} tp_u}{\\sum_{u \\in U} tp_u + \\sum_{u \\in U} fn_u} $$ $$ Recall_{sys} - macro = \\frac{\\sum_{u \\in U} Recall_u}{|U|} $$ \"\"\" def __str__ ( self ): return \"Recall - {} \" . format ( self . sys_avg ) def __repr__ ( self ): return f \"Recall(relevant_threshold= { self . relevant_threshold } , sys_average= { self . sys_avg } , \" \\ f \"precision= { self . precision } )\" def _calc_metric ( self , confusion_matrix : np . ndarray ): tp = confusion_matrix [ 0 , 0 ] fn = confusion_matrix [ 1 , 0 ] return self . precision (( tp + fn ) and tp / ( tp + fn ) or 0 ) # safediv between tp and (tp + fn) def _perform_single_user ( self , user_prediction : List [ Interaction ], user_truth_relevant_items : Set [ str ]): tp = len ([ prediction_interaction for prediction_interaction in user_prediction if prediction_interaction . item_id in user_truth_relevant_items ]) fn = len ( user_truth_relevant_items ) - tp useful_confusion_matrix_user = np . array ([[ tp , 0 ], [ fn , 0 ]], dtype = np . int32 ) return self . _calc_metric ( useful_confusion_matrix_user ), useful_confusion_matrix_user RecallAtK Bases: Recall The Recall@K metric is calculated as such for the single user : \\[ Recall@K_u = \\frac{tp@K_u}{tp@K_u + fn@K_u} \\] Where: \\(tp@K_u\\) is the number of items which are in the recommendation list of the user cutoff to the first K items and have a rating >= relevant_threshold in its 'ground truth' \\(tp@K_u\\) is the number of items which are NOT in the recommendation list of the user cutoff to the first K items and have a rating >= relevant_threshold in its 'ground truth' And it is calculated as such for the entire system , depending if 'macro' average or 'micro' average has been chosen: \\[ Recall@K_{sys} - micro = \\frac{\\sum_{u \\in U} tp@K_u}{\\sum_{u \\in U} tp@K_u + \\sum_{u \\in U} fn@K_u} \\] \\[ Recall@K_{sys} - macro = \\frac{\\sum_{u \\in U} Recall@K_u}{|U|} \\] Parameters: Name Type Description Default k int cutoff parameter. Only the first k items of the recommendation list will be considered required relevant_threshold float parameter needed to discern relevant items and non-relevant items for every user. If not specified, the mean rating score of every user will be used None sys_average str specify how the system average must be computed. Default is 'macro' 'macro' Source code in clayrs/evaluation/metrics/classification_metrics.py 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 class RecallAtK ( Recall ): r \"\"\" The Recall@K metric is calculated as such for the **single user**: $$ Recall@K_u = \\frac{tp@K_u}{tp@K_u + fn@K_u} $$ Where: - $tp@K_u$ is the number of items which are in the recommendation list of the user **cutoff to the first K items** and have a rating >= relevant_threshold in its 'ground truth' - $tp@K_u$ is the number of items which are NOT in the recommendation list of the user **cutoff to the first K items** and have a rating >= relevant_threshold in its 'ground truth' And it is calculated as such for the **entire system**, depending if 'macro' average or 'micro' average has been chosen: $$ Recall@K_{sys} - micro = \\frac{\\sum_{u \\in U} tp@K_u}{\\sum_{u \\in U} tp@K_u + \\sum_{u \\in U} fn@K_u} $$ $$ Recall@K_{sys} - macro = \\frac{\\sum_{u \\in U} Recall@K_u}{|U|} $$ Args: k (int): cutoff parameter. Only the first k items of the recommendation list will be considered relevant_threshold (float): parameter needed to discern relevant items and non-relevant items for every user. If not specified, the mean rating score of every user will be used sys_average (str): specify how the system average must be computed. Default is 'macro' \"\"\" def __init__ ( self , k : int , relevant_threshold : float = None , sys_average : str = 'macro' , precision : [ Callable ] = np . float64 ): super () . __init__ ( relevant_threshold , sys_average , precision ) if k < 1 : raise ValueError ( 'k= {} not valid! k must be >= 1!' . format ( k )) self . __k = k @property def k ( self ): return self . __k def __str__ ( self ): return \"Recall@ {} - {} \" . format ( self . k , self . sys_avg ) def __repr__ ( self ): return f \"RecallAtK(k= { self . k } , relevant_threshold= { self . relevant_threshold } , sys_average= { self . sys_avg } , \" \\ f \"precision= { self . precision } )\" def _perform_single_user ( self , user_prediction : List [ Interaction ], user_truth_relevant_items : Set [ str ]): user_prediction_cut = user_prediction [: self . k ] tp = len ([ prediction_interaction for prediction_interaction in user_prediction_cut if prediction_interaction . item_id in user_truth_relevant_items ]) fn = len ( user_truth_relevant_items ) - tp useful_confusion_matrix_user = np . array ([[ tp , 0 ], [ fn , 0 ]], dtype = np . int32 ) return self . _calc_metric ( useful_confusion_matrix_user ), useful_confusion_matrix_user","title":"Classification metrics"},{"location":"evaluation/metrics/classification_metrics/#classification-metrics","text":"A classification metric uses confusion matrix terminology (true positive, false positive, true negative, false negative) to classify each item predicted, and in general it needs a way to discern relevant items from non-relevant items for users","title":"Classification metrics"},{"location":"evaluation/metrics/classification_metrics/#clayrs.evaluation.metrics.classification_metrics.FMeasure","text":"Bases: ClassificationMetric The FMeasure metric combines Precision and Recall into a single metric. It is calculated as such for the single user : \\[ FMeasure_u = (1 + \\beta^2) \\cdot \\frac{P_u \\cdot R_u}{(\\beta^2 \\cdot P_u) + R_u} \\] Where: \\(P_u\\) is the Precision calculated for the user u \\(R_u\\) is the Recall calculated for the user u \\(\\beta\\) is a real factor which could weight differently Recall or Precision based on its value: \\(\\beta = 1\\) : Equally weight Precision and Recall \\(\\beta > 1\\) : Weight Recall more \\(\\beta < 1\\) : Weight Precision more A famous FMeasure is the F1 Metric, where \\(\\beta = 1\\) , which basically is the harmonic mean of recall and precision: \\[ F1_u = \\frac{2 \\cdot P_u \\cdot R_u}{P_u + R_u} \\] The FMeasure metric is calculated as such for the entire system , depending if 'macro' average or 'micro' average has been chosen: \\[ FMeasure_{sys} - micro = (1 + \\beta^2) \\cdot \\frac{P_u \\cdot R_u}{(\\beta^2 \\cdot P_u) + R_u} \\] \\[ FMeasure_{sys} - macro = \\frac{\\sum_{u \\in U} FMeasure_u}{|U|} \\] Parameters: Name Type Description Default beta float real factor which could weight differently Recall or Precision based on its value. Default is 1 1 relevant_threshold float parameter needed to discern relevant items and non-relevant items for every user. If not specified, the mean rating score of every user will be used None sys_average str specify how the system average must be computed. Default is 'macro' 'macro' Source code in clayrs/evaluation/metrics/classification_metrics.py 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 class FMeasure ( ClassificationMetric ): r \"\"\" The FMeasure metric combines Precision and Recall into a single metric. It is calculated as such for the **single user**: $$ FMeasure_u = (1 + \\beta^2) \\cdot \\frac{P_u \\cdot R_u}{(\\beta^2 \\cdot P_u) + R_u} $$ Where: - $P_u$ is the Precision calculated for the user *u* - $R_u$ is the Recall calculated for the user *u* - $\\beta$ is a real factor which could weight differently Recall or Precision based on its value: - $\\beta = 1$: Equally weight Precision and Recall - $\\beta > 1$: Weight Recall more - $\\beta < 1$: Weight Precision more A famous FMeasure is the F1 Metric, where $\\beta = 1$, which basically is the harmonic mean of recall and precision: $$ F1_u = \\frac{2 \\cdot P_u \\cdot R_u}{P_u + R_u} $$ The FMeasure metric is calculated as such for the **entire system**, depending if 'macro' average or 'micro' average has been chosen: $$ FMeasure_{sys} - micro = (1 + \\beta^2) \\cdot \\frac{P_u \\cdot R_u}{(\\beta^2 \\cdot P_u) + R_u} $$ $$ FMeasure_{sys} - macro = \\frac{\\sum_{u \\in U} FMeasure_u}{|U|} $$ Args: beta (float): real factor which could weight differently Recall or Precision based on its value. Default is 1 relevant_threshold (float): parameter needed to discern relevant items and non-relevant items for every user. If not specified, the mean rating score of every user will be used sys_average (str): specify how the system average must be computed. Default is 'macro' \"\"\" def __init__ ( self , beta : float = 1 , relevant_threshold : float = None , sys_average : str = 'macro' , precision : [ Callable ] = np . float64 ): super () . __init__ ( relevant_threshold , sys_average , precision ) self . __beta = beta @property def beta ( self ): return self . __beta def __str__ ( self ): return \"F {} - {} \" . format ( self . beta , self . sys_avg ) def __repr__ ( self ): return f \"FMeasure(beta= { self . beta } , relevant_threshold= { self . relevant_threshold } , \" \\ f \"sys_average= { self . sys_avg } , precision= { self . precision } )\" def _calc_metric ( self , confusion_matrix : np . ndarray ): prec = Precision () . _calc_metric ( confusion_matrix ) reca = Recall () . _calc_metric ( confusion_matrix ) beta_2 = self . beta ** 2 num = prec * reca den = ( beta_2 * prec ) + reca fbeta = self . precision (( 1 + beta_2 ) * ( den and num / den or 0 )) # safediv between num and den return fbeta def _perform_single_user ( self , user_prediction : List [ Interaction ], user_truth_relevant_items : Set [ str ]): tp = len ([ prediction_interaction for prediction_interaction in user_prediction if prediction_interaction . item_id in user_truth_relevant_items ]) fp = len ( user_prediction ) - tp fn = len ( user_truth_relevant_items ) - tp useful_confusion_matrix_user = np . array ([[ tp , fp ], [ fn , 0 ]], dtype = np . int32 ) return self . _calc_metric ( useful_confusion_matrix_user ), useful_confusion_matrix_user","title":"FMeasure"},{"location":"evaluation/metrics/classification_metrics/#clayrs.evaluation.metrics.classification_metrics.FMeasureAtK","text":"Bases: FMeasure The FMeasure@K metric combines Precision@K and Recall@K into a single metric. It is calculated as such for the single user : \\[ FMeasure_u = (1 + \\beta^2) \\cdot \\frac{P@K_u \\cdot R@K_u}{(\\beta^2 \\cdot P@K_u) + R@K_u} \\] Where: \\(P@K_u\\) is the Precision at K calculated for the user u \\(R@K_u\\) is the Recall at K calculated for the user u \\(\\beta\\) is a real factor which could weight differently Recall or Precision based on its value: \\(\\beta = 1\\) : Equally weight Precision and Recall \\(\\beta > 1\\) : Weight Recall more \\(\\beta < 1\\) : Weight Precision more A famous FMeasure@K is the F1@K Metric, where :math: \\beta = 1 , which basically is the harmonic mean of recall and precision: \\[ F1@K_u = \\frac{2 \\cdot P@K_u \\cdot R@K_u}{P@K_u + R@K_u} \\] The FMeasure@K metric is calculated as such for the entire system , depending if 'macro' average or 'micro' average has been chosen: \\[ FMeasure@K_{sys} - micro = (1 + \\beta^2) \\cdot \\frac{P@K_u \\cdot R@K_u}{(\\beta^2 \\cdot P@K_u) + R@K_u} \\] \\[ FMeasure@K_{sys} - macro = \\frac{\\sum_{u \\in U} FMeasure@K_u}{|U|} \\] Parameters: Name Type Description Default k int cutoff parameter. Will be used for the computation of Precision@K and Recall@K required beta float real factor which could weight differently Recall or Precision based on its value. Default is 1 1 relevant_threshold float parameter needed to discern relevant items and non-relevant items for every user. If not specified, the mean rating score of every user will be used None sys_average str specify how the system average must be computed. Default is 'macro' 'macro' Source code in clayrs/evaluation/metrics/classification_metrics.py 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 class FMeasureAtK ( FMeasure ): r \"\"\" The FMeasure@K metric combines Precision@K and Recall@K into a single metric. It is calculated as such for the **single user**: $$ FMeasure_u = (1 + \\beta^2) \\cdot \\frac{P@K_u \\cdot R@K_u}{(\\beta^2 \\cdot P@K_u) + R@K_u} $$ Where: - $P@K_u$ is the Precision at K calculated for the user *u* - $R@K_u$ is the Recall at K calculated for the user *u* - $\\beta$ is a real factor which could weight differently Recall or Precision based on its value: - $\\beta = 1$: Equally weight Precision and Recall - $\\beta > 1$: Weight Recall more - $\\beta < 1$: Weight Precision more A famous FMeasure@K is the F1@K Metric, where :math:`\\beta = 1`, which basically is the harmonic mean of recall and precision: $$ F1@K_u = \\frac{2 \\cdot P@K_u \\cdot R@K_u}{P@K_u + R@K_u} $$ The FMeasure@K metric is calculated as such for the **entire system**, depending if 'macro' average or 'micro' average has been chosen: $$ FMeasure@K_{sys} - micro = (1 + \\beta^2) \\cdot \\frac{P@K_u \\cdot R@K_u}{(\\beta^2 \\cdot P@K_u) + R@K_u} $$ $$ FMeasure@K_{sys} - macro = \\frac{\\sum_{u \\in U} FMeasure@K_u}{|U|} $$ Args: k (int): cutoff parameter. Will be used for the computation of Precision@K and Recall@K beta (float): real factor which could weight differently Recall or Precision based on its value. Default is 1 relevant_threshold (float): parameter needed to discern relevant items and non-relevant items for every user. If not specified, the mean rating score of every user will be used sys_average (str): specify how the system average must be computed. Default is 'macro' \"\"\" def __init__ ( self , k : int , beta : int = 1 , relevant_threshold : float = None , sys_average : str = 'macro' ): super () . __init__ ( beta , relevant_threshold , sys_average ) if k < 1 : raise ValueError ( 'k= {} not valid! k must be >= 1!' . format ( k )) self . __k = k @property def k ( self ): return self . __k def __str__ ( self ): return \"F {} @ {} - {} \" . format ( self . beta , self . k , self . sys_avg ) def __repr__ ( self ): return f \"FMeasureAtK(k= { self . k } , beta= { self . beta } , relevant_threshold= { self . relevant_threshold } , \" \\ f \"sys_average= { self . sys_avg } , precision= { self . precision } )\" def _perform_single_user ( self , user_prediction : List [ Interaction ], user_truth_relevant_items : Set [ str ]): user_prediction_cut = user_prediction [: self . k ] tp = len ([ prediction_interaction for prediction_interaction in user_prediction_cut if prediction_interaction . item_id in user_truth_relevant_items ]) fp = len ( user_prediction_cut ) - tp fn = len ( user_truth_relevant_items ) - tp useful_confusion_matrix_user = np . array ([[ tp , fp ], [ fn , 0 ]], dtype = np . int32 ) return self . _calc_metric ( useful_confusion_matrix_user ), useful_confusion_matrix_user","title":"FMeasureAtK"},{"location":"evaluation/metrics/classification_metrics/#clayrs.evaluation.metrics.classification_metrics.Precision","text":"Bases: ClassificationMetric The Precision metric is calculated as such for the single user : \\[ Precision_u = \\frac{tp_u}{tp_u + fp_u} \\] Where: \\(tp_u\\) is the number of items which are in the recommendation list of the user and have a rating >= relevant_threshold in its 'ground truth' \\(fp_u\\) is the number of items which are in the recommendation list of the user and have a rating < relevant_threshold in its 'ground truth' And it is calculated as such for the entire system , depending if 'macro' average or 'micro' average has been chosen: \\[ Precision_{sys} - micro = \\frac{\\sum_{u \\in U} tp_u}{\\sum_{u \\in U} tp_u + \\sum_{u \\in U} fp_u} \\] \\[ Precision_{sys} - macro = \\frac{\\sum_{u \\in U} Precision_u}{|U|} \\] Parameters: Name Type Description Default relevant_threshold float parameter needed to discern relevant items and non-relevant items for every user. If not specified, the mean rating score of every user will be used None sys_average str specify how the system average must be computed. Default is 'macro' 'macro' Source code in clayrs/evaluation/metrics/classification_metrics.py 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 class Precision ( ClassificationMetric ): r \"\"\" The Precision metric is calculated as such for the **single user**: $$ Precision_u = \\frac{tp_u}{tp_u + fp_u} $$ Where: - $tp_u$ is the number of items which are in the recommendation list of the user and have a rating >= relevant_threshold in its 'ground truth' - $fp_u$ is the number of items which are in the recommendation list of the user and have a rating < relevant_threshold in its 'ground truth' And it is calculated as such for the **entire system**, depending if 'macro' average or 'micro' average has been chosen: $$ Precision_{sys} - micro = \\frac{\\sum_{u \\in U} tp_u}{\\sum_{u \\in U} tp_u + \\sum_{u \\in U} fp_u} $$ $$ Precision_{sys} - macro = \\frac{\\sum_{u \\in U} Precision_u}{|U|} $$ Args: relevant_threshold: parameter needed to discern relevant items and non-relevant items for every user. If not specified, the mean rating score of every user will be used sys_average: specify how the system average must be computed. Default is 'macro' \"\"\" def __init__ ( self , relevant_threshold : float = None , sys_average : str = 'macro' , precision : [ Callable ] = np . float64 ): super ( Precision , self ) . __init__ ( relevant_threshold , sys_average , precision ) def __str__ ( self ): return \"Precision - {} \" . format ( self . sys_avg ) def __repr__ ( self ): return f \"Precision(relevant_threshold= { self . relevant_threshold } , sys_average= { self . sys_avg } , \" \\ f \"precision= { self . precision } )\" def _calc_metric ( self , confusion_matrix : np . ndarray ): tp = confusion_matrix [ 0 , 0 ] fp = confusion_matrix [ 0 , 1 ] return self . precision (( tp + fp ) and tp / ( tp + fp ) or 0 ) # safediv between tp and (tp + fp) def _perform_single_user ( self , user_prediction : List [ Interaction ], user_truth_relevant_items : Set [ str ]): tp = len ([ prediction_interaction for prediction_interaction in user_prediction if prediction_interaction . item_id in user_truth_relevant_items ]) fp = len ( user_prediction ) - tp # we do not compute the full confusion matrix for the user useful_confusion_matrix_user = np . array ([[ tp , fp ], [ 0 , 0 ]], dtype = np . int32 ) return self . _calc_metric ( useful_confusion_matrix_user ), useful_confusion_matrix_user","title":"Precision"},{"location":"evaluation/metrics/classification_metrics/#clayrs.evaluation.metrics.classification_metrics.PrecisionAtK","text":"Bases: Precision The Precision@K metric is calculated as such for the single user : \\[ Precision@K_u = \\frac{tp@K_u}{tp@K_u + fp@K_u} \\] Where: \\(tp@K_u\\) is the number of items which are in the recommendation list of the user cutoff to the first K items and have a rating >= relevant_threshold in its 'ground truth' \\(tp@K_u\\) is the number of items which are in the recommendation list of the user cutoff to the first K items and have a rating < relevant_threshold in its 'ground truth' And it is calculated as such for the entire system , depending if 'macro' average or 'micro' average has been chosen: \\[ Precision@K_{sys} - micro = \\frac{\\sum_{u \\in U} tp@K_u}{\\sum_{u \\in U} tp@K_u + \\sum_{u \\in U} fp@K_u} \\] \\[ Precision@K_{sys} - macro = \\frac{\\sum_{u \\in U} Precision@K_u}{|U|} \\] Parameters: Name Type Description Default k int cutoff parameter. Only the first k items of the recommendation list will be considered required relevant_threshold float parameter needed to discern relevant items and non-relevant items for every user. If not specified, the mean rating score of every user will be used None sys_average str specify how the system average must be computed. Default is 'macro' 'macro' Source code in clayrs/evaluation/metrics/classification_metrics.py 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 class PrecisionAtK ( Precision ): r \"\"\" The Precision@K metric is calculated as such for the **single user**: $$ Precision@K_u = \\frac{tp@K_u}{tp@K_u + fp@K_u} $$ Where: - $tp@K_u$ is the number of items which are in the recommendation list of the user **cutoff to the first K items** and have a rating >= relevant_threshold in its 'ground truth' - $tp@K_u$ is the number of items which are in the recommendation list of the user **cutoff to the first K items** and have a rating < relevant_threshold in its 'ground truth' And it is calculated as such for the **entire system**, depending if 'macro' average or 'micro' average has been chosen: $$ Precision@K_{sys} - micro = \\frac{\\sum_{u \\in U} tp@K_u}{\\sum_{u \\in U} tp@K_u + \\sum_{u \\in U} fp@K_u} $$ $$ Precision@K_{sys} - macro = \\frac{\\sum_{u \\in U} Precision@K_u}{|U|} $$ Args: k (int): cutoff parameter. Only the first k items of the recommendation list will be considered relevant_threshold (float): parameter needed to discern relevant items and non-relevant items for every user. If not specified, the mean rating score of every user will be used sys_average (str): specify how the system average must be computed. Default is 'macro' \"\"\" def __init__ ( self , k : int , relevant_threshold : float = None , sys_average : str = 'macro' , precision : [ Callable ] = np . float64 ): super () . __init__ ( relevant_threshold , sys_average , precision ) if k < 1 : raise ValueError ( 'k= {} not valid! k must be >= 1!' . format ( k )) self . __k = k @property def k ( self ): return self . __k def __str__ ( self ): return \"Precision@ {} - {} \" . format ( self . k , self . sys_avg ) def __repr__ ( self ): return f \"PrecisionAtK(k= { self . k } , relevant_threshold= { self . relevant_threshold } , sys_average= { self . sys_avg } , \" \\ f \"precision= { self . precision } )\" def _perform_single_user ( self , user_prediction : List [ Interaction ], user_truth_relevant_items : Set [ str ]): user_prediction_cut = user_prediction [: self . k ] tp = len ([ prediction_interaction for prediction_interaction in user_prediction_cut if prediction_interaction . item_id in user_truth_relevant_items ]) fp = len ( user_prediction_cut ) - tp useful_confusion_matrix_user = np . array ([[ tp , fp ], [ 0 , 0 ]], dtype = np . int32 ) return self . _calc_metric ( useful_confusion_matrix_user ), useful_confusion_matrix_user","title":"PrecisionAtK"},{"location":"evaluation/metrics/classification_metrics/#clayrs.evaluation.metrics.classification_metrics.RPrecision","text":"Bases: Precision The R-Precision metric is calculated as such for the single user : \\[ R-Precision_u = \\frac{tp@R_u}{tp@R_u + fp@R_u} \\] Where: \\(R\\) it's the number of relevant items for the user u \\(tp@R_u\\) is the number of items which are in the recommendation list of the user cutoff to the first R items and have a rating >= relevant_threshold in its 'ground truth' \\(tp@R_u\\) is the number of items which are in the recommendation list of the user cutoff to the first R items and have a rating < relevant_threshold in its 'ground truth' And it is calculated as such for the entire system , depending if 'macro' average or 'micro' average has been chosen: \\[ Precision@K_{sys} - micro = \\frac{\\sum_{u \\in U} tp@R_u}{\\sum_{u \\in U} tp@R_u + \\sum_{u \\in U} fp@R_u} \\] \\[ Precision@K_{sys} - macro = \\frac{\\sum_{u \\in U} R-Precision_u}{|U|} \\] Parameters: Name Type Description Default relevant_threshold parameter needed to discern relevant items and non-relevant items for every user. If not specified, the mean rating score of every user will be used required sys_average specify how the system average must be computed. Default is 'macro' required Source code in clayrs/evaluation/metrics/classification_metrics.py 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 class RPrecision ( Precision ): r \"\"\" The R-Precision metric is calculated as such for the **single user**: $$ R-Precision_u = \\frac{tp@R_u}{tp@R_u + fp@R_u} $$ Where: - $R$ it's the number of relevant items for the user *u* - $tp@R_u$ is the number of items which are in the recommendation list of the user **cutoff to the first R items** and have a rating >= relevant_threshold in its 'ground truth' - $tp@R_u$ is the number of items which are in the recommendation list of the user **cutoff to the first R items** and have a rating < relevant_threshold in its 'ground truth' And it is calculated as such for the **entire system**, depending if 'macro' average or 'micro' average has been chosen: $$ Precision@K_{sys} - micro = \\frac{\\sum_{u \\in U} tp@R_u}{\\sum_{u \\in U} tp@R_u + \\sum_{u \\in U} fp@R_u} $$ $$ Precision@K_{sys} - macro = \\frac{\\sum_{u \\in U} R-Precision_u}{|U|} $$ Args: relevant_threshold: parameter needed to discern relevant items and non-relevant items for every user. If not specified, the mean rating score of every user will be used sys_average: specify how the system average must be computed. Default is 'macro' \"\"\" def __str__ ( self ): return \"R-Precision - {} \" . format ( self . sys_avg ) def __repr__ ( self ): return f \"RPrecision(relevant_threshold= { self . relevant_threshold } , sys_average= { self . sys_avg } , \" \\ f \"precision= { self . precision } )\" def _perform_single_user ( self , user_prediction : List [ Interaction ], user_truth_relevant_items : Set [ str ]): r = len ( user_truth_relevant_items ) user_prediction_cut = user_prediction [: r ] tp = len ([ prediction_interaction for prediction_interaction in user_prediction_cut if prediction_interaction . item_id in user_truth_relevant_items ]) fp = len ( user_prediction_cut ) - tp useful_confusion_matrix_user = np . array ([[ tp , fp ], [ 0 , 0 ]], dtype = np . int32 ) return self . _calc_metric ( useful_confusion_matrix_user ), useful_confusion_matrix_user","title":"RPrecision"},{"location":"evaluation/metrics/classification_metrics/#clayrs.evaluation.metrics.classification_metrics.Recall","text":"Bases: ClassificationMetric The Recall metric is calculated as such for the single user : \\[ Recall_u = \\frac{tp_u}{tp_u + fn_u} \\] Where: \\(tp_u\\) is the number of items which are in the recommendation list of the user and have a rating >= relevant_threshold in its 'ground truth' \\(fn_u\\) is the number of items which are NOT in the recommendation list of the user and have a rating >= relevant_threshold in its 'ground truth' And it is calculated as such for the entire system , depending if 'macro' average or 'micro' average has been chosen: \\[ Recall_{sys} - micro = \\frac{\\sum_{u \\in U} tp_u}{\\sum_{u \\in U} tp_u + \\sum_{u \\in U} fn_u} \\] \\[ Recall_{sys} - macro = \\frac{\\sum_{u \\in U} Recall_u}{|U|} \\] Source code in clayrs/evaluation/metrics/classification_metrics.py 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 class Recall ( ClassificationMetric ): r \"\"\" The Recall metric is calculated as such for the **single user**: $$ Recall_u = \\frac{tp_u}{tp_u + fn_u} $$ Where: - $tp_u$ is the number of items which are in the recommendation list of the user and have a rating >= relevant_threshold in its 'ground truth' - $fn_u$ is the number of items which are NOT in the recommendation list of the user and have a rating >= relevant_threshold in its 'ground truth' And it is calculated as such for the **entire system**, depending if 'macro' average or 'micro' average has been chosen: $$ Recall_{sys} - micro = \\frac{\\sum_{u \\in U} tp_u}{\\sum_{u \\in U} tp_u + \\sum_{u \\in U} fn_u} $$ $$ Recall_{sys} - macro = \\frac{\\sum_{u \\in U} Recall_u}{|U|} $$ \"\"\" def __str__ ( self ): return \"Recall - {} \" . format ( self . sys_avg ) def __repr__ ( self ): return f \"Recall(relevant_threshold= { self . relevant_threshold } , sys_average= { self . sys_avg } , \" \\ f \"precision= { self . precision } )\" def _calc_metric ( self , confusion_matrix : np . ndarray ): tp = confusion_matrix [ 0 , 0 ] fn = confusion_matrix [ 1 , 0 ] return self . precision (( tp + fn ) and tp / ( tp + fn ) or 0 ) # safediv between tp and (tp + fn) def _perform_single_user ( self , user_prediction : List [ Interaction ], user_truth_relevant_items : Set [ str ]): tp = len ([ prediction_interaction for prediction_interaction in user_prediction if prediction_interaction . item_id in user_truth_relevant_items ]) fn = len ( user_truth_relevant_items ) - tp useful_confusion_matrix_user = np . array ([[ tp , 0 ], [ fn , 0 ]], dtype = np . int32 ) return self . _calc_metric ( useful_confusion_matrix_user ), useful_confusion_matrix_user","title":"Recall"},{"location":"evaluation/metrics/classification_metrics/#clayrs.evaluation.metrics.classification_metrics.RecallAtK","text":"Bases: Recall The Recall@K metric is calculated as such for the single user : \\[ Recall@K_u = \\frac{tp@K_u}{tp@K_u + fn@K_u} \\] Where: \\(tp@K_u\\) is the number of items which are in the recommendation list of the user cutoff to the first K items and have a rating >= relevant_threshold in its 'ground truth' \\(tp@K_u\\) is the number of items which are NOT in the recommendation list of the user cutoff to the first K items and have a rating >= relevant_threshold in its 'ground truth' And it is calculated as such for the entire system , depending if 'macro' average or 'micro' average has been chosen: \\[ Recall@K_{sys} - micro = \\frac{\\sum_{u \\in U} tp@K_u}{\\sum_{u \\in U} tp@K_u + \\sum_{u \\in U} fn@K_u} \\] \\[ Recall@K_{sys} - macro = \\frac{\\sum_{u \\in U} Recall@K_u}{|U|} \\] Parameters: Name Type Description Default k int cutoff parameter. Only the first k items of the recommendation list will be considered required relevant_threshold float parameter needed to discern relevant items and non-relevant items for every user. If not specified, the mean rating score of every user will be used None sys_average str specify how the system average must be computed. Default is 'macro' 'macro' Source code in clayrs/evaluation/metrics/classification_metrics.py 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 class RecallAtK ( Recall ): r \"\"\" The Recall@K metric is calculated as such for the **single user**: $$ Recall@K_u = \\frac{tp@K_u}{tp@K_u + fn@K_u} $$ Where: - $tp@K_u$ is the number of items which are in the recommendation list of the user **cutoff to the first K items** and have a rating >= relevant_threshold in its 'ground truth' - $tp@K_u$ is the number of items which are NOT in the recommendation list of the user **cutoff to the first K items** and have a rating >= relevant_threshold in its 'ground truth' And it is calculated as such for the **entire system**, depending if 'macro' average or 'micro' average has been chosen: $$ Recall@K_{sys} - micro = \\frac{\\sum_{u \\in U} tp@K_u}{\\sum_{u \\in U} tp@K_u + \\sum_{u \\in U} fn@K_u} $$ $$ Recall@K_{sys} - macro = \\frac{\\sum_{u \\in U} Recall@K_u}{|U|} $$ Args: k (int): cutoff parameter. Only the first k items of the recommendation list will be considered relevant_threshold (float): parameter needed to discern relevant items and non-relevant items for every user. If not specified, the mean rating score of every user will be used sys_average (str): specify how the system average must be computed. Default is 'macro' \"\"\" def __init__ ( self , k : int , relevant_threshold : float = None , sys_average : str = 'macro' , precision : [ Callable ] = np . float64 ): super () . __init__ ( relevant_threshold , sys_average , precision ) if k < 1 : raise ValueError ( 'k= {} not valid! k must be >= 1!' . format ( k )) self . __k = k @property def k ( self ): return self . __k def __str__ ( self ): return \"Recall@ {} - {} \" . format ( self . k , self . sys_avg ) def __repr__ ( self ): return f \"RecallAtK(k= { self . k } , relevant_threshold= { self . relevant_threshold } , sys_average= { self . sys_avg } , \" \\ f \"precision= { self . precision } )\" def _perform_single_user ( self , user_prediction : List [ Interaction ], user_truth_relevant_items : Set [ str ]): user_prediction_cut = user_prediction [: self . k ] tp = len ([ prediction_interaction for prediction_interaction in user_prediction_cut if prediction_interaction . item_id in user_truth_relevant_items ]) fn = len ( user_truth_relevant_items ) - tp useful_confusion_matrix_user = np . array ([[ tp , 0 ], [ fn , 0 ]], dtype = np . int32 ) return self . _calc_metric ( useful_confusion_matrix_user ), useful_confusion_matrix_user","title":"RecallAtK"},{"location":"evaluation/metrics/error_metrics/","text":"Error metrics MAE Bases: ErrorMetric The MAE (Mean Absolute Error) metric is calculated as such for the single user : \\[ MAE_u = \\sum_{i \\in T_u} \\frac{|r_{u,i} - \\hat{r}_{u,i}|}{|T_u|} \\] Where: \\(T_u\\) is the test set of the user \\(u\\) \\(r_{u, i}\\) is the actual score give by user \\(u\\) to item \\(i\\) \\(\\hat{r}_{u, i}\\) is the predicted score give by user \\(u\\) to item \\(i\\) And it is calculated as such for the entire system : \\[ MAE_{sys} = \\sum_{u \\in T} \\frac{MAE_u}{|T|} \\] Where: \\(T\\) is the test set \\(MAE_u\\) is the MAE calculated for user \\(u\\) There may be cases in which some items of the test set of the user could not be predicted (eg. A CBRS was chosen and items were not present locally, a methodology different than TestRatings was chosen). In those cases the \\(MAE_u\\) formula becomes \\[ MAE_u = \\sum_{i \\in T_u} \\frac{|r_{u,i} - \\hat{r}_{u,i}|}{|T_u| - unk} \\] Where: \\(unk\\) ( unknown ) is the number of items of the user test set that could not be predicted If no items of the user test set has been predicted ( \\(|T_u| - unk = 0\\) ), then: \\[ MAE_u = NaN \\] Source code in clayrs/evaluation/metrics/error_metrics.py 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 class MAE ( ErrorMetric ): r \"\"\" The MAE (Mean Absolute Error) metric is calculated as such for the **single user**: $$ MAE_u = \\sum_{i \\in T_u} \\frac{|r_{u,i} - \\hat{r}_{u,i}|}{|T_u|} $$ Where: - $T_u$ is the *test set* of the user $u$ - $r_{u, i}$ is the actual score give by user $u$ to item $i$ - $\\hat{r}_{u, i}$ is the predicted score give by user $u$ to item $i$ And it is calculated as such for the **entire system**: $$ MAE_{sys} = \\sum_{u \\in T} \\frac{MAE_u}{|T|} $$ Where: - $T$ is the *test set* - $MAE_u$ is the MAE calculated for user $u$ There may be cases in which some items of the *test set* of the user could not be predicted (eg. A CBRS was chosen and items were not present locally, a methodology different than *TestRatings* was chosen). In those cases the $MAE_u$ formula becomes $$ MAE_u = \\sum_{i \\in T_u} \\frac{|r_{u,i} - \\hat{r}_{u,i}|}{|T_u| - unk} $$ Where: - $unk$ (*unknown*) is the number of items of the *user test set* that could not be predicted If no items of the user test set has been predicted ($|T_u| - unk = 0$), then: $$ MAE_u = NaN $$ \"\"\" def __str__ ( self ): return \"MAE\" def __repr__ ( self ): return f \"MAE()\" def _calc_metric ( self , truth_scores : list , pred_scores : list ): return mean_absolute_error ( truth_scores , pred_scores ) MSE Bases: ErrorMetric The MSE (Mean Squared Error) metric is calculated as such for the single user : \\[ MSE_u = \\sum_{i \\in T_u} \\frac{(r_{u,i} - \\hat{r}_{u,i})^2}{|T_u|} \\] Where: \\(T_u\\) is the test set of the user :math: u \\(r_{u, i}\\) is the actual score give by user :math: u to item :math: i \\(\\hat{r}_{u, i}\\) is the predicted score give by user :math: u to item :math: i And it is calculated as such for the entire system : $$ MSE_{sys} = \\sum_{u \\in T} \\frac{MSE_u}{|T|} $$ Where: \\(T\\) is the test set \\(MSE_u\\) is the MSE calculated for user \\(u\\) There may be cases in which some items of the test set of the user could not be predicted (eg. A CBRS was chosen and items were not present locally) In those cases the \\(MSE_u\\) formula becomes \\[ MSE_u = \\sum_{i \\in T_u} \\frac{(r_{u,i} - \\hat{r}_{u,i})^2}{|T_u| - unk} \\] Where: \\(unk\\) ( unknown ) is the number of items of the user test set that could not be predicted If no items of the user test set has been predicted ( \\(|T_u| - unk = 0\\) ), then: \\[ MSE_u = NaN \\] Source code in clayrs/evaluation/metrics/error_metrics.py 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 class MSE ( ErrorMetric ): r \"\"\" The MSE (Mean Squared Error) metric is calculated as such for the **single user**: $$ MSE_u = \\sum_{i \\in T_u} \\frac{(r_{u,i} - \\hat{r}_{u,i})^2}{|T_u|} $$ Where: - $T_u$ is the *test set* of the user :math:`u` - $r_{u, i}$ is the actual score give by user :math:`u` to item :math:`i` - $\\hat{r}_{u, i}$ is the predicted score give by user :math:`u` to item :math:`i` And it is calculated as such for the **entire system**: $$ MSE_{sys} = \\sum_{u \\in T} \\frac{MSE_u}{|T|} $$ Where: - $T$ is the *test set* - $MSE_u$ is the MSE calculated for user $u$ There may be cases in which some items of the *test set* of the user could not be predicted (eg. A CBRS was chosen and items were not present locally) In those cases the $MSE_u$ formula becomes $$ MSE_u = \\sum_{i \\in T_u} \\frac{(r_{u,i} - \\hat{r}_{u,i})^2}{|T_u| - unk} $$ Where: - $unk$ (*unknown*) is the number of items of the *user test set* that could not be predicted If no items of the user test set has been predicted ($|T_u| - unk = 0$), then: $$ MSE_u = NaN $$ \"\"\" def __str__ ( self ): return \"MSE\" def __repr__ ( self ): return \"MSE()\" def _calc_metric ( self , truth_scores : list , pred_scores : list ): return mean_squared_error ( truth_scores , pred_scores ) RMSE Bases: ErrorMetric The RMSE (Root Mean Squared Error) metric is calculated as such for the single user : \\[ RMSE_u = \\sqrt{\\sum_{i \\in T_u} \\frac{(r_{u,i} - \\hat{r}_{u,i})^2}{|T_u|}} \\] Where: \\(T_u\\) is the test set of the user :math: u \\(r_{u, i}\\) is the actual score give by user :math: u to item :math: i \\(\\hat{r}_{u, i}\\) is the predicted score give by user :math: u to item :math: i And it is calculated as such for the entire system : \\[ RMSE_{sys} = \\sum_{u \\in T} \\frac{RMSE_u}{|T|} \\] Where: \\(T\\) is the test set \\(RMSE_u\\) is the RMSE calculated for user :math: u There may be cases in which some items of the test set of the user could not be predicted (eg. A CBRS was chosen and items were not present locally, a methodology different than TestRatings was chosen). In those cases the \\(RMSE_u\\) formula becomes \\[ RMSE_u = \\sqrt{\\sum_{i \\in T_u} \\frac{(r_{u,i} - \\hat{r}_{u,i})^2}{|T_u| - unk}} \\] Where: \\(unk\\) ( unknown ) is the number of items of the user test set that could not be predicted If no items of the user test set has been predicted ( \\(|T_u| - unk = 0\\) ), then: \\[ RMSE_u = NaN \\] Source code in clayrs/evaluation/metrics/error_metrics.py 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 class RMSE ( ErrorMetric ): r \"\"\" The RMSE (Root Mean Squared Error) metric is calculated as such for the **single user**: $$ RMSE_u = \\sqrt{\\sum_{i \\in T_u} \\frac{(r_{u,i} - \\hat{r}_{u,i})^2}{|T_u|}} $$ Where: - $T_u$ is the *test set* of the user :math:`u` - $r_{u, i}$ is the actual score give by user :math:`u` to item :math:`i` - $\\hat{r}_{u, i}$ is the predicted score give by user :math:`u` to item :math:`i` And it is calculated as such for the **entire system**: $$ RMSE_{sys} = \\sum_{u \\in T} \\frac{RMSE_u}{|T|} $$ Where: - $T$ is the *test set* - $RMSE_u$ is the RMSE calculated for user :math:`u` There may be cases in which some items of the *test set* of the user could not be predicted (eg. A CBRS was chosen and items were not present locally, a methodology different than *TestRatings* was chosen). In those cases the $RMSE_u$ formula becomes $$ RMSE_u = \\sqrt{\\sum_{i \\in T_u} \\frac{(r_{u,i} - \\hat{r}_{u,i})^2}{|T_u| - unk}} $$ Where: - $unk$ (*unknown*) is the number of items of the *user test set* that could not be predicted If no items of the user test set has been predicted ($|T_u| - unk = 0$), then: $$ RMSE_u = NaN $$ \"\"\" def __str__ ( self ): return \"RMSE\" def __repr__ ( self ): return f \"RMSE()\" def _calc_metric ( self , truth_scores : list , pred_scores : list ): return mean_squared_error ( truth_scores , pred_scores , squared = False )","title":"Error metrics"},{"location":"evaluation/metrics/error_metrics/#error-metrics","text":"","title":"Error metrics"},{"location":"evaluation/metrics/error_metrics/#clayrs.evaluation.metrics.error_metrics.MAE","text":"Bases: ErrorMetric The MAE (Mean Absolute Error) metric is calculated as such for the single user : \\[ MAE_u = \\sum_{i \\in T_u} \\frac{|r_{u,i} - \\hat{r}_{u,i}|}{|T_u|} \\] Where: \\(T_u\\) is the test set of the user \\(u\\) \\(r_{u, i}\\) is the actual score give by user \\(u\\) to item \\(i\\) \\(\\hat{r}_{u, i}\\) is the predicted score give by user \\(u\\) to item \\(i\\) And it is calculated as such for the entire system : \\[ MAE_{sys} = \\sum_{u \\in T} \\frac{MAE_u}{|T|} \\] Where: \\(T\\) is the test set \\(MAE_u\\) is the MAE calculated for user \\(u\\) There may be cases in which some items of the test set of the user could not be predicted (eg. A CBRS was chosen and items were not present locally, a methodology different than TestRatings was chosen). In those cases the \\(MAE_u\\) formula becomes \\[ MAE_u = \\sum_{i \\in T_u} \\frac{|r_{u,i} - \\hat{r}_{u,i}|}{|T_u| - unk} \\] Where: \\(unk\\) ( unknown ) is the number of items of the user test set that could not be predicted If no items of the user test set has been predicted ( \\(|T_u| - unk = 0\\) ), then: \\[ MAE_u = NaN \\] Source code in clayrs/evaluation/metrics/error_metrics.py 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 class MAE ( ErrorMetric ): r \"\"\" The MAE (Mean Absolute Error) metric is calculated as such for the **single user**: $$ MAE_u = \\sum_{i \\in T_u} \\frac{|r_{u,i} - \\hat{r}_{u,i}|}{|T_u|} $$ Where: - $T_u$ is the *test set* of the user $u$ - $r_{u, i}$ is the actual score give by user $u$ to item $i$ - $\\hat{r}_{u, i}$ is the predicted score give by user $u$ to item $i$ And it is calculated as such for the **entire system**: $$ MAE_{sys} = \\sum_{u \\in T} \\frac{MAE_u}{|T|} $$ Where: - $T$ is the *test set* - $MAE_u$ is the MAE calculated for user $u$ There may be cases in which some items of the *test set* of the user could not be predicted (eg. A CBRS was chosen and items were not present locally, a methodology different than *TestRatings* was chosen). In those cases the $MAE_u$ formula becomes $$ MAE_u = \\sum_{i \\in T_u} \\frac{|r_{u,i} - \\hat{r}_{u,i}|}{|T_u| - unk} $$ Where: - $unk$ (*unknown*) is the number of items of the *user test set* that could not be predicted If no items of the user test set has been predicted ($|T_u| - unk = 0$), then: $$ MAE_u = NaN $$ \"\"\" def __str__ ( self ): return \"MAE\" def __repr__ ( self ): return f \"MAE()\" def _calc_metric ( self , truth_scores : list , pred_scores : list ): return mean_absolute_error ( truth_scores , pred_scores )","title":"MAE"},{"location":"evaluation/metrics/error_metrics/#clayrs.evaluation.metrics.error_metrics.MSE","text":"Bases: ErrorMetric The MSE (Mean Squared Error) metric is calculated as such for the single user : \\[ MSE_u = \\sum_{i \\in T_u} \\frac{(r_{u,i} - \\hat{r}_{u,i})^2}{|T_u|} \\] Where: \\(T_u\\) is the test set of the user :math: u \\(r_{u, i}\\) is the actual score give by user :math: u to item :math: i \\(\\hat{r}_{u, i}\\) is the predicted score give by user :math: u to item :math: i And it is calculated as such for the entire system : $$ MSE_{sys} = \\sum_{u \\in T} \\frac{MSE_u}{|T|} $$ Where: \\(T\\) is the test set \\(MSE_u\\) is the MSE calculated for user \\(u\\) There may be cases in which some items of the test set of the user could not be predicted (eg. A CBRS was chosen and items were not present locally) In those cases the \\(MSE_u\\) formula becomes \\[ MSE_u = \\sum_{i \\in T_u} \\frac{(r_{u,i} - \\hat{r}_{u,i})^2}{|T_u| - unk} \\] Where: \\(unk\\) ( unknown ) is the number of items of the user test set that could not be predicted If no items of the user test set has been predicted ( \\(|T_u| - unk = 0\\) ), then: \\[ MSE_u = NaN \\] Source code in clayrs/evaluation/metrics/error_metrics.py 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 class MSE ( ErrorMetric ): r \"\"\" The MSE (Mean Squared Error) metric is calculated as such for the **single user**: $$ MSE_u = \\sum_{i \\in T_u} \\frac{(r_{u,i} - \\hat{r}_{u,i})^2}{|T_u|} $$ Where: - $T_u$ is the *test set* of the user :math:`u` - $r_{u, i}$ is the actual score give by user :math:`u` to item :math:`i` - $\\hat{r}_{u, i}$ is the predicted score give by user :math:`u` to item :math:`i` And it is calculated as such for the **entire system**: $$ MSE_{sys} = \\sum_{u \\in T} \\frac{MSE_u}{|T|} $$ Where: - $T$ is the *test set* - $MSE_u$ is the MSE calculated for user $u$ There may be cases in which some items of the *test set* of the user could not be predicted (eg. A CBRS was chosen and items were not present locally) In those cases the $MSE_u$ formula becomes $$ MSE_u = \\sum_{i \\in T_u} \\frac{(r_{u,i} - \\hat{r}_{u,i})^2}{|T_u| - unk} $$ Where: - $unk$ (*unknown*) is the number of items of the *user test set* that could not be predicted If no items of the user test set has been predicted ($|T_u| - unk = 0$), then: $$ MSE_u = NaN $$ \"\"\" def __str__ ( self ): return \"MSE\" def __repr__ ( self ): return \"MSE()\" def _calc_metric ( self , truth_scores : list , pred_scores : list ): return mean_squared_error ( truth_scores , pred_scores )","title":"MSE"},{"location":"evaluation/metrics/error_metrics/#clayrs.evaluation.metrics.error_metrics.RMSE","text":"Bases: ErrorMetric The RMSE (Root Mean Squared Error) metric is calculated as such for the single user : \\[ RMSE_u = \\sqrt{\\sum_{i \\in T_u} \\frac{(r_{u,i} - \\hat{r}_{u,i})^2}{|T_u|}} \\] Where: \\(T_u\\) is the test set of the user :math: u \\(r_{u, i}\\) is the actual score give by user :math: u to item :math: i \\(\\hat{r}_{u, i}\\) is the predicted score give by user :math: u to item :math: i And it is calculated as such for the entire system : \\[ RMSE_{sys} = \\sum_{u \\in T} \\frac{RMSE_u}{|T|} \\] Where: \\(T\\) is the test set \\(RMSE_u\\) is the RMSE calculated for user :math: u There may be cases in which some items of the test set of the user could not be predicted (eg. A CBRS was chosen and items were not present locally, a methodology different than TestRatings was chosen). In those cases the \\(RMSE_u\\) formula becomes \\[ RMSE_u = \\sqrt{\\sum_{i \\in T_u} \\frac{(r_{u,i} - \\hat{r}_{u,i})^2}{|T_u| - unk}} \\] Where: \\(unk\\) ( unknown ) is the number of items of the user test set that could not be predicted If no items of the user test set has been predicted ( \\(|T_u| - unk = 0\\) ), then: \\[ RMSE_u = NaN \\] Source code in clayrs/evaluation/metrics/error_metrics.py 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 class RMSE ( ErrorMetric ): r \"\"\" The RMSE (Root Mean Squared Error) metric is calculated as such for the **single user**: $$ RMSE_u = \\sqrt{\\sum_{i \\in T_u} \\frac{(r_{u,i} - \\hat{r}_{u,i})^2}{|T_u|}} $$ Where: - $T_u$ is the *test set* of the user :math:`u` - $r_{u, i}$ is the actual score give by user :math:`u` to item :math:`i` - $\\hat{r}_{u, i}$ is the predicted score give by user :math:`u` to item :math:`i` And it is calculated as such for the **entire system**: $$ RMSE_{sys} = \\sum_{u \\in T} \\frac{RMSE_u}{|T|} $$ Where: - $T$ is the *test set* - $RMSE_u$ is the RMSE calculated for user :math:`u` There may be cases in which some items of the *test set* of the user could not be predicted (eg. A CBRS was chosen and items were not present locally, a methodology different than *TestRatings* was chosen). In those cases the $RMSE_u$ formula becomes $$ RMSE_u = \\sqrt{\\sum_{i \\in T_u} \\frac{(r_{u,i} - \\hat{r}_{u,i})^2}{|T_u| - unk}} $$ Where: - $unk$ (*unknown*) is the number of items of the *user test set* that could not be predicted If no items of the user test set has been predicted ($|T_u| - unk = 0$), then: $$ RMSE_u = NaN $$ \"\"\" def __str__ ( self ): return \"RMSE\" def __repr__ ( self ): return f \"RMSE()\" def _calc_metric ( self , truth_scores : list , pred_scores : list ): return mean_squared_error ( truth_scores , pred_scores , squared = False )","title":"RMSE"},{"location":"evaluation/metrics/fairness_metrics/","text":"Fairness metrics CatalogCoverage Bases: PredictionCoverage The Catalog Coverage metric measures in percentage how many distinct items are being recommended in relation to all available items. It's a system wide metric, so only its result it will be returned and not those of every user. It differs from the Prediction Coverage since it allows for different parameters to come into play. If no parameter is passed then it's a simple Prediction Coverage. The metric is calculated as such: \\[ Catalog Coverage_{sys} = (\\frac{|\\bigcup_{j=1...N}reclist(u_j)|}{|I|})\\cdot100 \\] Where: \\(N\\) is the total number of users \\(reclist(u_j)\\) is the set of items contained in the recommendation list of user \\(j\\) \\(I\\) is the set of all available items The \\(I\\) must be specified through the 'catalog' parameter The recommendation list of every user ( \\(reclist(u_j)\\) ) can be reduced to the first n parameter with the top-n parameter, so that catalog coverage is measured considering only the most highest ranked items. With the 'k' parameter one could specify the number of users that will be used to calculate catalog coverage: k users will be randomly sampled and their recommendation lists will be used. The formula above becomes: \\[ Catalog Coverage_{sys} = (\\frac{|\\bigcup_{j=1...k}reclist(u_j)|}{|I|})\\cdot100 \\] Where: \\(k\\) is the parameter specified Obviously 'k' < N, else simply recommendation lists of all users will be used Check the 'Beyond Accuracy: Evaluating Recommender Systems by Coverage and Serendipity' paper and page 13 of the 'Comparison of group recommendation algorithms' paper for more Source code in clayrs/evaluation/metrics/fairness_metrics.py 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 class CatalogCoverage ( PredictionCoverage ): r \"\"\" The Catalog Coverage metric measures in percentage how many distinct items are being recommended in relation to all available items. It's a system wide metric, so only its result it will be returned and not those of every user. It differs from the Prediction Coverage since it allows for different parameters to come into play. If no parameter is passed then it's a simple Prediction Coverage. The metric is calculated as such: $$ Catalog Coverage_{sys} = (\\frac{|\\bigcup_{j=1...N}reclist(u_j)|}{|I|})\\cdot100 $$ Where: - $N$ is the total number of users - $reclist(u_j)$ is the set of items contained in the recommendation list of user $j$ - $I$ is the set of all available items The $I$ must be specified through the 'catalog' parameter The recommendation list of every user ($reclist(u_j)$) can be reduced to the first *n* parameter with the top-n parameter, so that catalog coverage is measured considering only the most highest ranked items. With the 'k' parameter one could specify the number of users that will be used to calculate catalog coverage: k users will be randomly sampled and their recommendation lists will be used. The formula above becomes: $$ Catalog Coverage_{sys} = (\\frac{|\\bigcup_{j=1...k}reclist(u_j)|}{|I|})\\cdot100 $$ Where: - $k$ is the parameter specified Obviously 'k' < N, else simply recommendation lists of all users will be used Check the 'Beyond Accuracy: Evaluating Recommender Systems by Coverage and Serendipity' paper and page 13 of the 'Comparison of group recommendation algorithms' paper for more \"\"\" def __init__ ( self , catalog : Set [ str ], top_n : int = None , k : int = None ): super () . __init__ ( catalog ) self . __top_n = top_n self . __k = k def __str__ ( self ): # If none of the parameter is passed, then it's simply a PredictionCoverage name = \"CatalogCoverage (PredictionCov)\" if self . __top_n : name = \"CatalogCoverage\" name += \" - Top {} \" . format ( self . __top_n ) if self . __k : name = \"CatalogCoverage\" name += \" - {} sampled users\" . format ( self . __k ) return name def __repr__ ( self ): return f 'CatalogCoverage(catalog= { self . catalog } , top_n= { self . __top_n } , k= { self . __k } )' def _get_covered ( self , pred : Ratings ): catalog = self . catalog if self . __top_n is not None : pred = list ( itertools . chain . from_iterable ([ pred . get_user_interactions ( user_id , self . __top_n ) for user_id in set ( pred . user_id_column )])) # IF k is passed, then we choose randomly k users and calc catalog coverage # based on their predictions. We check that k is < n_user since if it's the equal # or it's greater, then all predictions generated for all user must be used if self . __k is not None and self . __k < len ( pred ): user_list = list ( set ([ interaction_pred . user_id for interaction_pred in pred ])) sampling = random . choices ( user_list , k = self . __k ) predicted_items = set ([ interaction_pred . item_id for interaction_pred in pred if interaction_pred . user_id in sampling ]) covered_items = predicted_items . intersection ( catalog ) else : predicted_items = set ([ interaction_pred . item_id for interaction_pred in pred ]) covered_items = predicted_items . intersection ( catalog ) return covered_items DeltaGap Bases: GroupFairnessMetric The Delta GAP (Group Average popularity) metric lets you compare the average popularity \"requested\" by one or multiple groups of users and the average popularity \"obtained\" with the recommendation given by the recsys. It's a system wide metric and results of every group will be returned. It is calculated as such: \\[ \\Delta GAP = \\frac{recs_GAP - profile_GAP}{profile_GAP} \\] Users are splitted into groups based on the user_groups parameter, which contains names of the groups as keys, and percentage of how many user must contain a group as values. For example:: user_groups = {'popular_users': 0.3, 'medium_popular_users': 0.2, 'low_popular_users': 0.5} Every user will be inserted in a group based on how many popular items the user has rated (in relation to the percentage of users we specified as value in the dictionary): users with many popular items will be inserted into the first group, users with niche items rated will be inserted into one of the last groups If the 'top_n' parameter is specified, then the Delta GAP will be calculated considering only the first n items of every recommendation list of all users Parameters: Name Type Description Default user_groups Dict<str, float> Dict containing group names as keys and percentage of users as value, used to split users in groups. Users with more popular items rated are grouped into the first group, users with slightly less popular items rated are grouped into the second one, etc. required top_n int it's a cutoff parameter, if specified the Gini index will be calculated considering only ther first 'n' items of every recommendation list of all users. Default is None None pop_percentage float How many (in percentage) 'most popular items' must be considered. Default is 0.2 0.2 Source code in clayrs/evaluation/metrics/fairness_metrics.py 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 class DeltaGap ( GroupFairnessMetric ): r \"\"\" The Delta GAP (Group Average popularity) metric lets you compare the average popularity \"requested\" by one or multiple groups of users and the average popularity \"obtained\" with the recommendation given by the recsys. It's a system wide metric and results of every group will be returned. It is calculated as such: $$ \\Delta GAP = \\frac{recs_GAP - profile_GAP}{profile_GAP} $$ Users are splitted into groups based on the *user_groups* parameter, which contains names of the groups as keys, and percentage of how many user must contain a group as values. For example:: user_groups = {'popular_users': 0.3, 'medium_popular_users': 0.2, 'low_popular_users': 0.5} Every user will be inserted in a group based on how many popular items the user has rated (in relation to the percentage of users we specified as value in the dictionary): users with many popular items will be inserted into the first group, users with niche items rated will be inserted into one of the last groups If the 'top_n' parameter is specified, then the Delta GAP will be calculated considering only the first *n* items of every recommendation list of all users Args: user_groups (Dict<str, float>): Dict containing group names as keys and percentage of users as value, used to split users in groups. Users with more popular items rated are grouped into the first group, users with slightly less popular items rated are grouped into the second one, etc. top_n (int): it's a cutoff parameter, if specified the Gini index will be calculated considering only ther first 'n' items of every recommendation list of all users. Default is None pop_percentage (float): How many (in percentage) 'most popular items' must be considered. Default is 0.2 \"\"\" def __init__ ( self , user_groups : Dict [ str , float ], top_n : int = None , pop_percentage : float = 0.2 ): if not 0 < pop_percentage <= 1 : raise ValueError ( 'Incorrect percentage! Valid percentage range: 0 < percentage <= 1' ) self . __pop_percentage = pop_percentage self . __top_n = top_n super () . __init__ ( user_groups ) def __str__ ( self ): name = \"DeltaGap\" if self . __top_n : name += \" - Top {} \" . format ( self . __top_n ) return name def __repr__ ( self ): return f 'DeltaGap(user_groups= { self . user_groups } , pop_percentage= { self . __pop_percentage } , top_n= { self . __top_n } )' @staticmethod def calculate_gap ( group : Set [ str ], avg_pop_by_users : Dict [ str , object ]) -> float : r \"\"\" Compute the GAP (Group Average Popularity) formula $$ GAP = \\frac{\\sum_{u \\in U}\\cdot \\frac{\\sum_{i \\in i_u} pop_i}{|iu|}}{|G|} $$ Where: - $G$ is the set of users - $i_u$ is the set of items rated by user u - $pop_i$ is the popularity of item i Args: group (Set<str>): the set of users (user_id) avg_pop_by_users (Dict<str, object>): average popularity by user Returns: score (float): gap score \"\"\" total_pop = 0 for element in group : if avg_pop_by_users . get ( element ): total_pop += avg_pop_by_users [ element ] return total_pop / len ( group ) @staticmethod def calculate_delta_gap ( recs_gap : float , profile_gap : float ) -> float : \"\"\" Compute the ratio between the recommendation gap and the user profiles gap Args: recs_gap (float): recommendation gap profile_gap: user profiles gap Returns: score (float): delta gap measure \"\"\" result = 0 if profile_gap != 0.0 : result = ( recs_gap - profile_gap ) / profile_gap return result def perform ( self , split : Split ) -> pd . DataFrame : predictions = split . pred truth = split . truth if self . __top_n : predictions = predictions . take_head_all ( self . __top_n ) most_popular_items = popular_items ( score_frame = truth , pop_percentage = self . __pop_percentage ) user_groups = self . split_user_in_groups ( score_frame = predictions , groups = self . user_groups , pop_items = most_popular_items ) split_result = { \" {} | {} \" . format ( str ( self ), group ): [] for group in user_groups } split_result [ 'user_id' ] = [ 'sys' ] pop_by_items = Counter ( list ( truth . item_id_column )) for group_name in user_groups : # Computing avg pop by users recs for delta gap avg_pop_by_users_recs = self . get_avg_pop_by_users ( predictions , pop_by_items , user_groups [ group_name ]) # Computing avg pop by users profiles for delta gap avg_pop_by_users_profiles = self . get_avg_pop_by_users ( truth , pop_by_items , user_groups [ group_name ]) # Computing delta gap for every group recs_gap = self . calculate_gap ( group = user_groups [ group_name ], avg_pop_by_users = avg_pop_by_users_recs ) profile_gap = self . calculate_gap ( group = user_groups [ group_name ], avg_pop_by_users = avg_pop_by_users_profiles ) group_delta_gap = self . calculate_delta_gap ( recs_gap = recs_gap , profile_gap = profile_gap ) split_result [ ' {} | {} ' . format ( str ( self ), group_name )] . append ( group_delta_gap ) return pd . DataFrame ( split_result ) calculate_delta_gap ( recs_gap , profile_gap ) staticmethod Compute the ratio between the recommendation gap and the user profiles gap Parameters: Name Type Description Default recs_gap float recommendation gap required profile_gap float user profiles gap required Returns: Name Type Description score float delta gap measure Source code in clayrs/evaluation/metrics/fairness_metrics.py 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 @staticmethod def calculate_delta_gap ( recs_gap : float , profile_gap : float ) -> float : \"\"\" Compute the ratio between the recommendation gap and the user profiles gap Args: recs_gap (float): recommendation gap profile_gap: user profiles gap Returns: score (float): delta gap measure \"\"\" result = 0 if profile_gap != 0.0 : result = ( recs_gap - profile_gap ) / profile_gap return result calculate_gap ( group , avg_pop_by_users ) staticmethod Compute the GAP (Group Average Popularity) formula \\[ GAP = \\frac{\\sum_{u \\in U}\\cdot \\frac{\\sum_{i \\in i_u} pop_i}{|iu|}}{|G|} \\] Where: \\(G\\) is the set of users \\(i_u\\) is the set of items rated by user u \\(pop_i\\) is the popularity of item i Parameters: Name Type Description Default group Set<str> the set of users (user_id) required avg_pop_by_users Dict<str, object> average popularity by user required Returns: Name Type Description score float gap score Source code in clayrs/evaluation/metrics/fairness_metrics.py 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 @staticmethod def calculate_gap ( group : Set [ str ], avg_pop_by_users : Dict [ str , object ]) -> float : r \"\"\" Compute the GAP (Group Average Popularity) formula $$ GAP = \\frac{\\sum_{u \\in U}\\cdot \\frac{\\sum_{i \\in i_u} pop_i}{|iu|}}{|G|} $$ Where: - $G$ is the set of users - $i_u$ is the set of items rated by user u - $pop_i$ is the popularity of item i Args: group (Set<str>): the set of users (user_id) avg_pop_by_users (Dict<str, object>): average popularity by user Returns: score (float): gap score \"\"\" total_pop = 0 for element in group : if avg_pop_by_users . get ( element ): total_pop += avg_pop_by_users [ element ] return total_pop / len ( group ) GiniIndex Bases: FairnessMetric The Gini Index metric measures inequality in recommendation lists. It's a system wide metric, so only its result it will be returned and not those of every user. The metric is calculated as such: \\[ Gini_{sys} = \\frac{\\sum_i(2i - n - 1)x_i}{n\\cdot\\sum_i x_i} \\] Where: \\(n\\) is the total number of distinct items that are being recommended \\(x_i\\) is the number of times that the item \\(i\\) has been recommended A perfectly equal recommender system should recommend every item the same number of times, in which case the Gini index would be equal to 0. The more the recsys is \"disegual\", the more the Gini Index is closer to 1 If the 'top_n' parameter is specified, then the Gini index will measure inequality considering only the first n items of every recommendation list of all users Parameters: Name Type Description Default top_n int it's a cutoff parameter, if specified the Gini index will be calculated considering only ther first 'n' items of every recommendation list of all users. Default is None None Source code in clayrs/evaluation/metrics/fairness_metrics.py 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 class GiniIndex ( FairnessMetric ): r \"\"\" The Gini Index metric measures inequality in recommendation lists. It's a system wide metric, so only its result it will be returned and not those of every user. The metric is calculated as such: $$ Gini_{sys} = \\frac{\\sum_i(2i - n - 1)x_i}{n\\cdot\\sum_i x_i} $$ Where: - $n$ is the total number of distinct items that are being recommended - $x_i$ is the number of times that the item $i$ has been recommended A perfectly equal recommender system should recommend every item the same number of times, in which case the Gini index would be equal to 0. The more the recsys is \"disegual\", the more the Gini Index is closer to 1 If the 'top_n' parameter is specified, then the Gini index will measure inequality considering only the first *n* items of every recommendation list of all users Args: top_n (int): it's a cutoff parameter, if specified the Gini index will be calculated considering only ther first 'n' items of every recommendation list of all users. Default is None \"\"\" def __init__ ( self , top_n : int = None ): self . __top_n = top_n def __str__ ( self ): name = \"Gini\" if self . __top_n : name += \" - Top {} \" . format ( self . __top_n ) return name def __repr__ ( self ): return f 'GiniIndex(top_n= { self . __top_n } )' def perform ( self , split : Split ): def gini ( x : List ): \"\"\" Inner method which given a list of values, calculates the gini index Args: x (list): list of values of which we want to measure inequality \"\"\" # The rest of the code requires numpy arrays. x = np . asarray ( x ) sorted_x = np . sort ( x ) n = len ( x ) cumx = np . cumsum ( sorted_x , dtype = float ) # The above formula, with all weights equal to 1 simplifies to: return ( n + 1 - 2 * np . sum ( cumx ) / cumx [ - 1 ]) / n predictions = split . pred score_dict = { 'user_id' : [], str ( self ): []} if self . __top_n is not None : predictions = itertools . chain . from_iterable ([ predictions . get_user_interactions ( user_id , self . __top_n ) for user_id in set ( predictions . user_id_column )]) coun = Counter ([ prediction . item_id for prediction in predictions ]) result = gini ( list ( coun . values ())) score_dict [ 'user_id' ] . append ( 'sys' ) score_dict [ str ( self )] . append ( result ) return pd . DataFrame ( score_dict ) PredictionCoverage Bases: FairnessMetric The Prediction Coverage metric measures in percentage how many distinct items are being recommended in relation to all available items. It's a system wide metric, so only its result it will be returned and not those of every user. The metric is calculated as such: \\[ Prediction Coverage_{sys} = (\\frac{|I_p|}{|I|})\\cdot100 \\] Where: \\(I\\) is the set of all available items \\(I_p\\) is the set of recommended items The \\(I\\) must be specified through the 'catalog' parameter Check the 'Beyond Accuracy: Evaluating Recommender Systems by Coverage and Serendipity' paper for more Source code in clayrs/evaluation/metrics/fairness_metrics.py 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 class PredictionCoverage ( FairnessMetric ): r \"\"\" The Prediction Coverage metric measures in percentage how many distinct items are being recommended in relation to all available items. It's a system wide metric, so only its result it will be returned and not those of every user. The metric is calculated as such: $$ Prediction Coverage_{sys} = (\\frac{|I_p|}{|I|})\\cdot100 $$ Where: - $I$ is the set of all available items - $I_p$ is the set of recommended items The $I$ must be specified through the 'catalog' parameter Check the 'Beyond Accuracy: Evaluating Recommender Systems by Coverage and Serendipity' paper for more \"\"\" def __init__ ( self , catalog : Set [ str ]): self . __catalog = catalog def __str__ ( self ): return \"PredictionCoverage\" def __repr__ ( self ): return f 'PredictionCoverage(catalog= { self . __catalog } )' @property def catalog ( self ): return self . __catalog def _get_covered ( self , pred : Ratings ): \"\"\" Private function which calculates all recommended items given a catalog of all available items (specified in the constructor) Args: pred: DataFrame containing recommendation lists of all users Returns: Set of distinct items that have been recommended that also appear in the catalog \"\"\" catalog = self . catalog pred_items = set ( pred . item_id_column ) return pred_items . intersection ( catalog ) def perform ( self , split : Split ) -> pd . DataFrame : prediction = { 'user_id' : [], str ( self ): []} catalog = { str ( item ) for item in self . __catalog } # cast to string in case user is not careful pred = split . pred covered_items = self . _get_covered ( pred ) percentage = ( len ( covered_items ) / len ( catalog )) * 100 coverage_percentage = np . round ( percentage , 2 ) prediction [ 'user_id' ] . append ( 'sys' ) prediction [ str ( self )] . append ( coverage_percentage ) return pd . DataFrame ( prediction )","title":"Fairness metrics"},{"location":"evaluation/metrics/fairness_metrics/#fairness-metrics","text":"","title":"Fairness metrics"},{"location":"evaluation/metrics/fairness_metrics/#clayrs.evaluation.metrics.fairness_metrics.CatalogCoverage","text":"Bases: PredictionCoverage The Catalog Coverage metric measures in percentage how many distinct items are being recommended in relation to all available items. It's a system wide metric, so only its result it will be returned and not those of every user. It differs from the Prediction Coverage since it allows for different parameters to come into play. If no parameter is passed then it's a simple Prediction Coverage. The metric is calculated as such: \\[ Catalog Coverage_{sys} = (\\frac{|\\bigcup_{j=1...N}reclist(u_j)|}{|I|})\\cdot100 \\] Where: \\(N\\) is the total number of users \\(reclist(u_j)\\) is the set of items contained in the recommendation list of user \\(j\\) \\(I\\) is the set of all available items The \\(I\\) must be specified through the 'catalog' parameter The recommendation list of every user ( \\(reclist(u_j)\\) ) can be reduced to the first n parameter with the top-n parameter, so that catalog coverage is measured considering only the most highest ranked items. With the 'k' parameter one could specify the number of users that will be used to calculate catalog coverage: k users will be randomly sampled and their recommendation lists will be used. The formula above becomes: \\[ Catalog Coverage_{sys} = (\\frac{|\\bigcup_{j=1...k}reclist(u_j)|}{|I|})\\cdot100 \\] Where: \\(k\\) is the parameter specified Obviously 'k' < N, else simply recommendation lists of all users will be used Check the 'Beyond Accuracy: Evaluating Recommender Systems by Coverage and Serendipity' paper and page 13 of the 'Comparison of group recommendation algorithms' paper for more Source code in clayrs/evaluation/metrics/fairness_metrics.py 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 class CatalogCoverage ( PredictionCoverage ): r \"\"\" The Catalog Coverage metric measures in percentage how many distinct items are being recommended in relation to all available items. It's a system wide metric, so only its result it will be returned and not those of every user. It differs from the Prediction Coverage since it allows for different parameters to come into play. If no parameter is passed then it's a simple Prediction Coverage. The metric is calculated as such: $$ Catalog Coverage_{sys} = (\\frac{|\\bigcup_{j=1...N}reclist(u_j)|}{|I|})\\cdot100 $$ Where: - $N$ is the total number of users - $reclist(u_j)$ is the set of items contained in the recommendation list of user $j$ - $I$ is the set of all available items The $I$ must be specified through the 'catalog' parameter The recommendation list of every user ($reclist(u_j)$) can be reduced to the first *n* parameter with the top-n parameter, so that catalog coverage is measured considering only the most highest ranked items. With the 'k' parameter one could specify the number of users that will be used to calculate catalog coverage: k users will be randomly sampled and their recommendation lists will be used. The formula above becomes: $$ Catalog Coverage_{sys} = (\\frac{|\\bigcup_{j=1...k}reclist(u_j)|}{|I|})\\cdot100 $$ Where: - $k$ is the parameter specified Obviously 'k' < N, else simply recommendation lists of all users will be used Check the 'Beyond Accuracy: Evaluating Recommender Systems by Coverage and Serendipity' paper and page 13 of the 'Comparison of group recommendation algorithms' paper for more \"\"\" def __init__ ( self , catalog : Set [ str ], top_n : int = None , k : int = None ): super () . __init__ ( catalog ) self . __top_n = top_n self . __k = k def __str__ ( self ): # If none of the parameter is passed, then it's simply a PredictionCoverage name = \"CatalogCoverage (PredictionCov)\" if self . __top_n : name = \"CatalogCoverage\" name += \" - Top {} \" . format ( self . __top_n ) if self . __k : name = \"CatalogCoverage\" name += \" - {} sampled users\" . format ( self . __k ) return name def __repr__ ( self ): return f 'CatalogCoverage(catalog= { self . catalog } , top_n= { self . __top_n } , k= { self . __k } )' def _get_covered ( self , pred : Ratings ): catalog = self . catalog if self . __top_n is not None : pred = list ( itertools . chain . from_iterable ([ pred . get_user_interactions ( user_id , self . __top_n ) for user_id in set ( pred . user_id_column )])) # IF k is passed, then we choose randomly k users and calc catalog coverage # based on their predictions. We check that k is < n_user since if it's the equal # or it's greater, then all predictions generated for all user must be used if self . __k is not None and self . __k < len ( pred ): user_list = list ( set ([ interaction_pred . user_id for interaction_pred in pred ])) sampling = random . choices ( user_list , k = self . __k ) predicted_items = set ([ interaction_pred . item_id for interaction_pred in pred if interaction_pred . user_id in sampling ]) covered_items = predicted_items . intersection ( catalog ) else : predicted_items = set ([ interaction_pred . item_id for interaction_pred in pred ]) covered_items = predicted_items . intersection ( catalog ) return covered_items","title":"CatalogCoverage"},{"location":"evaluation/metrics/fairness_metrics/#clayrs.evaluation.metrics.fairness_metrics.DeltaGap","text":"Bases: GroupFairnessMetric The Delta GAP (Group Average popularity) metric lets you compare the average popularity \"requested\" by one or multiple groups of users and the average popularity \"obtained\" with the recommendation given by the recsys. It's a system wide metric and results of every group will be returned. It is calculated as such: \\[ \\Delta GAP = \\frac{recs_GAP - profile_GAP}{profile_GAP} \\] Users are splitted into groups based on the user_groups parameter, which contains names of the groups as keys, and percentage of how many user must contain a group as values. For example:: user_groups = {'popular_users': 0.3, 'medium_popular_users': 0.2, 'low_popular_users': 0.5} Every user will be inserted in a group based on how many popular items the user has rated (in relation to the percentage of users we specified as value in the dictionary): users with many popular items will be inserted into the first group, users with niche items rated will be inserted into one of the last groups If the 'top_n' parameter is specified, then the Delta GAP will be calculated considering only the first n items of every recommendation list of all users Parameters: Name Type Description Default user_groups Dict<str, float> Dict containing group names as keys and percentage of users as value, used to split users in groups. Users with more popular items rated are grouped into the first group, users with slightly less popular items rated are grouped into the second one, etc. required top_n int it's a cutoff parameter, if specified the Gini index will be calculated considering only ther first 'n' items of every recommendation list of all users. Default is None None pop_percentage float How many (in percentage) 'most popular items' must be considered. Default is 0.2 0.2 Source code in clayrs/evaluation/metrics/fairness_metrics.py 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 class DeltaGap ( GroupFairnessMetric ): r \"\"\" The Delta GAP (Group Average popularity) metric lets you compare the average popularity \"requested\" by one or multiple groups of users and the average popularity \"obtained\" with the recommendation given by the recsys. It's a system wide metric and results of every group will be returned. It is calculated as such: $$ \\Delta GAP = \\frac{recs_GAP - profile_GAP}{profile_GAP} $$ Users are splitted into groups based on the *user_groups* parameter, which contains names of the groups as keys, and percentage of how many user must contain a group as values. For example:: user_groups = {'popular_users': 0.3, 'medium_popular_users': 0.2, 'low_popular_users': 0.5} Every user will be inserted in a group based on how many popular items the user has rated (in relation to the percentage of users we specified as value in the dictionary): users with many popular items will be inserted into the first group, users with niche items rated will be inserted into one of the last groups If the 'top_n' parameter is specified, then the Delta GAP will be calculated considering only the first *n* items of every recommendation list of all users Args: user_groups (Dict<str, float>): Dict containing group names as keys and percentage of users as value, used to split users in groups. Users with more popular items rated are grouped into the first group, users with slightly less popular items rated are grouped into the second one, etc. top_n (int): it's a cutoff parameter, if specified the Gini index will be calculated considering only ther first 'n' items of every recommendation list of all users. Default is None pop_percentage (float): How many (in percentage) 'most popular items' must be considered. Default is 0.2 \"\"\" def __init__ ( self , user_groups : Dict [ str , float ], top_n : int = None , pop_percentage : float = 0.2 ): if not 0 < pop_percentage <= 1 : raise ValueError ( 'Incorrect percentage! Valid percentage range: 0 < percentage <= 1' ) self . __pop_percentage = pop_percentage self . __top_n = top_n super () . __init__ ( user_groups ) def __str__ ( self ): name = \"DeltaGap\" if self . __top_n : name += \" - Top {} \" . format ( self . __top_n ) return name def __repr__ ( self ): return f 'DeltaGap(user_groups= { self . user_groups } , pop_percentage= { self . __pop_percentage } , top_n= { self . __top_n } )' @staticmethod def calculate_gap ( group : Set [ str ], avg_pop_by_users : Dict [ str , object ]) -> float : r \"\"\" Compute the GAP (Group Average Popularity) formula $$ GAP = \\frac{\\sum_{u \\in U}\\cdot \\frac{\\sum_{i \\in i_u} pop_i}{|iu|}}{|G|} $$ Where: - $G$ is the set of users - $i_u$ is the set of items rated by user u - $pop_i$ is the popularity of item i Args: group (Set<str>): the set of users (user_id) avg_pop_by_users (Dict<str, object>): average popularity by user Returns: score (float): gap score \"\"\" total_pop = 0 for element in group : if avg_pop_by_users . get ( element ): total_pop += avg_pop_by_users [ element ] return total_pop / len ( group ) @staticmethod def calculate_delta_gap ( recs_gap : float , profile_gap : float ) -> float : \"\"\" Compute the ratio between the recommendation gap and the user profiles gap Args: recs_gap (float): recommendation gap profile_gap: user profiles gap Returns: score (float): delta gap measure \"\"\" result = 0 if profile_gap != 0.0 : result = ( recs_gap - profile_gap ) / profile_gap return result def perform ( self , split : Split ) -> pd . DataFrame : predictions = split . pred truth = split . truth if self . __top_n : predictions = predictions . take_head_all ( self . __top_n ) most_popular_items = popular_items ( score_frame = truth , pop_percentage = self . __pop_percentage ) user_groups = self . split_user_in_groups ( score_frame = predictions , groups = self . user_groups , pop_items = most_popular_items ) split_result = { \" {} | {} \" . format ( str ( self ), group ): [] for group in user_groups } split_result [ 'user_id' ] = [ 'sys' ] pop_by_items = Counter ( list ( truth . item_id_column )) for group_name in user_groups : # Computing avg pop by users recs for delta gap avg_pop_by_users_recs = self . get_avg_pop_by_users ( predictions , pop_by_items , user_groups [ group_name ]) # Computing avg pop by users profiles for delta gap avg_pop_by_users_profiles = self . get_avg_pop_by_users ( truth , pop_by_items , user_groups [ group_name ]) # Computing delta gap for every group recs_gap = self . calculate_gap ( group = user_groups [ group_name ], avg_pop_by_users = avg_pop_by_users_recs ) profile_gap = self . calculate_gap ( group = user_groups [ group_name ], avg_pop_by_users = avg_pop_by_users_profiles ) group_delta_gap = self . calculate_delta_gap ( recs_gap = recs_gap , profile_gap = profile_gap ) split_result [ ' {} | {} ' . format ( str ( self ), group_name )] . append ( group_delta_gap ) return pd . DataFrame ( split_result )","title":"DeltaGap"},{"location":"evaluation/metrics/fairness_metrics/#clayrs.evaluation.metrics.fairness_metrics.DeltaGap.calculate_delta_gap","text":"Compute the ratio between the recommendation gap and the user profiles gap Parameters: Name Type Description Default recs_gap float recommendation gap required profile_gap float user profiles gap required Returns: Name Type Description score float delta gap measure Source code in clayrs/evaluation/metrics/fairness_metrics.py 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 @staticmethod def calculate_delta_gap ( recs_gap : float , profile_gap : float ) -> float : \"\"\" Compute the ratio between the recommendation gap and the user profiles gap Args: recs_gap (float): recommendation gap profile_gap: user profiles gap Returns: score (float): delta gap measure \"\"\" result = 0 if profile_gap != 0.0 : result = ( recs_gap - profile_gap ) / profile_gap return result","title":"calculate_delta_gap()"},{"location":"evaluation/metrics/fairness_metrics/#clayrs.evaluation.metrics.fairness_metrics.DeltaGap.calculate_gap","text":"Compute the GAP (Group Average Popularity) formula \\[ GAP = \\frac{\\sum_{u \\in U}\\cdot \\frac{\\sum_{i \\in i_u} pop_i}{|iu|}}{|G|} \\] Where: \\(G\\) is the set of users \\(i_u\\) is the set of items rated by user u \\(pop_i\\) is the popularity of item i Parameters: Name Type Description Default group Set<str> the set of users (user_id) required avg_pop_by_users Dict<str, object> average popularity by user required Returns: Name Type Description score float gap score Source code in clayrs/evaluation/metrics/fairness_metrics.py 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 @staticmethod def calculate_gap ( group : Set [ str ], avg_pop_by_users : Dict [ str , object ]) -> float : r \"\"\" Compute the GAP (Group Average Popularity) formula $$ GAP = \\frac{\\sum_{u \\in U}\\cdot \\frac{\\sum_{i \\in i_u} pop_i}{|iu|}}{|G|} $$ Where: - $G$ is the set of users - $i_u$ is the set of items rated by user u - $pop_i$ is the popularity of item i Args: group (Set<str>): the set of users (user_id) avg_pop_by_users (Dict<str, object>): average popularity by user Returns: score (float): gap score \"\"\" total_pop = 0 for element in group : if avg_pop_by_users . get ( element ): total_pop += avg_pop_by_users [ element ] return total_pop / len ( group )","title":"calculate_gap()"},{"location":"evaluation/metrics/fairness_metrics/#clayrs.evaluation.metrics.fairness_metrics.GiniIndex","text":"Bases: FairnessMetric The Gini Index metric measures inequality in recommendation lists. It's a system wide metric, so only its result it will be returned and not those of every user. The metric is calculated as such: \\[ Gini_{sys} = \\frac{\\sum_i(2i - n - 1)x_i}{n\\cdot\\sum_i x_i} \\] Where: \\(n\\) is the total number of distinct items that are being recommended \\(x_i\\) is the number of times that the item \\(i\\) has been recommended A perfectly equal recommender system should recommend every item the same number of times, in which case the Gini index would be equal to 0. The more the recsys is \"disegual\", the more the Gini Index is closer to 1 If the 'top_n' parameter is specified, then the Gini index will measure inequality considering only the first n items of every recommendation list of all users Parameters: Name Type Description Default top_n int it's a cutoff parameter, if specified the Gini index will be calculated considering only ther first 'n' items of every recommendation list of all users. Default is None None Source code in clayrs/evaluation/metrics/fairness_metrics.py 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 class GiniIndex ( FairnessMetric ): r \"\"\" The Gini Index metric measures inequality in recommendation lists. It's a system wide metric, so only its result it will be returned and not those of every user. The metric is calculated as such: $$ Gini_{sys} = \\frac{\\sum_i(2i - n - 1)x_i}{n\\cdot\\sum_i x_i} $$ Where: - $n$ is the total number of distinct items that are being recommended - $x_i$ is the number of times that the item $i$ has been recommended A perfectly equal recommender system should recommend every item the same number of times, in which case the Gini index would be equal to 0. The more the recsys is \"disegual\", the more the Gini Index is closer to 1 If the 'top_n' parameter is specified, then the Gini index will measure inequality considering only the first *n* items of every recommendation list of all users Args: top_n (int): it's a cutoff parameter, if specified the Gini index will be calculated considering only ther first 'n' items of every recommendation list of all users. Default is None \"\"\" def __init__ ( self , top_n : int = None ): self . __top_n = top_n def __str__ ( self ): name = \"Gini\" if self . __top_n : name += \" - Top {} \" . format ( self . __top_n ) return name def __repr__ ( self ): return f 'GiniIndex(top_n= { self . __top_n } )' def perform ( self , split : Split ): def gini ( x : List ): \"\"\" Inner method which given a list of values, calculates the gini index Args: x (list): list of values of which we want to measure inequality \"\"\" # The rest of the code requires numpy arrays. x = np . asarray ( x ) sorted_x = np . sort ( x ) n = len ( x ) cumx = np . cumsum ( sorted_x , dtype = float ) # The above formula, with all weights equal to 1 simplifies to: return ( n + 1 - 2 * np . sum ( cumx ) / cumx [ - 1 ]) / n predictions = split . pred score_dict = { 'user_id' : [], str ( self ): []} if self . __top_n is not None : predictions = itertools . chain . from_iterable ([ predictions . get_user_interactions ( user_id , self . __top_n ) for user_id in set ( predictions . user_id_column )]) coun = Counter ([ prediction . item_id for prediction in predictions ]) result = gini ( list ( coun . values ())) score_dict [ 'user_id' ] . append ( 'sys' ) score_dict [ str ( self )] . append ( result ) return pd . DataFrame ( score_dict )","title":"GiniIndex"},{"location":"evaluation/metrics/fairness_metrics/#clayrs.evaluation.metrics.fairness_metrics.PredictionCoverage","text":"Bases: FairnessMetric The Prediction Coverage metric measures in percentage how many distinct items are being recommended in relation to all available items. It's a system wide metric, so only its result it will be returned and not those of every user. The metric is calculated as such: \\[ Prediction Coverage_{sys} = (\\frac{|I_p|}{|I|})\\cdot100 \\] Where: \\(I\\) is the set of all available items \\(I_p\\) is the set of recommended items The \\(I\\) must be specified through the 'catalog' parameter Check the 'Beyond Accuracy: Evaluating Recommender Systems by Coverage and Serendipity' paper for more Source code in clayrs/evaluation/metrics/fairness_metrics.py 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 class PredictionCoverage ( FairnessMetric ): r \"\"\" The Prediction Coverage metric measures in percentage how many distinct items are being recommended in relation to all available items. It's a system wide metric, so only its result it will be returned and not those of every user. The metric is calculated as such: $$ Prediction Coverage_{sys} = (\\frac{|I_p|}{|I|})\\cdot100 $$ Where: - $I$ is the set of all available items - $I_p$ is the set of recommended items The $I$ must be specified through the 'catalog' parameter Check the 'Beyond Accuracy: Evaluating Recommender Systems by Coverage and Serendipity' paper for more \"\"\" def __init__ ( self , catalog : Set [ str ]): self . __catalog = catalog def __str__ ( self ): return \"PredictionCoverage\" def __repr__ ( self ): return f 'PredictionCoverage(catalog= { self . __catalog } )' @property def catalog ( self ): return self . __catalog def _get_covered ( self , pred : Ratings ): \"\"\" Private function which calculates all recommended items given a catalog of all available items (specified in the constructor) Args: pred: DataFrame containing recommendation lists of all users Returns: Set of distinct items that have been recommended that also appear in the catalog \"\"\" catalog = self . catalog pred_items = set ( pred . item_id_column ) return pred_items . intersection ( catalog ) def perform ( self , split : Split ) -> pd . DataFrame : prediction = { 'user_id' : [], str ( self ): []} catalog = { str ( item ) for item in self . __catalog } # cast to string in case user is not careful pred = split . pred covered_items = self . _get_covered ( pred ) percentage = ( len ( covered_items ) / len ( catalog )) * 100 coverage_percentage = np . round ( percentage , 2 ) prediction [ 'user_id' ] . append ( 'sys' ) prediction [ str ( self )] . append ( coverage_percentage ) return pd . DataFrame ( prediction )","title":"PredictionCoverage"},{"location":"evaluation/metrics/plot_metrics/","text":"Plot metrics LongTailDistr Bases: PlotMetric This metric generates the Long Tail Distribution plot and saves it in the output directory with the file name specified. The plot can be generated both for the truth set or the predictions set (based on the on parameter): on = 'truth' : in this case the long tail distribution is useful to see which are the most popular items (the most rated ones) on = 'pred' : in this case the long tail distribution is useful to see which are the most recommended items The plot file will be saved as out_dir/file_name.format Since multiple split could be evaluated at once, the overwrite parameter comes into play: if is set to False, file with the same name will be saved as file_name (1).format , file_name (2).format , etc. so that for every split a plot is generated without overwriting any file previously generated Parameters: Name Type Description Default out_dir str Directory where the plot will be saved. Default is '.', meaning that the plot will be saved in the same directory where the python script it's being executed '.' file_name str Name of the plot file. Default is 'long_tail_distr' 'long_tail_distr' on str Set on which the Long Tail Distribution plot will be generated. Values accepted are 'truth' or 'pred' 'truth' format str Format of the plot file. Could be 'jpg', 'svg', 'png'. Default is 'png' 'png' overwrite bool parameter which specifies if the plot saved must overwrite any file that as the same name ('file_name.format'). Default is False False Raises: Type Description ValueError exception raised when a invalid value for the 'on' parameter is specified Source code in clayrs/evaluation/metrics/plot_metrics.py 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 class LongTailDistr ( PlotMetric ): \"\"\" This metric generates the Long Tail Distribution plot and saves it in the output directory with the file name specified. The plot can be generated both for the *truth set* or the *predictions set* (based on the *on* parameter): - **on = 'truth'**: in this case the long tail distribution is useful to see which are the most popular items (the most rated ones) - **on = 'pred'**: in this case the long tail distribution is useful to see which are the most recommended items The plot file will be saved as *out_dir/file_name.format* Since multiple split could be evaluated at once, the *overwrite* parameter comes into play: if is set to False, file with the same name will be saved as *file_name (1).format*, *file_name (2).format*, etc. so that for every split a plot is generated without overwriting any file previously generated Args: out_dir (str): Directory where the plot will be saved. Default is '.', meaning that the plot will be saved in the same directory where the python script it's being executed file_name (str): Name of the plot file. Default is 'long_tail_distr' on (str): Set on which the Long Tail Distribution plot will be generated. Values accepted are 'truth' or 'pred' format (str): Format of the plot file. Could be 'jpg', 'svg', 'png'. Default is 'png' overwrite (bool): parameter which specifies if the plot saved must overwrite any file that as the same name ('file_name.format'). Default is False Raises: ValueError: exception raised when a invalid value for the 'on' parameter is specified \"\"\" def __init__ ( self , out_dir : str = '.' , file_name : str = 'long_tail_distr' , on : str = 'truth' , format : str = 'png' , overwrite : bool = False ): valid = { 'truth' , 'pred' } self . __on = on . lower () if self . __on not in valid : raise ValueError ( \"on= {} is not supported! Long Tail can be calculated only on: \\n \" \" {} \" . format ( on , valid )) super () . __init__ ( out_dir , file_name , format , overwrite ) def __str__ ( self ): return \"LongTailDistr\" def __repr__ ( self ): return f 'LongTailDistr(out_dir= { self . __out_dir } , ' \\ f 'file_name= { self . __file_name } , ' \\ f 'format= { self . __format } , ' \\ f 'overwrite= { self . __overwrite } )' def perform ( self , split : Split ) -> pd . DataFrame : if self . __on == 'truth' : frame = split . truth else : frame = split . pred counts_by_item = Counter ( frame . item_id_column ) ordered_item_count_pairs = counts_by_item . most_common () ordered_counts = [] labels = [] for item_count_pair in ordered_item_count_pairs : labels . append ( item_count_pair [ 0 ]) ordered_counts . append ( item_count_pair [ 1 ]) x = [ i for i in range ( len ( labels ))] y = ordered_counts fig = plt . figure () ax = fig . add_subplot () if self . __on == 'truth' : ax . set ( xlabel = 'Item ID' , ylabel = '# of item ratings' , title = 'Long Tail Distribution - {} ' . format ( self . __on )) else : ax . set ( xlabel = 'Item ID' , ylabel = '# of item recommendations' , title = 'Long Tail Distribution - {} ' . format ( self . __on )) ax . fill_between ( x , y , color = \"orange\" , alpha = 0.2 ) ax . set_xticks ([]) ax . plot ( x , y ) file_name = self . file_name + '_ {} ' . format ( self . __on ) self . save_figure ( fig , file_name = file_name ) return pd . DataFrame ( columns = [ 'user_id' , 'item_id' , 'score' ]) PopProfileVsRecs Bases: GroupFairnessMetric , PlotMetric This metric generates a plot where users are splitted into groups and, for every group, a boxplot comparing profile popularity and recommendations popularity is drawn Users are splitted into groups based on the user_groups parameter, which contains names of the groups as keys, and percentage of how many user must contain a group as values. For example:: user_groups = {'popular_users': 0.3, 'medium_popular_users': 0.2, 'low_popular_users': 0.5} Every user will be inserted in a group based on how many popular items the user has rated (in relation to the percentage of users we specified as value in the dictionary): users with many popular items will be inserted into the first group, users with niche items rated will be inserted into one of the last groups You could also specify how many most popular items must be considered with the 'pop_percentage' parameter. By default is set to 0.2 which means that the top 20% items are considered as most popular The plot file will be saved as out_dir/file_name.format Since multiple split could be evaluated at once, the overwrite parameter comes into play: if is set to False, file with the same name will be saved as file_name (1).format , file_name (2).format , etc. so that for every split a plot is generated without overwriting any file previously generated Thanks to the 'store_frame' parameter it's also possible to store a csv containing the calculations done in order to build every boxplot. Will be saved in the same directory and with the same file name as the plot itself (but with the .csv format): The csv will be saved as out_dir/file_name.csv Parameters: Name Type Description Default user_groups Dict<str, float> Dict containing group names as keys and percentage of users as value, used to split users in groups. Users with more popular items rated are grouped into the first group, users with slightly less popular items rated are grouped into the second one, etc. required out_dir str Directory where the plot will be saved. Default is '.', meaning that the plot will be saved in the same directory where the python script it's being executed '.' file_name str Name of the plot file. Default is 'pop_ratio_profile_vs_recs' 'pop_ratio_profile_vs_recs' pop_percentage float How many (in percentage) 'most popular items' must be considered. Default is 0.2 0.2 store_frame bool True if you want to store calculations done in order to build every boxplot in a csv file, False otherwise. Default is set to False False format str Format of the plot file. Could be 'jpg', 'svg', 'png'. Default is 'png' 'png' overwrite bool parameter which specifies if the plot saved must overwrite any file that as the same name ('file_name.format'). Default is False False Source code in clayrs/evaluation/metrics/plot_metrics.py 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 class PopProfileVsRecs ( GroupFairnessMetric , PlotMetric ): \"\"\" This metric generates a plot where users are splitted into groups and, for every group, a boxplot comparing profile popularity and recommendations popularity is drawn Users are splitted into groups based on the *user_groups* parameter, which contains names of the groups as keys, and percentage of how many user must contain a group as values. For example:: user_groups = {'popular_users': 0.3, 'medium_popular_users': 0.2, 'low_popular_users': 0.5} Every user will be inserted in a group based on how many popular items the user has rated (in relation to the percentage of users we specified as value in the dictionary): users with many popular items will be inserted into the first group, users with niche items rated will be inserted into one of the last groups You could also specify how many *most popular items* must be considered with the 'pop_percentage' parameter. By default is set to 0.2 which means that the top 20% items are considered as most popular The plot file will be saved as *out_dir/file_name.format* Since multiple split could be evaluated at once, the *overwrite* parameter comes into play: if is set to False, file with the same name will be saved as *file_name (1).format*, *file_name (2).format*, etc. so that for every split a plot is generated without overwriting any file previously generated Thanks to the 'store_frame' parameter it's also possible to store a csv containing the calculations done in order to build every boxplot. Will be saved in the same directory and with the same file name as the plot itself (but with the .csv format): The csv will be saved as *out_dir/file_name.csv* Args: user_groups (Dict<str, float>): Dict containing group names as keys and percentage of users as value, used to split users in groups. Users with more popular items rated are grouped into the first group, users with slightly less popular items rated are grouped into the second one, etc. out_dir (str): Directory where the plot will be saved. Default is '.', meaning that the plot will be saved in the same directory where the python script it's being executed file_name (str): Name of the plot file. Default is 'pop_ratio_profile_vs_recs' pop_percentage (float): How many (in percentage) 'most popular items' must be considered. Default is 0.2 store_frame (bool): True if you want to store calculations done in order to build every boxplot in a csv file, False otherwise. Default is set to False format (str): Format of the plot file. Could be 'jpg', 'svg', 'png'. Default is 'png' overwrite (bool): parameter which specifies if the plot saved must overwrite any file that as the same name ('file_name.format'). Default is False \"\"\" def __init__ ( self , user_groups : Dict [ str , float ], out_dir : str = '.' , file_name : str = 'pop_ratio_profile_vs_recs' , pop_percentage : float = 0.2 , store_frame : bool = False , format : str = 'png' , overwrite : bool = False ): PlotMetric . __init__ ( self , out_dir , file_name , format , overwrite ) GroupFairnessMetric . __init__ ( self , user_groups ) if not 0 < pop_percentage <= 1 : raise ValueError ( 'Incorrect percentage! Valid percentage range: 0 < percentage <= 1' ) self . __pop_percentage = pop_percentage self . __user_groups = user_groups self . __store_frame = store_frame def __str__ ( self ): return \"PopProfileVsRecs\" def __repr__ ( self ): return f 'PopProfileVsRecs(' \\ f 'user_groups= { self . __user_groups } , ' \\ f 'out_dir= { self . output_directory } , ' \\ f 'file_name= { self . file_name } , ' \\ f 'pop_percentage= { self . __pop_percentage } , ' \\ f 'store_frame= { self . __store_frame } , ' \\ f 'format= { self . format } , ' \\ f 'overwrite= { self . overwrite } )' def perform ( self , split : Split ) -> pd . DataFrame : predictions = split . pred truth = split . truth most_popular_items = popular_items ( score_frame = truth , pop_percentage = self . __pop_percentage ) user_groups = self . split_user_in_groups ( score_frame = predictions , groups = self . user_groups , pop_items = most_popular_items ) split_result = { 'user_group' : [], 'profile_pop_ratio' : [], 'recs_pop_ratio' : []} data_to_plot = [] labels = [] for group_name in user_groups : truth_group = truth . filter_ratings ( user_list = user_groups [ group_name ]) pred_group = predictions . filter_ratings ( user_list = user_groups [ group_name ]) profile_pop_ratios_frame = pop_ratio_by_user ( truth_group , most_popular_items ) recs_pop_ratios_frame = pop_ratio_by_user ( pred_group , most_popular_items ) profile_pop_ratios = list ( profile_pop_ratios_frame . values ()) recs_pop_ratios = list ( recs_pop_ratios_frame . values ()) split_result [ 'user_group' ] . append ( group_name ) split_result [ 'profile_pop_ratio' ] . append ( profile_pop_ratios ) split_result [ 'recs_pop_ratio' ] . append ( recs_pop_ratios ) profile_data = profile_pop_ratios data_to_plot . append ( profile_data ) recs_data = recs_pop_ratios data_to_plot . append ( recs_data ) labels . append ( ' {} \\n group' . format ( group_name )) # agg backend is used to create plot as a .png file mpl . use ( 'agg' ) # Create a figure instance fig = plt . figure () # Create an axes instance ax = fig . add_subplot () ax . set ( title = 'Popularity ratio Profile vs Recs' ) # add patch_artist=True option to ax.boxplot() # to get fill color bp = ax . boxplot ( np . array ( data_to_plot , dtype = object ), patch_artist = True ) first_color = '#7570b3' second_color = '#b2df8a' fill_color_profile = '#004e98' fill_color_recs = '#ff6700' # change outline color, fill color and linewidth of the boxes for i , box in enumerate ( bp [ 'boxes' ]): # change outline color box . set ( color = first_color , linewidth = 2 ) # change fill color if i % 2 == 0 : box . set ( facecolor = fill_color_profile ) else : box . set ( facecolor = fill_color_recs ) # change color and linewidth of the whiskers for whisker in bp [ 'whiskers' ]: whisker . set ( color = first_color , linewidth = 2 ) # change color and linewidth of the caps for cap in bp [ 'caps' ]: cap . set ( color = first_color , linewidth = 2 ) # change color and linewidth of the medians for median in bp [ 'medians' ]: median . set ( color = second_color , linewidth = 2 ) # change the style of fliers and their fill for flier in bp [ 'fliers' ]: flier . set ( marker = 'o' , color = '#e7298a' , alpha = 0.5 ) # x ticks minor contains the vertical lines for better separate the various groups # a vertical line is 0.5 at the left of the first profile boxplot and another vertical line is at 0.5 of the # second boxplot for each group xticks_minor_tuples = [( i - 0.5 , i + 1 + 0.5 ) for i in range ( 1 , len ( bp [ 'boxes' ]) + 1 , 2 )] xticks_minor = list ( itertools . chain . from_iterable ( xticks_minor_tuples )) # x ticks contains the \"middle point\" between the profile boxplot and recs boxplot # for each group x_ticks = [( i + ( i + 1 )) / 2 for i in range ( 1 , len ( bp [ 'boxes' ]) + 1 , 2 )] ax . set_xticks ( x_ticks ) ax . set_xticks ( xticks_minor , minor = True ) ax . set_xticklabels ( labels ) # make x_ticks_minor bigger, they are basically the vertical lines ax . tick_params ( axis = 'x' , which = 'minor' , direction = 'out' , length = 25 ) # remove the tick and show only the label for the main ticks ax . tick_params ( axis = 'x' , which = 'major' , length = 0 ) # Remove top axes and right axes ticks ax . get_xaxis () . tick_bottom () ax . get_yaxis () . tick_left () # create the legend profile_patch = mpatches . Patch ( color = fill_color_profile , label = 'Profile popularity' ) recs_patch = mpatches . Patch ( color = fill_color_recs , label = 'Recs popularity' ) ax . legend ( handles = [ profile_patch , recs_patch ], loc = 'upper center' , bbox_to_anchor = ( 0.5 , - 0.15 )) plt . tight_layout () file_name = self . file_name self . save_figure ( fig , file_name = file_name ) score_frame = pd . DataFrame ( split_result ) if self . __store_frame : file_name = get_valid_filename ( self . output_directory , file_name , 'csv' , self . overwrite ) score_frame . to_csv ( os . path . join ( self . output_directory , file_name ), index = False ) return pd . DataFrame ( columns = [ 'user_id' , 'item_id' , 'score' ]) PopRecsCorrelation Bases: PlotMetric This metric generates a plot which has as the X-axis the popularity and as Y-axis number of recommendations, so that it can be easily seen the correlation between popular (niche) items and how many times are being recommended by the recsys The plot file will be saved as out_dir/file_name.format Since multiple split could be evaluated at once, the overwrite parameter comes into play: if is set to False, file with the same name will be saved as file_name (1).format , file_name (2).format , etc. so that for every split a plot is generated without overwriting any file previously generated There exists cases in which some items are not recommended even once, so in the graph could appear zero recommendations . One could change this behaviour thanks to the 'mode' parameter: mode='both' : two graphs will be created, the first one containing eventual zero recommendations , the second one where zero recommendations are excluded. This additional graph will be stored as out_dir/file_name_no_zeros.format (the string '_no_zeros' will be added to the file_name chosen automatically) mode='w_zeros' : only a graph containing eventual zero recommendations will be created mode='no_zeros' : only a graph excluding eventual zero recommendations will be created. The graph will be saved as out_dir/file_name_no_zeros.format (the string '_no_zeros' will be added to the file_name chosen automatically) Parameters: Name Type Description Default out_dir str Directory where the plot will be saved. Default is '.', meaning that the plot will be saved in the same directory where the python script it's being executed '.' file_name str Name of the plot file. Default is 'pop_recs_correlation' 'pop_recs_correlation' mode str Parameter which dictates which graph must be created. By default is 'both', so the graph with eventual zero recommendations as well as the graph excluding eventual zero recommendations will be created. Check the class documentation for more 'both' format str Format of the plot file. Could be 'jpg', 'svg', 'png'. Default is 'png' 'png' overwrite bool parameter which specifies if the plot saved must overwrite any file that as the same name ('file_name.format'). Default is False False Source code in clayrs/evaluation/metrics/plot_metrics.py 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 class PopRecsCorrelation ( PlotMetric ): \"\"\" This metric generates a plot which has as the X-axis the popularity and as Y-axis number of recommendations, so that it can be easily seen the correlation between popular (niche) items and how many times are being recommended by the recsys The plot file will be saved as *out_dir/file_name.format* Since multiple split could be evaluated at once, the *overwrite* parameter comes into play: if is set to False, file with the same name will be saved as *file_name (1).format*, *file_name (2).format*, etc. so that for every split a plot is generated without overwriting any file previously generated There exists cases in which some items are not recommended even once, so in the graph could appear **zero recommendations**. One could change this behaviour thanks to the 'mode' parameter: - **mode='both'**: two graphs will be created, the first one containing eventual *zero recommendations*, the second one where *zero recommendations* are excluded. This additional graph will be stored as *out_dir/file_name_no_zeros.format* (the string '_no_zeros' will be added to the file_name chosen automatically) - **mode='w_zeros'**: only a graph containing eventual *zero recommendations* will be created - **mode='no_zeros'**: only a graph excluding eventual *zero recommendations* will be created. The graph will be saved as *out_dir/file_name_no_zeros.format* (the string '_no_zeros' will be added to the file_name chosen automatically) Args: out_dir (str): Directory where the plot will be saved. Default is '.', meaning that the plot will be saved in the same directory where the python script it's being executed file_name (str): Name of the plot file. Default is 'pop_recs_correlation' mode (str): Parameter which dictates which graph must be created. By default is 'both', so the graph with eventual zero recommendations as well as the graph excluding eventual zero recommendations will be created. Check the class documentation for more format (str): Format of the plot file. Could be 'jpg', 'svg', 'png'. Default is 'png' overwrite (bool): parameter which specifies if the plot saved must overwrite any file that as the same name ('file_name.format'). Default is False \"\"\" def __init__ ( self , out_dir : str = '.' , file_name : str = 'pop_recs_correlation' , mode : str = 'both' , format : str = 'png' , overwrite : bool = False ): valid = { 'both' , 'no_zeros' , 'w_zeros' } self . __mode = mode . lower () if self . __mode not in valid : raise ValueError ( \"Mode {} is not supported! Modes available: \\n \" \" {} \" . format ( mode , valid )) super () . __init__ ( out_dir , file_name , format , overwrite ) def __str__ ( self ): return \"PopRecsCorrelation\" def __repr__ ( self ): return f 'PopRecsCorrelation(' \\ f 'out_dir= { self . output_directory } , ' \\ f 'file_name= { self . file_name } , ' \\ f 'mode= { self . __mode } , ' \\ f 'format= { self . format } , ' \\ f 'overwrite= { self . overwrite } )' def build_plot ( self , x : list , y : list , title : str ): \"\"\" Method which builds a matplotlib plot given x-axis values, y-axis values and the title of the plot. X-axis label and Y-axis label are hard-coded as 'Popularity' and 'Recommendation frequency' respectively. Args: x (list): List containing x-axis values y (list): List containing y-axis values title (str): title of the plot Returns: The matplotlib figure \"\"\" fig = plt . figure () ax = fig . add_subplot () ax . set ( xlabel = 'Popularity' , ylabel = 'Recommendation frequency' , title = title ) ax . scatter ( x , y , marker = 'o' , s = 20 , c = 'orange' , edgecolors = 'black' , linewidths = 0.05 ) return fig def build_w_zeros_plot ( self , popularities : list , recommendations : list ): \"\"\" Method which builds and saves the plot containing eventual *zero recommendations* It saves the plot as *out_dir/filename.format*, according to their value passed in the constructor Args: popularities (list): x-axis values representing popularity of every item recommendations (list): y-axis values representing number of times every item has been recommended \"\"\" title = 'Popularity-Recommendations Correlation' fig = self . build_plot ( popularities , recommendations , title ) file_name = self . file_name self . save_figure ( fig , file_name ) def build_no_zeros_plot ( self , popularities : list , recommendations : list ): \"\"\" Method which builds and saves the plot **excluding** eventual *zero recommendations* It saves the plot as *out_dir/filename_no_zeros.format*, according to their value passed in the constructor. Note that the '_no_zeros' string is automatically added to the file_name chosen Args: popularities (list): x-axis values representing popularity of every item recommendations (list): y-axis values representing number of times every item has been recommended \"\"\" title = 'Popularity-Recommendations Correlation (No zeros)' fig = self . build_plot ( popularities , recommendations , title ) file_name = self . file_name + '_no_zeros' self . save_figure ( fig , file_name ) def perform ( self , split : Split ): predictions = split . pred truth = split . truth # Calculating popularity by item items = truth . item_id_column pop_by_items = Counter ( items ) # Calculating num of recommendations by item pop_by_items = pop_by_items . most_common () recs_by_item = Counter ( predictions . item_id_column ) popularities = list () recommendations = list () popularities_no_zeros = list () recommendations_no_zeros = list () at_least_one_zero = False for item , pop in pop_by_items : num_of_recs = recs_by_item [ item ] popularities . append ( pop ) recommendations . append ( num_of_recs ) if num_of_recs != 0 : popularities_no_zeros . append ( pop ) recommendations_no_zeros . append ( num_of_recs ) else : at_least_one_zero = True # Both when possible if self . __mode == 'both' : self . build_w_zeros_plot ( popularities , recommendations ) if at_least_one_zero : self . build_no_zeros_plot ( popularities_no_zeros , recommendations_no_zeros ) else : self . build_no_zeros_plot ( popularities , recommendations ) logger . warning ( \"There's no zero recommendation! \\n \" \"The graph with 'no-zero' is identical to the 'w-zero' one!\" ) elif self . __mode == 'w_zeros' : self . build_w_zeros_plot ( popularities , recommendations ) elif self . __mode == 'no_zeros' : self . build_no_zeros_plot ( popularities_no_zeros , recommendations_no_zeros ) return pd . DataFrame ( columns = [ 'user_id' , 'item_id' , 'score' ]) build_no_zeros_plot ( popularities , recommendations ) Method which builds and saves the plot excluding eventual zero recommendations It saves the plot as out_dir/filename_no_zeros.format , according to their value passed in the constructor. Note that the '_no_zeros' string is automatically added to the file_name chosen Parameters: Name Type Description Default popularities list x-axis values representing popularity of every item required recommendations list y-axis values representing number of times every item has been recommended required Source code in clayrs/evaluation/metrics/plot_metrics.py 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 def build_no_zeros_plot ( self , popularities : list , recommendations : list ): \"\"\" Method which builds and saves the plot **excluding** eventual *zero recommendations* It saves the plot as *out_dir/filename_no_zeros.format*, according to their value passed in the constructor. Note that the '_no_zeros' string is automatically added to the file_name chosen Args: popularities (list): x-axis values representing popularity of every item recommendations (list): y-axis values representing number of times every item has been recommended \"\"\" title = 'Popularity-Recommendations Correlation (No zeros)' fig = self . build_plot ( popularities , recommendations , title ) file_name = self . file_name + '_no_zeros' self . save_figure ( fig , file_name ) build_plot ( x , y , title ) Method which builds a matplotlib plot given x-axis values, y-axis values and the title of the plot. X-axis label and Y-axis label are hard-coded as 'Popularity' and 'Recommendation frequency' respectively. Parameters: Name Type Description Default x list List containing x-axis values required y list List containing y-axis values required title str title of the plot required Returns: Type Description The matplotlib figure Source code in clayrs/evaluation/metrics/plot_metrics.py 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 def build_plot ( self , x : list , y : list , title : str ): \"\"\" Method which builds a matplotlib plot given x-axis values, y-axis values and the title of the plot. X-axis label and Y-axis label are hard-coded as 'Popularity' and 'Recommendation frequency' respectively. Args: x (list): List containing x-axis values y (list): List containing y-axis values title (str): title of the plot Returns: The matplotlib figure \"\"\" fig = plt . figure () ax = fig . add_subplot () ax . set ( xlabel = 'Popularity' , ylabel = 'Recommendation frequency' , title = title ) ax . scatter ( x , y , marker = 'o' , s = 20 , c = 'orange' , edgecolors = 'black' , linewidths = 0.05 ) return fig build_w_zeros_plot ( popularities , recommendations ) Method which builds and saves the plot containing eventual zero recommendations It saves the plot as out_dir/filename.format , according to their value passed in the constructor Parameters: Name Type Description Default popularities list x-axis values representing popularity of every item required recommendations list y-axis values representing number of times every item has been recommended required Source code in clayrs/evaluation/metrics/plot_metrics.py 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 def build_w_zeros_plot ( self , popularities : list , recommendations : list ): \"\"\" Method which builds and saves the plot containing eventual *zero recommendations* It saves the plot as *out_dir/filename.format*, according to their value passed in the constructor Args: popularities (list): x-axis values representing popularity of every item recommendations (list): y-axis values representing number of times every item has been recommended \"\"\" title = 'Popularity-Recommendations Correlation' fig = self . build_plot ( popularities , recommendations , title ) file_name = self . file_name self . save_figure ( fig , file_name )","title":"Plot metrics"},{"location":"evaluation/metrics/plot_metrics/#plot-metrics","text":"","title":"Plot metrics"},{"location":"evaluation/metrics/plot_metrics/#clayrs.evaluation.metrics.plot_metrics.LongTailDistr","text":"Bases: PlotMetric This metric generates the Long Tail Distribution plot and saves it in the output directory with the file name specified. The plot can be generated both for the truth set or the predictions set (based on the on parameter): on = 'truth' : in this case the long tail distribution is useful to see which are the most popular items (the most rated ones) on = 'pred' : in this case the long tail distribution is useful to see which are the most recommended items The plot file will be saved as out_dir/file_name.format Since multiple split could be evaluated at once, the overwrite parameter comes into play: if is set to False, file with the same name will be saved as file_name (1).format , file_name (2).format , etc. so that for every split a plot is generated without overwriting any file previously generated Parameters: Name Type Description Default out_dir str Directory where the plot will be saved. Default is '.', meaning that the plot will be saved in the same directory where the python script it's being executed '.' file_name str Name of the plot file. Default is 'long_tail_distr' 'long_tail_distr' on str Set on which the Long Tail Distribution plot will be generated. Values accepted are 'truth' or 'pred' 'truth' format str Format of the plot file. Could be 'jpg', 'svg', 'png'. Default is 'png' 'png' overwrite bool parameter which specifies if the plot saved must overwrite any file that as the same name ('file_name.format'). Default is False False Raises: Type Description ValueError exception raised when a invalid value for the 'on' parameter is specified Source code in clayrs/evaluation/metrics/plot_metrics.py 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 class LongTailDistr ( PlotMetric ): \"\"\" This metric generates the Long Tail Distribution plot and saves it in the output directory with the file name specified. The plot can be generated both for the *truth set* or the *predictions set* (based on the *on* parameter): - **on = 'truth'**: in this case the long tail distribution is useful to see which are the most popular items (the most rated ones) - **on = 'pred'**: in this case the long tail distribution is useful to see which are the most recommended items The plot file will be saved as *out_dir/file_name.format* Since multiple split could be evaluated at once, the *overwrite* parameter comes into play: if is set to False, file with the same name will be saved as *file_name (1).format*, *file_name (2).format*, etc. so that for every split a plot is generated without overwriting any file previously generated Args: out_dir (str): Directory where the plot will be saved. Default is '.', meaning that the plot will be saved in the same directory where the python script it's being executed file_name (str): Name of the plot file. Default is 'long_tail_distr' on (str): Set on which the Long Tail Distribution plot will be generated. Values accepted are 'truth' or 'pred' format (str): Format of the plot file. Could be 'jpg', 'svg', 'png'. Default is 'png' overwrite (bool): parameter which specifies if the plot saved must overwrite any file that as the same name ('file_name.format'). Default is False Raises: ValueError: exception raised when a invalid value for the 'on' parameter is specified \"\"\" def __init__ ( self , out_dir : str = '.' , file_name : str = 'long_tail_distr' , on : str = 'truth' , format : str = 'png' , overwrite : bool = False ): valid = { 'truth' , 'pred' } self . __on = on . lower () if self . __on not in valid : raise ValueError ( \"on= {} is not supported! Long Tail can be calculated only on: \\n \" \" {} \" . format ( on , valid )) super () . __init__ ( out_dir , file_name , format , overwrite ) def __str__ ( self ): return \"LongTailDistr\" def __repr__ ( self ): return f 'LongTailDistr(out_dir= { self . __out_dir } , ' \\ f 'file_name= { self . __file_name } , ' \\ f 'format= { self . __format } , ' \\ f 'overwrite= { self . __overwrite } )' def perform ( self , split : Split ) -> pd . DataFrame : if self . __on == 'truth' : frame = split . truth else : frame = split . pred counts_by_item = Counter ( frame . item_id_column ) ordered_item_count_pairs = counts_by_item . most_common () ordered_counts = [] labels = [] for item_count_pair in ordered_item_count_pairs : labels . append ( item_count_pair [ 0 ]) ordered_counts . append ( item_count_pair [ 1 ]) x = [ i for i in range ( len ( labels ))] y = ordered_counts fig = plt . figure () ax = fig . add_subplot () if self . __on == 'truth' : ax . set ( xlabel = 'Item ID' , ylabel = '# of item ratings' , title = 'Long Tail Distribution - {} ' . format ( self . __on )) else : ax . set ( xlabel = 'Item ID' , ylabel = '# of item recommendations' , title = 'Long Tail Distribution - {} ' . format ( self . __on )) ax . fill_between ( x , y , color = \"orange\" , alpha = 0.2 ) ax . set_xticks ([]) ax . plot ( x , y ) file_name = self . file_name + '_ {} ' . format ( self . __on ) self . save_figure ( fig , file_name = file_name ) return pd . DataFrame ( columns = [ 'user_id' , 'item_id' , 'score' ])","title":"LongTailDistr"},{"location":"evaluation/metrics/plot_metrics/#clayrs.evaluation.metrics.plot_metrics.PopProfileVsRecs","text":"Bases: GroupFairnessMetric , PlotMetric This metric generates a plot where users are splitted into groups and, for every group, a boxplot comparing profile popularity and recommendations popularity is drawn Users are splitted into groups based on the user_groups parameter, which contains names of the groups as keys, and percentage of how many user must contain a group as values. For example:: user_groups = {'popular_users': 0.3, 'medium_popular_users': 0.2, 'low_popular_users': 0.5} Every user will be inserted in a group based on how many popular items the user has rated (in relation to the percentage of users we specified as value in the dictionary): users with many popular items will be inserted into the first group, users with niche items rated will be inserted into one of the last groups You could also specify how many most popular items must be considered with the 'pop_percentage' parameter. By default is set to 0.2 which means that the top 20% items are considered as most popular The plot file will be saved as out_dir/file_name.format Since multiple split could be evaluated at once, the overwrite parameter comes into play: if is set to False, file with the same name will be saved as file_name (1).format , file_name (2).format , etc. so that for every split a plot is generated without overwriting any file previously generated Thanks to the 'store_frame' parameter it's also possible to store a csv containing the calculations done in order to build every boxplot. Will be saved in the same directory and with the same file name as the plot itself (but with the .csv format): The csv will be saved as out_dir/file_name.csv Parameters: Name Type Description Default user_groups Dict<str, float> Dict containing group names as keys and percentage of users as value, used to split users in groups. Users with more popular items rated are grouped into the first group, users with slightly less popular items rated are grouped into the second one, etc. required out_dir str Directory where the plot will be saved. Default is '.', meaning that the plot will be saved in the same directory where the python script it's being executed '.' file_name str Name of the plot file. Default is 'pop_ratio_profile_vs_recs' 'pop_ratio_profile_vs_recs' pop_percentage float How many (in percentage) 'most popular items' must be considered. Default is 0.2 0.2 store_frame bool True if you want to store calculations done in order to build every boxplot in a csv file, False otherwise. Default is set to False False format str Format of the plot file. Could be 'jpg', 'svg', 'png'. Default is 'png' 'png' overwrite bool parameter which specifies if the plot saved must overwrite any file that as the same name ('file_name.format'). Default is False False Source code in clayrs/evaluation/metrics/plot_metrics.py 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 class PopProfileVsRecs ( GroupFairnessMetric , PlotMetric ): \"\"\" This metric generates a plot where users are splitted into groups and, for every group, a boxplot comparing profile popularity and recommendations popularity is drawn Users are splitted into groups based on the *user_groups* parameter, which contains names of the groups as keys, and percentage of how many user must contain a group as values. For example:: user_groups = {'popular_users': 0.3, 'medium_popular_users': 0.2, 'low_popular_users': 0.5} Every user will be inserted in a group based on how many popular items the user has rated (in relation to the percentage of users we specified as value in the dictionary): users with many popular items will be inserted into the first group, users with niche items rated will be inserted into one of the last groups You could also specify how many *most popular items* must be considered with the 'pop_percentage' parameter. By default is set to 0.2 which means that the top 20% items are considered as most popular The plot file will be saved as *out_dir/file_name.format* Since multiple split could be evaluated at once, the *overwrite* parameter comes into play: if is set to False, file with the same name will be saved as *file_name (1).format*, *file_name (2).format*, etc. so that for every split a plot is generated without overwriting any file previously generated Thanks to the 'store_frame' parameter it's also possible to store a csv containing the calculations done in order to build every boxplot. Will be saved in the same directory and with the same file name as the plot itself (but with the .csv format): The csv will be saved as *out_dir/file_name.csv* Args: user_groups (Dict<str, float>): Dict containing group names as keys and percentage of users as value, used to split users in groups. Users with more popular items rated are grouped into the first group, users with slightly less popular items rated are grouped into the second one, etc. out_dir (str): Directory where the plot will be saved. Default is '.', meaning that the plot will be saved in the same directory where the python script it's being executed file_name (str): Name of the plot file. Default is 'pop_ratio_profile_vs_recs' pop_percentage (float): How many (in percentage) 'most popular items' must be considered. Default is 0.2 store_frame (bool): True if you want to store calculations done in order to build every boxplot in a csv file, False otherwise. Default is set to False format (str): Format of the plot file. Could be 'jpg', 'svg', 'png'. Default is 'png' overwrite (bool): parameter which specifies if the plot saved must overwrite any file that as the same name ('file_name.format'). Default is False \"\"\" def __init__ ( self , user_groups : Dict [ str , float ], out_dir : str = '.' , file_name : str = 'pop_ratio_profile_vs_recs' , pop_percentage : float = 0.2 , store_frame : bool = False , format : str = 'png' , overwrite : bool = False ): PlotMetric . __init__ ( self , out_dir , file_name , format , overwrite ) GroupFairnessMetric . __init__ ( self , user_groups ) if not 0 < pop_percentage <= 1 : raise ValueError ( 'Incorrect percentage! Valid percentage range: 0 < percentage <= 1' ) self . __pop_percentage = pop_percentage self . __user_groups = user_groups self . __store_frame = store_frame def __str__ ( self ): return \"PopProfileVsRecs\" def __repr__ ( self ): return f 'PopProfileVsRecs(' \\ f 'user_groups= { self . __user_groups } , ' \\ f 'out_dir= { self . output_directory } , ' \\ f 'file_name= { self . file_name } , ' \\ f 'pop_percentage= { self . __pop_percentage } , ' \\ f 'store_frame= { self . __store_frame } , ' \\ f 'format= { self . format } , ' \\ f 'overwrite= { self . overwrite } )' def perform ( self , split : Split ) -> pd . DataFrame : predictions = split . pred truth = split . truth most_popular_items = popular_items ( score_frame = truth , pop_percentage = self . __pop_percentage ) user_groups = self . split_user_in_groups ( score_frame = predictions , groups = self . user_groups , pop_items = most_popular_items ) split_result = { 'user_group' : [], 'profile_pop_ratio' : [], 'recs_pop_ratio' : []} data_to_plot = [] labels = [] for group_name in user_groups : truth_group = truth . filter_ratings ( user_list = user_groups [ group_name ]) pred_group = predictions . filter_ratings ( user_list = user_groups [ group_name ]) profile_pop_ratios_frame = pop_ratio_by_user ( truth_group , most_popular_items ) recs_pop_ratios_frame = pop_ratio_by_user ( pred_group , most_popular_items ) profile_pop_ratios = list ( profile_pop_ratios_frame . values ()) recs_pop_ratios = list ( recs_pop_ratios_frame . values ()) split_result [ 'user_group' ] . append ( group_name ) split_result [ 'profile_pop_ratio' ] . append ( profile_pop_ratios ) split_result [ 'recs_pop_ratio' ] . append ( recs_pop_ratios ) profile_data = profile_pop_ratios data_to_plot . append ( profile_data ) recs_data = recs_pop_ratios data_to_plot . append ( recs_data ) labels . append ( ' {} \\n group' . format ( group_name )) # agg backend is used to create plot as a .png file mpl . use ( 'agg' ) # Create a figure instance fig = plt . figure () # Create an axes instance ax = fig . add_subplot () ax . set ( title = 'Popularity ratio Profile vs Recs' ) # add patch_artist=True option to ax.boxplot() # to get fill color bp = ax . boxplot ( np . array ( data_to_plot , dtype = object ), patch_artist = True ) first_color = '#7570b3' second_color = '#b2df8a' fill_color_profile = '#004e98' fill_color_recs = '#ff6700' # change outline color, fill color and linewidth of the boxes for i , box in enumerate ( bp [ 'boxes' ]): # change outline color box . set ( color = first_color , linewidth = 2 ) # change fill color if i % 2 == 0 : box . set ( facecolor = fill_color_profile ) else : box . set ( facecolor = fill_color_recs ) # change color and linewidth of the whiskers for whisker in bp [ 'whiskers' ]: whisker . set ( color = first_color , linewidth = 2 ) # change color and linewidth of the caps for cap in bp [ 'caps' ]: cap . set ( color = first_color , linewidth = 2 ) # change color and linewidth of the medians for median in bp [ 'medians' ]: median . set ( color = second_color , linewidth = 2 ) # change the style of fliers and their fill for flier in bp [ 'fliers' ]: flier . set ( marker = 'o' , color = '#e7298a' , alpha = 0.5 ) # x ticks minor contains the vertical lines for better separate the various groups # a vertical line is 0.5 at the left of the first profile boxplot and another vertical line is at 0.5 of the # second boxplot for each group xticks_minor_tuples = [( i - 0.5 , i + 1 + 0.5 ) for i in range ( 1 , len ( bp [ 'boxes' ]) + 1 , 2 )] xticks_minor = list ( itertools . chain . from_iterable ( xticks_minor_tuples )) # x ticks contains the \"middle point\" between the profile boxplot and recs boxplot # for each group x_ticks = [( i + ( i + 1 )) / 2 for i in range ( 1 , len ( bp [ 'boxes' ]) + 1 , 2 )] ax . set_xticks ( x_ticks ) ax . set_xticks ( xticks_minor , minor = True ) ax . set_xticklabels ( labels ) # make x_ticks_minor bigger, they are basically the vertical lines ax . tick_params ( axis = 'x' , which = 'minor' , direction = 'out' , length = 25 ) # remove the tick and show only the label for the main ticks ax . tick_params ( axis = 'x' , which = 'major' , length = 0 ) # Remove top axes and right axes ticks ax . get_xaxis () . tick_bottom () ax . get_yaxis () . tick_left () # create the legend profile_patch = mpatches . Patch ( color = fill_color_profile , label = 'Profile popularity' ) recs_patch = mpatches . Patch ( color = fill_color_recs , label = 'Recs popularity' ) ax . legend ( handles = [ profile_patch , recs_patch ], loc = 'upper center' , bbox_to_anchor = ( 0.5 , - 0.15 )) plt . tight_layout () file_name = self . file_name self . save_figure ( fig , file_name = file_name ) score_frame = pd . DataFrame ( split_result ) if self . __store_frame : file_name = get_valid_filename ( self . output_directory , file_name , 'csv' , self . overwrite ) score_frame . to_csv ( os . path . join ( self . output_directory , file_name ), index = False ) return pd . DataFrame ( columns = [ 'user_id' , 'item_id' , 'score' ])","title":"PopProfileVsRecs"},{"location":"evaluation/metrics/plot_metrics/#clayrs.evaluation.metrics.plot_metrics.PopRecsCorrelation","text":"Bases: PlotMetric This metric generates a plot which has as the X-axis the popularity and as Y-axis number of recommendations, so that it can be easily seen the correlation between popular (niche) items and how many times are being recommended by the recsys The plot file will be saved as out_dir/file_name.format Since multiple split could be evaluated at once, the overwrite parameter comes into play: if is set to False, file with the same name will be saved as file_name (1).format , file_name (2).format , etc. so that for every split a plot is generated without overwriting any file previously generated There exists cases in which some items are not recommended even once, so in the graph could appear zero recommendations . One could change this behaviour thanks to the 'mode' parameter: mode='both' : two graphs will be created, the first one containing eventual zero recommendations , the second one where zero recommendations are excluded. This additional graph will be stored as out_dir/file_name_no_zeros.format (the string '_no_zeros' will be added to the file_name chosen automatically) mode='w_zeros' : only a graph containing eventual zero recommendations will be created mode='no_zeros' : only a graph excluding eventual zero recommendations will be created. The graph will be saved as out_dir/file_name_no_zeros.format (the string '_no_zeros' will be added to the file_name chosen automatically) Parameters: Name Type Description Default out_dir str Directory where the plot will be saved. Default is '.', meaning that the plot will be saved in the same directory where the python script it's being executed '.' file_name str Name of the plot file. Default is 'pop_recs_correlation' 'pop_recs_correlation' mode str Parameter which dictates which graph must be created. By default is 'both', so the graph with eventual zero recommendations as well as the graph excluding eventual zero recommendations will be created. Check the class documentation for more 'both' format str Format of the plot file. Could be 'jpg', 'svg', 'png'. Default is 'png' 'png' overwrite bool parameter which specifies if the plot saved must overwrite any file that as the same name ('file_name.format'). Default is False False Source code in clayrs/evaluation/metrics/plot_metrics.py 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 class PopRecsCorrelation ( PlotMetric ): \"\"\" This metric generates a plot which has as the X-axis the popularity and as Y-axis number of recommendations, so that it can be easily seen the correlation between popular (niche) items and how many times are being recommended by the recsys The plot file will be saved as *out_dir/file_name.format* Since multiple split could be evaluated at once, the *overwrite* parameter comes into play: if is set to False, file with the same name will be saved as *file_name (1).format*, *file_name (2).format*, etc. so that for every split a plot is generated without overwriting any file previously generated There exists cases in which some items are not recommended even once, so in the graph could appear **zero recommendations**. One could change this behaviour thanks to the 'mode' parameter: - **mode='both'**: two graphs will be created, the first one containing eventual *zero recommendations*, the second one where *zero recommendations* are excluded. This additional graph will be stored as *out_dir/file_name_no_zeros.format* (the string '_no_zeros' will be added to the file_name chosen automatically) - **mode='w_zeros'**: only a graph containing eventual *zero recommendations* will be created - **mode='no_zeros'**: only a graph excluding eventual *zero recommendations* will be created. The graph will be saved as *out_dir/file_name_no_zeros.format* (the string '_no_zeros' will be added to the file_name chosen automatically) Args: out_dir (str): Directory where the plot will be saved. Default is '.', meaning that the plot will be saved in the same directory where the python script it's being executed file_name (str): Name of the plot file. Default is 'pop_recs_correlation' mode (str): Parameter which dictates which graph must be created. By default is 'both', so the graph with eventual zero recommendations as well as the graph excluding eventual zero recommendations will be created. Check the class documentation for more format (str): Format of the plot file. Could be 'jpg', 'svg', 'png'. Default is 'png' overwrite (bool): parameter which specifies if the plot saved must overwrite any file that as the same name ('file_name.format'). Default is False \"\"\" def __init__ ( self , out_dir : str = '.' , file_name : str = 'pop_recs_correlation' , mode : str = 'both' , format : str = 'png' , overwrite : bool = False ): valid = { 'both' , 'no_zeros' , 'w_zeros' } self . __mode = mode . lower () if self . __mode not in valid : raise ValueError ( \"Mode {} is not supported! Modes available: \\n \" \" {} \" . format ( mode , valid )) super () . __init__ ( out_dir , file_name , format , overwrite ) def __str__ ( self ): return \"PopRecsCorrelation\" def __repr__ ( self ): return f 'PopRecsCorrelation(' \\ f 'out_dir= { self . output_directory } , ' \\ f 'file_name= { self . file_name } , ' \\ f 'mode= { self . __mode } , ' \\ f 'format= { self . format } , ' \\ f 'overwrite= { self . overwrite } )' def build_plot ( self , x : list , y : list , title : str ): \"\"\" Method which builds a matplotlib plot given x-axis values, y-axis values and the title of the plot. X-axis label and Y-axis label are hard-coded as 'Popularity' and 'Recommendation frequency' respectively. Args: x (list): List containing x-axis values y (list): List containing y-axis values title (str): title of the plot Returns: The matplotlib figure \"\"\" fig = plt . figure () ax = fig . add_subplot () ax . set ( xlabel = 'Popularity' , ylabel = 'Recommendation frequency' , title = title ) ax . scatter ( x , y , marker = 'o' , s = 20 , c = 'orange' , edgecolors = 'black' , linewidths = 0.05 ) return fig def build_w_zeros_plot ( self , popularities : list , recommendations : list ): \"\"\" Method which builds and saves the plot containing eventual *zero recommendations* It saves the plot as *out_dir/filename.format*, according to their value passed in the constructor Args: popularities (list): x-axis values representing popularity of every item recommendations (list): y-axis values representing number of times every item has been recommended \"\"\" title = 'Popularity-Recommendations Correlation' fig = self . build_plot ( popularities , recommendations , title ) file_name = self . file_name self . save_figure ( fig , file_name ) def build_no_zeros_plot ( self , popularities : list , recommendations : list ): \"\"\" Method which builds and saves the plot **excluding** eventual *zero recommendations* It saves the plot as *out_dir/filename_no_zeros.format*, according to their value passed in the constructor. Note that the '_no_zeros' string is automatically added to the file_name chosen Args: popularities (list): x-axis values representing popularity of every item recommendations (list): y-axis values representing number of times every item has been recommended \"\"\" title = 'Popularity-Recommendations Correlation (No zeros)' fig = self . build_plot ( popularities , recommendations , title ) file_name = self . file_name + '_no_zeros' self . save_figure ( fig , file_name ) def perform ( self , split : Split ): predictions = split . pred truth = split . truth # Calculating popularity by item items = truth . item_id_column pop_by_items = Counter ( items ) # Calculating num of recommendations by item pop_by_items = pop_by_items . most_common () recs_by_item = Counter ( predictions . item_id_column ) popularities = list () recommendations = list () popularities_no_zeros = list () recommendations_no_zeros = list () at_least_one_zero = False for item , pop in pop_by_items : num_of_recs = recs_by_item [ item ] popularities . append ( pop ) recommendations . append ( num_of_recs ) if num_of_recs != 0 : popularities_no_zeros . append ( pop ) recommendations_no_zeros . append ( num_of_recs ) else : at_least_one_zero = True # Both when possible if self . __mode == 'both' : self . build_w_zeros_plot ( popularities , recommendations ) if at_least_one_zero : self . build_no_zeros_plot ( popularities_no_zeros , recommendations_no_zeros ) else : self . build_no_zeros_plot ( popularities , recommendations ) logger . warning ( \"There's no zero recommendation! \\n \" \"The graph with 'no-zero' is identical to the 'w-zero' one!\" ) elif self . __mode == 'w_zeros' : self . build_w_zeros_plot ( popularities , recommendations ) elif self . __mode == 'no_zeros' : self . build_no_zeros_plot ( popularities_no_zeros , recommendations_no_zeros ) return pd . DataFrame ( columns = [ 'user_id' , 'item_id' , 'score' ])","title":"PopRecsCorrelation"},{"location":"evaluation/metrics/plot_metrics/#clayrs.evaluation.metrics.plot_metrics.PopRecsCorrelation.build_no_zeros_plot","text":"Method which builds and saves the plot excluding eventual zero recommendations It saves the plot as out_dir/filename_no_zeros.format , according to their value passed in the constructor. Note that the '_no_zeros' string is automatically added to the file_name chosen Parameters: Name Type Description Default popularities list x-axis values representing popularity of every item required recommendations list y-axis values representing number of times every item has been recommended required Source code in clayrs/evaluation/metrics/plot_metrics.py 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 def build_no_zeros_plot ( self , popularities : list , recommendations : list ): \"\"\" Method which builds and saves the plot **excluding** eventual *zero recommendations* It saves the plot as *out_dir/filename_no_zeros.format*, according to their value passed in the constructor. Note that the '_no_zeros' string is automatically added to the file_name chosen Args: popularities (list): x-axis values representing popularity of every item recommendations (list): y-axis values representing number of times every item has been recommended \"\"\" title = 'Popularity-Recommendations Correlation (No zeros)' fig = self . build_plot ( popularities , recommendations , title ) file_name = self . file_name + '_no_zeros' self . save_figure ( fig , file_name )","title":"build_no_zeros_plot()"},{"location":"evaluation/metrics/plot_metrics/#clayrs.evaluation.metrics.plot_metrics.PopRecsCorrelation.build_plot","text":"Method which builds a matplotlib plot given x-axis values, y-axis values and the title of the plot. X-axis label and Y-axis label are hard-coded as 'Popularity' and 'Recommendation frequency' respectively. Parameters: Name Type Description Default x list List containing x-axis values required y list List containing y-axis values required title str title of the plot required Returns: Type Description The matplotlib figure Source code in clayrs/evaluation/metrics/plot_metrics.py 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 def build_plot ( self , x : list , y : list , title : str ): \"\"\" Method which builds a matplotlib plot given x-axis values, y-axis values and the title of the plot. X-axis label and Y-axis label are hard-coded as 'Popularity' and 'Recommendation frequency' respectively. Args: x (list): List containing x-axis values y (list): List containing y-axis values title (str): title of the plot Returns: The matplotlib figure \"\"\" fig = plt . figure () ax = fig . add_subplot () ax . set ( xlabel = 'Popularity' , ylabel = 'Recommendation frequency' , title = title ) ax . scatter ( x , y , marker = 'o' , s = 20 , c = 'orange' , edgecolors = 'black' , linewidths = 0.05 ) return fig","title":"build_plot()"},{"location":"evaluation/metrics/plot_metrics/#clayrs.evaluation.metrics.plot_metrics.PopRecsCorrelation.build_w_zeros_plot","text":"Method which builds and saves the plot containing eventual zero recommendations It saves the plot as out_dir/filename.format , according to their value passed in the constructor Parameters: Name Type Description Default popularities list x-axis values representing popularity of every item required recommendations list y-axis values representing number of times every item has been recommended required Source code in clayrs/evaluation/metrics/plot_metrics.py 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 def build_w_zeros_plot ( self , popularities : list , recommendations : list ): \"\"\" Method which builds and saves the plot containing eventual *zero recommendations* It saves the plot as *out_dir/filename.format*, according to their value passed in the constructor Args: popularities (list): x-axis values representing popularity of every item recommendations (list): y-axis values representing number of times every item has been recommended \"\"\" title = 'Popularity-Recommendations Correlation' fig = self . build_plot ( popularities , recommendations , title ) file_name = self . file_name self . save_figure ( fig , file_name )","title":"build_w_zeros_plot()"},{"location":"evaluation/metrics/ranking_metrics/","text":"Ranking metrics Correlation Bases: RankingMetric The Correlation metric calculates the correlation between the ranking of a user and its ideal ranking. The currently correlation methods implemented are: pearson kendall spearman Every correlation method is implemented by the scipy library, so read its documentation for more The correlation metric is calculated as such for the single user : \\[ Corr_u = Corr(ranking_u, ideal\\_ranking_u) \\] Where: \\(ranking_u\\) is ranking of the user \\(ideal\\_ranking_u\\) is the ideal ranking for the user The ideal ranking is calculated based on the rating inside the ground truth of the user The Correlation metric calculated for the entire system is simply the average of every \\(Corr\\) : \\[ Corr_{sys} = \\frac{\\sum_{u} Corr_u}{|U|} \\] Where: \\(Corr_u\\) is the correlation of the user \\(u\\) \\(U\\) is the set of all users The system average excludes NaN values. It's also possible to specify a cutoff parameter thanks to the 'top_n' parameter: if specified, only the first \\(n\\) results of the recommendation list will be used in order to calculate the correlation Parameters: Name Type Description Default method str The correlation method to use. It must be 'pearson', 'kendall' or 'spearman', otherwise a ValueError exception is raised. By default is 'pearson' 'pearson' top_n int Cutoff parameter, if specified only the first n items of the recommendation list will be used in order to calculate the correlation None Raises: Type Description ValueError if an invalid method parameter is passed Source code in clayrs/evaluation/metrics/ranking_metrics.py 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 class Correlation ( RankingMetric ): r \"\"\" The Correlation metric calculates the correlation between the ranking of a user and its ideal ranking. The currently correlation methods implemented are: - pearson - kendall - spearman Every correlation method is implemented by the scipy library, so read its documentation for more The correlation metric is calculated as such for the **single user**: $$ Corr_u = Corr(ranking_u, ideal\\_ranking_u) $$ Where: - $ranking_u$ is ranking of the user - $ideal\\_ranking_u$ is the ideal ranking for the user The ideal ranking is calculated based on the rating inside the *ground truth* of the user The Correlation metric calculated for the **entire system** is simply the average of every $Corr$: $$ Corr_{sys} = \\frac{\\sum_{u} Corr_u}{|U|} $$ Where: - $Corr_u$ is the correlation of the user $u$ - $U$ is the set of all users The system average excludes NaN values. It's also possible to specify a cutoff parameter thanks to the 'top_n' parameter: if specified, only the first $n$ results of the recommendation list will be used in order to calculate the correlation Args: method (str): The correlation method to use. It must be 'pearson', 'kendall' or 'spearman', otherwise a ValueError exception is raised. By default is 'pearson' top_n (int): Cutoff parameter, if specified only the first n items of the recommendation list will be used in order to calculate the correlation Raises: ValueError: if an invalid method parameter is passed \"\"\" def __init__ ( self , method : str = 'pearson' , top_n : int = None ): valid = { 'pearson' , 'kendall' , 'spearman' } self . __method = method . lower () if self . __method not in valid : raise ValueError ( \"Method {} is not supported! Methods available: \\n \" \" {} \" . format ( method , valid )) self . __top_n = top_n def __str__ ( self ): name = self . __method if self . __top_n is not None : name += \" - Top {} \" . format ( self . __top_n ) return name def __repr__ ( self ): return f 'Correlation(method= { self . __method } , top_n= { self . __top_n } )' def perform ( self , split : Split ) -> pd . DataFrame : pred = split . pred truth = split . truth split_result = { 'user_id' : [], str ( self ): []} for user in set ( truth . user_id_column ): user_predictions = pred . get_user_interactions ( user ) user_truth = truth . get_user_interactions ( user ) ideal , actual = self . _get_ideal_actual_rank ( user_predictions , user_truth ) if len ( actual ) < 2 : coef = np . nan else : ideal_ranking = pd . Series ( ideal ) actual_ranking = pd . Series ( actual ) coef = actual_ranking . corr ( ideal_ranking , method = self . __method ) split_result [ 'user_id' ] . append ( user ) split_result [ str ( self )] . append ( coef ) split_result [ 'user_id' ] . append ( 'sys' ) split_result [ str ( self )] . append ( np . nanmean ( split_result [ str ( self )])) return pd . DataFrame ( split_result ) def _get_ideal_actual_rank ( self , user_predictions : List [ Interaction ], user_truth : List [ Interaction ]): if self . __top_n is not None : user_predictions = user_predictions [: self . __top_n ] # sorting truth on score values user_truth_ordered = sorted ( user_truth , key = lambda interaction : interaction . score , reverse = True ) ideal_rank = [ interaction . item_id for interaction in user_truth_ordered ] predicted_items = [ interaction . item_id for interaction in user_predictions ] actual_rank = [ predicted_items . index ( item ) for item in ideal_rank if item in set ( predicted_items )] # the ideal rank is basically 0, 1, 2, 3 etc. return [ i for i in range ( len ( ideal_rank ))], actual_rank MRR Bases: RankingMetric The MRR (Mean Reciprocal Rank) metric is a system wide metric, so only its result it will be returned and not those of every user. MRR is calculated as such \\[ MRR_{sys} = \\frac{1}{|Q|}\\cdot\\sum_{i=1}^{|Q|}\\frac{1}{rank(i)} \\] Where: \\(Q\\) is the set of recommendation lists \\(rank(i)\\) is the position of the first relevant item in the i-th recommendation list The MRR metric needs to discern relevant items from the not relevant ones: in order to do that, one could pass a custom relevant_threshold parameter that will be applied to every user, so that if a rating of an item is >= relevant_threshold, then it's relevant, otherwise it's not. If no relevant_threshold parameter is passed then, for every user, its mean rating score will be used Parameters: Name Type Description Default relevant_threshold float parameter needed to discern relevant items and non-relevant items for every user. If not specified, the mean rating score of every user will be used None Source code in clayrs/evaluation/metrics/ranking_metrics.py 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 class MRR ( RankingMetric ): r \"\"\" The MRR (Mean Reciprocal Rank) metric is a system wide metric, so only its result it will be returned and not those of every user. MRR is calculated as such $$ MRR_{sys} = \\frac{1}{|Q|}\\cdot\\sum_{i=1}^{|Q|}\\frac{1}{rank(i)} $$ Where: - $Q$ is the set of recommendation lists - $rank(i)$ is the position of the first relevant item in the i-th recommendation list The MRR metric needs to discern relevant items from the not relevant ones: in order to do that, one could pass a custom relevant_threshold parameter that will be applied to every user, so that if a rating of an item is >= relevant_threshold, then it's relevant, otherwise it's not. If no relevant_threshold parameter is passed then, for every user, its mean rating score will be used Args: relevant_threshold (float): parameter needed to discern relevant items and non-relevant items for every user. If not specified, the mean rating score of every user will be used \"\"\" def __init__ ( self , relevant_threshold : float = None ): self . __relevant_threshold = relevant_threshold @property def relevant_threshold ( self ): return self . __relevant_threshold def __str__ ( self ): return \"MRR\" def __repr__ ( self ): return f 'MRR(relevant_threshold= { self . relevant_threshold } )' def calc_reciprocal_rank ( self , user_predictions : List [ Interaction ], user_truth_relevant_items : Set [ Interaction ], relevant_threshold : float ): \"\"\" Method which calculates the RR (Reciprocal Rank) for a single user Args: valid (pd.DataFrame): a DataFrame containing the recommendation list and the truth of a single user \"\"\" reciprocal_rank = 0 i = 1 for interaction_pred in user_predictions : if interaction_pred . item_id in user_truth_relevant_items : reciprocal_rank = 1 / i break # We only need the first relevant item position in the rank i += 1 return reciprocal_rank def perform ( self , split : Split ) -> pd . DataFrame : pred = split . pred truth = split . truth split_result = { 'user_id' : [], str ( self ): []} rr_list = [] for user in set ( truth . user_id_column ): user_predictions = pred . get_user_interactions ( user ) user_truth = truth . get_user_interactions ( user ) relevant_threshold = self . relevant_threshold if relevant_threshold is None : relevant_threshold = np . nanmean ([ interaction . score for interaction in user_truth ]) user_truth_relevant_items = set ( interaction . item_id for interaction in user_truth if interaction . score >= relevant_threshold ) if len ( user_truth_relevant_items ) != 0 : user_reciprocal_rank = self . calc_reciprocal_rank ( user_predictions , user_truth_relevant_items , relevant_threshold ) else : user_reciprocal_rank = np . nan rr_list . append ( user_reciprocal_rank ) # trick to check for nan values, if all values are nan then an exception is thrown if all ( rr != rr for rr in rr_list ): raise ValueError ( \"No user has a rating above the given threshold! Try lower it\" ) mrr = np . nanmean ( rr_list ) split_result [ 'user_id' ] . append ( 'sys' ) split_result [ str ( self )] . append ( mrr ) return pd . DataFrame ( split_result ) calc_reciprocal_rank ( user_predictions , user_truth_relevant_items , relevant_threshold ) Method which calculates the RR (Reciprocal Rank) for a single user Parameters: Name Type Description Default valid pd . DataFrame a DataFrame containing the recommendation list and the truth of a single user required Source code in clayrs/evaluation/metrics/ranking_metrics.py 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 def calc_reciprocal_rank ( self , user_predictions : List [ Interaction ], user_truth_relevant_items : Set [ Interaction ], relevant_threshold : float ): \"\"\" Method which calculates the RR (Reciprocal Rank) for a single user Args: valid (pd.DataFrame): a DataFrame containing the recommendation list and the truth of a single user \"\"\" reciprocal_rank = 0 i = 1 for interaction_pred in user_predictions : if interaction_pred . item_id in user_truth_relevant_items : reciprocal_rank = 1 / i break # We only need the first relevant item position in the rank i += 1 return reciprocal_rank MRRAtK Bases: MRR The MRR@K (Mean Reciprocal Rank at K) metric is a system wide metric, so only its result will be returned and not those of every user. MRR@K is calculated as such \\[ MRR@K_{sys} = \\frac{1}{|Q|}\\cdot\\sum_{i=1}^{K}\\frac{1}{rank(i)} \\] Where: \\(K\\) is the cutoff parameter \\(Q\\) is the set of recommendation lists \\(rank(i)\\) is the position of the first relevant item in the i-th recommendation list Parameters: Name Type Description Default k int the cutoff parameter. It must be >= 1, otherwise a ValueError exception is raised required relevant_threshold float parameter needed to discern relevant items and non-relevant items for every user. If not specified, the mean rating score of every user will be used None Raises: Type Description ValueError if an invalid cutoff parameter is passed (0 or negative) Source code in clayrs/evaluation/metrics/ranking_metrics.py 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 class MRRAtK ( MRR ): r \"\"\" The MRR@K (Mean Reciprocal Rank at K) metric is a system wide metric, so only its result will be returned and not those of every user. MRR@K is calculated as such $$ MRR@K_{sys} = \\frac{1}{|Q|}\\cdot\\sum_{i=1}^{K}\\frac{1}{rank(i)} $$ Where: - $K$ is the cutoff parameter - $Q$ is the set of recommendation lists - $rank(i)$ is the position of the first relevant item in the i-th recommendation list Args: k (int): the cutoff parameter. It must be >= 1, otherwise a ValueError exception is raised relevant_threshold (float): parameter needed to discern relevant items and non-relevant items for every user. If not specified, the mean rating score of every user will be used Raises: ValueError: if an invalid cutoff parameter is passed (0 or negative) \"\"\" def __init__ ( self , k : int , relevant_threshold : float = None ): if k < 1 : raise ValueError ( 'k= {} not valid! k must be >= 1!' . format ( k )) self . __k = k super () . __init__ ( relevant_threshold ) @property def k ( self ): return self . __k def __str__ ( self ): return \"MRR@ {} \" . format ( self . k ) def __repr__ ( self ): return f 'MRRAtK(relevant_threshold= { self . relevant_threshold } )' def calc_reciprocal_rank ( self , user_predictions : List [ Interaction ], user_truth_relevant_items : Set [ Interaction ], relevant_threshold : float ): \"\"\" Method which calculates the RR (Reciprocal Rank) for a single user Args: valid (pd.DataFrame): a DataFrame containing the recommendation list and the truth of a single user \"\"\" user_predictions_cut = user_predictions [: self . k ] return super () . calc_reciprocal_rank ( user_predictions_cut , user_truth_relevant_items , relevant_threshold ) calc_reciprocal_rank ( user_predictions , user_truth_relevant_items , relevant_threshold ) Method which calculates the RR (Reciprocal Rank) for a single user Parameters: Name Type Description Default valid pd . DataFrame a DataFrame containing the recommendation list and the truth of a single user required Source code in clayrs/evaluation/metrics/ranking_metrics.py 279 280 281 282 283 284 285 286 287 288 def calc_reciprocal_rank ( self , user_predictions : List [ Interaction ], user_truth_relevant_items : Set [ Interaction ], relevant_threshold : float ): \"\"\" Method which calculates the RR (Reciprocal Rank) for a single user Args: valid (pd.DataFrame): a DataFrame containing the recommendation list and the truth of a single user \"\"\" user_predictions_cut = user_predictions [: self . k ] return super () . calc_reciprocal_rank ( user_predictions_cut , user_truth_relevant_items , relevant_threshold ) NDCG Bases: RankingMetric The NDCG (Normalized Discounted Cumulative Gain) metric is calculated for the single user by using the sklearn implementation, so be sure to check its documentation for more. The NDCG of the entire system is calculated instead as such: \\[ NDCG_{sys} = \\frac{\\sum_{u} NDCG_u}{|U|} \\] Where: \\(NDCG_u\\) is the NDCG calculated for user :math: u \\(U\\) is the set of all users The system average excludes NaN values. Source code in clayrs/evaluation/metrics/ranking_metrics.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 class NDCG ( RankingMetric ): r \"\"\" The NDCG (Normalized Discounted Cumulative Gain) metric is calculated for the **single user** by using the sklearn implementation, so be sure to check its documentation for more. The NDCG of the **entire system** is calculated instead as such: $$ NDCG_{sys} = \\frac{\\sum_{u} NDCG_u}{|U|} $$ Where: - $NDCG_u$ is the NDCG calculated for user :math:`u` - $U$ is the set of all users The system average excludes NaN values. \"\"\" def __str__ ( self ): return \"NDCG\" def __repr__ ( self ): return \"NDCG()\" def _calc_ndcg ( self , ideal_rank : np . array , actual_rank : np . array ): \"\"\" Private method which calculates the NDCG for a single user using sklearn implementation \"\"\" return ndcg_score ( ideal_rank , actual_rank ) def _get_ideal_actual_rank ( self , user_predictions : List [ Interaction ], user_truth : List [ Interaction ]): \"\"\" Private method which calculates two lists, actual_rank list and ideal_rank list. actual_rank - given the ranking of the user, for every item 'i' in the ranking it extracts the rating that the user has effectively given to 'i' and adds it to the actual_rank list. If the item is not present in the truth, a 0 is added to the list. ideal_rank - it's the actual_rank list ordered from the highest rating to the lowest one. It represents the perfect ranking for the user Args: valid (pd.DataFrame): DataFrame which contains ranking for a user and its test set \"\"\" # important that predicted items is a list, we must maintain the order predicted_items = [ interaction . item_id for interaction in user_predictions ] item_score_truth = { interaction . item_id : interaction . score for interaction in user_truth } actual_rank = [ item_score_truth . get ( item_id ) if item_score_truth . get ( item_id ) is not None else 0 for item_id in predicted_items ] ideal_rank = sorted ( actual_rank , reverse = True ) return ideal_rank , actual_rank def perform ( self , split : Split ): pred = split . pred truth = split . truth split_result = { 'user_id' : [], str ( self ): []} for user in set ( truth . user_id_column ): user_predictions = pred . get_user_interactions ( user ) user_truth = truth . get_user_interactions ( user ) ideal , actual = self . _get_ideal_actual_rank ( user_predictions , user_truth ) if len ( ideal ) == 1 : user_ndcg = 1 else : ideal_rank = np . array ([ ideal ]) actual_rank = np . array ([ actual ]) user_ndcg = self . _calc_ndcg ( ideal_rank , actual_rank ) split_result [ 'user_id' ] . append ( user ) split_result [ str ( self )] . append ( user_ndcg ) split_result [ 'user_id' ] . append ( 'sys' ) split_result [ str ( self )] . append ( np . nanmean ( split_result [ str ( self )])) return pd . DataFrame ( split_result ) NDCGAtK Bases: NDCG The NDCG@K (Normalized Discounted Cumulative Gain at K) metric is calculated for the single user by using the sklearn implementation, so be sure to check its documentation for more. The NDCG@K of the entire system is calculated instead as such: \\[ NDCG@K_{sys} = \\frac{\\sum_{u} NDCG@K_u}{|U|} \\] Where: \\(NDCG@K_u\\) is the NDCG@K calculated for user \\(u\\) \\(U\\) is the set of all users The system average excludes NaN values. Parameters: Name Type Description Default k int the cutoff parameter required Source code in clayrs/evaluation/metrics/ranking_metrics.py 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 class NDCGAtK ( NDCG ): r \"\"\" The NDCG@K (Normalized Discounted Cumulative Gain at K) metric is calculated for the **single user** by using the sklearn implementation, so be sure to check its documentation for more. The NDCG@K of the **entire system** is calculated instead as such: $$ NDCG@K_{sys} = \\frac{\\sum_{u} NDCG@K_u}{|U|} $$ Where: - $NDCG@K_u$ is the NDCG@K calculated for user $u$ - $U$ is the set of all users The system average excludes NaN values. Args: k (int): the cutoff parameter \"\"\" def __init__ ( self , k : int ): self . __k = k def __str__ ( self ): return \"NDCG@ {} \" . format ( self . __k ) def __repr__ ( self ): return f 'NDCGAtK(k= { self . __k } )' def _calc_ndcg ( self , ideal_rank : np . array , actual_rank : np . array ): return ndcg_score ( ideal_rank , actual_rank , k = self . __k )","title":"Ranking metrics"},{"location":"evaluation/metrics/ranking_metrics/#ranking-metrics","text":"","title":"Ranking metrics"},{"location":"evaluation/metrics/ranking_metrics/#clayrs.evaluation.metrics.ranking_metrics.Correlation","text":"Bases: RankingMetric The Correlation metric calculates the correlation between the ranking of a user and its ideal ranking. The currently correlation methods implemented are: pearson kendall spearman Every correlation method is implemented by the scipy library, so read its documentation for more The correlation metric is calculated as such for the single user : \\[ Corr_u = Corr(ranking_u, ideal\\_ranking_u) \\] Where: \\(ranking_u\\) is ranking of the user \\(ideal\\_ranking_u\\) is the ideal ranking for the user The ideal ranking is calculated based on the rating inside the ground truth of the user The Correlation metric calculated for the entire system is simply the average of every \\(Corr\\) : \\[ Corr_{sys} = \\frac{\\sum_{u} Corr_u}{|U|} \\] Where: \\(Corr_u\\) is the correlation of the user \\(u\\) \\(U\\) is the set of all users The system average excludes NaN values. It's also possible to specify a cutoff parameter thanks to the 'top_n' parameter: if specified, only the first \\(n\\) results of the recommendation list will be used in order to calculate the correlation Parameters: Name Type Description Default method str The correlation method to use. It must be 'pearson', 'kendall' or 'spearman', otherwise a ValueError exception is raised. By default is 'pearson' 'pearson' top_n int Cutoff parameter, if specified only the first n items of the recommendation list will be used in order to calculate the correlation None Raises: Type Description ValueError if an invalid method parameter is passed Source code in clayrs/evaluation/metrics/ranking_metrics.py 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 class Correlation ( RankingMetric ): r \"\"\" The Correlation metric calculates the correlation between the ranking of a user and its ideal ranking. The currently correlation methods implemented are: - pearson - kendall - spearman Every correlation method is implemented by the scipy library, so read its documentation for more The correlation metric is calculated as such for the **single user**: $$ Corr_u = Corr(ranking_u, ideal\\_ranking_u) $$ Where: - $ranking_u$ is ranking of the user - $ideal\\_ranking_u$ is the ideal ranking for the user The ideal ranking is calculated based on the rating inside the *ground truth* of the user The Correlation metric calculated for the **entire system** is simply the average of every $Corr$: $$ Corr_{sys} = \\frac{\\sum_{u} Corr_u}{|U|} $$ Where: - $Corr_u$ is the correlation of the user $u$ - $U$ is the set of all users The system average excludes NaN values. It's also possible to specify a cutoff parameter thanks to the 'top_n' parameter: if specified, only the first $n$ results of the recommendation list will be used in order to calculate the correlation Args: method (str): The correlation method to use. It must be 'pearson', 'kendall' or 'spearman', otherwise a ValueError exception is raised. By default is 'pearson' top_n (int): Cutoff parameter, if specified only the first n items of the recommendation list will be used in order to calculate the correlation Raises: ValueError: if an invalid method parameter is passed \"\"\" def __init__ ( self , method : str = 'pearson' , top_n : int = None ): valid = { 'pearson' , 'kendall' , 'spearman' } self . __method = method . lower () if self . __method not in valid : raise ValueError ( \"Method {} is not supported! Methods available: \\n \" \" {} \" . format ( method , valid )) self . __top_n = top_n def __str__ ( self ): name = self . __method if self . __top_n is not None : name += \" - Top {} \" . format ( self . __top_n ) return name def __repr__ ( self ): return f 'Correlation(method= { self . __method } , top_n= { self . __top_n } )' def perform ( self , split : Split ) -> pd . DataFrame : pred = split . pred truth = split . truth split_result = { 'user_id' : [], str ( self ): []} for user in set ( truth . user_id_column ): user_predictions = pred . get_user_interactions ( user ) user_truth = truth . get_user_interactions ( user ) ideal , actual = self . _get_ideal_actual_rank ( user_predictions , user_truth ) if len ( actual ) < 2 : coef = np . nan else : ideal_ranking = pd . Series ( ideal ) actual_ranking = pd . Series ( actual ) coef = actual_ranking . corr ( ideal_ranking , method = self . __method ) split_result [ 'user_id' ] . append ( user ) split_result [ str ( self )] . append ( coef ) split_result [ 'user_id' ] . append ( 'sys' ) split_result [ str ( self )] . append ( np . nanmean ( split_result [ str ( self )])) return pd . DataFrame ( split_result ) def _get_ideal_actual_rank ( self , user_predictions : List [ Interaction ], user_truth : List [ Interaction ]): if self . __top_n is not None : user_predictions = user_predictions [: self . __top_n ] # sorting truth on score values user_truth_ordered = sorted ( user_truth , key = lambda interaction : interaction . score , reverse = True ) ideal_rank = [ interaction . item_id for interaction in user_truth_ordered ] predicted_items = [ interaction . item_id for interaction in user_predictions ] actual_rank = [ predicted_items . index ( item ) for item in ideal_rank if item in set ( predicted_items )] # the ideal rank is basically 0, 1, 2, 3 etc. return [ i for i in range ( len ( ideal_rank ))], actual_rank","title":"Correlation"},{"location":"evaluation/metrics/ranking_metrics/#clayrs.evaluation.metrics.ranking_metrics.MRR","text":"Bases: RankingMetric The MRR (Mean Reciprocal Rank) metric is a system wide metric, so only its result it will be returned and not those of every user. MRR is calculated as such \\[ MRR_{sys} = \\frac{1}{|Q|}\\cdot\\sum_{i=1}^{|Q|}\\frac{1}{rank(i)} \\] Where: \\(Q\\) is the set of recommendation lists \\(rank(i)\\) is the position of the first relevant item in the i-th recommendation list The MRR metric needs to discern relevant items from the not relevant ones: in order to do that, one could pass a custom relevant_threshold parameter that will be applied to every user, so that if a rating of an item is >= relevant_threshold, then it's relevant, otherwise it's not. If no relevant_threshold parameter is passed then, for every user, its mean rating score will be used Parameters: Name Type Description Default relevant_threshold float parameter needed to discern relevant items and non-relevant items for every user. If not specified, the mean rating score of every user will be used None Source code in clayrs/evaluation/metrics/ranking_metrics.py 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 class MRR ( RankingMetric ): r \"\"\" The MRR (Mean Reciprocal Rank) metric is a system wide metric, so only its result it will be returned and not those of every user. MRR is calculated as such $$ MRR_{sys} = \\frac{1}{|Q|}\\cdot\\sum_{i=1}^{|Q|}\\frac{1}{rank(i)} $$ Where: - $Q$ is the set of recommendation lists - $rank(i)$ is the position of the first relevant item in the i-th recommendation list The MRR metric needs to discern relevant items from the not relevant ones: in order to do that, one could pass a custom relevant_threshold parameter that will be applied to every user, so that if a rating of an item is >= relevant_threshold, then it's relevant, otherwise it's not. If no relevant_threshold parameter is passed then, for every user, its mean rating score will be used Args: relevant_threshold (float): parameter needed to discern relevant items and non-relevant items for every user. If not specified, the mean rating score of every user will be used \"\"\" def __init__ ( self , relevant_threshold : float = None ): self . __relevant_threshold = relevant_threshold @property def relevant_threshold ( self ): return self . __relevant_threshold def __str__ ( self ): return \"MRR\" def __repr__ ( self ): return f 'MRR(relevant_threshold= { self . relevant_threshold } )' def calc_reciprocal_rank ( self , user_predictions : List [ Interaction ], user_truth_relevant_items : Set [ Interaction ], relevant_threshold : float ): \"\"\" Method which calculates the RR (Reciprocal Rank) for a single user Args: valid (pd.DataFrame): a DataFrame containing the recommendation list and the truth of a single user \"\"\" reciprocal_rank = 0 i = 1 for interaction_pred in user_predictions : if interaction_pred . item_id in user_truth_relevant_items : reciprocal_rank = 1 / i break # We only need the first relevant item position in the rank i += 1 return reciprocal_rank def perform ( self , split : Split ) -> pd . DataFrame : pred = split . pred truth = split . truth split_result = { 'user_id' : [], str ( self ): []} rr_list = [] for user in set ( truth . user_id_column ): user_predictions = pred . get_user_interactions ( user ) user_truth = truth . get_user_interactions ( user ) relevant_threshold = self . relevant_threshold if relevant_threshold is None : relevant_threshold = np . nanmean ([ interaction . score for interaction in user_truth ]) user_truth_relevant_items = set ( interaction . item_id for interaction in user_truth if interaction . score >= relevant_threshold ) if len ( user_truth_relevant_items ) != 0 : user_reciprocal_rank = self . calc_reciprocal_rank ( user_predictions , user_truth_relevant_items , relevant_threshold ) else : user_reciprocal_rank = np . nan rr_list . append ( user_reciprocal_rank ) # trick to check for nan values, if all values are nan then an exception is thrown if all ( rr != rr for rr in rr_list ): raise ValueError ( \"No user has a rating above the given threshold! Try lower it\" ) mrr = np . nanmean ( rr_list ) split_result [ 'user_id' ] . append ( 'sys' ) split_result [ str ( self )] . append ( mrr ) return pd . DataFrame ( split_result )","title":"MRR"},{"location":"evaluation/metrics/ranking_metrics/#clayrs.evaluation.metrics.ranking_metrics.MRR.calc_reciprocal_rank","text":"Method which calculates the RR (Reciprocal Rank) for a single user Parameters: Name Type Description Default valid pd . DataFrame a DataFrame containing the recommendation list and the truth of a single user required Source code in clayrs/evaluation/metrics/ranking_metrics.py 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 def calc_reciprocal_rank ( self , user_predictions : List [ Interaction ], user_truth_relevant_items : Set [ Interaction ], relevant_threshold : float ): \"\"\" Method which calculates the RR (Reciprocal Rank) for a single user Args: valid (pd.DataFrame): a DataFrame containing the recommendation list and the truth of a single user \"\"\" reciprocal_rank = 0 i = 1 for interaction_pred in user_predictions : if interaction_pred . item_id in user_truth_relevant_items : reciprocal_rank = 1 / i break # We only need the first relevant item position in the rank i += 1 return reciprocal_rank","title":"calc_reciprocal_rank()"},{"location":"evaluation/metrics/ranking_metrics/#clayrs.evaluation.metrics.ranking_metrics.MRRAtK","text":"Bases: MRR The MRR@K (Mean Reciprocal Rank at K) metric is a system wide metric, so only its result will be returned and not those of every user. MRR@K is calculated as such \\[ MRR@K_{sys} = \\frac{1}{|Q|}\\cdot\\sum_{i=1}^{K}\\frac{1}{rank(i)} \\] Where: \\(K\\) is the cutoff parameter \\(Q\\) is the set of recommendation lists \\(rank(i)\\) is the position of the first relevant item in the i-th recommendation list Parameters: Name Type Description Default k int the cutoff parameter. It must be >= 1, otherwise a ValueError exception is raised required relevant_threshold float parameter needed to discern relevant items and non-relevant items for every user. If not specified, the mean rating score of every user will be used None Raises: Type Description ValueError if an invalid cutoff parameter is passed (0 or negative) Source code in clayrs/evaluation/metrics/ranking_metrics.py 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 class MRRAtK ( MRR ): r \"\"\" The MRR@K (Mean Reciprocal Rank at K) metric is a system wide metric, so only its result will be returned and not those of every user. MRR@K is calculated as such $$ MRR@K_{sys} = \\frac{1}{|Q|}\\cdot\\sum_{i=1}^{K}\\frac{1}{rank(i)} $$ Where: - $K$ is the cutoff parameter - $Q$ is the set of recommendation lists - $rank(i)$ is the position of the first relevant item in the i-th recommendation list Args: k (int): the cutoff parameter. It must be >= 1, otherwise a ValueError exception is raised relevant_threshold (float): parameter needed to discern relevant items and non-relevant items for every user. If not specified, the mean rating score of every user will be used Raises: ValueError: if an invalid cutoff parameter is passed (0 or negative) \"\"\" def __init__ ( self , k : int , relevant_threshold : float = None ): if k < 1 : raise ValueError ( 'k= {} not valid! k must be >= 1!' . format ( k )) self . __k = k super () . __init__ ( relevant_threshold ) @property def k ( self ): return self . __k def __str__ ( self ): return \"MRR@ {} \" . format ( self . k ) def __repr__ ( self ): return f 'MRRAtK(relevant_threshold= { self . relevant_threshold } )' def calc_reciprocal_rank ( self , user_predictions : List [ Interaction ], user_truth_relevant_items : Set [ Interaction ], relevant_threshold : float ): \"\"\" Method which calculates the RR (Reciprocal Rank) for a single user Args: valid (pd.DataFrame): a DataFrame containing the recommendation list and the truth of a single user \"\"\" user_predictions_cut = user_predictions [: self . k ] return super () . calc_reciprocal_rank ( user_predictions_cut , user_truth_relevant_items , relevant_threshold )","title":"MRRAtK"},{"location":"evaluation/metrics/ranking_metrics/#clayrs.evaluation.metrics.ranking_metrics.MRRAtK.calc_reciprocal_rank","text":"Method which calculates the RR (Reciprocal Rank) for a single user Parameters: Name Type Description Default valid pd . DataFrame a DataFrame containing the recommendation list and the truth of a single user required Source code in clayrs/evaluation/metrics/ranking_metrics.py 279 280 281 282 283 284 285 286 287 288 def calc_reciprocal_rank ( self , user_predictions : List [ Interaction ], user_truth_relevant_items : Set [ Interaction ], relevant_threshold : float ): \"\"\" Method which calculates the RR (Reciprocal Rank) for a single user Args: valid (pd.DataFrame): a DataFrame containing the recommendation list and the truth of a single user \"\"\" user_predictions_cut = user_predictions [: self . k ] return super () . calc_reciprocal_rank ( user_predictions_cut , user_truth_relevant_items , relevant_threshold )","title":"calc_reciprocal_rank()"},{"location":"evaluation/metrics/ranking_metrics/#clayrs.evaluation.metrics.ranking_metrics.NDCG","text":"Bases: RankingMetric The NDCG (Normalized Discounted Cumulative Gain) metric is calculated for the single user by using the sklearn implementation, so be sure to check its documentation for more. The NDCG of the entire system is calculated instead as such: \\[ NDCG_{sys} = \\frac{\\sum_{u} NDCG_u}{|U|} \\] Where: \\(NDCG_u\\) is the NDCG calculated for user :math: u \\(U\\) is the set of all users The system average excludes NaN values. Source code in clayrs/evaluation/metrics/ranking_metrics.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 class NDCG ( RankingMetric ): r \"\"\" The NDCG (Normalized Discounted Cumulative Gain) metric is calculated for the **single user** by using the sklearn implementation, so be sure to check its documentation for more. The NDCG of the **entire system** is calculated instead as such: $$ NDCG_{sys} = \\frac{\\sum_{u} NDCG_u}{|U|} $$ Where: - $NDCG_u$ is the NDCG calculated for user :math:`u` - $U$ is the set of all users The system average excludes NaN values. \"\"\" def __str__ ( self ): return \"NDCG\" def __repr__ ( self ): return \"NDCG()\" def _calc_ndcg ( self , ideal_rank : np . array , actual_rank : np . array ): \"\"\" Private method which calculates the NDCG for a single user using sklearn implementation \"\"\" return ndcg_score ( ideal_rank , actual_rank ) def _get_ideal_actual_rank ( self , user_predictions : List [ Interaction ], user_truth : List [ Interaction ]): \"\"\" Private method which calculates two lists, actual_rank list and ideal_rank list. actual_rank - given the ranking of the user, for every item 'i' in the ranking it extracts the rating that the user has effectively given to 'i' and adds it to the actual_rank list. If the item is not present in the truth, a 0 is added to the list. ideal_rank - it's the actual_rank list ordered from the highest rating to the lowest one. It represents the perfect ranking for the user Args: valid (pd.DataFrame): DataFrame which contains ranking for a user and its test set \"\"\" # important that predicted items is a list, we must maintain the order predicted_items = [ interaction . item_id for interaction in user_predictions ] item_score_truth = { interaction . item_id : interaction . score for interaction in user_truth } actual_rank = [ item_score_truth . get ( item_id ) if item_score_truth . get ( item_id ) is not None else 0 for item_id in predicted_items ] ideal_rank = sorted ( actual_rank , reverse = True ) return ideal_rank , actual_rank def perform ( self , split : Split ): pred = split . pred truth = split . truth split_result = { 'user_id' : [], str ( self ): []} for user in set ( truth . user_id_column ): user_predictions = pred . get_user_interactions ( user ) user_truth = truth . get_user_interactions ( user ) ideal , actual = self . _get_ideal_actual_rank ( user_predictions , user_truth ) if len ( ideal ) == 1 : user_ndcg = 1 else : ideal_rank = np . array ([ ideal ]) actual_rank = np . array ([ actual ]) user_ndcg = self . _calc_ndcg ( ideal_rank , actual_rank ) split_result [ 'user_id' ] . append ( user ) split_result [ str ( self )] . append ( user_ndcg ) split_result [ 'user_id' ] . append ( 'sys' ) split_result [ str ( self )] . append ( np . nanmean ( split_result [ str ( self )])) return pd . DataFrame ( split_result )","title":"NDCG"},{"location":"evaluation/metrics/ranking_metrics/#clayrs.evaluation.metrics.ranking_metrics.NDCGAtK","text":"Bases: NDCG The NDCG@K (Normalized Discounted Cumulative Gain at K) metric is calculated for the single user by using the sklearn implementation, so be sure to check its documentation for more. The NDCG@K of the entire system is calculated instead as such: \\[ NDCG@K_{sys} = \\frac{\\sum_{u} NDCG@K_u}{|U|} \\] Where: \\(NDCG@K_u\\) is the NDCG@K calculated for user \\(u\\) \\(U\\) is the set of all users The system average excludes NaN values. Parameters: Name Type Description Default k int the cutoff parameter required Source code in clayrs/evaluation/metrics/ranking_metrics.py 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 class NDCGAtK ( NDCG ): r \"\"\" The NDCG@K (Normalized Discounted Cumulative Gain at K) metric is calculated for the **single user** by using the sklearn implementation, so be sure to check its documentation for more. The NDCG@K of the **entire system** is calculated instead as such: $$ NDCG@K_{sys} = \\frac{\\sum_{u} NDCG@K_u}{|U|} $$ Where: - $NDCG@K_u$ is the NDCG@K calculated for user $u$ - $U$ is the set of all users The system average excludes NaN values. Args: k (int): the cutoff parameter \"\"\" def __init__ ( self , k : int ): self . __k = k def __str__ ( self ): return \"NDCG@ {} \" . format ( self . __k ) def __repr__ ( self ): return f 'NDCGAtK(k= { self . __k } )' def _calc_ndcg ( self , ideal_rank : np . array , actual_rank : np . array ): return ndcg_score ( ideal_rank , actual_rank , k = self . __k )","title":"NDCGAtK"},{"location":"first_steps/installation/","text":"Installation Via PIP recommended ClayRS requires Python 3.7 or later, while package dependencies are in requirements.txt and are all installable via pip , as ClayRS itself. To install it execute the following command: Latest pip install git+https://github.com/SwapUniba/clayrs.git This will automatically install compatible versions of all dependencies. Tip : We suggest to install ClayRS (or any python package, for that matters) in a virtual environment Virtual environments are special isolated environments where all the packages and versions you install only apply to that specific environment. It\u2019s like a private island! \u2014 but for code. Read this Medium article for understanding all the advantages and the official python guide on how to setup one","title":"Installation"},{"location":"first_steps/installation/#installation","text":"","title":"Installation"},{"location":"first_steps/installation/#via-pip-recommended","text":"ClayRS requires Python 3.7 or later, while package dependencies are in requirements.txt and are all installable via pip , as ClayRS itself. To install it execute the following command: Latest pip install git+https://github.com/SwapUniba/clayrs.git This will automatically install compatible versions of all dependencies. Tip : We suggest to install ClayRS (or any python package, for that matters) in a virtual environment Virtual environments are special isolated environments where all the packages and versions you install only apply to that specific environment. It\u2019s like a private island! \u2014 but for code. Read this Medium article for understanding all the advantages and the official python guide on how to setup one","title":"Via PIP"},{"location":"first_steps/quickstart/","text":"Quickstart Content Analyzer The first thing to do is to import the Content Analyzer module * We will access its methods and classes via dot notation import clayrs.content_analyzer as ca First let's point to the source containing raw information to process raw_source = ca . JSONFile ( 'items_info.json' ) Then let's start building the configuration for the items Info Note that same operations that can be specified for items could be also specified for users via the ca . UserAnalyzerConfig class # Configuration of item representation movies_ca_config = ca . ItemAnalyzerConfig ( source = raw_source , id = 'movielens_id' , # (1) output_directory = 'movies_codified/' # (2) ) The id in the raw source which uniquely identifies each item Directory which will contain items complexly represented Let's represent the plot field of each content with a TfIdf representation * * Since the preprocessing parameter has been specified, then each field is first preprocessed with the specified operations movies_ca_config . add_single_config ( 'plot' , ca . FieldConfig ( ca . SkLearnTfIdf (), preprocessing = ca . NLTK ( stopwords_removal = True , lemmatization = True ), id = 'tfidf' ) # (1) ) User defined id for the representation To finalize the Content Analyzer part, let's instantiate the ContentAnalyzer class by passing the built configuration and by calling its fit() method ca . ContentAnalyzer ( movies_ca_config ) . fit () The items will be created with the specified representations and serialized RecSys Similarly above, we must first import the RecSys module import clayrs.recsys as rs Then we load the rating frame from a TSV file Info In this case in our file the first three columns are user_id, item_id, score in this order If your file has a different structure you must specify how to map the column via parameters, check documentation for more ratings = ca . Ratings ( ca . CSVFile ( 'ratings.tsv' , separator = ' \\t ' )) Let's split with the KFold technique the loaded rating frame into train set and test set since n_splits=2 , train_list will contain two train_sets and test_list will contain two test_sets train_list , test_list = rs . KFoldPartitioning ( n_splits = 2 ) . split_all ( ratings ) In order to recommend items to users, we must choose an algorithm to use In this case we are using the CentroidVector algorithm which will work by using the first representation specified for the plot field You can freely choose which representation to use among all representation codified for the fields in the Content Analyzer phase centroid_vec = rs . CentroidVector ( { 'plot' : 'tfidf' }, # (1) similarity = rs . CosineSimilarity () ) We can reference the representation specified for the 'plot' field with the assigned custom id in the Content Analyzer phase Let's now compute the top-10 ranking for each user of the train set By default the candidate items are those in the test set of the user, but you can change this behaviour with the methodology parameter Since we used the kfold technique, we iterate over all train sets and test sets result_list = [] for train_set , test_set in zip ( train_list , test_list ): cbrs = rs . ContentBasedRS ( centroid_vec , train_set , 'movies_codified/' ) rank = cbrs . fit_rank ( test_set , n_recs = 10 ) result_list . append ( rank ) result_list will contain two Rank objects in this case, one for each split Evaluation module Similarly to the Content Analyzer and RecSys module, we must first import the evaluation module import clayrs.evaluation as eva The class responsible for evaluating recommendation lists is the EvalModel class. It needs the following parameters: A list of computed rank/predictions (in case multiple splits must be evaluated) A list of truths (in case multiple splits must be evaluated) List of metrics to compute Obviously the list of computed rank/predictions and list of truths must have the same length, and the rank/prediction in position \\(i\\) will be compared with the truth at position \\(i\\) em = eva . EvalModel ( pred_list = result_list , truth_list = test_list , metric_list = [ eva . NDCG (), eva . Precision (), eva . RecallAtK ( k = 5 ) ] ) Then simply call the fit () method of the instantiated object It will return two pandas DataFrame: the first one contains the metrics aggregated for the system, while the second contains the metrics computed for each user (where possible) sys_result , users_result = em . fit () Note Note that the EvalModel is able to compute evaluation of recommendations generated by other tools/frameworks, check documentation for more","title":"Quickstart"},{"location":"first_steps/quickstart/#quickstart","text":"","title":"Quickstart"},{"location":"first_steps/quickstart/#content-analyzer","text":"The first thing to do is to import the Content Analyzer module * We will access its methods and classes via dot notation import clayrs.content_analyzer as ca First let's point to the source containing raw information to process raw_source = ca . JSONFile ( 'items_info.json' ) Then let's start building the configuration for the items Info Note that same operations that can be specified for items could be also specified for users via the ca . UserAnalyzerConfig class # Configuration of item representation movies_ca_config = ca . ItemAnalyzerConfig ( source = raw_source , id = 'movielens_id' , # (1) output_directory = 'movies_codified/' # (2) ) The id in the raw source which uniquely identifies each item Directory which will contain items complexly represented Let's represent the plot field of each content with a TfIdf representation * * Since the preprocessing parameter has been specified, then each field is first preprocessed with the specified operations movies_ca_config . add_single_config ( 'plot' , ca . FieldConfig ( ca . SkLearnTfIdf (), preprocessing = ca . NLTK ( stopwords_removal = True , lemmatization = True ), id = 'tfidf' ) # (1) ) User defined id for the representation To finalize the Content Analyzer part, let's instantiate the ContentAnalyzer class by passing the built configuration and by calling its fit() method ca . ContentAnalyzer ( movies_ca_config ) . fit () The items will be created with the specified representations and serialized","title":"Content Analyzer"},{"location":"first_steps/quickstart/#recsys","text":"Similarly above, we must first import the RecSys module import clayrs.recsys as rs Then we load the rating frame from a TSV file Info In this case in our file the first three columns are user_id, item_id, score in this order If your file has a different structure you must specify how to map the column via parameters, check documentation for more ratings = ca . Ratings ( ca . CSVFile ( 'ratings.tsv' , separator = ' \\t ' )) Let's split with the KFold technique the loaded rating frame into train set and test set since n_splits=2 , train_list will contain two train_sets and test_list will contain two test_sets train_list , test_list = rs . KFoldPartitioning ( n_splits = 2 ) . split_all ( ratings ) In order to recommend items to users, we must choose an algorithm to use In this case we are using the CentroidVector algorithm which will work by using the first representation specified for the plot field You can freely choose which representation to use among all representation codified for the fields in the Content Analyzer phase centroid_vec = rs . CentroidVector ( { 'plot' : 'tfidf' }, # (1) similarity = rs . CosineSimilarity () ) We can reference the representation specified for the 'plot' field with the assigned custom id in the Content Analyzer phase Let's now compute the top-10 ranking for each user of the train set By default the candidate items are those in the test set of the user, but you can change this behaviour with the methodology parameter Since we used the kfold technique, we iterate over all train sets and test sets result_list = [] for train_set , test_set in zip ( train_list , test_list ): cbrs = rs . ContentBasedRS ( centroid_vec , train_set , 'movies_codified/' ) rank = cbrs . fit_rank ( test_set , n_recs = 10 ) result_list . append ( rank ) result_list will contain two Rank objects in this case, one for each split","title":"RecSys"},{"location":"first_steps/quickstart/#evaluation-module","text":"Similarly to the Content Analyzer and RecSys module, we must first import the evaluation module import clayrs.evaluation as eva The class responsible for evaluating recommendation lists is the EvalModel class. It needs the following parameters: A list of computed rank/predictions (in case multiple splits must be evaluated) A list of truths (in case multiple splits must be evaluated) List of metrics to compute Obviously the list of computed rank/predictions and list of truths must have the same length, and the rank/prediction in position \\(i\\) will be compared with the truth at position \\(i\\) em = eva . EvalModel ( pred_list = result_list , truth_list = test_list , metric_list = [ eva . NDCG (), eva . Precision (), eva . RecallAtK ( k = 5 ) ] ) Then simply call the fit () method of the instantiated object It will return two pandas DataFrame: the first one contains the metrics aggregated for the system, while the second contains the metrics computed for each user (where possible) sys_result , users_result = em . fit () Note Note that the EvalModel is able to compute evaluation of recommendations generated by other tools/frameworks, check documentation for more","title":"Evaluation module"},{"location":"recsys/introduction/","text":"Introduction ClayRS is a python framework for (mainly) content-based recommender systems which allows you to perform several operations, starting from a raw representation of users and items to building and evaluating a recommender system. It also supports graph-based recommendation with feature selection algorithms and graph manipulation methods. The framework has three main modules, which you can also use individually: Given a raw source, the Content Analyzer : * Creates and serializes contents, * Using the chosen configuration The RecSys module allows to: * Instantiate a recommender system * Using items and users serialized by the Content Analyzer * Make score prediction or recommend items for the active user(s) The EvalModel has the task of evaluating a recommender system, using several state-of-the-art metrics Code examples for all three modules will follow in the Usage section","title":"Introduction"},{"location":"recsys/introduction/#introduction","text":"ClayRS is a python framework for (mainly) content-based recommender systems which allows you to perform several operations, starting from a raw representation of users and items to building and evaluating a recommender system. It also supports graph-based recommendation with feature selection algorithms and graph manipulation methods. The framework has three main modules, which you can also use individually: Given a raw source, the Content Analyzer : * Creates and serializes contents, * Using the chosen configuration The RecSys module allows to: * Instantiate a recommender system * Using items and users serialized by the Content Analyzer * Make score prediction or recommend items for the active user(s) The EvalModel has the task of evaluating a recommender system, using several state-of-the-art metrics Code examples for all three modules will follow in the Usage section","title":"Introduction"},{"location":"recsys/recsys/","text":"Recsys ContentBasedRS Bases: RecSys Class for recommender systems which use the items' content in order to make predictions, some algorithms may also use users' content Every CBRS differ from each other based the algorithm chosen Parameters: Name Type Description Default algorithm ContentBasedAlgorithm the content based algorithm that will be used in order to rank or make score prediction required train_set pd . DataFrame a DataFrame containing interactions between users and items required items_directory str the path of the items serialized by the Content Analyzer required users_directory str the path of the users serialized by the Content Analyzer None Source code in clayrs/recsys/recsys.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 class ContentBasedRS ( RecSys ): \"\"\" Class for recommender systems which use the items' content in order to make predictions, some algorithms may also use users' content Every CBRS differ from each other based the algorithm chosen Args: algorithm (ContentBasedAlgorithm): the content based algorithm that will be used in order to rank or make score prediction train_set (pd.DataFrame): a DataFrame containing interactions between users and items items_directory (str): the path of the items serialized by the Content Analyzer users_directory (str): the path of the users serialized by the Content Analyzer \"\"\" def __init__ ( self , algorithm : ContentBasedAlgorithm , train_set : Ratings , items_directory : str , users_directory : str = None ): super () . __init__ ( algorithm ) self . __train_set = train_set self . __items_directory = items_directory self . __users_directory = users_directory self . _user_fit_dic = {} @property def algorithm ( self ): \"\"\" The content based algorithm chosen \"\"\" alg : ContentBasedAlgorithm = super () . algorithm return alg @property def train_set ( self ): return self . __train_set @property def items_directory ( self ): \"\"\" Path of the serialized items \"\"\" return self . __items_directory @property def users_directory ( self ): \"\"\" Path of the serialized users \"\"\" return self . __users_directory def fit ( self ): \"\"\" Method that divides the train set into as many parts as there are different users. then it proceeds with the fit for each user and saves the result in the dictionary \"user_fit_dic\" \"\"\" items_to_load = set ( self . train_set . item_id_column ) loaded_items_interface = self . algorithm . _load_available_contents ( self . items_directory , items_to_load ) with get_progbar ( set ( self . train_set . user_id_column )) as pbar : pbar . set_description ( \"Fitting algorithm\" ) for user_id in pbar : user_train = self . train_set . get_user_interactions ( user_id ) try : user_alg = deepcopy ( self . algorithm ) user_alg . process_rated ( user_train , loaded_items_interface ) user_alg . fit () self . _user_fit_dic [ user_id ] = user_alg except UserSkipAlgFit as e : warning_message = str ( e ) + f \" \\n No algorithm will be fitted for the user { user_id } \" logger . warning ( warning_message ) self . _user_fit_dic [ user_id ] = None # we force the garbage collector after freeing loaded items del loaded_items_interface gc . collect () return self def rank ( self , test_set : Ratings , n_recs : int = None , user_id_list : List = None , methodology : Union [ Methodology , None ] = TestRatingsMethodology ()) -> Rank : \"\"\" Method used to calculate ranking for the user in test set If the recs_number is specified, then the rank will contain the top-n items for the users. Otherwise the rank will contain all unrated items of the particular users if the items evaluated are present for each user, the filter list is calculated, and score prediction is executed only for the items inside the filter list. Otherwise, score prediction is executed for all unrated items of the particular user Args: test_set: set of users for which to calculate the rank n_recs: number of the top items that will be present in the ranking Returns: concat_rank: list of the items ranked for each user \"\"\" if len ( self . _user_fit_dic ) == 0 : raise NotFittedAlg ( \"Algorithm not fit! You must call the fit() method first, or fit_rank().\" ) all_users = set ( test_set . user_id_column ) if user_id_list is not None : all_users = set ( user_id_list ) loaded_items_interface = self . algorithm . _load_available_contents ( self . items_directory , set ()) rank = [] logger . info ( \"Don't worry if it looks stuck at first\" ) logger . info ( \"First iterations will stabilize the estimated remaining time\" ) with get_progbar ( all_users ) as pbar : pbar . set_description ( f \"Loading first items from memory...\" ) for user_id in pbar : user_id = str ( user_id ) user_train = self . train_set . get_user_interactions ( user_id ) filter_list = None if methodology is not None : filter_list = set ( methodology . filter_single ( user_id , self . train_set , test_set )) user_fitted_alg = self . _user_fit_dic . get ( user_id ) if user_fitted_alg is not None : user_rank = user_fitted_alg . rank ( user_train , loaded_items_interface , n_recs , filter_list = filter_list ) else : user_rank = [] logger . warning ( f \"No algorithm fitted for user { user_id } ! It will be skipped\" ) rank . extend ( user_rank ) pbar . set_description ( f \"Computing rank for { user_id } \" ) rank = Rank . from_list ( rank ) # we force the garbage collector after freeing loaded items del loaded_items_interface gc . collect () self . _yaml_report = { 'mode' : 'rank' , 'n_recs' : repr ( n_recs ), 'methodology' : repr ( methodology )} return rank def predict ( self , test_set : Ratings , user_id_list : List = None , methodology : Union [ Methodology , None ] = TestRatingsMethodology ()) -> Prediction : \"\"\" Method to call when score prediction must be done for the users in test set If the items evaluated are present for each user, the filter list is calculated, and score prediction is executed only for the items inside the filter list. Otherwise, score prediction is executed for all unrated items of the particular user Args: test_set: set of users for which to calculate the predictions Returns: concat_score_preds: prediction for each user \"\"\" if len ( self . _user_fit_dic ) == 0 : raise NotFittedAlg ( \"Algorithm not fit! You must call the fit() method first, or fit_rank().\" ) all_users = set ( test_set . user_id_column ) if user_id_list is not None : all_users = set ( user_id_list ) loaded_items_interface = self . algorithm . _load_available_contents ( self . items_directory , set ()) pred = [] logger . info ( \"Don't worry if it looks stuck at first\" ) logger . info ( \"First iterations will stabilize the estimated remaining time\" ) with get_progbar ( all_users ) as pbar : pbar . set_description ( f \"Loading first items from memory...\" ) for user_id in pbar : user_id = str ( user_id ) user_train = self . train_set . get_user_interactions ( user_id ) filter_list = None if methodology is not None : filter_list = set ( methodology . filter_single ( user_id , self . train_set , test_set )) user_fitted_alg = self . _user_fit_dic . get ( user_id ) if user_fitted_alg is not None : user_pred = user_fitted_alg . predict ( user_train , loaded_items_interface , filter_list = filter_list ) else : user_pred = [] logger . warning ( f \"No algorithm fitted for user { user_id } ! It will be skipped\" ) pred . extend ( user_pred ) pbar . set_description ( f \"Computing predictions for user { user_id } \" ) pred = Prediction . from_list ( pred ) # we force the garbage collector after freeing loaded items del loaded_items_interface gc . collect () self . _yaml_report = { 'mode' : 'score_prediction' , 'methodology' : repr ( methodology )} return pred def fit_predict ( self , test_set : Ratings , user_id_list : List = None , methodology : Union [ Methodology , None ] = TestRatingsMethodology (), save_fit : bool = False ) -> Prediction : \"\"\" The method fits the algorithm and then calculates the prediction for each user Args: test_set: set of users for which to calculate the prediction Returns: prediction: prediction for each user \"\"\" all_users = set ( test_set . user_id_column ) if user_id_list is not None : all_users = set ( user_id_list ) loaded_items_interface = self . algorithm . _load_available_contents ( self . items_directory , set ()) pred = [] logger . info ( \"Don't worry if it looks stuck at first\" ) logger . info ( \"First iterations will stabilize the estimated remaining time\" ) with get_progbar ( all_users ) as pbar : pbar . set_description ( f \"Loading first items from memory...\" ) for user_id in pbar : user_id = str ( user_id ) user_train = self . train_set . get_user_interactions ( user_id ) try : if save_fit : user_alg = deepcopy ( self . algorithm ) self . _user_fit_dic [ user_id ] = user_alg alg = user_alg else : alg = self . algorithm alg . process_rated ( user_train , loaded_items_interface ) alg . fit () except UserSkipAlgFit as e : warning_message = str ( e ) + f \" \\n The algorithm can't be fitted for the user { user_id } \" logger . warning ( warning_message ) if save_fit : self . _user_fit_dic [ user_id ] = None continue filter_list = None if methodology is not None : filter_list = set ( methodology . filter_single ( user_id , self . train_set , test_set )) user_pred = alg . predict ( user_train , loaded_items_interface , filter_list = filter_list ) pred . extend ( user_pred ) pbar . set_description ( f \"Computing fit_rank for user { user_id } \" ) pred = Prediction . from_list ( pred ) # we force the garbage collector after freeing loaded items del loaded_items_interface gc . collect () self . _yaml_report = { 'mode' : 'score_prediction' , 'methodology' : repr ( methodology )} return pred def fit_rank ( self , test_set : Ratings , n_recs : int = None , user_id_list : List = None , methodology : Union [ Methodology , None ] = TestRatingsMethodology (), save_fit : bool = False ) -> Rank : \"\"\" The method fits the algorithm and then calculates the rank for each user Args: test_set: set of users for which to calculate the rank n_recs: number of the top items that will be present in the ranking Returns: rank: ranked items for each user \"\"\" all_users = set ( test_set . user_id_column ) if user_id_list is not None : all_users = set ( user_id_list ) loaded_items_interface = self . algorithm . _load_available_contents ( self . items_directory , set ()) rank = [] logger . info ( \"Don't worry if it looks stuck at first\" ) logger . info ( \"First iterations will stabilize the estimated remaining time\" ) with get_progbar ( all_users ) as pbar : pbar . set_description ( f \"Loading first items from memory...\" ) for user_id in pbar : user_id = str ( user_id ) user_train = self . train_set . get_user_interactions ( user_id ) try : if save_fit : user_alg = deepcopy ( self . algorithm ) self . _user_fit_dic [ user_id ] = user_alg alg = user_alg else : alg = self . algorithm alg . process_rated ( user_train , loaded_items_interface ) alg . fit () except UserSkipAlgFit as e : warning_message = str ( e ) + f \" \\n The algorithm can't be fitted for the user { user_id } \" logger . warning ( warning_message ) if save_fit : self . _user_fit_dic [ user_id ] = None continue filter_list = None if methodology is not None : filter_list = set ( methodology . filter_single ( user_id , self . train_set , test_set )) user_rank = alg . rank ( user_train , loaded_items_interface , n_recs , filter_list = filter_list ) rank . extend ( user_rank ) pbar . set_description ( f \"Computing fit_rank for user { user_id } \" ) rank = Rank . from_list ( rank ) # we force the garbage collector after freeing loaded items del loaded_items_interface gc . collect () self . _yaml_report = { 'mode' : 'rank' , 'n_recs' : repr ( n_recs ), 'methodology' : repr ( methodology )} return rank def __repr__ ( self ): return f \"ContentBasedRS(algorithm= { self . algorithm } , train_set= { self . train_set } , \" \\ f \"items_directory= { self . items_directory } , users_directory= { self . users_directory } )\" algorithm () property The content based algorithm chosen Source code in clayrs/recsys/recsys.py 80 81 82 83 84 85 86 @property def algorithm ( self ): \"\"\" The content based algorithm chosen \"\"\" alg : ContentBasedAlgorithm = super () . algorithm return alg fit () Method that divides the train set into as many parts as there are different users. then it proceeds with the fit for each user and saves the result in the dictionary \"user_fit_dic\" Source code in clayrs/recsys/recsys.py 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 def fit ( self ): \"\"\" Method that divides the train set into as many parts as there are different users. then it proceeds with the fit for each user and saves the result in the dictionary \"user_fit_dic\" \"\"\" items_to_load = set ( self . train_set . item_id_column ) loaded_items_interface = self . algorithm . _load_available_contents ( self . items_directory , items_to_load ) with get_progbar ( set ( self . train_set . user_id_column )) as pbar : pbar . set_description ( \"Fitting algorithm\" ) for user_id in pbar : user_train = self . train_set . get_user_interactions ( user_id ) try : user_alg = deepcopy ( self . algorithm ) user_alg . process_rated ( user_train , loaded_items_interface ) user_alg . fit () self . _user_fit_dic [ user_id ] = user_alg except UserSkipAlgFit as e : warning_message = str ( e ) + f \" \\n No algorithm will be fitted for the user { user_id } \" logger . warning ( warning_message ) self . _user_fit_dic [ user_id ] = None # we force the garbage collector after freeing loaded items del loaded_items_interface gc . collect () return self fit_predict ( test_set , user_id_list = None , methodology = TestRatingsMethodology (), save_fit = False ) The method fits the algorithm and then calculates the prediction for each user Parameters: Name Type Description Default test_set Ratings set of users for which to calculate the prediction required Returns: Name Type Description prediction Prediction prediction for each user Source code in clayrs/recsys/recsys.py 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 def fit_predict ( self , test_set : Ratings , user_id_list : List = None , methodology : Union [ Methodology , None ] = TestRatingsMethodology (), save_fit : bool = False ) -> Prediction : \"\"\" The method fits the algorithm and then calculates the prediction for each user Args: test_set: set of users for which to calculate the prediction Returns: prediction: prediction for each user \"\"\" all_users = set ( test_set . user_id_column ) if user_id_list is not None : all_users = set ( user_id_list ) loaded_items_interface = self . algorithm . _load_available_contents ( self . items_directory , set ()) pred = [] logger . info ( \"Don't worry if it looks stuck at first\" ) logger . info ( \"First iterations will stabilize the estimated remaining time\" ) with get_progbar ( all_users ) as pbar : pbar . set_description ( f \"Loading first items from memory...\" ) for user_id in pbar : user_id = str ( user_id ) user_train = self . train_set . get_user_interactions ( user_id ) try : if save_fit : user_alg = deepcopy ( self . algorithm ) self . _user_fit_dic [ user_id ] = user_alg alg = user_alg else : alg = self . algorithm alg . process_rated ( user_train , loaded_items_interface ) alg . fit () except UserSkipAlgFit as e : warning_message = str ( e ) + f \" \\n The algorithm can't be fitted for the user { user_id } \" logger . warning ( warning_message ) if save_fit : self . _user_fit_dic [ user_id ] = None continue filter_list = None if methodology is not None : filter_list = set ( methodology . filter_single ( user_id , self . train_set , test_set )) user_pred = alg . predict ( user_train , loaded_items_interface , filter_list = filter_list ) pred . extend ( user_pred ) pbar . set_description ( f \"Computing fit_rank for user { user_id } \" ) pred = Prediction . from_list ( pred ) # we force the garbage collector after freeing loaded items del loaded_items_interface gc . collect () self . _yaml_report = { 'mode' : 'score_prediction' , 'methodology' : repr ( methodology )} return pred fit_rank ( test_set , n_recs = None , user_id_list = None , methodology = TestRatingsMethodology (), save_fit = False ) The method fits the algorithm and then calculates the rank for each user Parameters: Name Type Description Default test_set Ratings set of users for which to calculate the rank required n_recs int number of the top items that will be present in the ranking None Returns: Name Type Description rank Rank ranked items for each user Source code in clayrs/recsys/recsys.py 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 def fit_rank ( self , test_set : Ratings , n_recs : int = None , user_id_list : List = None , methodology : Union [ Methodology , None ] = TestRatingsMethodology (), save_fit : bool = False ) -> Rank : \"\"\" The method fits the algorithm and then calculates the rank for each user Args: test_set: set of users for which to calculate the rank n_recs: number of the top items that will be present in the ranking Returns: rank: ranked items for each user \"\"\" all_users = set ( test_set . user_id_column ) if user_id_list is not None : all_users = set ( user_id_list ) loaded_items_interface = self . algorithm . _load_available_contents ( self . items_directory , set ()) rank = [] logger . info ( \"Don't worry if it looks stuck at first\" ) logger . info ( \"First iterations will stabilize the estimated remaining time\" ) with get_progbar ( all_users ) as pbar : pbar . set_description ( f \"Loading first items from memory...\" ) for user_id in pbar : user_id = str ( user_id ) user_train = self . train_set . get_user_interactions ( user_id ) try : if save_fit : user_alg = deepcopy ( self . algorithm ) self . _user_fit_dic [ user_id ] = user_alg alg = user_alg else : alg = self . algorithm alg . process_rated ( user_train , loaded_items_interface ) alg . fit () except UserSkipAlgFit as e : warning_message = str ( e ) + f \" \\n The algorithm can't be fitted for the user { user_id } \" logger . warning ( warning_message ) if save_fit : self . _user_fit_dic [ user_id ] = None continue filter_list = None if methodology is not None : filter_list = set ( methodology . filter_single ( user_id , self . train_set , test_set )) user_rank = alg . rank ( user_train , loaded_items_interface , n_recs , filter_list = filter_list ) rank . extend ( user_rank ) pbar . set_description ( f \"Computing fit_rank for user { user_id } \" ) rank = Rank . from_list ( rank ) # we force the garbage collector after freeing loaded items del loaded_items_interface gc . collect () self . _yaml_report = { 'mode' : 'rank' , 'n_recs' : repr ( n_recs ), 'methodology' : repr ( methodology )} return rank items_directory () property Path of the serialized items Source code in clayrs/recsys/recsys.py 92 93 94 95 96 97 @property def items_directory ( self ): \"\"\" Path of the serialized items \"\"\" return self . __items_directory predict ( test_set , user_id_list = None , methodology = TestRatingsMethodology ()) Method to call when score prediction must be done for the users in test set If the items evaluated are present for each user, the filter list is calculated, and score prediction is executed only for the items inside the filter list. Otherwise, score prediction is executed for all unrated items of the particular user Parameters: Name Type Description Default test_set Ratings set of users for which to calculate the predictions required Returns: Name Type Description concat_score_preds Prediction prediction for each user Source code in clayrs/recsys/recsys.py 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 def predict ( self , test_set : Ratings , user_id_list : List = None , methodology : Union [ Methodology , None ] = TestRatingsMethodology ()) -> Prediction : \"\"\" Method to call when score prediction must be done for the users in test set If the items evaluated are present for each user, the filter list is calculated, and score prediction is executed only for the items inside the filter list. Otherwise, score prediction is executed for all unrated items of the particular user Args: test_set: set of users for which to calculate the predictions Returns: concat_score_preds: prediction for each user \"\"\" if len ( self . _user_fit_dic ) == 0 : raise NotFittedAlg ( \"Algorithm not fit! You must call the fit() method first, or fit_rank().\" ) all_users = set ( test_set . user_id_column ) if user_id_list is not None : all_users = set ( user_id_list ) loaded_items_interface = self . algorithm . _load_available_contents ( self . items_directory , set ()) pred = [] logger . info ( \"Don't worry if it looks stuck at first\" ) logger . info ( \"First iterations will stabilize the estimated remaining time\" ) with get_progbar ( all_users ) as pbar : pbar . set_description ( f \"Loading first items from memory...\" ) for user_id in pbar : user_id = str ( user_id ) user_train = self . train_set . get_user_interactions ( user_id ) filter_list = None if methodology is not None : filter_list = set ( methodology . filter_single ( user_id , self . train_set , test_set )) user_fitted_alg = self . _user_fit_dic . get ( user_id ) if user_fitted_alg is not None : user_pred = user_fitted_alg . predict ( user_train , loaded_items_interface , filter_list = filter_list ) else : user_pred = [] logger . warning ( f \"No algorithm fitted for user { user_id } ! It will be skipped\" ) pred . extend ( user_pred ) pbar . set_description ( f \"Computing predictions for user { user_id } \" ) pred = Prediction . from_list ( pred ) # we force the garbage collector after freeing loaded items del loaded_items_interface gc . collect () self . _yaml_report = { 'mode' : 'score_prediction' , 'methodology' : repr ( methodology )} return pred rank ( test_set , n_recs = None , user_id_list = None , methodology = TestRatingsMethodology ()) Method used to calculate ranking for the user in test set If the recs_number is specified, then the rank will contain the top-n items for the users. Otherwise the rank will contain all unrated items of the particular users if the items evaluated are present for each user, the filter list is calculated, and score prediction is executed only for the items inside the filter list. Otherwise, score prediction is executed for all unrated items of the particular user Parameters: Name Type Description Default test_set Ratings set of users for which to calculate the rank required n_recs int number of the top items that will be present in the ranking None Returns: Name Type Description concat_rank Rank list of the items ranked for each user Source code in clayrs/recsys/recsys.py 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 def rank ( self , test_set : Ratings , n_recs : int = None , user_id_list : List = None , methodology : Union [ Methodology , None ] = TestRatingsMethodology ()) -> Rank : \"\"\" Method used to calculate ranking for the user in test set If the recs_number is specified, then the rank will contain the top-n items for the users. Otherwise the rank will contain all unrated items of the particular users if the items evaluated are present for each user, the filter list is calculated, and score prediction is executed only for the items inside the filter list. Otherwise, score prediction is executed for all unrated items of the particular user Args: test_set: set of users for which to calculate the rank n_recs: number of the top items that will be present in the ranking Returns: concat_rank: list of the items ranked for each user \"\"\" if len ( self . _user_fit_dic ) == 0 : raise NotFittedAlg ( \"Algorithm not fit! You must call the fit() method first, or fit_rank().\" ) all_users = set ( test_set . user_id_column ) if user_id_list is not None : all_users = set ( user_id_list ) loaded_items_interface = self . algorithm . _load_available_contents ( self . items_directory , set ()) rank = [] logger . info ( \"Don't worry if it looks stuck at first\" ) logger . info ( \"First iterations will stabilize the estimated remaining time\" ) with get_progbar ( all_users ) as pbar : pbar . set_description ( f \"Loading first items from memory...\" ) for user_id in pbar : user_id = str ( user_id ) user_train = self . train_set . get_user_interactions ( user_id ) filter_list = None if methodology is not None : filter_list = set ( methodology . filter_single ( user_id , self . train_set , test_set )) user_fitted_alg = self . _user_fit_dic . get ( user_id ) if user_fitted_alg is not None : user_rank = user_fitted_alg . rank ( user_train , loaded_items_interface , n_recs , filter_list = filter_list ) else : user_rank = [] logger . warning ( f \"No algorithm fitted for user { user_id } ! It will be skipped\" ) rank . extend ( user_rank ) pbar . set_description ( f \"Computing rank for { user_id } \" ) rank = Rank . from_list ( rank ) # we force the garbage collector after freeing loaded items del loaded_items_interface gc . collect () self . _yaml_report = { 'mode' : 'rank' , 'n_recs' : repr ( n_recs ), 'methodology' : repr ( methodology )} return rank users_directory () property Path of the serialized users Source code in clayrs/recsys/recsys.py 99 100 101 102 103 104 @property def users_directory ( self ): \"\"\" Path of the serialized users \"\"\" return self . __users_directory GraphBasedRS Bases: RecSys Class for recommender systems which use a graph in order to make predictions Every graph based recommender system differ from each other based the algorithm chosen Parameters: Name Type Description Default algorithm GraphBasedAlgorithm the graph based algorithm that will be used in order to rank or make score prediction required graph FullGraph a FullGraph containing interactions required Source code in clayrs/recsys/recsys.py 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 class GraphBasedRS ( RecSys ): \"\"\" Class for recommender systems which use a graph in order to make predictions Every graph based recommender system differ from each other based the algorithm chosen Args: algorithm (GraphBasedAlgorithm): the graph based algorithm that will be used in order to rank or make score prediction graph (FullGraph): a FullGraph containing interactions \"\"\" def __init__ ( self , algorithm : GraphBasedAlgorithm , graph : FullDiGraph ): self . __graph = graph super () . __init__ ( algorithm ) @property def users ( self ): return self . __graph . user_nodes @property def graph ( self ): \"\"\" The graph containing interactions \"\"\" return self . __graph @property def algorithm ( self ): \"\"\" The content based algorithm chosen \"\"\" alg : GraphBasedAlgorithm = super () . algorithm return alg def predict ( self , test_set : Ratings , user_id_list : List = None , methodology : Union [ Methodology , None ] = TestRatingsMethodology ()): \"\"\" Method used to predict the rating of the users If the items evaluated are present for each user, the filter list is calculated, and score prediction is executed only for the items inside the filter list. Otherwise, score prediction is executed for all unrated items of the particular user Args: test_set: set of users for which to calculate the predictions Returns: concate_score_preds: list of predictions for each user \"\"\" all_users = set ( test_set . user_id_column ) if user_id_list is not None : all_users = set ( user_id_list ) filter_dict : Union [ Dict , None ] = None if methodology is not None : train_set = self . graph . to_ratings () filter_dict = methodology . filter_all ( train_set , test_set , result_as_iter_dict = True ) total_predict_list = self . algorithm . predict ( all_users , self . graph , filter_dict ) total_predict = Prediction . from_list ( total_predict_list ) self . _yaml_report = { 'graph' : repr ( self . graph ), 'mode' : 'score_prediction' , 'methodology' : repr ( methodology )} return total_predict def rank ( self , test_set : Ratings , n_recs : int = None , user_id_list : List = None , methodology : Union [ Methodology , None ] = TestRatingsMethodology ()): \"\"\" Method used to rank the rating of the users If the items evaluated are present for each user, the filter list is calculated, and score prediction is executed only for the items inside the filter list. Otherwise, score prediction is executed for all unrated items of the particular user Args: test_set: set of users for which to calculate the rank n_recs: number of the top items that will be present in the ranking Returns: concate_rank: list of the items ranked for each user \"\"\" all_users = set ( test_set . user_id_column ) if user_id_list is not None : all_users = set ( user_id_list ) filter_dict : Union [ Dict , None ] = None if methodology is not None : train_set = self . graph . to_ratings () filter_dict = methodology . filter_all ( train_set , test_set , result_as_iter_dict = True ) total_rank_list = self . algorithm . rank ( all_users , self . graph , n_recs , filter_dict ) total_rank = Rank . from_list ( total_rank_list ) if len ( total_rank ) == 0 : logger . warning ( \"No items could be ranked for any users! Remember that items to rank must be present \" \"in the graph. \\n \" \"Try changing methodology!\" ) elif len ( set ( total_rank . user_id_column )) != len ( all_users ): logger . warning ( f \"No items could be ranked for users { all_users - set ( total_rank . user_id_column ) } \\n \" f \"No nodes to rank for them found in the graph. Try changing methodology! \" ) self . _yaml_report = { 'graph' : repr ( self . graph ), 'mode' : 'rank' , 'n_recs' : repr ( n_recs ), 'methodology' : repr ( methodology )} return total_rank def __repr__ ( self ): return f \"GraphBasedRS(algorithm= { self . algorithm } , graph= { self . graph } )\" algorithm () property The content based algorithm chosen Source code in clayrs/recsys/recsys.py 441 442 443 444 445 446 447 @property def algorithm ( self ): \"\"\" The content based algorithm chosen \"\"\" alg : GraphBasedAlgorithm = super () . algorithm return alg graph () property The graph containing interactions Source code in clayrs/recsys/recsys.py 434 435 436 437 438 439 @property def graph ( self ): \"\"\" The graph containing interactions \"\"\" return self . __graph predict ( test_set , user_id_list = None , methodology = TestRatingsMethodology ()) Method used to predict the rating of the users If the items evaluated are present for each user, the filter list is calculated, and score prediction is executed only for the items inside the filter list. Otherwise, score prediction is executed for all unrated items of the particular user Parameters: Name Type Description Default test_set Ratings set of users for which to calculate the predictions required Returns: Name Type Description concate_score_preds list of predictions for each user Source code in clayrs/recsys/recsys.py 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 def predict ( self , test_set : Ratings , user_id_list : List = None , methodology : Union [ Methodology , None ] = TestRatingsMethodology ()): \"\"\" Method used to predict the rating of the users If the items evaluated are present for each user, the filter list is calculated, and score prediction is executed only for the items inside the filter list. Otherwise, score prediction is executed for all unrated items of the particular user Args: test_set: set of users for which to calculate the predictions Returns: concate_score_preds: list of predictions for each user \"\"\" all_users = set ( test_set . user_id_column ) if user_id_list is not None : all_users = set ( user_id_list ) filter_dict : Union [ Dict , None ] = None if methodology is not None : train_set = self . graph . to_ratings () filter_dict = methodology . filter_all ( train_set , test_set , result_as_iter_dict = True ) total_predict_list = self . algorithm . predict ( all_users , self . graph , filter_dict ) total_predict = Prediction . from_list ( total_predict_list ) self . _yaml_report = { 'graph' : repr ( self . graph ), 'mode' : 'score_prediction' , 'methodology' : repr ( methodology )} return total_predict rank ( test_set , n_recs = None , user_id_list = None , methodology = TestRatingsMethodology ()) Method used to rank the rating of the users If the items evaluated are present for each user, the filter list is calculated, and score prediction is executed only for the items inside the filter list. Otherwise, score prediction is executed for all unrated items of the particular user Parameters: Name Type Description Default test_set Ratings set of users for which to calculate the rank required n_recs int number of the top items that will be present in the ranking None Returns: Name Type Description concate_rank list of the items ranked for each user Source code in clayrs/recsys/recsys.py 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 def rank ( self , test_set : Ratings , n_recs : int = None , user_id_list : List = None , methodology : Union [ Methodology , None ] = TestRatingsMethodology ()): \"\"\" Method used to rank the rating of the users If the items evaluated are present for each user, the filter list is calculated, and score prediction is executed only for the items inside the filter list. Otherwise, score prediction is executed for all unrated items of the particular user Args: test_set: set of users for which to calculate the rank n_recs: number of the top items that will be present in the ranking Returns: concate_rank: list of the items ranked for each user \"\"\" all_users = set ( test_set . user_id_column ) if user_id_list is not None : all_users = set ( user_id_list ) filter_dict : Union [ Dict , None ] = None if methodology is not None : train_set = self . graph . to_ratings () filter_dict = methodology . filter_all ( train_set , test_set , result_as_iter_dict = True ) total_rank_list = self . algorithm . rank ( all_users , self . graph , n_recs , filter_dict ) total_rank = Rank . from_list ( total_rank_list ) if len ( total_rank ) == 0 : logger . warning ( \"No items could be ranked for any users! Remember that items to rank must be present \" \"in the graph. \\n \" \"Try changing methodology!\" ) elif len ( set ( total_rank . user_id_column )) != len ( all_users ): logger . warning ( f \"No items could be ranked for users { all_users - set ( total_rank . user_id_column ) } \\n \" f \"No nodes to rank for them found in the graph. Try changing methodology! \" ) self . _yaml_report = { 'graph' : repr ( self . graph ), 'mode' : 'rank' , 'n_recs' : repr ( n_recs ), 'methodology' : repr ( methodology )} return total_rank RecSys Bases: ABC Abstract class for a Recommender System There exists various type of recommender systems, content-based, graph-based, etc. so extend this class if another type must be implemented into the framework. Every recommender system do its prediction based on a rating frame, containing interactions between users and items Parameters: Name Type Description Default rating_frame pd . DataFrame a dataframe containing interactions between users and items required Source code in clayrs/recsys/recsys.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 class RecSys ( ABC ): \"\"\" Abstract class for a Recommender System There exists various type of recommender systems, content-based, graph-based, etc. so extend this class if another type must be implemented into the framework. Every recommender system do its prediction based on a rating frame, containing interactions between users and items Args: rating_frame (pd.DataFrame): a dataframe containing interactions between users and items \"\"\" def __init__ ( self , algorithm : Union [ ContentBasedAlgorithm , GraphBasedAlgorithm ]): self . __alg = algorithm self . _yaml_report : Optional [ Dict ] = None @property def algorithm ( self ): return self . __alg @abc . abstractmethod def rank ( self , test_set : pd . DataFrame , n_recs : int = None ) -> Rank : raise NotImplementedError @abc . abstractmethod def predict ( self , test_set : pd . DataFrame ) -> Prediction : raise NotImplementedError","title":"RecSys"},{"location":"recsys/recsys/#recsys","text":"","title":"Recsys"},{"location":"recsys/recsys/#clayrs.recsys.recsys.ContentBasedRS","text":"Bases: RecSys Class for recommender systems which use the items' content in order to make predictions, some algorithms may also use users' content Every CBRS differ from each other based the algorithm chosen Parameters: Name Type Description Default algorithm ContentBasedAlgorithm the content based algorithm that will be used in order to rank or make score prediction required train_set pd . DataFrame a DataFrame containing interactions between users and items required items_directory str the path of the items serialized by the Content Analyzer required users_directory str the path of the users serialized by the Content Analyzer None Source code in clayrs/recsys/recsys.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 class ContentBasedRS ( RecSys ): \"\"\" Class for recommender systems which use the items' content in order to make predictions, some algorithms may also use users' content Every CBRS differ from each other based the algorithm chosen Args: algorithm (ContentBasedAlgorithm): the content based algorithm that will be used in order to rank or make score prediction train_set (pd.DataFrame): a DataFrame containing interactions between users and items items_directory (str): the path of the items serialized by the Content Analyzer users_directory (str): the path of the users serialized by the Content Analyzer \"\"\" def __init__ ( self , algorithm : ContentBasedAlgorithm , train_set : Ratings , items_directory : str , users_directory : str = None ): super () . __init__ ( algorithm ) self . __train_set = train_set self . __items_directory = items_directory self . __users_directory = users_directory self . _user_fit_dic = {} @property def algorithm ( self ): \"\"\" The content based algorithm chosen \"\"\" alg : ContentBasedAlgorithm = super () . algorithm return alg @property def train_set ( self ): return self . __train_set @property def items_directory ( self ): \"\"\" Path of the serialized items \"\"\" return self . __items_directory @property def users_directory ( self ): \"\"\" Path of the serialized users \"\"\" return self . __users_directory def fit ( self ): \"\"\" Method that divides the train set into as many parts as there are different users. then it proceeds with the fit for each user and saves the result in the dictionary \"user_fit_dic\" \"\"\" items_to_load = set ( self . train_set . item_id_column ) loaded_items_interface = self . algorithm . _load_available_contents ( self . items_directory , items_to_load ) with get_progbar ( set ( self . train_set . user_id_column )) as pbar : pbar . set_description ( \"Fitting algorithm\" ) for user_id in pbar : user_train = self . train_set . get_user_interactions ( user_id ) try : user_alg = deepcopy ( self . algorithm ) user_alg . process_rated ( user_train , loaded_items_interface ) user_alg . fit () self . _user_fit_dic [ user_id ] = user_alg except UserSkipAlgFit as e : warning_message = str ( e ) + f \" \\n No algorithm will be fitted for the user { user_id } \" logger . warning ( warning_message ) self . _user_fit_dic [ user_id ] = None # we force the garbage collector after freeing loaded items del loaded_items_interface gc . collect () return self def rank ( self , test_set : Ratings , n_recs : int = None , user_id_list : List = None , methodology : Union [ Methodology , None ] = TestRatingsMethodology ()) -> Rank : \"\"\" Method used to calculate ranking for the user in test set If the recs_number is specified, then the rank will contain the top-n items for the users. Otherwise the rank will contain all unrated items of the particular users if the items evaluated are present for each user, the filter list is calculated, and score prediction is executed only for the items inside the filter list. Otherwise, score prediction is executed for all unrated items of the particular user Args: test_set: set of users for which to calculate the rank n_recs: number of the top items that will be present in the ranking Returns: concat_rank: list of the items ranked for each user \"\"\" if len ( self . _user_fit_dic ) == 0 : raise NotFittedAlg ( \"Algorithm not fit! You must call the fit() method first, or fit_rank().\" ) all_users = set ( test_set . user_id_column ) if user_id_list is not None : all_users = set ( user_id_list ) loaded_items_interface = self . algorithm . _load_available_contents ( self . items_directory , set ()) rank = [] logger . info ( \"Don't worry if it looks stuck at first\" ) logger . info ( \"First iterations will stabilize the estimated remaining time\" ) with get_progbar ( all_users ) as pbar : pbar . set_description ( f \"Loading first items from memory...\" ) for user_id in pbar : user_id = str ( user_id ) user_train = self . train_set . get_user_interactions ( user_id ) filter_list = None if methodology is not None : filter_list = set ( methodology . filter_single ( user_id , self . train_set , test_set )) user_fitted_alg = self . _user_fit_dic . get ( user_id ) if user_fitted_alg is not None : user_rank = user_fitted_alg . rank ( user_train , loaded_items_interface , n_recs , filter_list = filter_list ) else : user_rank = [] logger . warning ( f \"No algorithm fitted for user { user_id } ! It will be skipped\" ) rank . extend ( user_rank ) pbar . set_description ( f \"Computing rank for { user_id } \" ) rank = Rank . from_list ( rank ) # we force the garbage collector after freeing loaded items del loaded_items_interface gc . collect () self . _yaml_report = { 'mode' : 'rank' , 'n_recs' : repr ( n_recs ), 'methodology' : repr ( methodology )} return rank def predict ( self , test_set : Ratings , user_id_list : List = None , methodology : Union [ Methodology , None ] = TestRatingsMethodology ()) -> Prediction : \"\"\" Method to call when score prediction must be done for the users in test set If the items evaluated are present for each user, the filter list is calculated, and score prediction is executed only for the items inside the filter list. Otherwise, score prediction is executed for all unrated items of the particular user Args: test_set: set of users for which to calculate the predictions Returns: concat_score_preds: prediction for each user \"\"\" if len ( self . _user_fit_dic ) == 0 : raise NotFittedAlg ( \"Algorithm not fit! You must call the fit() method first, or fit_rank().\" ) all_users = set ( test_set . user_id_column ) if user_id_list is not None : all_users = set ( user_id_list ) loaded_items_interface = self . algorithm . _load_available_contents ( self . items_directory , set ()) pred = [] logger . info ( \"Don't worry if it looks stuck at first\" ) logger . info ( \"First iterations will stabilize the estimated remaining time\" ) with get_progbar ( all_users ) as pbar : pbar . set_description ( f \"Loading first items from memory...\" ) for user_id in pbar : user_id = str ( user_id ) user_train = self . train_set . get_user_interactions ( user_id ) filter_list = None if methodology is not None : filter_list = set ( methodology . filter_single ( user_id , self . train_set , test_set )) user_fitted_alg = self . _user_fit_dic . get ( user_id ) if user_fitted_alg is not None : user_pred = user_fitted_alg . predict ( user_train , loaded_items_interface , filter_list = filter_list ) else : user_pred = [] logger . warning ( f \"No algorithm fitted for user { user_id } ! It will be skipped\" ) pred . extend ( user_pred ) pbar . set_description ( f \"Computing predictions for user { user_id } \" ) pred = Prediction . from_list ( pred ) # we force the garbage collector after freeing loaded items del loaded_items_interface gc . collect () self . _yaml_report = { 'mode' : 'score_prediction' , 'methodology' : repr ( methodology )} return pred def fit_predict ( self , test_set : Ratings , user_id_list : List = None , methodology : Union [ Methodology , None ] = TestRatingsMethodology (), save_fit : bool = False ) -> Prediction : \"\"\" The method fits the algorithm and then calculates the prediction for each user Args: test_set: set of users for which to calculate the prediction Returns: prediction: prediction for each user \"\"\" all_users = set ( test_set . user_id_column ) if user_id_list is not None : all_users = set ( user_id_list ) loaded_items_interface = self . algorithm . _load_available_contents ( self . items_directory , set ()) pred = [] logger . info ( \"Don't worry if it looks stuck at first\" ) logger . info ( \"First iterations will stabilize the estimated remaining time\" ) with get_progbar ( all_users ) as pbar : pbar . set_description ( f \"Loading first items from memory...\" ) for user_id in pbar : user_id = str ( user_id ) user_train = self . train_set . get_user_interactions ( user_id ) try : if save_fit : user_alg = deepcopy ( self . algorithm ) self . _user_fit_dic [ user_id ] = user_alg alg = user_alg else : alg = self . algorithm alg . process_rated ( user_train , loaded_items_interface ) alg . fit () except UserSkipAlgFit as e : warning_message = str ( e ) + f \" \\n The algorithm can't be fitted for the user { user_id } \" logger . warning ( warning_message ) if save_fit : self . _user_fit_dic [ user_id ] = None continue filter_list = None if methodology is not None : filter_list = set ( methodology . filter_single ( user_id , self . train_set , test_set )) user_pred = alg . predict ( user_train , loaded_items_interface , filter_list = filter_list ) pred . extend ( user_pred ) pbar . set_description ( f \"Computing fit_rank for user { user_id } \" ) pred = Prediction . from_list ( pred ) # we force the garbage collector after freeing loaded items del loaded_items_interface gc . collect () self . _yaml_report = { 'mode' : 'score_prediction' , 'methodology' : repr ( methodology )} return pred def fit_rank ( self , test_set : Ratings , n_recs : int = None , user_id_list : List = None , methodology : Union [ Methodology , None ] = TestRatingsMethodology (), save_fit : bool = False ) -> Rank : \"\"\" The method fits the algorithm and then calculates the rank for each user Args: test_set: set of users for which to calculate the rank n_recs: number of the top items that will be present in the ranking Returns: rank: ranked items for each user \"\"\" all_users = set ( test_set . user_id_column ) if user_id_list is not None : all_users = set ( user_id_list ) loaded_items_interface = self . algorithm . _load_available_contents ( self . items_directory , set ()) rank = [] logger . info ( \"Don't worry if it looks stuck at first\" ) logger . info ( \"First iterations will stabilize the estimated remaining time\" ) with get_progbar ( all_users ) as pbar : pbar . set_description ( f \"Loading first items from memory...\" ) for user_id in pbar : user_id = str ( user_id ) user_train = self . train_set . get_user_interactions ( user_id ) try : if save_fit : user_alg = deepcopy ( self . algorithm ) self . _user_fit_dic [ user_id ] = user_alg alg = user_alg else : alg = self . algorithm alg . process_rated ( user_train , loaded_items_interface ) alg . fit () except UserSkipAlgFit as e : warning_message = str ( e ) + f \" \\n The algorithm can't be fitted for the user { user_id } \" logger . warning ( warning_message ) if save_fit : self . _user_fit_dic [ user_id ] = None continue filter_list = None if methodology is not None : filter_list = set ( methodology . filter_single ( user_id , self . train_set , test_set )) user_rank = alg . rank ( user_train , loaded_items_interface , n_recs , filter_list = filter_list ) rank . extend ( user_rank ) pbar . set_description ( f \"Computing fit_rank for user { user_id } \" ) rank = Rank . from_list ( rank ) # we force the garbage collector after freeing loaded items del loaded_items_interface gc . collect () self . _yaml_report = { 'mode' : 'rank' , 'n_recs' : repr ( n_recs ), 'methodology' : repr ( methodology )} return rank def __repr__ ( self ): return f \"ContentBasedRS(algorithm= { self . algorithm } , train_set= { self . train_set } , \" \\ f \"items_directory= { self . items_directory } , users_directory= { self . users_directory } )\"","title":"ContentBasedRS"},{"location":"recsys/recsys/#clayrs.recsys.recsys.ContentBasedRS.algorithm","text":"The content based algorithm chosen Source code in clayrs/recsys/recsys.py 80 81 82 83 84 85 86 @property def algorithm ( self ): \"\"\" The content based algorithm chosen \"\"\" alg : ContentBasedAlgorithm = super () . algorithm return alg","title":"algorithm()"},{"location":"recsys/recsys/#clayrs.recsys.recsys.ContentBasedRS.fit","text":"Method that divides the train set into as many parts as there are different users. then it proceeds with the fit for each user and saves the result in the dictionary \"user_fit_dic\" Source code in clayrs/recsys/recsys.py 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 def fit ( self ): \"\"\" Method that divides the train set into as many parts as there are different users. then it proceeds with the fit for each user and saves the result in the dictionary \"user_fit_dic\" \"\"\" items_to_load = set ( self . train_set . item_id_column ) loaded_items_interface = self . algorithm . _load_available_contents ( self . items_directory , items_to_load ) with get_progbar ( set ( self . train_set . user_id_column )) as pbar : pbar . set_description ( \"Fitting algorithm\" ) for user_id in pbar : user_train = self . train_set . get_user_interactions ( user_id ) try : user_alg = deepcopy ( self . algorithm ) user_alg . process_rated ( user_train , loaded_items_interface ) user_alg . fit () self . _user_fit_dic [ user_id ] = user_alg except UserSkipAlgFit as e : warning_message = str ( e ) + f \" \\n No algorithm will be fitted for the user { user_id } \" logger . warning ( warning_message ) self . _user_fit_dic [ user_id ] = None # we force the garbage collector after freeing loaded items del loaded_items_interface gc . collect () return self","title":"fit()"},{"location":"recsys/recsys/#clayrs.recsys.recsys.ContentBasedRS.fit_predict","text":"The method fits the algorithm and then calculates the prediction for each user Parameters: Name Type Description Default test_set Ratings set of users for which to calculate the prediction required Returns: Name Type Description prediction Prediction prediction for each user Source code in clayrs/recsys/recsys.py 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 def fit_predict ( self , test_set : Ratings , user_id_list : List = None , methodology : Union [ Methodology , None ] = TestRatingsMethodology (), save_fit : bool = False ) -> Prediction : \"\"\" The method fits the algorithm and then calculates the prediction for each user Args: test_set: set of users for which to calculate the prediction Returns: prediction: prediction for each user \"\"\" all_users = set ( test_set . user_id_column ) if user_id_list is not None : all_users = set ( user_id_list ) loaded_items_interface = self . algorithm . _load_available_contents ( self . items_directory , set ()) pred = [] logger . info ( \"Don't worry if it looks stuck at first\" ) logger . info ( \"First iterations will stabilize the estimated remaining time\" ) with get_progbar ( all_users ) as pbar : pbar . set_description ( f \"Loading first items from memory...\" ) for user_id in pbar : user_id = str ( user_id ) user_train = self . train_set . get_user_interactions ( user_id ) try : if save_fit : user_alg = deepcopy ( self . algorithm ) self . _user_fit_dic [ user_id ] = user_alg alg = user_alg else : alg = self . algorithm alg . process_rated ( user_train , loaded_items_interface ) alg . fit () except UserSkipAlgFit as e : warning_message = str ( e ) + f \" \\n The algorithm can't be fitted for the user { user_id } \" logger . warning ( warning_message ) if save_fit : self . _user_fit_dic [ user_id ] = None continue filter_list = None if methodology is not None : filter_list = set ( methodology . filter_single ( user_id , self . train_set , test_set )) user_pred = alg . predict ( user_train , loaded_items_interface , filter_list = filter_list ) pred . extend ( user_pred ) pbar . set_description ( f \"Computing fit_rank for user { user_id } \" ) pred = Prediction . from_list ( pred ) # we force the garbage collector after freeing loaded items del loaded_items_interface gc . collect () self . _yaml_report = { 'mode' : 'score_prediction' , 'methodology' : repr ( methodology )} return pred","title":"fit_predict()"},{"location":"recsys/recsys/#clayrs.recsys.recsys.ContentBasedRS.fit_rank","text":"The method fits the algorithm and then calculates the rank for each user Parameters: Name Type Description Default test_set Ratings set of users for which to calculate the rank required n_recs int number of the top items that will be present in the ranking None Returns: Name Type Description rank Rank ranked items for each user Source code in clayrs/recsys/recsys.py 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 def fit_rank ( self , test_set : Ratings , n_recs : int = None , user_id_list : List = None , methodology : Union [ Methodology , None ] = TestRatingsMethodology (), save_fit : bool = False ) -> Rank : \"\"\" The method fits the algorithm and then calculates the rank for each user Args: test_set: set of users for which to calculate the rank n_recs: number of the top items that will be present in the ranking Returns: rank: ranked items for each user \"\"\" all_users = set ( test_set . user_id_column ) if user_id_list is not None : all_users = set ( user_id_list ) loaded_items_interface = self . algorithm . _load_available_contents ( self . items_directory , set ()) rank = [] logger . info ( \"Don't worry if it looks stuck at first\" ) logger . info ( \"First iterations will stabilize the estimated remaining time\" ) with get_progbar ( all_users ) as pbar : pbar . set_description ( f \"Loading first items from memory...\" ) for user_id in pbar : user_id = str ( user_id ) user_train = self . train_set . get_user_interactions ( user_id ) try : if save_fit : user_alg = deepcopy ( self . algorithm ) self . _user_fit_dic [ user_id ] = user_alg alg = user_alg else : alg = self . algorithm alg . process_rated ( user_train , loaded_items_interface ) alg . fit () except UserSkipAlgFit as e : warning_message = str ( e ) + f \" \\n The algorithm can't be fitted for the user { user_id } \" logger . warning ( warning_message ) if save_fit : self . _user_fit_dic [ user_id ] = None continue filter_list = None if methodology is not None : filter_list = set ( methodology . filter_single ( user_id , self . train_set , test_set )) user_rank = alg . rank ( user_train , loaded_items_interface , n_recs , filter_list = filter_list ) rank . extend ( user_rank ) pbar . set_description ( f \"Computing fit_rank for user { user_id } \" ) rank = Rank . from_list ( rank ) # we force the garbage collector after freeing loaded items del loaded_items_interface gc . collect () self . _yaml_report = { 'mode' : 'rank' , 'n_recs' : repr ( n_recs ), 'methodology' : repr ( methodology )} return rank","title":"fit_rank()"},{"location":"recsys/recsys/#clayrs.recsys.recsys.ContentBasedRS.items_directory","text":"Path of the serialized items Source code in clayrs/recsys/recsys.py 92 93 94 95 96 97 @property def items_directory ( self ): \"\"\" Path of the serialized items \"\"\" return self . __items_directory","title":"items_directory()"},{"location":"recsys/recsys/#clayrs.recsys.recsys.ContentBasedRS.predict","text":"Method to call when score prediction must be done for the users in test set If the items evaluated are present for each user, the filter list is calculated, and score prediction is executed only for the items inside the filter list. Otherwise, score prediction is executed for all unrated items of the particular user Parameters: Name Type Description Default test_set Ratings set of users for which to calculate the predictions required Returns: Name Type Description concat_score_preds Prediction prediction for each user Source code in clayrs/recsys/recsys.py 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 def predict ( self , test_set : Ratings , user_id_list : List = None , methodology : Union [ Methodology , None ] = TestRatingsMethodology ()) -> Prediction : \"\"\" Method to call when score prediction must be done for the users in test set If the items evaluated are present for each user, the filter list is calculated, and score prediction is executed only for the items inside the filter list. Otherwise, score prediction is executed for all unrated items of the particular user Args: test_set: set of users for which to calculate the predictions Returns: concat_score_preds: prediction for each user \"\"\" if len ( self . _user_fit_dic ) == 0 : raise NotFittedAlg ( \"Algorithm not fit! You must call the fit() method first, or fit_rank().\" ) all_users = set ( test_set . user_id_column ) if user_id_list is not None : all_users = set ( user_id_list ) loaded_items_interface = self . algorithm . _load_available_contents ( self . items_directory , set ()) pred = [] logger . info ( \"Don't worry if it looks stuck at first\" ) logger . info ( \"First iterations will stabilize the estimated remaining time\" ) with get_progbar ( all_users ) as pbar : pbar . set_description ( f \"Loading first items from memory...\" ) for user_id in pbar : user_id = str ( user_id ) user_train = self . train_set . get_user_interactions ( user_id ) filter_list = None if methodology is not None : filter_list = set ( methodology . filter_single ( user_id , self . train_set , test_set )) user_fitted_alg = self . _user_fit_dic . get ( user_id ) if user_fitted_alg is not None : user_pred = user_fitted_alg . predict ( user_train , loaded_items_interface , filter_list = filter_list ) else : user_pred = [] logger . warning ( f \"No algorithm fitted for user { user_id } ! It will be skipped\" ) pred . extend ( user_pred ) pbar . set_description ( f \"Computing predictions for user { user_id } \" ) pred = Prediction . from_list ( pred ) # we force the garbage collector after freeing loaded items del loaded_items_interface gc . collect () self . _yaml_report = { 'mode' : 'score_prediction' , 'methodology' : repr ( methodology )} return pred","title":"predict()"},{"location":"recsys/recsys/#clayrs.recsys.recsys.ContentBasedRS.rank","text":"Method used to calculate ranking for the user in test set If the recs_number is specified, then the rank will contain the top-n items for the users. Otherwise the rank will contain all unrated items of the particular users if the items evaluated are present for each user, the filter list is calculated, and score prediction is executed only for the items inside the filter list. Otherwise, score prediction is executed for all unrated items of the particular user Parameters: Name Type Description Default test_set Ratings set of users for which to calculate the rank required n_recs int number of the top items that will be present in the ranking None Returns: Name Type Description concat_rank Rank list of the items ranked for each user Source code in clayrs/recsys/recsys.py 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 def rank ( self , test_set : Ratings , n_recs : int = None , user_id_list : List = None , methodology : Union [ Methodology , None ] = TestRatingsMethodology ()) -> Rank : \"\"\" Method used to calculate ranking for the user in test set If the recs_number is specified, then the rank will contain the top-n items for the users. Otherwise the rank will contain all unrated items of the particular users if the items evaluated are present for each user, the filter list is calculated, and score prediction is executed only for the items inside the filter list. Otherwise, score prediction is executed for all unrated items of the particular user Args: test_set: set of users for which to calculate the rank n_recs: number of the top items that will be present in the ranking Returns: concat_rank: list of the items ranked for each user \"\"\" if len ( self . _user_fit_dic ) == 0 : raise NotFittedAlg ( \"Algorithm not fit! You must call the fit() method first, or fit_rank().\" ) all_users = set ( test_set . user_id_column ) if user_id_list is not None : all_users = set ( user_id_list ) loaded_items_interface = self . algorithm . _load_available_contents ( self . items_directory , set ()) rank = [] logger . info ( \"Don't worry if it looks stuck at first\" ) logger . info ( \"First iterations will stabilize the estimated remaining time\" ) with get_progbar ( all_users ) as pbar : pbar . set_description ( f \"Loading first items from memory...\" ) for user_id in pbar : user_id = str ( user_id ) user_train = self . train_set . get_user_interactions ( user_id ) filter_list = None if methodology is not None : filter_list = set ( methodology . filter_single ( user_id , self . train_set , test_set )) user_fitted_alg = self . _user_fit_dic . get ( user_id ) if user_fitted_alg is not None : user_rank = user_fitted_alg . rank ( user_train , loaded_items_interface , n_recs , filter_list = filter_list ) else : user_rank = [] logger . warning ( f \"No algorithm fitted for user { user_id } ! It will be skipped\" ) rank . extend ( user_rank ) pbar . set_description ( f \"Computing rank for { user_id } \" ) rank = Rank . from_list ( rank ) # we force the garbage collector after freeing loaded items del loaded_items_interface gc . collect () self . _yaml_report = { 'mode' : 'rank' , 'n_recs' : repr ( n_recs ), 'methodology' : repr ( methodology )} return rank","title":"rank()"},{"location":"recsys/recsys/#clayrs.recsys.recsys.ContentBasedRS.users_directory","text":"Path of the serialized users Source code in clayrs/recsys/recsys.py 99 100 101 102 103 104 @property def users_directory ( self ): \"\"\" Path of the serialized users \"\"\" return self . __users_directory","title":"users_directory()"},{"location":"recsys/recsys/#clayrs.recsys.recsys.GraphBasedRS","text":"Bases: RecSys Class for recommender systems which use a graph in order to make predictions Every graph based recommender system differ from each other based the algorithm chosen Parameters: Name Type Description Default algorithm GraphBasedAlgorithm the graph based algorithm that will be used in order to rank or make score prediction required graph FullGraph a FullGraph containing interactions required Source code in clayrs/recsys/recsys.py 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 class GraphBasedRS ( RecSys ): \"\"\" Class for recommender systems which use a graph in order to make predictions Every graph based recommender system differ from each other based the algorithm chosen Args: algorithm (GraphBasedAlgorithm): the graph based algorithm that will be used in order to rank or make score prediction graph (FullGraph): a FullGraph containing interactions \"\"\" def __init__ ( self , algorithm : GraphBasedAlgorithm , graph : FullDiGraph ): self . __graph = graph super () . __init__ ( algorithm ) @property def users ( self ): return self . __graph . user_nodes @property def graph ( self ): \"\"\" The graph containing interactions \"\"\" return self . __graph @property def algorithm ( self ): \"\"\" The content based algorithm chosen \"\"\" alg : GraphBasedAlgorithm = super () . algorithm return alg def predict ( self , test_set : Ratings , user_id_list : List = None , methodology : Union [ Methodology , None ] = TestRatingsMethodology ()): \"\"\" Method used to predict the rating of the users If the items evaluated are present for each user, the filter list is calculated, and score prediction is executed only for the items inside the filter list. Otherwise, score prediction is executed for all unrated items of the particular user Args: test_set: set of users for which to calculate the predictions Returns: concate_score_preds: list of predictions for each user \"\"\" all_users = set ( test_set . user_id_column ) if user_id_list is not None : all_users = set ( user_id_list ) filter_dict : Union [ Dict , None ] = None if methodology is not None : train_set = self . graph . to_ratings () filter_dict = methodology . filter_all ( train_set , test_set , result_as_iter_dict = True ) total_predict_list = self . algorithm . predict ( all_users , self . graph , filter_dict ) total_predict = Prediction . from_list ( total_predict_list ) self . _yaml_report = { 'graph' : repr ( self . graph ), 'mode' : 'score_prediction' , 'methodology' : repr ( methodology )} return total_predict def rank ( self , test_set : Ratings , n_recs : int = None , user_id_list : List = None , methodology : Union [ Methodology , None ] = TestRatingsMethodology ()): \"\"\" Method used to rank the rating of the users If the items evaluated are present for each user, the filter list is calculated, and score prediction is executed only for the items inside the filter list. Otherwise, score prediction is executed for all unrated items of the particular user Args: test_set: set of users for which to calculate the rank n_recs: number of the top items that will be present in the ranking Returns: concate_rank: list of the items ranked for each user \"\"\" all_users = set ( test_set . user_id_column ) if user_id_list is not None : all_users = set ( user_id_list ) filter_dict : Union [ Dict , None ] = None if methodology is not None : train_set = self . graph . to_ratings () filter_dict = methodology . filter_all ( train_set , test_set , result_as_iter_dict = True ) total_rank_list = self . algorithm . rank ( all_users , self . graph , n_recs , filter_dict ) total_rank = Rank . from_list ( total_rank_list ) if len ( total_rank ) == 0 : logger . warning ( \"No items could be ranked for any users! Remember that items to rank must be present \" \"in the graph. \\n \" \"Try changing methodology!\" ) elif len ( set ( total_rank . user_id_column )) != len ( all_users ): logger . warning ( f \"No items could be ranked for users { all_users - set ( total_rank . user_id_column ) } \\n \" f \"No nodes to rank for them found in the graph. Try changing methodology! \" ) self . _yaml_report = { 'graph' : repr ( self . graph ), 'mode' : 'rank' , 'n_recs' : repr ( n_recs ), 'methodology' : repr ( methodology )} return total_rank def __repr__ ( self ): return f \"GraphBasedRS(algorithm= { self . algorithm } , graph= { self . graph } )\"","title":"GraphBasedRS"},{"location":"recsys/recsys/#clayrs.recsys.recsys.GraphBasedRS.algorithm","text":"The content based algorithm chosen Source code in clayrs/recsys/recsys.py 441 442 443 444 445 446 447 @property def algorithm ( self ): \"\"\" The content based algorithm chosen \"\"\" alg : GraphBasedAlgorithm = super () . algorithm return alg","title":"algorithm()"},{"location":"recsys/recsys/#clayrs.recsys.recsys.GraphBasedRS.graph","text":"The graph containing interactions Source code in clayrs/recsys/recsys.py 434 435 436 437 438 439 @property def graph ( self ): \"\"\" The graph containing interactions \"\"\" return self . __graph","title":"graph()"},{"location":"recsys/recsys/#clayrs.recsys.recsys.GraphBasedRS.predict","text":"Method used to predict the rating of the users If the items evaluated are present for each user, the filter list is calculated, and score prediction is executed only for the items inside the filter list. Otherwise, score prediction is executed for all unrated items of the particular user Parameters: Name Type Description Default test_set Ratings set of users for which to calculate the predictions required Returns: Name Type Description concate_score_preds list of predictions for each user Source code in clayrs/recsys/recsys.py 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 def predict ( self , test_set : Ratings , user_id_list : List = None , methodology : Union [ Methodology , None ] = TestRatingsMethodology ()): \"\"\" Method used to predict the rating of the users If the items evaluated are present for each user, the filter list is calculated, and score prediction is executed only for the items inside the filter list. Otherwise, score prediction is executed for all unrated items of the particular user Args: test_set: set of users for which to calculate the predictions Returns: concate_score_preds: list of predictions for each user \"\"\" all_users = set ( test_set . user_id_column ) if user_id_list is not None : all_users = set ( user_id_list ) filter_dict : Union [ Dict , None ] = None if methodology is not None : train_set = self . graph . to_ratings () filter_dict = methodology . filter_all ( train_set , test_set , result_as_iter_dict = True ) total_predict_list = self . algorithm . predict ( all_users , self . graph , filter_dict ) total_predict = Prediction . from_list ( total_predict_list ) self . _yaml_report = { 'graph' : repr ( self . graph ), 'mode' : 'score_prediction' , 'methodology' : repr ( methodology )} return total_predict","title":"predict()"},{"location":"recsys/recsys/#clayrs.recsys.recsys.GraphBasedRS.rank","text":"Method used to rank the rating of the users If the items evaluated are present for each user, the filter list is calculated, and score prediction is executed only for the items inside the filter list. Otherwise, score prediction is executed for all unrated items of the particular user Parameters: Name Type Description Default test_set Ratings set of users for which to calculate the rank required n_recs int number of the top items that will be present in the ranking None Returns: Name Type Description concate_rank list of the items ranked for each user Source code in clayrs/recsys/recsys.py 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 def rank ( self , test_set : Ratings , n_recs : int = None , user_id_list : List = None , methodology : Union [ Methodology , None ] = TestRatingsMethodology ()): \"\"\" Method used to rank the rating of the users If the items evaluated are present for each user, the filter list is calculated, and score prediction is executed only for the items inside the filter list. Otherwise, score prediction is executed for all unrated items of the particular user Args: test_set: set of users for which to calculate the rank n_recs: number of the top items that will be present in the ranking Returns: concate_rank: list of the items ranked for each user \"\"\" all_users = set ( test_set . user_id_column ) if user_id_list is not None : all_users = set ( user_id_list ) filter_dict : Union [ Dict , None ] = None if methodology is not None : train_set = self . graph . to_ratings () filter_dict = methodology . filter_all ( train_set , test_set , result_as_iter_dict = True ) total_rank_list = self . algorithm . rank ( all_users , self . graph , n_recs , filter_dict ) total_rank = Rank . from_list ( total_rank_list ) if len ( total_rank ) == 0 : logger . warning ( \"No items could be ranked for any users! Remember that items to rank must be present \" \"in the graph. \\n \" \"Try changing methodology!\" ) elif len ( set ( total_rank . user_id_column )) != len ( all_users ): logger . warning ( f \"No items could be ranked for users { all_users - set ( total_rank . user_id_column ) } \\n \" f \"No nodes to rank for them found in the graph. Try changing methodology! \" ) self . _yaml_report = { 'graph' : repr ( self . graph ), 'mode' : 'rank' , 'n_recs' : repr ( n_recs ), 'methodology' : repr ( methodology )} return total_rank","title":"rank()"},{"location":"recsys/recsys/#clayrs.recsys.recsys.RecSys","text":"Bases: ABC Abstract class for a Recommender System There exists various type of recommender systems, content-based, graph-based, etc. so extend this class if another type must be implemented into the framework. Every recommender system do its prediction based on a rating frame, containing interactions between users and items Parameters: Name Type Description Default rating_frame pd . DataFrame a dataframe containing interactions between users and items required Source code in clayrs/recsys/recsys.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 class RecSys ( ABC ): \"\"\" Abstract class for a Recommender System There exists various type of recommender systems, content-based, graph-based, etc. so extend this class if another type must be implemented into the framework. Every recommender system do its prediction based on a rating frame, containing interactions between users and items Args: rating_frame (pd.DataFrame): a dataframe containing interactions between users and items \"\"\" def __init__ ( self , algorithm : Union [ ContentBasedAlgorithm , GraphBasedAlgorithm ]): self . __alg = algorithm self . _yaml_report : Optional [ Dict ] = None @property def algorithm ( self ): return self . __alg @abc . abstractmethod def rank ( self , test_set : pd . DataFrame , n_recs : int = None ) -> Rank : raise NotImplementedError @abc . abstractmethod def predict ( self , test_set : pd . DataFrame ) -> Prediction : raise NotImplementedError","title":"RecSys"}]}